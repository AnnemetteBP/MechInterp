{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "from typing import Tuple, List, Dict, Optional, Any\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from pathlib import Path\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from datasets import load_dataset, DownloadMode\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import tqdm\n",
    "\n",
    "from helper_utils.enum_keys import (\n",
    "    FPKey,\n",
    "    ModelKey,\n",
    "    QuantStyle,\n",
    "    MiscPrompts,\n",
    "    Contexts,\n",
    "    Texts\n",
    ")\n",
    "\n",
    "from PTQ.bitlinear_wrapper_class import BitLinear\n",
    "from PTQ.apply_ptq import applyPTQ\n",
    "from PTQ.olmo_act_fns import patch_olmo_mlp\n",
    "import helper_utils.utils as utils\n",
    "from helper_utils.models_loader import load_4bit_auto, load_8bit_auto\n",
    "from mech_interp_utils.utils_main.src.transformer_utils import (\n",
    "    logit_lens,\n",
    "    activation_lens,\n",
    "    dictionary_learning,\n",
    "    chatbot_analysis\n",
    ")\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "original = torch.randn(512) * 0.5  # Original activations\n",
    "\n",
    "def quantize_dequantize(tensor, scale_value):\n",
    "    scale = max(scale_value, 1e-8)\n",
    "    qmin, qmax = -127, 127\n",
    "    tensor_int = (tensor / scale).round().clamp(qmin, qmax).to(torch.int8)\n",
    "    tensor_dequant = tensor_int.float() * scale\n",
    "    return tensor_int, tensor_dequant\n",
    "\n",
    "# Quantize with different scales\n",
    "_, dequant_1e2 = quantize_dequantize(original, 1e-2)\n",
    "_, dequant_1e5 = quantize_dequantize(original, 1e-5)\n",
    "\n",
    "# L2 distance\n",
    "print(\"L2 Distance (scale=1e-2):\", torch.norm(original - dequant_1e2).item())\n",
    "print(\"L2 Distance (scale=1e-5):\", torch.norm(original - dequant_1e5).item())\n",
    "\n",
    "# Plot histograms + KDEs\n",
    "plt.figure(figsize=(14, 6))\n",
    "sns.histplot(original.numpy(),bel='Original', kde=True, stat=\"count\", bins=50, color='black', alpha=0.5)\n",
    "sns.histplot(dequant_1e2.numpy(), label='Dequant (scale=1e-2)', kde=True, stat=\"count\", bins=50, color='red', alpha=0.5)\n",
    "sns.histplot(dequant_1e5.numpy(), label='Dequant (scale=1e-5)', kde=True, stat=\"count\", bins=50, color='blue', alpha=0.5)\n",
    "\n",
    "plt.title(\"Histogram (Count) + KDE of Quantized vs Original Activations\")\n",
    "plt.xlabel(\"Activation Value\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.ylim(0, 40)  \n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch._dynamo\n",
    "torch._dynamo.config.suppress_errors = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datasets for calibrating activations and analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = r'D:\\ThesisData\\wikitext'\n",
    "\n",
    "destination_path = str(Path(filepath))\n",
    "dataset = load_dataset(\n",
    "    'wikitext', 'wikitext-103-raw-v1',\n",
    "    split={\n",
    "        'train': 'train[:200]',\n",
    "    },\n",
    "    cache_dir=destination_path,\n",
    "    download_mode=DownloadMode.REUSE_DATASET_IF_EXISTS,\n",
    "    keep_in_memory=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_texts = dataset['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calibration_texts = [t for t in dataset['train'][\"text\"] if isinstance(t, str) and t.strip()]\n",
    "#calibration_texts = [t for t in sub_txts[\"text\"] if isinstance(t, str) and t.strip()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_txts = train_texts.take(200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GSM8K (Math) \"gsm8k\"\n",
    "#### LogiQA (Logic & Reasoning): \"logiq\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = r'D:\\ThesisData\\nq'\n",
    "\n",
    "destination_path = str(Path(filepath))\n",
    "nq_dataset = load_dataset(\n",
    "    'sentence-transformers/natural-questions',\n",
    "    split={\n",
    "        'train': 'train[:20]'\n",
    "    },\n",
    "    cache_dir=destination_path,\n",
    "    download_mode=DownloadMode.REUSE_DATASET_IF_EXISTS,\n",
    "    keep_in_memory=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nq_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nq_queries= nq_dataset['train']['query']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nq_queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nq_answers = nq_dataset['train']['answer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nq_answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = r'D:\\ThesisData\\logiqa'\n",
    "\n",
    "destination_path = str(Path(filepath))\n",
    "logiqa_dataset = load_dataset(\n",
    "    'lucasmccabe/logiqa',\n",
    "    split={\n",
    "        'train': 'train[:20]'\n",
    "    },\n",
    "    cache_dir=destination_path,\n",
    "    download_mode=DownloadMode.REUSE_DATASET_IF_EXISTS,\n",
    "    keep_in_memory=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logiqa_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = r'D:\\ThesisData\\gsm8k'\n",
    "\n",
    "destination_path = str(Path(filepath))\n",
    "gsm8k_dataset = load_dataset(\n",
    "    'gsm8k', 'main',\n",
    "    split={\n",
    "        'train': 'train[:20]'\n",
    "    },\n",
    "    cache_dir=destination_path,\n",
    "    download_mode=DownloadMode.REUSE_DATASET_IF_EXISTS,\n",
    "    keep_in_memory=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gsm8k_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gsm8k_questions = gsm8k_dataset['train']['question']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gsm8k_answers = gsm8k_dataset['train']['answer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gsm8k_answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gsm8k_questions_sae = \"\"\"\n",
    "Natalia sold clips to 48 of her friends in April, and then she sold half as many clips in May. How many clips did Natalia sell altogether in April and May?\n",
    "Weng earns $12 an hour for babysitting. Yesterday, she just did 50 minutes of babysitting. How much did she earn?\n",
    "Betty is saving money for a new wallet which costs $100. Betty has only half of the money she needs. Her parents decided to give her $15 for that purpose, and her grandparents twice as much as her parents. How much more money does Betty need to buy the wallet?\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models and Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_test_model(model_path:str, dtype=torch.dtype) -> AutoModelForCausalLM:\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_path,\n",
    "        return_dict=True,\n",
    "        output_hidden_states=True,\n",
    "        torch_dtype=dtype,\n",
    "        low_cpu_mem_usage=True,\n",
    "        local_files_only=True,\n",
    "        use_safetensors=True,\n",
    "        #trust_remote_code=True\n",
    "    )\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hfbit1_tokenizer = AutoTokenizer.from_pretrained(FPKey.HFBIT1_TOKENIZER.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hfbit1_fp32 = load_test_model(FPKey.HFBIT1_8B.value, dtype=torch.float32) # https://huggingface.co/HF1BitLLM/Llama3-8B-1.58-100B-tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print model layers to inspect their names\n",
    "for name, module in hfbit1_fp32.named_modules():\n",
    "    print(f\"Layer name: {name}, Module: {module}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llama8b_tokenizer = AutoTokenizer.from_pretrained(FPKey.LINSTRUCT_TOKENIZER.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llama8b_fp32 = load_test_model(FPKey.LINSTRUCT_8B.value, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model with 8-bit quantization using BNB\n",
    "llama8b_bnb8_float32 = AutoModelForCausalLM.from_pretrained(\n",
    "    ModelKey.LLINSTRUCT8B.value,           # Replace with your actual model\n",
    "    torch_dtype=torch.float32,     # Specify the dtype (for 8-bit, use uint8)\n",
    "    #device_map=\"cpu\",           # Automatically map the model to available devices (GPU/CPU)\n",
    "    load_in_8bit=True,\n",
    "    return_dict=True,\n",
    "    output_hidden_states=True,            # Set to True for 8-bit quantization\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llama8b_bnb8_float32.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model with 4-bit quantization using BNB\n",
    "llama8b_bnb4_float32 = AutoModelForCausalLM.from_pretrained(\n",
    "    ModelKey.LLINSTRUCT8B.value,           # Replace with your actual model\n",
    "    torch_dtype=torch.float32,     # This would still use uint8 or another type based on quantization method\n",
    "    #device_map=\"auto\",           # Automatically map the model to available devices (GPU/CPU)\n",
    "    load_in_4bit=True,            # Set to True for 4-bit quantization (if available)\n",
    "    return_dict=True,\n",
    "    output_hidden_states=True, \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llama8b_ptsq_float32 = applyPTQ(\n",
    "    load_test_model(ModelKey.LLINSTRUCT8B.value, dtype=torch.float32),\n",
    "    tokenizer=llama8b_tokenizer,\n",
    "    #calibration_input=None,\n",
    "    #calibration_input=sub_txts['text'],\n",
    "    calibration_input=Texts.T1.value,\n",
    "    mode='1.58bit',\n",
    "    safer_quant=True,\n",
    "    q_lmhead=True,\n",
    "    model_half=False,\n",
    "    quant_half=False,\n",
    "    layers_to_quant_weights=QuantStyle.BITNET.value,\n",
    "    layers_to_quant_activations=QuantStyle.BITNET.value,\n",
    "    fragile_layers=False,\n",
    "    act_quant=True,\n",
    "    act_bits=8,\n",
    "    torch_backends=False,\n",
    "    debugging=True,\n",
    "    plot_debugging=False,\n",
    "    plot_quantization=False,\n",
    "    freeze_modules=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hfbit1_tokenizer.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llama8b_tokenizer.vocab_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### allenai/OLMo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "olmo1b_tokenizer = AutoTokenizer.from_pretrained(FPKey.OLMO1B_TOKENIZER.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "olmo2t_tokenizer = AutoTokenizer.from_pretrained(FPKey.OLMO7B2T_TOKENIZER.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "olmo1b_fp32 = load_test_model(FPKey.OLMO1B_FP.value, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "olmo2t_fp32 = load_test_model(FPKey.OLMO7B2T_FP.value, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "olmo1b_bitnet_fp32_ptsq = applyPTQ(\n",
    "    load_test_model(FPKey.OLMO1B_FP.value, dtype=torch.float32),\n",
    "    tokenizer=olmo1b_tokenizer,\n",
    "    #calibration_input=None,\n",
    "    #calibration_input=sub_txts['text'],\n",
    "    calibration_input=Texts.T1.value,\n",
    "    mode='1.58bit',\n",
    "    safer_quant=True,\n",
    "    q_lmhead=True,\n",
    "    model_half=False,\n",
    "    quant_half=False,\n",
    "    layers_to_quant_weights=QuantStyle.BITNET.value,\n",
    "    layers_to_quant_activations=QuantStyle.BITNET.value,\n",
    "    fragile_layers=False,\n",
    "    act_quant=True,\n",
    "    act_bits=8,\n",
    "    torch_backends=True,\n",
    "    debugging=True,\n",
    "    plot_debugging=False,\n",
    "    plot_quantization=False,\n",
    "    freeze_modules=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "olmo2t_bitnet_fp32_ptsq = applyPTQ(\n",
    "    load_test_model(FPKey.OLMO7B2T_FP.value, dtype=torch.float32),\n",
    "    tokenizer=olmo2t_tokenizer,\n",
    "    #calibration_input=None,\n",
    "    #calibration_input=sub_txts['text'],\n",
    "    calibration_input=Texts.T1.value,\n",
    "    mode='1.58bit',\n",
    "    safer_quant=True,\n",
    "    q_lmhead=True,\n",
    "    model_half=False,\n",
    "    quant_half=False,\n",
    "    layers_to_quant_weights=QuantStyle.BITNET.value,\n",
    "    layers_to_quant_activations=QuantStyle.BITNET.value,\n",
    "    fragile_layers=False,\n",
    "    act_quant=True,\n",
    "    act_bits=8,\n",
    "    torch_backends=False,\n",
    "    debugging=True,\n",
    "    plot_debugging=False,\n",
    "    plot_quantization=False,\n",
    "    freeze_modules=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model with 8-bit quantization using BNB\n",
    "olmo1b_bnb8_float32 = AutoModelForCausalLM.from_pretrained(\n",
    "    FPKey.OLMO1B_FP.value,           # Replace with your actual model\n",
    "    torch_dtype=torch.float32,     # Specify the dtype (for 8-bit, use uint8)\n",
    "    #device_map=\"cpu\",           # Automatically map the model to available devices (GPU/CPU)\n",
    "    load_in_8bit=True,\n",
    "    return_dict=True,\n",
    "    output_hidden_states=True,            # Set to True for 8-bit quantization\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model with 4-bit quantization using BNB\n",
    "olmo1b_bnb4_float32 = AutoModelForCausalLM.from_pretrained(\n",
    "    FPKey.OLMO1B_FP.value,           # Replace with your actual model\n",
    "    torch_dtype=torch.float32,     # This would still use uint8 or another type based on quantization method\n",
    "    #device_map=\"auto\",           # Automatically map the model to available devices (GPU/CPU)\n",
    "    load_in_4bit=True,            # Set to True for 4-bit quantization (if available)\n",
    "    return_dict=True,\n",
    "    output_hidden_states=True, \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NousResearch/DeepHermes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dh3b_tokenizer = AutoTokenizer.from_pretrained(FPKey.TOKENIZER_3B.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dh8b_tokenizer = AutoTokenizer.from_pretrained(FPKey.TOKENIZER_8B.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dh3b_fp32 = load_test_model(FPKey.FP_3B.value, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dh8b_fp32 = load_test_model(FPKey.FP_8B.value, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dh3b_bitnet_fp32_ptsq = applyPTQ(\n",
    "    load_test_model(FPKey.FP_3B.value, dtype=torch.float32),\n",
    "    tokenizer=dh3b_tokenizer,\n",
    "    #calibration_input=None,\n",
    "    #calibration_input=sub_txts['text'],\n",
    "    calibration_input=Texts.T1.value,\n",
    "    mode='1.58bit',\n",
    "    safer_quant=True,\n",
    "    q_lmhead=True,\n",
    "    model_half=False,\n",
    "    quant_half=False,\n",
    "    layers_to_quant_weights=QuantStyle.BITNET.value,\n",
    "    layers_to_quant_activations=QuantStyle.BITNET.value,\n",
    "    fragile_layers=False,\n",
    "    act_quant=True,\n",
    "    act_bits=8,\n",
    "    torch_backends=False,\n",
    "    debugging=True,\n",
    "    plot_debugging=False,\n",
    "    plot_quantization=False,\n",
    "    freeze_modules=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model with 8-bit quantization using BNB\n",
    "dh3b_bnb8_float32 = AutoModelForCausalLM.from_pretrained(\n",
    "    FPKey.FP_3B.value,           # Replace with your actual model\n",
    "    torch_dtype=torch.float32,     # Specify the dtype (for 8-bit, use uint8)\n",
    "    #device_map=\"cpu\",           # Automatically map the model to available devices (GPU/CPU)\n",
    "    load_in_8bit=True,\n",
    "    return_dict=True,\n",
    "    output_hidden_states=True,            # Set to True for 8-bit quantization\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model with 4-bit quantization using BNB\n",
    "dh3b_bnb4_float32 = AutoModelForCausalLM.from_pretrained(\n",
    "    FPKey.FP_3B.value,           # Replace with your actual model\n",
    "    torch_dtype=torch.float32,     # This would still use uint8 or another type based on quantization method\n",
    "    #device_map=\"auto\",           # Automatically map the model to available devices (GPU/CPU)\n",
    "    load_in_4bit=True,            # Set to True for 4-bit quantization (if available)\n",
    "    return_dict=True,\n",
    "    output_hidden_states=True, \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model with 8-bit quantization using BNB\n",
    "dh8b_bnb8_float32 = AutoModelForCausalLM.from_pretrained(\n",
    "    FPKey.FP_8B.value,           # Replace with your actual model\n",
    "    torch_dtype=torch.float32,     # Specify the dtype (for 8-bit, use uint8)\n",
    "    #device_map=\"cpu\",           # Automatically map the model to available devices (GPU/CPU)\n",
    "    load_in_8bit=True,\n",
    "    return_dict=True,\n",
    "    output_hidden_states=True,            # Set to True for 8-bit quantization\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model with 4-bit quantization using BNB\n",
    "dh8b_bnb4_float32 = AutoModelForCausalLM.from_pretrained(\n",
    "    FPKey.FP_8B.value,           # Replace with your actual model\n",
    "    torch_dtype=torch.float32,     # This would still use uint8 or another type based on quantization method\n",
    "    #device_map=\"auto\",           # Automatically map the model to available devices (GPU/CPU)\n",
    "    load_in_4bit=True,            # Set to True for 4-bit quantization (if available)\n",
    "    return_dict=True,\n",
    "    output_hidden_states=True, \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dh8b_bnb4_float32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activation Lens and Logit Lens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_inputs = [\n",
    "    # Language understanding\n",
    "    \"The quick brown fox jumps over the lazy dog.\",\n",
    "    \"Despite the rain, the event continued as planned.\",\n",
    "    \n",
    "    # Logic/reasoning\n",
    "    \"If all humans are mortal and Socrates is a human, then Socrates is mortal.\",\n",
    "    \"Either the lights are off or the power is out. The lights are on, so the power must be out.\",\n",
    "\n",
    "    # Math/numerical\n",
    "    \"The derivative of sin(x) with respect to x is cos(x).\",\n",
    "    \"What is the sum of the first 100 natural numbers?\",\n",
    "\n",
    "    # Programming\n",
    "    \"In Python, list comprehensions provide a concise way to create lists.\",\n",
    "    \"To define a function in JavaScript, use the 'function' keyword.\",\n",
    "\n",
    "    # Commonsense knowledge\n",
    "    \"You should refrigerate milk after opening it to keep it fresh.\",\n",
    "    \"People usually eat breakfast in the morning before starting their day.\",\n",
    "\n",
    "    # Scientific knowledge\n",
    "    \"Water boils at 100 degrees Celsius under standard atmospheric pressure.\",\n",
    "    \"Photosynthesis is the process by which plants convert sunlight into chemical energy.\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gsm8k_questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gsm8k_answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nq_queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [\"The quick brown fox jumps over the lazy dog.\", \"Despite the rain, the event continued as planned.\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch._dynamo\n",
    "torch._dynamo.config.suppress_errors = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logit_lens.plot_logit_lens_plotly(\n",
    "    model=dh3b_bnb4_float32,\n",
    "    tokenizer=dh3b_tokenizer,\n",
    "    inputs=nq_answers,\n",
    "    start_ix=0, end_ix=15,\n",
    "    topk=5,\n",
    "    plot_topk_lens=False,\n",
    "    #json_log_path=None,\n",
    "    json_log_path='logs/nq_answers/dh.3b-bnb4bit.fp32', # 20 samples\n",
    "    #save_fig_path=None,\n",
    "    #save_fig_path='Outputs/LogitLens/DH3B/logits_3b_fp32_math.jpg',\n",
    "    #entropy=True,\n",
    "    model_precision=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dh8b_bnb8_float32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "json_path = 'logs/gsm8k/llama.8b-1.58.fp32'\n",
    "# Load your JSON file\n",
    "with open(json_path, 'r') as f:\n",
    "    log_data = json.load(f)\n",
    "\n",
    "# Convert the loaded JSON data to a DataFrame\n",
    "df = pd.json_normalize(log_data)\n",
    "\n",
    "# If you want to see the dataframe\n",
    "print(df)\n",
    "\n",
    "# Optionally, display it in a Jupyter notebook in a more readable format\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "# Load the JSON file and normalize into DataFrame\n",
    "with open('logs/gsm8k/dh.3b-ptsq.fp32', 'r') as f:\n",
    "    log_data = json.load(f)\n",
    "df = pd.json_normalize(log_data)\n",
    "\n",
    "# Ensure each row's layer_names and entropy have matching length\n",
    "num_layers = len(df.loc[0, 'layer_names'])\n",
    "sum_entropy = [0.0] * num_layers\n",
    "valid_rows = 0\n",
    "\n",
    "for _, row in df.iterrows():\n",
    "    layer_names = row['layer_names']\n",
    "    entropy = row['entropy']\n",
    "    \n",
    "    # Validate that both lists are the expected length\n",
    "    if isinstance(entropy, list) and len(entropy) == num_layers:\n",
    "        sum_entropy = [s + e for s, e in zip(sum_entropy, entropy)]\n",
    "        valid_rows += 1\n",
    "\n",
    "# Compute average\n",
    "if valid_rows > 0:\n",
    "    avg_entropy = [e / valid_rows for e in sum_entropy]\n",
    "    layer_labels = df.loc[0, 'layer_names']\n",
    "\n",
    "    # Plotting\n",
    "    plt.figure(figsize=(14, 6))\n",
    "    plt.plot(layer_labels, avg_entropy, marker='o', linestyle='-', color='b')\n",
    "    plt.xlabel('Layer Name')\n",
    "    plt.ylabel('Average Entropy')\n",
    "    plt.title(f'Average Entropy Across Layers (n = {valid_rows} samples)')\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No valid rows matched expected layer length.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['entropy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['normalized_entropy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logit_lens.plot_logit_lens(\n",
    "    model=llama8b_fp32,\n",
    "    tokenizer=llama8b_tokenizer,\n",
    "    input_ids=MiscPrompts.Q12.value,\n",
    "    start_ix=0, end_ix=15,\n",
    "    save_fig_path=None,\n",
    "    #save_fig_path='Outputs/LogitLens/LI8B/probs_hf1bit_fp32_qa_blockstep10.jpg',\n",
    "    probs=True,\n",
    "    block_step=5,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logit_lens.plot_logit_lens(\n",
    "    model=hfbit1_fp32,\n",
    "    tokenizer=hfbit1_tokenizer,\n",
    "    input_ids=MiscPrompts.Q12.value,\n",
    "    start_ix=0, end_ix=15,\n",
    "    save_fig_path=None,\n",
    "    #save_fig_path='Outputs/LogitLens/LI8B/probs_hf1bit_fp32_qa_blockstep10.jpg',\n",
    "    #probs=True,\n",
    "    block_step=5,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logit_lens.plot_logit_lens(\n",
    "    model=dh3b_bitnet_fp32_qlmhead,\n",
    "    tokenizer=dh3b_tokenizer,\n",
    "    input_ids=MiscPrompts.Q12.value,\n",
    "    start_ix=0, end_ix=15,\n",
    "    save_fig_path=None,\n",
    "    #save_fig_path='Outputs/LogitLens/LI8B/probs_hf1bit_fp32_qa_blockstep10.jpg',\n",
    "    entropy=True,\n",
    "    block_step=5,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logit_lens.plot_logit_lens(\n",
    "    model=dh3b_bitnet_fp32_flmhead,\n",
    "    tokenizer=dh3b_tokenizer,\n",
    "    input_ids=MiscPrompts.Q12.value,\n",
    "    start_ix=0, end_ix=15,\n",
    "    #save_fig_path=None,\n",
    "    save_fig_path='Outputs/LogitLens/DH3B/logits_flmhead_fp32_qa.jpg',\n",
    "    #kl=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logit_lens.plot_comparing_lens(\n",
    "    models=(dh3b_fp32, dh3b_bitnet_fp32_qlmhead),\n",
    "    tokenizer=dh3b_tokenizer,\n",
    "    input_ids=MiscPrompts.Q12.value,\n",
    "    start_ix=0, end_ix=6,\n",
    "    #save_fig_path='Outputs/LogitLens/LI8B/nwd_llama_hf1bit_qa_blockstep10.jpg',\n",
    "    save_fig_path=None,\n",
    "    wasserstein=True,\n",
    "    #top_down=False,\n",
    "    block_step=5,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logit_lens.plot_comparing_lens(\n",
    "    models=(olmo1b_fp32, olmo1b_bitnet_fp32_qlmhead),\n",
    "    tokenizer=olmo1b_tokenizer,\n",
    "    input_ids=MiscPrompts.Q12.value,\n",
    "    start_ix=0, end_ix=15,\n",
    "    #save_fig_path='Outputs/LogitLens/OLMo1B/nwd_flmhead_qlmhead_math.jpg',\n",
    "    #save_fig_path=None,\n",
    "    wasserstein=True,\n",
    "    #top_down=False,\n",
    "    #block_step=3,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logit_lens.plot_comparing_lens(\n",
    "    models=(llama8b_fp32, hfbit1_fp32),\n",
    "    tokenizer=hfbit1_tokenizer,\n",
    "    input_ids=MiscPrompts.Q12.value,\n",
    "    start_ix=0, end_ix=15,\n",
    "    #save_fig_path='Outputs/LogitLens/LI8B/nwd_instruct_bitnet_fp32_qa.jpg',\n",
    "    save_fig_path=None,\n",
    "    wasserstein=True,\n",
    "    #top_down=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logit_lens.plot_comparing_lens(\n",
    "    models=(llama8b_fp32, llama8b_bnb4_fp16),\n",
    "    tokenizer=llama8b_tokenizer,\n",
    "    input_ids=MiscPrompts.Q12.value,\n",
    "    start_ix=0, end_ix=15,\n",
    "    #save_fig_path='Outputs/LogitLens/DH3B/nwd_3bfp32_ptdq_math.jpg',\n",
    "    save_fig_path=None,\n",
    "    wasserstein=True,\n",
    "    #top_down=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logit_lens.plot_topk_lens(\n",
    "    model=olmo1b_fp32,\n",
    "    tokenizer=olmo1b_tokenizer,\n",
    "    input_ids=MiscPrompts.Q12.value,\n",
    "    start_ix=0, end_ix=15,\n",
    "    topk_n=5,\n",
    "    #save_fig_path='Outputs/LogitLens/LI8B/topk5logits_bnb4bit_qa.jpg'\n",
    "    #save_fig_path=None,\n",
    "    #top_down=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logit_lens.plot_topk_lens(\n",
    "    model=olmo1b_bitnet_fp32_qlmhead,\n",
    "    tokenizer=olmo1b_tokenizer,\n",
    "    input_ids=MiscPrompts.Q12.value,\n",
    "    start_ix=0, end_ix=15,\n",
    "    topk_n=5,\n",
    "    #save_fig_path='Outputs/LogitLens/LI8B/topk5logits_ptsq_fp32_math.jpg'\n",
    "    #save_fig_path=None,\n",
    "    #entropy=True,\n",
    "    #top_down=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activation_lens.plot_activation_lens(\n",
    "    model=olmo1b_fp32,\n",
    "    tokenizer=olmo1b_tokenizer,\n",
    "    input_ids=MiscPrompts.Q12.value,\n",
    "    start_ix=0, end_ix=15,\n",
    "    metric='norm',\n",
    "    save_fig_path='Outputs/LogitLens/OLMo1B/actnorm_fp_qa.jpg', #HUSK FOR BITNET hfb1 Q12 qa!\n",
    "    #save_fig_path=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activation_lens.plot_activation_lens(\n",
    "    model=olmo1b_bitnet_fp32_qlmhead,\n",
    "    tokenizer=olmo1b_tokenizer,\n",
    "    input_ids=MiscPrompts.Q12.value,\n",
    "    start_ix=0, end_ix=15,\n",
    "    metric='norm',\n",
    "    save_fig_path='Outputs/LogitLens/OLMo1B/actnorm_ptsq_qa.jpg',\n",
    "    #save_fig_path=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activation_lens.plot_comparing_act_lens(\n",
    "    models=(olmo1b_fp32, olmo1b_bitnet_fp32_qlmhead),\n",
    "    tokenizer=olmo1b_tokenizer,\n",
    "    input_ids=MiscPrompts.Q12.value,\n",
    "    start_ix=0, end_ix=15,\n",
    "    metric='norm',\n",
    "    metric_name='l2',\n",
    "    save_fig_path='Outputs/LogitLens/OLMo1B/actnorm_fp_compare_ptsq_qa.jpg',\n",
    "    #save_fig_path=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dictionary Learning: SAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch._dynamo\n",
    "torch._dynamo.config.suppress_errors = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dh3b_bitnet_fp32_ptsq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary_learning.plot_sae_heatmap(\n",
    "    model=dh3b_fp32,\n",
    "    tokenizer=dh3b_tokenizer,\n",
    "    inputs=gsm8k_questions_sae,\n",
    "    plot_sae=True,\n",
    "    do_log=True,\n",
    "    top_k=5,\n",
    "    tokens_per_row=30,\n",
    "    target_layers=[2, 9, 16, 20, 26],\n",
    "    log_path='logs/sae_logs/DH3B/fp',\n",
    "    log_name='dh.3b-ptsq.fp32',\n",
    "    fig_path=None,\n",
    "    deterministic_sae=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary_learning.plot_sae_tokens(\n",
    "    model=dh3b_fp32,\n",
    "    tokenizer=dh3b_tokenizer,\n",
    "    inputs=PARAMS.get('prompt'),\n",
    "    multi_tokens=False,\n",
    "    do_log=False,\n",
    "    target_layers=[5],\n",
    "    vis_projection=None,\n",
    "    log_path=None,\n",
    "    log_name=None,\n",
    "    fig_path=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary_learning.plot_sae_tokens(\n",
    "    model=dh3b_fp32,\n",
    "    tokenizer=dh3b_tokenizer,\n",
    "    inputs=PARAMS.get('prompt'),\n",
    "    multi_tokens=True,\n",
    "    do_log=False,\n",
    "    target_layers=[5],\n",
    "    vis_projection=None,\n",
    "    log_path=None,\n",
    "    log_name=None,\n",
    "    fig_path=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary_learning.plot_sae_tokens(\n",
    "    model=dh3b_bitnet_fp32,\n",
    "    tokenizer=dh3b_tokenizer,\n",
    "    inputs=PARAMS.get('prompt'),\n",
    "    multi_tokens=True,\n",
    "    do_log=False,\n",
    "    target_layers=[5],\n",
    "    vis_projection=None,\n",
    "    log_path=None,\n",
    "    log_name=None,\n",
    "    fig_path=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary_learning.plot_sae_heatmap(\n",
    "    model=olmo1b_fp32,\n",
    "    tokenizer=olmo1b_tokenizer,\n",
    "    inputs=Texts.T1.value,\n",
    "    do_log=False,\n",
    "    top_k=5,\n",
    "    tokens_per_row=30,\n",
    "    target_layers=[5, 10, 15],\n",
    "    log_path=None,\n",
    "    log_name=None,\n",
    "    fig_path=None,\n",
    "    deterministic_sae=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary_learning.plot_sae_heatmap(\n",
    "    model=olmo1b_fp32,\n",
    "    tokenizer=olmo1b_tokenizer,\n",
    "    inputs=Texts.T1.value,\n",
    "    do_log=False,\n",
    "    top_k=5,\n",
    "    tokens_per_row=30,\n",
    "    target_layers=[5, 10, 15],\n",
    "    log_path=None,\n",
    "    log_name=None,\n",
    "    fig_path=None,\n",
    "    deterministic_sae=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary_learning.plot_sae_heatmap(\n",
    "    model=olmo2t_fp32,\n",
    "    tokenizer=olmo2t_tokenizer,\n",
    "    inputs=Texts.T1.value,\n",
    "    do_log=False,\n",
    "    top_k=5,\n",
    "    tokens_per_row=30,\n",
    "    target_layers=[5, 10, 15],\n",
    "    log_path=None,\n",
    "    log_name=None,\n",
    "    fig_path=None,\n",
    "    deterministic_sae=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary_learning.plot_comparing_heatmap(\n",
    "    models=(dh3b_fp32, dh3b_bitnet_fp32_qlmhead),\n",
    "    tokenizer=dh3b_tokenizer,\n",
    "    inputs=Texts.T1.value,\n",
    "    top_k=5,\n",
    "    tokens_per_row=30,\n",
    "    target_layers=[5, 15, 25],\n",
    "    fig_path=None,\n",
    "    deterministic_sae=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary_learning.plot_comparing_heatmap(\n",
    "    models=(dh3b_fp32, dh3b_bitnet_fp32_ptsq),\n",
    "    tokenizer=dh3b_tokenizer,\n",
    "    inputs=gsm8k_questions_sae,\n",
    "    top_k=5,\n",
    "    tokens_per_row=30,\n",
    "    target_layers=[2, 9, 16, 20, 26],\n",
    "    fig_path=None,\n",
    "    deterministic_sae=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary_learning.plot_comparing_heatmap(\n",
    "    models=(llama8b_fp32, hfbit1_fp32),\n",
    "    tokenizer=llama8b_tokenizer,\n",
    "    inputs=gsm8k_questions_sae,\n",
    "    top_k=5,\n",
    "    tokens_per_row=30,\n",
    "    target_layers=[2, 9, 16, 23, 30],\n",
    "    fig_path=None\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Hermes Chatbot Analysis (template only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_dict = {\n",
    "    #'dh.3b-llama.fp32': dh3b_fp32,\n",
    "    #'dh.3b-bnb4bit.fp16': dh3b_bnb4_fp16,\n",
    "    #'dh.3b-1.58.ptdq': dh3b_bitnet_fp32, \n",
    "    #'dh.3b-1.58.ptsq': dh3b_bitnet_fp32,\n",
    "    #'dh.8b-llama.fp32': dh8b_fp32,\n",
    "    #'dh.8b-bnb4bit.fp16': dh8b_bnb4_fp16,\n",
    "    #'dh.8b-1.58.ptdq': dh8b_bitnet_fp32,\n",
    "    #'dh.8b-1.58.ptsq': dh8b_bitnet_fp32,\n",
    "    #'llama.8b-instruct.fp32': llama8b_fp32,\n",
    "    #'llama.8b-bnb4bit.fp16': llama8b_bnb4_fp16,\n",
    "    #'llama.8b-1.58.fp32': hfbit1_fp32,\n",
    "    #'llama.8b-1.58.ptdq': llama8b_bitnet_fp32,\n",
    "    #'llama.8b-1.58.ptsq': llama8b_bitnet_fp32,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PARAMS:Dict = {\n",
    "    'context': Contexts.C1.value,\n",
    "    'prompt': MiscPrompts.Q2.value,\n",
    "    'max_new_tokens': 100,\n",
    "    'temperature': 0.8,\n",
    "    'repetition_penalty': 1.1,\n",
    "    'sample': True,\n",
    "    'device': None\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chatbot_analysis.run_gsm8k_analysis(\n",
    "    model=dh3b_bnb4_float32,\n",
    "    tokenizer=dh3b_tokenizer,\n",
    "    model_name='dh.3b-bnb4bit.fp32',\n",
    "    dataset=gsm8k_dataset['train'],\n",
    "    save_path='logs/gsm8k_logs/DH3B',\n",
    "    num_samples=10,\n",
    "    deterministic_backend=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chatbot_analysis.run_chatbot_analysis(\n",
    "    models=chat_dict,\n",
    "    tokenizer=dh3b_tokenizer,\n",
    "    deep_thinking=False,\n",
    "    full_path='logs/chatbot_logs',\n",
    "    deterministic_backend=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chatbot_analysis.plot_chatbot_analysis(\n",
    "    json_logs='logs/gsm8k_logs',\n",
    "    parallel_plot=True,\n",
    "    reference_file='logs/gsm8k_logs/llama.8b-1.58.fp32.json'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chatbot_analysis.plot_chatbot_analysis(\n",
    "    json_logs='logs/gsm8k_logs',\n",
    "    parallel_plot=False,\n",
    "    reference_file='logs/gsm8k_logs/llama.8b-1.58.fp32.json',\n",
    "    title=\"Model Metrics ('What is y if y=2*2-4+(3*2)')\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import matplotlib.cm as cm\n",
    "# Path to your results folder\n",
    "results_dir = \"logs/gsm8k_logs\"\n",
    "\n",
    "# Load all JSONs into a DataFrame\n",
    "all_results = []\n",
    "for filename in os.listdir(results_dir):\n",
    "    if filename.endswith(\".json\"):\n",
    "        with open(os.path.join(results_dir, filename), 'r') as f:\n",
    "            data = json.load(f)\n",
    "            all_results.append(data)\n",
    "\n",
    "df = pd.DataFrame(all_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define metrics\n",
    "metrics = [\n",
    "    'Perplexity',\n",
    "    'CPU Usage (%)',\n",
    "    'RAM Usage (%)',\n",
    "    'GPU Memory (MB)',\n",
    "    'Activation Similarity',\n",
    "    'Latency (s)'\n",
    "]\n",
    "\n",
    "# Generate a color map\n",
    "models = df['Model'].tolist()\n",
    "num_models = len(models)\n",
    "colors = cm.get_cmap('tab20c', num_models)  # You can try 'Set2', 'hsv', 'tab10', etc.\n",
    "\n",
    "model_colors = {model: colors(i) for i, model in enumerate(models)}\n",
    "\n",
    "# Plot\n",
    "fig, axes = plt.subplots(nrows=2, ncols=3, figsize=(18, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, metric in enumerate(metrics):\n",
    "    ax = axes[i]\n",
    "    for j, model in enumerate(models):\n",
    "        ax.bar(model, df.loc[j, metric], color=model_colors[model])\n",
    "    ax.set_title(metric)\n",
    "    ax.set_xticklabels(models, rotation=45, ha='right')\n",
    "    ax.grid(True)\n",
    "\n",
    "# Custom legend\n",
    "handles = [plt.Rectangle((0,0),1,1, color=model_colors[model]) for model in models]\n",
    "fig.legend(handles, models, loc='upper center', ncol=num_models)\n",
    "\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.92])\n",
    "plt.suptitle(\"GSM8K for n=10\", fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = [\n",
    "    'Perplexity',\n",
    "    'CPU Usage (%)',\n",
    "    'RAM Usage (%)',\n",
    "    'GPU Memory (MB)',\n",
    "    'Activation Similarity',\n",
    "    'Latency (s)',\n",
    "    \"Last Layer Mean Activation\",\n",
    "    \"Last Layer Activation Std\",\n",
    "    \"Mean Logits\",\n",
    "    \"Logit Std\",\n",
    "]\n",
    "\n",
    "# Generate a color map\n",
    "models = df['Model'].tolist()\n",
    "num_models = len(models)\n",
    "colors = cm.get_cmap('coolwarm', num_models)\n",
    "model_colors = {model: colors(i) for i, model in enumerate(models)}\n",
    "\n",
    "# Prepare subplot grid dynamically\n",
    "n_metrics = len(metrics)\n",
    "ncols = 5\n",
    "nrows = int(np.ceil(n_metrics / ncols))\n",
    "\n",
    "fig, axes = plt.subplots(nrows=nrows, ncols=ncols, figsize=(20, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, metric in enumerate(metrics):\n",
    "    ax = axes[i]\n",
    "    if metric in df.columns:\n",
    "        for j, model in enumerate(models):\n",
    "            ax.bar(model, df.loc[j, metric], color=model_colors[model])\n",
    "        ax.set_title(metric, fontsize=10)\n",
    "        ax.set_xticks(range(len(models)))\n",
    "        ax.set_xticklabels(models, rotation=45, ha='right', fontsize=8)\n",
    "        ax.grid(True)\n",
    "\n",
    "# Hide unused axes\n",
    "for j in range(i + 1, len(axes)):\n",
    "    fig.delaxes(axes[j])\n",
    "\n",
    "# Legend\n",
    "##handles = [plt.Rectangle((0, 0), 1, 1, color=model_colors[model]) for model in models]\n",
    "#fig.legend(handles, models, loc='upper center', ncol=min(num_models, 5))\n",
    "\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.93])\n",
    "plt.suptitle(\"Deep Hermes LLaMA 3B & LLaMA Instruct 8B GSM8K (n=10)\", fontsize=12)\n",
    "plt.savefig('Outputs/Report/llama8bdh3b_subplots.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = [\n",
    "    'Perplexity',\n",
    "    'CPU Usage (%)',\n",
    "    'RAM Usage (%)',\n",
    "    'GPU Memory (MB)',\n",
    "    'Activation Similarity',\n",
    "    'Latency (s)',\n",
    "    \"Last Layer Mean Activation\",\n",
    "    \"Last Layer Activation Std\",\n",
    "    \"Mean Logits\",\n",
    "    \"Logit Std\",\n",
    "]\n",
    "\n",
    "# Generate a color map\n",
    "models = df['Model'].tolist()\n",
    "num_models = len(models)\n",
    "colors = cm.get_cmap('coolwarm', num_models)\n",
    "model_colors = {model: colors(i) for i, model in enumerate(models)}\n",
    "\n",
    "# Prepare subplot grid dynamically\n",
    "n_metrics = len(metrics)\n",
    "ncols = 5\n",
    "nrows = int(np.ceil(n_metrics / ncols))\n",
    "\n",
    "fig, axes = plt.subplots(nrows=nrows, ncols=ncols, figsize=(20, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, metric in enumerate(metrics):\n",
    "    ax = axes[i]\n",
    "    if metric in df.columns:\n",
    "        values = df[metric].copy()\n",
    "\n",
    "        # Normalize perplexity via log-scale\n",
    "        if metric == \"Perplexity\":\n",
    "            values = np.log1p(values)  # log1p handles 0 gracefully\n",
    "\n",
    "        for j, model in enumerate(models):\n",
    "            ax.bar(model, values[j], color=model_colors[model])\n",
    "        ax.set_title(metric + (\" (log)\" if metric == \"Perplexity\" else \"\"), fontsize=10)\n",
    "        ax.set_xticks(range(len(models)))\n",
    "        ax.set_xticklabels(models, rotation=45, ha='right', fontsize=8)\n",
    "        ax.grid(True)\n",
    "\n",
    "# Hide unused axes\n",
    "for j in range(i + 1, len(axes)):\n",
    "    fig.delaxes(axes[j])\n",
    "\n",
    "# Legend\n",
    "##handles = [plt.Rectangle((0, 0), 1, 1, color=model_colors[model]) for model in models]\n",
    "#fig.legend(handles, models, loc='upper center', ncol=min(num_models, 5))\n",
    "\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.93])\n",
    "plt.suptitle(\"Deep Hermes LLaMA 3B & LLaMA Instruct 8B GSM8K (n=10)\", fontsize=12)\n",
    "plt.savefig('Outputs/Report/llama8bdh3b_subplots_normalized_perplexity.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "# Metrics to include (excluding 'Activation Similarity')\n",
    "metrics_for_corr = [\n",
    "    'Perplexity',\n",
    "    'CPU Usage (%)',\n",
    "    'RAM Usage (%)',\n",
    "    'GPU Memory (MB)',\n",
    "    'Latency (s)',\n",
    "    \"Last Layer Mean Activation\",\n",
    "    \"Last Layer Activation Std\",\n",
    "    \"Mean Logits\",\n",
    "    \"Logit Std\",\n",
    "]\n",
    "\n",
    "# Compute correlation\n",
    "corr_matrix = df[metrics_for_corr].corr()\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(\n",
    "    corr_matrix, \n",
    "    annot=True, \n",
    "    cmap='coolwarm', \n",
    "    fmt=\".2f\", \n",
    "    linewidths=0.5, \n",
    "    square=True,\n",
    "    cbar_kws={\"shrink\": 0.75}\n",
    ")\n",
    "plt.title(\"Deep Hermes LLaMA 3B & LLaMA Instruct 8B GSM8K Correlation (n=10)\", fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.savefig('Outputs/Report/llama8bdh3b_corr_heatmap.png')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MechInterp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
