{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ampir\\anaconda3\\envs\\MechInterp\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from __future__ import annotations\n",
    "from typing import Tuple, List, Dict, Optional, Any\n",
    "\n",
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from datasets import load_dataset, DownloadMode\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import tqdm\n",
    "\n",
    "from helper_utils.enum_keys import (\n",
    "    FPKey,\n",
    "    ModelKey,\n",
    "    QuantStyle,\n",
    "    MiscPrompts,\n",
    "    Contexts,\n",
    "    Texts\n",
    ")\n",
    "\n",
    "from PTQ.apply_ptq import applyPTQ\n",
    "from PTQ.olmo_act_fns import patch_olmo_mlp\n",
    "import helper_utils.utils as utils\n",
    "\n",
    "from mech_interp_utils.utils_main.src.transformer_utils import (\n",
    "    logit_lens,\n",
    "    activation_lens,\n",
    "    dictionary_learning,\n",
    "    chatbot_analysis\n",
    ")\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "def load_metrics_from_directory(directory_path):\n",
    "    \"\"\"\n",
    "    Load metrics from a directory of JSON files (list of dicts per file).\n",
    "    \"\"\"\n",
    "    all_metrics = []\n",
    "    for filename in os.listdir(directory_path):\n",
    "        if not filename.endswith(\".json\"):\n",
    "            continue\n",
    "        filepath = os.path.join(directory_path, filename)\n",
    "        with open(filepath, 'r') as f:\n",
    "            try:\n",
    "                data = json.load(f)\n",
    "                if isinstance(data, list) and all(isinstance(item, dict) for item in data):\n",
    "                    for item in data:\n",
    "                        item['file_name'] = filename\n",
    "                    all_metrics.extend(data)\n",
    "                else:\n",
    "                    print(f\"[!] Unexpected structure in {filename}\")\n",
    "            except Exception as e:\n",
    "                print(f\"[!] Failed to load {filename}: {e}\")\n",
    "    print(f\"Loaded {len(all_metrics)} total entries from {directory_path}\")\n",
    "    return all_metrics\n",
    "\n",
    "\n",
    "def load_all_model_metrics(source_map):\n",
    "    \"\"\"\n",
    "    Load metrics from either individual JSON files or directories,\n",
    "    tagging each record with a model_id for later analysis.\n",
    "\n",
    "    Parameters:\n",
    "        source_map (dict): Mapping from model name to path (file or directory)\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: All metrics combined and annotated with model identity\n",
    "    \"\"\"\n",
    "    records = []\n",
    "\n",
    "    for model_name, path in source_map.items():\n",
    "        path_obj = Path(path)\n",
    "        if path_obj.is_file():\n",
    "            try:\n",
    "                with open(path, \"r\") as f:\n",
    "                    data = json.load(f)\n",
    "                for entry in data:\n",
    "                    entry['model_id'] = model_name\n",
    "                    entry['file_name'] = path_obj.name\n",
    "                    records.append(entry)\n",
    "            except Exception as e:\n",
    "                print(f\"[!] Failed to load {path}: {e}\")\n",
    "        elif path_obj.is_dir():\n",
    "            data = load_metrics_from_directory(path)\n",
    "            for entry in data:\n",
    "                entry['model_id'] = model_name\n",
    "            records.extend(data)\n",
    "        else:\n",
    "            print(f\"[!] Invalid path: {path}\")\n",
    "\n",
    "    print(f\"Total entries combined: {len(records)}\")\n",
    "    return pd.DataFrame(records)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_sources = {\n",
    "    'OLMo-1B-hf': 'logs/gsm8k_answers/olmo.1b.fp32.json',\n",
    "    'OLMo-1B-8bit': 'logs/gsm8k_answers/olmo.1b-bnb8bit.fp32.json',\n",
    "    'OLMo-1B-4bit': 'logs/gsm8k_answers/olmo.1b-bnb4bit.fp32.json',\n",
    "    'OLMo-1B-1.58bit': 'logs/gsm8k_answers/olmo.1b-ptsq.fp32.json',\n",
    "    'DeepHermes-LLaMA-3B': 'logs/gsm8k_answers/dh.3b-llama.fp32.json',\n",
    "    'DeepHermes-3B-8bit': 'logs/gsm8k_answers/dh.3b-bnb8bit.fp32.json',\n",
    "    'DeepHermes-3B-4bit': 'logs/gsm8k_answers/dh.3b-bnb4bit.fp32.json',\n",
    "    'DeepHermes-3B-1.58bit': 'logs/gsm8k_answers/dh.3b-ptsq.fp32.json',\n",
    "    'LLaMA-Instruct-8B': 'logs/gsm8k_answers/llama.8b-instruct.fp32.json',\n",
    "    'LLaMA-Instruct-8B-8bit': 'logs/gsm8k_answers/llama.8b-bnb8bit.fp32.json',\n",
    "    'LLaMA-Instruct-8B-4bit': 'logs/gsm8k_answers/llama.8b-bnb4bit.fp32.json',\n",
    "    'HF1BitLLM': 'logs/gsm8k_answers/llama.8b-1.58.fp32.json'\n",
    "}\n",
    "\n",
    "df_all = load_all_model_metrics(model_sources)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_sources = {\n",
    "    'OLMo-1B-hf': 'logs/gsm8k_questions/olmo.1b.fp32.json',\n",
    "    'OLMo-1B-8bit': 'logs/gsm8k_questions/olmo.1b-bnb8bit.fp32.json',\n",
    "    'OLMo-1B-4bit': 'logs/gsm8k_questions/olmo.1b-bnb4bit.fp32.json',\n",
    "    'OLMo-1B-1.58bit': 'logs/gsm8k_questions/olmo.1b-ptsq.fp32.json',\n",
    "    'DeepHermes-LLaMA-3B': 'logs/gsm8k_questions/dh.3b-llama.fp32.json',\n",
    "    'DeepHermes-3B-8bit': 'logs/gsm8k_questions/dh.3b-bnb8bit.fp32.json',\n",
    "    'DeepHermes-3B-4bit': 'logs/gsm8k_questions/dh.3b-bnb4bit.fp32.json',\n",
    "    'DeepHermes-3B-1.58bit': 'logs/gsm8k_questions/dh.3b-ptsq.fp32.json',\n",
    "    'LLaMA-Instruct-8B': 'logs/gsm8k_questions/llama.8b-instruct.fp32.json',\n",
    "    'LLaMA-Instruct-8B-8bit': 'logs/gsm8k_questions/llama.8b-bnb8bit.fp32.json',\n",
    "    'LLaMA-Instruct-8B-4bit': 'logs/gsm8k_questions/llama.8b-bnb4bit.fp32.json',\n",
    "    'HF1BitLLM': 'logs/gsm8k_questions/llama.8b-1.58.fp32.json'\n",
    "}\n",
    "\n",
    "df_all = load_all_model_metrics(model_sources)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_sources = {\n",
    "    'OLMo-1B-hf': 'logs/nq_queries/olmo.1b.fp32.json',\n",
    "    'OLMo-1B-8bit': 'logs/nq_queries/olmo.1b-bnb8bit.fp32.json',\n",
    "    'OLMo-1B-4bit': 'logs/nq_queries/olmo.1b-bnb4bit.fp32.json',\n",
    "    'OLMo-1B-1.58bit': 'logs/nq_queries/olmo.1b-ptsq.fp32.json',\n",
    "    'DeepHermes-LLaMA-3B': 'logs/nq_queries/dh.3b-llama.fp32.json',\n",
    "    'DeepHermes-3B-8bit': 'logs/nq_queries/dh.3b-bnb8bit.fp32.json',\n",
    "    'DeepHermes-3B-4bit': 'logs/nq_queries/dh.3b-bnb4bit.fp32.json',\n",
    "    'DeepHermes-3B-1.58bit': 'logs/nq_queries/dh.3b-ptsq.fp32.json',\n",
    "    'LLaMA-Instruct-8B': 'logs/nq_queries/llama.8b-instruct.fp32.json',\n",
    "    'LLaMA-Instruct-8B-8bit': 'logs/nq_queries/llama.8b-bnb8bit.fp32.json',\n",
    "    'LLaMA-Instruct-8B-4bit': 'logs/nq_queries/llama.8b-bnb4bit.fp32.json',\n",
    "    'HF1BitLLM': 'logs/nq_queries/llama.8b-1.58.fp32.json'\n",
    "}\n",
    "\n",
    "df_all = load_all_model_metrics(model_sources)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total entries combined: 240\n"
     ]
    }
   ],
   "source": [
    "model_sources = {\n",
    "    'OLMo-1B-hf': 'logs/nq_answers/olmo.1b.fp32.json',\n",
    "    'OLMo-1B-8bit': 'logs/nq_answers/olmo.1b-bnb8bit.fp32.json',\n",
    "    'OLMo-1B-4bit': 'logs/nq_answers/olmo.1b-bnb4bit.fp32.json',\n",
    "    'OLMo-1B-1.58bit': 'logs/nq_answers/olmo.1b-ptsq.fp32.json',\n",
    "    'DeepHermes-LLaMA-3B': 'logs/nq_answers/dh.3b-llama.fp32.json',\n",
    "    'DeepHermes-3B-8bit': 'logs/nq_answers/dh.3b-bnb8bit.fp32.json',\n",
    "    'DeepHermes-3B-4bit': 'logs/nq_answers/dh.3b-bnb4bit.fp32.json',\n",
    "    'DeepHermes-3B-1.58bit': 'logs/nq_answers/dh.3b-ptsq.fp32.json',\n",
    "    'LLaMA-Instruct-8B': 'logs/nq_answers/llama.8b-instruct.fp32.json',\n",
    "    'LLaMA-Instruct-8B-8bit': 'logs/nq_answers/llama.8b-bnb8bit.fp32.json',\n",
    "    'LLaMA-Instruct-8B-4bit': 'logs/nq_answers/llama.8b-bnb4bit.fp32.json',\n",
    "    'HF1BitLLM': 'logs/nq_answers/llama.8b-1.58.fp32.json'\n",
    "}\n",
    "\n",
    "df_all = load_all_model_metrics(model_sources)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prompt</th>\n",
       "      <th>decoded_prompt_str</th>\n",
       "      <th>tokens</th>\n",
       "      <th>prompt_type</th>\n",
       "      <th>target_ids</th>\n",
       "      <th>target_tokens</th>\n",
       "      <th>layer_names</th>\n",
       "      <th>correct_1</th>\n",
       "      <th>correct_topk</th>\n",
       "      <th>correct_1_std</th>\n",
       "      <th>...</th>\n",
       "      <th>correct_topk_by_position</th>\n",
       "      <th>entropy</th>\n",
       "      <th>normalized_entropy</th>\n",
       "      <th>logit_mean</th>\n",
       "      <th>prob_mean</th>\n",
       "      <th>stability_top1</th>\n",
       "      <th>stability_topk</th>\n",
       "      <th>layer_kl_divergences</th>\n",
       "      <th>model_id</th>\n",
       "      <th>file_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[[17389, 9456, 15794, 9585, 25380, 3407, 4240,...</td>\n",
       "      <td>Richmond Football Club Richmond began 2017 wit...</td>\n",
       "      <td>[Rich, mond, ĠFootball, ĠClub, ĠRichmond, Ġbeg...</td>\n",
       "      <td>text</td>\n",
       "      <td>[9456, 15794, 9585, 25380, 3407, 4240, 342, 60...</td>\n",
       "      <td>[mond, ĠFootball, ĠClub, ĠRichmond, Ġbegan, Ġ2...</td>\n",
       "      <td>[model.layers.0, model.layers.1, model.layers....</td>\n",
       "      <td>[1.0, 1.0, 0.8, 0.6666666666666666, 0.53333333...</td>\n",
       "      <td>[1.0, 1.0, 0.9333333333333333, 0.6666666666666...</td>\n",
       "      <td>[0.0, 0.0, 0.4, 0.47140452079103173, 0.4988876...</td>\n",
       "      <td>...</td>\n",
       "      <td>[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0,...</td>\n",
       "      <td>[10.756144523620605, 10.807570457458496, 10.81...</td>\n",
       "      <td>[[0.9778485298156738, 0.993316650390625, 0.983...</td>\n",
       "      <td>[5.030283451080322, 2.4894778728485107, 1.7052...</td>\n",
       "      <td>[0.007354279048740864, 0.0003936590801458806, ...</td>\n",
       "      <td>[-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -...</td>\n",
       "      <td>[-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -...</td>\n",
       "      <td>[[0.11137033998966217, 0.04020148515701294, 0....</td>\n",
       "      <td>OLMo-1B-hf</td>\n",
       "      <td>olmo.1b.fp32.json</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[[16082, 7493, 313, 4093, 254, 10, 2058, 253, ...</td>\n",
       "      <td>Jack Scott (singer) At the beginning of 1960, ...</td>\n",
       "      <td>[Jack, ĠScott, Ġ(, sing, er, ), ĠAt, Ġthe, Ġbe...</td>\n",
       "      <td>text</td>\n",
       "      <td>[7493, 313, 4093, 254, 10, 2058, 253, 5068, 27...</td>\n",
       "      <td>[ĠScott, Ġ(, sing, er, ), ĠAt, Ġthe, Ġbeginnin...</td>\n",
       "      <td>[model.layers.0, model.layers.1, model.layers....</td>\n",
       "      <td>[1.0, 0.9333333333333333, 0.8, 0.6666666666666...</td>\n",
       "      <td>[1.0, 1.0, 0.8666666666666667, 0.6666666666666...</td>\n",
       "      <td>[0.0, 0.24944382578492943, 0.4, 0.471404520791...</td>\n",
       "      <td>...</td>\n",
       "      <td>[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0,...</td>\n",
       "      <td>[10.724294662475586, 10.8092622756958, 10.8132...</td>\n",
       "      <td>[[0.9967544674873352, 0.9980196952819824, 0.95...</td>\n",
       "      <td>[5.072450637817383, 2.083895206451416, 1.41339...</td>\n",
       "      <td>[0.011206414550542831, 0.00023871877056080848,...</td>\n",
       "      <td>[-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -...</td>\n",
       "      <td>[-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -...</td>\n",
       "      <td>[[0.028099993243813515, 0.03677203878760338, 0...</td>\n",
       "      <td>OLMo-1B-hf</td>\n",
       "      <td>olmo.1b.fp32.json</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[[56, 1062, 11841, 25818, 3275, 310, 670, 374,...</td>\n",
       "      <td>Wool Global wool production is about 2 million...</td>\n",
       "      <td>[W, ool, ĠGlobal, Ġwool, Ġproduction, Ġis, Ġab...</td>\n",
       "      <td>text</td>\n",
       "      <td>[1062, 11841, 25818, 3275, 310, 670, 374, 3041...</td>\n",
       "      <td>[ool, ĠGlobal, Ġwool, Ġproduction, Ġis, Ġabout...</td>\n",
       "      <td>[model.layers.0, model.layers.1, model.layers....</td>\n",
       "      <td>[1.0, 1.0, 0.8666666666666667, 0.6666666666666...</td>\n",
       "      <td>[1.0, 1.0, 1.0, 0.7333333333333333, 0.66666666...</td>\n",
       "      <td>[0.0, 0.0, 0.339934634239519, 0.47140452079103...</td>\n",
       "      <td>...</td>\n",
       "      <td>[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0,...</td>\n",
       "      <td>[10.71219539642334, 10.80852222442627, 10.8123...</td>\n",
       "      <td>[[0.9076886773109436, 0.9939619898796082, 0.99...</td>\n",
       "      <td>[5.187832355499268, 2.4061098098754883, 1.6752...</td>\n",
       "      <td>[0.012746719643473625, 0.0003259088553022593, ...</td>\n",
       "      <td>[-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -...</td>\n",
       "      <td>[-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -...</td>\n",
       "      <td>[[0.4904257357120514, 0.037792764604091644, 0....</td>\n",
       "      <td>OLMo-1B-hf</td>\n",
       "      <td>olmo.1b.fp32.json</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[[2422, 11304, 27, 380, 9859, 48688, 20829, 27...</td>\n",
       "      <td>Alaska: The Last Frontier Alaska: The Last Fro...</td>\n",
       "      <td>[Al, aska, :, ĠThe, ĠLast, ĠFrontier, ĠAlaska,...</td>\n",
       "      <td>text</td>\n",
       "      <td>[11304, 27, 380, 9859, 48688, 20829, 27, 380, ...</td>\n",
       "      <td>[aska, :, ĠThe, ĠLast, ĠFrontier, ĠAlaska, :, ...</td>\n",
       "      <td>[model.layers.0, model.layers.1, model.layers....</td>\n",
       "      <td>[1.0, 1.0, 0.9333333333333333, 0.6666666666666...</td>\n",
       "      <td>[1.0, 1.0, 1.0, 0.8, 0.6666666666666666, 0.533...</td>\n",
       "      <td>[0.0, 0.0, 0.24944382578492943, 0.471404520791...</td>\n",
       "      <td>...</td>\n",
       "      <td>[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0,...</td>\n",
       "      <td>[10.770737648010254, 10.80851936340332, 10.813...</td>\n",
       "      <td>[[0.9897329211235046, 0.9984946250915527, 0.99...</td>\n",
       "      <td>[5.026289463043213, 2.6048905849456787, 1.7857...</td>\n",
       "      <td>[0.006153601221740246, 0.0005233273259364069, ...</td>\n",
       "      <td>[-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -...</td>\n",
       "      <td>[-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -...</td>\n",
       "      <td>[[0.04795553907752037, 0.02789744734764099, 0....</td>\n",
       "      <td>OLMo-1B-hf</td>\n",
       "      <td>olmo.1b.fp32.json</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[[3074, 309, 29430, 313, 34, 6258, 281, 18741,...</td>\n",
       "      <td>All I Want (A Day to Remember song) The music ...</td>\n",
       "      <td>[All, ĠI, ĠWant, Ġ(, A, ĠDay, Ġto, ĠRemember, ...</td>\n",
       "      <td>text</td>\n",
       "      <td>[309, 29430, 313, 34, 6258, 281, 18741, 4498, ...</td>\n",
       "      <td>[ĠI, ĠWant, Ġ(, A, ĠDay, Ġto, ĠRemember, Ġsong...</td>\n",
       "      <td>[model.layers.0, model.layers.1, model.layers....</td>\n",
       "      <td>[1.0, 1.0, 0.8666666666666667, 0.4666666666666...</td>\n",
       "      <td>[1.0, 1.0, 0.9333333333333333, 0.6, 0.4, 0.4, ...</td>\n",
       "      <td>[0.0, 0.0, 0.33993463423951903, 0.498887651569...</td>\n",
       "      <td>...</td>\n",
       "      <td>[[1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...</td>\n",
       "      <td>[10.788894653320312, 10.810789108276367, 10.81...</td>\n",
       "      <td>[[0.9987165927886963, 0.99701327085495, 0.9980...</td>\n",
       "      <td>[4.419623851776123, 2.0161120891571045, 1.3066...</td>\n",
       "      <td>[0.0028756733518093824, 0.00020052891341038048...</td>\n",
       "      <td>[-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -...</td>\n",
       "      <td>[-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -...</td>\n",
       "      <td>[[0.025843387469649315, 0.02116398699581623, 0...</td>\n",
       "      <td>OLMo-1B-hf</td>\n",
       "      <td>olmo.1b.fp32.json</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              prompt  \\\n",
       "0  [[17389, 9456, 15794, 9585, 25380, 3407, 4240,...   \n",
       "1  [[16082, 7493, 313, 4093, 254, 10, 2058, 253, ...   \n",
       "2  [[56, 1062, 11841, 25818, 3275, 310, 670, 374,...   \n",
       "3  [[2422, 11304, 27, 380, 9859, 48688, 20829, 27...   \n",
       "4  [[3074, 309, 29430, 313, 34, 6258, 281, 18741,...   \n",
       "\n",
       "                                  decoded_prompt_str  \\\n",
       "0  Richmond Football Club Richmond began 2017 wit...   \n",
       "1  Jack Scott (singer) At the beginning of 1960, ...   \n",
       "2  Wool Global wool production is about 2 million...   \n",
       "3  Alaska: The Last Frontier Alaska: The Last Fro...   \n",
       "4  All I Want (A Day to Remember song) The music ...   \n",
       "\n",
       "                                              tokens prompt_type  \\\n",
       "0  [Rich, mond, ĠFootball, ĠClub, ĠRichmond, Ġbeg...        text   \n",
       "1  [Jack, ĠScott, Ġ(, sing, er, ), ĠAt, Ġthe, Ġbe...        text   \n",
       "2  [W, ool, ĠGlobal, Ġwool, Ġproduction, Ġis, Ġab...        text   \n",
       "3  [Al, aska, :, ĠThe, ĠLast, ĠFrontier, ĠAlaska,...        text   \n",
       "4  [All, ĠI, ĠWant, Ġ(, A, ĠDay, Ġto, ĠRemember, ...        text   \n",
       "\n",
       "                                          target_ids  \\\n",
       "0  [9456, 15794, 9585, 25380, 3407, 4240, 342, 60...   \n",
       "1  [7493, 313, 4093, 254, 10, 2058, 253, 5068, 27...   \n",
       "2  [1062, 11841, 25818, 3275, 310, 670, 374, 3041...   \n",
       "3  [11304, 27, 380, 9859, 48688, 20829, 27, 380, ...   \n",
       "4  [309, 29430, 313, 34, 6258, 281, 18741, 4498, ...   \n",
       "\n",
       "                                       target_tokens  \\\n",
       "0  [mond, ĠFootball, ĠClub, ĠRichmond, Ġbegan, Ġ2...   \n",
       "1  [ĠScott, Ġ(, sing, er, ), ĠAt, Ġthe, Ġbeginnin...   \n",
       "2  [ool, ĠGlobal, Ġwool, Ġproduction, Ġis, Ġabout...   \n",
       "3  [aska, :, ĠThe, ĠLast, ĠFrontier, ĠAlaska, :, ...   \n",
       "4  [ĠI, ĠWant, Ġ(, A, ĠDay, Ġto, ĠRemember, Ġsong...   \n",
       "\n",
       "                                         layer_names  \\\n",
       "0  [model.layers.0, model.layers.1, model.layers....   \n",
       "1  [model.layers.0, model.layers.1, model.layers....   \n",
       "2  [model.layers.0, model.layers.1, model.layers....   \n",
       "3  [model.layers.0, model.layers.1, model.layers....   \n",
       "4  [model.layers.0, model.layers.1, model.layers....   \n",
       "\n",
       "                                           correct_1  \\\n",
       "0  [1.0, 1.0, 0.8, 0.6666666666666666, 0.53333333...   \n",
       "1  [1.0, 0.9333333333333333, 0.8, 0.6666666666666...   \n",
       "2  [1.0, 1.0, 0.8666666666666667, 0.6666666666666...   \n",
       "3  [1.0, 1.0, 0.9333333333333333, 0.6666666666666...   \n",
       "4  [1.0, 1.0, 0.8666666666666667, 0.4666666666666...   \n",
       "\n",
       "                                        correct_topk  \\\n",
       "0  [1.0, 1.0, 0.9333333333333333, 0.6666666666666...   \n",
       "1  [1.0, 1.0, 0.8666666666666667, 0.6666666666666...   \n",
       "2  [1.0, 1.0, 1.0, 0.7333333333333333, 0.66666666...   \n",
       "3  [1.0, 1.0, 1.0, 0.8, 0.6666666666666666, 0.533...   \n",
       "4  [1.0, 1.0, 0.9333333333333333, 0.6, 0.4, 0.4, ...   \n",
       "\n",
       "                                       correct_1_std  ...  \\\n",
       "0  [0.0, 0.0, 0.4, 0.47140452079103173, 0.4988876...  ...   \n",
       "1  [0.0, 0.24944382578492943, 0.4, 0.471404520791...  ...   \n",
       "2  [0.0, 0.0, 0.339934634239519, 0.47140452079103...  ...   \n",
       "3  [0.0, 0.0, 0.24944382578492943, 0.471404520791...  ...   \n",
       "4  [0.0, 0.0, 0.33993463423951903, 0.498887651569...  ...   \n",
       "\n",
       "                            correct_topk_by_position  \\\n",
       "0  [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0,...   \n",
       "1  [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0,...   \n",
       "2  [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0,...   \n",
       "3  [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0,...   \n",
       "4  [[1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...   \n",
       "\n",
       "                                             entropy  \\\n",
       "0  [10.756144523620605, 10.807570457458496, 10.81...   \n",
       "1  [10.724294662475586, 10.8092622756958, 10.8132...   \n",
       "2  [10.71219539642334, 10.80852222442627, 10.8123...   \n",
       "3  [10.770737648010254, 10.80851936340332, 10.813...   \n",
       "4  [10.788894653320312, 10.810789108276367, 10.81...   \n",
       "\n",
       "                                  normalized_entropy  \\\n",
       "0  [[0.9778485298156738, 0.993316650390625, 0.983...   \n",
       "1  [[0.9967544674873352, 0.9980196952819824, 0.95...   \n",
       "2  [[0.9076886773109436, 0.9939619898796082, 0.99...   \n",
       "3  [[0.9897329211235046, 0.9984946250915527, 0.99...   \n",
       "4  [[0.9987165927886963, 0.99701327085495, 0.9980...   \n",
       "\n",
       "                                          logit_mean  \\\n",
       "0  [5.030283451080322, 2.4894778728485107, 1.7052...   \n",
       "1  [5.072450637817383, 2.083895206451416, 1.41339...   \n",
       "2  [5.187832355499268, 2.4061098098754883, 1.6752...   \n",
       "3  [5.026289463043213, 2.6048905849456787, 1.7857...   \n",
       "4  [4.419623851776123, 2.0161120891571045, 1.3066...   \n",
       "\n",
       "                                           prob_mean  \\\n",
       "0  [0.007354279048740864, 0.0003936590801458806, ...   \n",
       "1  [0.011206414550542831, 0.00023871877056080848,...   \n",
       "2  [0.012746719643473625, 0.0003259088553022593, ...   \n",
       "3  [0.006153601221740246, 0.0005233273259364069, ...   \n",
       "4  [0.0028756733518093824, 0.00020052891341038048...   \n",
       "\n",
       "                                      stability_top1  \\\n",
       "0  [-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -...   \n",
       "1  [-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -...   \n",
       "2  [-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -...   \n",
       "3  [-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -...   \n",
       "4  [-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -...   \n",
       "\n",
       "                                      stability_topk  \\\n",
       "0  [-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -...   \n",
       "1  [-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -...   \n",
       "2  [-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -...   \n",
       "3  [-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -...   \n",
       "4  [-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -...   \n",
       "\n",
       "                                layer_kl_divergences    model_id  \\\n",
       "0  [[0.11137033998966217, 0.04020148515701294, 0....  OLMo-1B-hf   \n",
       "1  [[0.028099993243813515, 0.03677203878760338, 0...  OLMo-1B-hf   \n",
       "2  [[0.4904257357120514, 0.037792764604091644, 0....  OLMo-1B-hf   \n",
       "3  [[0.04795553907752037, 0.02789744734764099, 0....  OLMo-1B-hf   \n",
       "4  [[0.025843387469649315, 0.02116398699581623, 0...  OLMo-1B-hf   \n",
       "\n",
       "           file_name  \n",
       "0  olmo.1b.fp32.json  \n",
       "1  olmo.1b.fp32.json  \n",
       "2  olmo.1b.fp32.json  \n",
       "3  olmo.1b.fp32.json  \n",
       "4  olmo.1b.fp32.json  \n",
       "\n",
       "[5 rows x 22 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_id\n",
      "OLMo-1B-hf                20\n",
      "OLMo-1B-8bit              20\n",
      "OLMo-1B-4bit              20\n",
      "OLMo-1B-1.58bit           20\n",
      "DeepHermes-LLaMA-3B       20\n",
      "DeepHermes-3B-8bit        20\n",
      "DeepHermes-3B-4bit        20\n",
      "DeepHermes-3B-1.58bit     20\n",
      "LLaMA-Instruct-8B         20\n",
      "LLaMA-Instruct-8B-8bit    20\n",
      "LLaMA-Instruct-8B-4bit    20\n",
      "HF1BitLLM                 20\n",
      "Name: count, dtype: int64\n",
      "Index(['prompt', 'decoded_prompt_str', 'tokens', 'prompt_type', 'target_ids',\n",
      "       'target_tokens', 'layer_names', 'correct_1', 'correct_topk',\n",
      "       'correct_1_std', 'correct_topk_std', 'correct_1_by_position',\n",
      "       'correct_topk_by_position', 'entropy', 'normalized_entropy',\n",
      "       'logit_mean', 'prob_mean', 'stability_top1', 'stability_topk',\n",
      "       'layer_kl_divergences', 'model_id', 'file_name'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(df_all['model_id'].value_counts())\n",
    "print(df_all.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_singleton_lists(df, columns):\n",
    "    \"\"\"\n",
    "    Replace singleton lists (e.g., [0.5]) with scalar values in specified columns.\n",
    "    \"\"\"\n",
    "    for col in columns:\n",
    "        df[col] = df[col].apply(lambda x: x[0] if isinstance(x, list) and len(x) == 1 else x)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "singleton_cols = ['entropy', 'normalized_entropy', 'logit_mean', 'prob_mean',\n",
    "                  'stability_top1', 'stability_topk']  # Add others if needed\n",
    "\n",
    "df_all = flatten_singleton_lists(df_all, singleton_cols)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "singleton_cols = ['prompt', 'decoded_prompt_str', 'tokens', 'prompt_type', 'target_ids',\n",
    "       'target_tokens', 'layer_names', 'correct_1', 'correct_topk',\n",
    "       'correct_1_std', 'correct_topk_std', 'correct_1_by_position',\n",
    "       'correct_topk_by_position', 'entropy', 'normalized_entropy',\n",
    "       'logit_mean', 'prob_mean', 'stability_top1', 'stability_topk',\n",
    "       'layer_kl_divergences', 'model_id', 'file_name']  # Add others if needed\n",
    "\n",
    "df_all = flatten_singleton_lists(df_all, singleton_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all['kl_layer_last'] = df_all['layer_kl_divergences'].apply(lambda x: x[-1] if isinstance(x, list) else np.nan)\n",
    "df_all['kl_layer_mean'] = df_all['layer_kl_divergences'].apply(lambda x: np.mean(x) if isinstance(x, list) else np.nan)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all['logit_std'] = df_all['logit_mean'].apply(np.std)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Language and Cosine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "def compute_cosine_similarity_per_position(probs1, probs2):\n",
    "    return [1 - cosine(p1, p2) if not (np.all(p1 == 0) or np.all(p2 == 0)) else 0.0\n",
    "            for p1, p2 in zip(probs1, probs2)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langdetect import detect, DetectorFactory\n",
    "DetectorFactory.seed = 42  # ensure consistent results\n",
    "\n",
    "#df_all['language'] = df_all['decoded_prompt_str'].apply(lambda x: detect(x) if isinstance(x, str) else 'unknown')\n",
    "#df_all['language'] = df_all['tokens'].apply(lambda x: detect(x) if isinstance(x, str) else 'unknown')\n",
    "df_all['language'] = df_all['target_tokens'].apply(lambda x: detect(x) if isinstance(x, str) else 'unknown')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for w in df_all['language']:\n",
    "    if w != 'en':\n",
    "        print(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_config = [\n",
    "    #(\"prob_mean\", \"e) Mean Prob of TopK\", \"Mean Prob.\"),\n",
    "    (\"logit_mean\", \"a) Mean Logit Magnitude\", \"Mean Logit\"),\n",
    "    (\"normalized_entropy\", \"b) Normalized Entropy per Layer\", \"Norm. Entropy\"),\n",
    "    (\"layer_kl_divergences\", \"c) KL Divergence per Layer\", \"KL Divergence\"),\n",
    "    (\"correct_1\", \"d) Correct TopK-1\", \"Correct\"),\n",
    "    (\"correct_1_std\", \"e) Correct TopK-1 Std.\", \"Std.\"),\n",
    "    (\"stability_top1\", \"f) TopK-1 Stability\", \"Stability\"),\n",
    "    (\"correct_topk\", \"g) Correct TopK-5\", \"Correct\"),\n",
    "    (\"correct_topk_std\", \"h) Correct TopK-5 Std.\", \"Std.\"),\n",
    "    (\"stability_topk\", \"i) TopK-5 Stability\", \"Stability\"),\n",
    "]\n",
    "\n",
    "#plot_model_diagnostics_variable_depth(df_all, metrics_config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract final-layer KL for overview\n",
    "df_all['kl_final_layer'] = df_all['layer_kl_divergences'].apply(lambda x: x[-1] if isinstance(x, list) else np.nan)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## With Font Controls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "# Set seaborn style\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "def flatten_layerwise_metric(row):\n",
    "    \"\"\"Flattens overly nested [[...]] or [[[...]]] metrics.\"\"\"\n",
    "    while isinstance(row, list) and len(row) == 1 and isinstance(row[0], list):\n",
    "        row = row[0]\n",
    "    return row\n",
    "\n",
    "def average_nested_per_layer(data):\n",
    "    \"\"\"Convert list-of-lists (per layer) to averaged list per layer.\"\"\"\n",
    "    return [np.mean(layer) if isinstance(layer, list) and layer else np.nan for layer in data]\n",
    "\n",
    "def plot_model_diagnostics_variable_depth(\n",
    "    df,\n",
    "    metrics_config,\n",
    "    title=\"TopK-5 Quantized Model Diagnostic Landscape\",\n",
    "    max_layer_len=32,\n",
    "    font_sizes=None\n",
    "):\n",
    "    # Default font sizes if none provided\n",
    "    if font_sizes is None:\n",
    "        font_sizes = {\n",
    "            \"title\": 24,\n",
    "            \"subtitle\": 18,\n",
    "            \"xlabel\": 16,\n",
    "            \"ylabel\": 16,\n",
    "            \"xtick\": 14,\n",
    "            \"ytick\": 14,\n",
    "            \"legend\": 16\n",
    "        }\n",
    "\n",
    "    num_plots = len(metrics_config)\n",
    "    num_rows = (num_plots + 2) // 3\n",
    "\n",
    "    # Use Times New Roman and set figure size\n",
    "    plt.rcParams['font.family'] = 'Times New Roman'\n",
    "    fig, axs = plt.subplots(num_rows, 3, figsize=(22, 5 * num_rows), sharex=False)\n",
    "    axs = axs.flatten()\n",
    "\n",
    "    model_ids = df['model_id'].unique()\n",
    "    model_colors = {\n",
    "        model: color for model, color in zip(model_ids, sns.color_palette(\"tab20\", len(model_ids)))\n",
    "    }\n",
    "\n",
    "    double_nested_metrics = {\"layer_kl_divergences\", \"normalized_entropy\"}\n",
    "\n",
    "    for i, (metric_key, subplot_title, y_label) in enumerate(metrics_config):\n",
    "        ax = axs[i]\n",
    "\n",
    "        for model_id, group in df.groupby(\"model_id\"):\n",
    "            layerwise_aggregates = []\n",
    "\n",
    "            for row in group[metric_key].dropna():\n",
    "                row = flatten_layerwise_metric(row)\n",
    "\n",
    "                if metric_key in double_nested_metrics:\n",
    "                    if not isinstance(row, list) or not all(isinstance(r, list) for r in row):\n",
    "                        continue\n",
    "                    averaged = average_nested_per_layer(row)\n",
    "                else:\n",
    "                    try:\n",
    "                        averaged = np.asarray(row, dtype=np.float32).flatten().tolist()\n",
    "                    except Exception:\n",
    "                        continue\n",
    "\n",
    "                if not averaged or np.any(np.isnan(averaged)) or np.any(np.isinf(averaged)):\n",
    "                    continue\n",
    "\n",
    "                if len(averaged) > max_layer_len:\n",
    "                    averaged = averaged[:max_layer_len]\n",
    "\n",
    "                layerwise_aggregates.append(averaged)\n",
    "\n",
    "            if not layerwise_aggregates:\n",
    "                continue\n",
    "\n",
    "            max_len = max(len(v) for v in layerwise_aggregates)\n",
    "            per_layer_values = [[] for _ in range(max_len)]\n",
    "\n",
    "            for v in layerwise_aggregates:\n",
    "                for j, val in enumerate(v):\n",
    "                    per_layer_values[j].append(val)\n",
    "\n",
    "            means = np.array([np.mean(l) if l else np.nan for l in per_layer_values])\n",
    "            stds = np.array([np.std(l) if l else np.nan for l in per_layer_values])\n",
    "            x = np.arange(len(means))\n",
    "\n",
    "            ax.plot(x, means, label=model_id, color=model_colors[model_id])\n",
    "            ax.fill_between(x, means - stds, means + stds, color=model_colors[model_id], alpha=0.2)\n",
    "\n",
    "            last_valid = np.where(~np.isnan(means))[0][-1]\n",
    "            ax.plot(x[last_valid], means[last_valid], 'o', color=model_colors[model_id], markersize=8)\n",
    "\n",
    "        ax.set_title(subplot_title, fontsize=font_sizes[\"subtitle\"])\n",
    "        ax.set_xlabel(\"Layer Index\", fontsize=font_sizes[\"xlabel\"])\n",
    "        ax.set_ylabel(y_label, fontsize=font_sizes[\"ylabel\"])\n",
    "        ax.tick_params(axis='x', labelsize=font_sizes[\"xtick\"])\n",
    "        ax.tick_params(axis='y', labelsize=font_sizes[\"ytick\"])\n",
    "        ax.grid(True)\n",
    "\n",
    "    # Remove unused axes\n",
    "    for j in range(len(metrics_config), len(axs)):\n",
    "        fig.delaxes(axs[j])\n",
    "\n",
    "    # Shared legend and figure title\n",
    "    handles, labels = axs[0].get_legend_handles_labels()\n",
    "    fig.legend(handles, labels, loc='upper center', ncol=4, fontsize=font_sizes[\"legend\"],\n",
    "               frameon=False, bbox_to_anchor=(0.5, 1.05))\n",
    "    #fig.suptitle(title, fontsize=font_sizes[\"title\"])\n",
    "\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.93])  # adjust to leave space for legend and title\n",
    "    fig.savefig('nq_answers_lineplots.jpg', bbox_inches='tight', dpi=300)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 2200x1500 with 9 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_model_diagnostics_variable_depth(\n",
    "    df_all,\n",
    "    metrics_config,\n",
    "    font_sizes={\n",
    "        \"title\": 36,\n",
    "        \"subtitle\": 28,\n",
    "        \"xlabel\": 24,\n",
    "        \"ylabel\": 24,\n",
    "        \"xtick\": 20,\n",
    "        \"ytick\": 20,\n",
    "        \"legend\": 26\n",
    "    }\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "def compute_layerwise_metric_correlations(df, metrics_to_compare, max_layer_len=32):\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "\n",
    "    metric_data = {metric: [] for metric in metrics_to_compare}\n",
    "\n",
    "    for metric in metrics_to_compare:\n",
    "        for _, row in df.iterrows():\n",
    "            data = row[metric]\n",
    "            if data is None or (isinstance(data, float) and np.isnan(data)):\n",
    "                continue\n",
    "\n",
    "            data = flatten_layerwise_metric(data)\n",
    "\n",
    "            if isinstance(data, list) and all(isinstance(d, list) for d in data):\n",
    "                data = average_nested_per_layer(data)\n",
    "            try:\n",
    "                flattened = np.asarray(data, dtype=np.float32).flatten()\n",
    "                if len(flattened) < max_layer_len:\n",
    "                    padded = np.full((max_layer_len,), np.nan, dtype=np.float32)\n",
    "                    padded[:len(flattened)] = flattened\n",
    "                    flattened = padded\n",
    "                elif len(flattened) > max_layer_len:\n",
    "                    flattened = flattened[:max_layer_len]\n",
    "\n",
    "            except Exception:\n",
    "                continue\n",
    "\n",
    "            if len(flattened) > max_layer_len:\n",
    "                flattened = flattened[:max_layer_len]\n",
    "\n",
    "            if not len(flattened) or np.any(np.isnan(flattened)) or np.any(np.isinf(flattened)):\n",
    "                continue\n",
    "\n",
    "            metric_data[metric].append(flattened)\n",
    "\n",
    "    # Align all metrics by truncating/padding each run\n",
    "    min_samples = min(len(v) for v in metric_data.values())\n",
    "    for key in metric_data:\n",
    "        metric_data[key] = metric_data[key][:min_samples]\n",
    "\n",
    "    # Convert to matrix: shape = (num_samples, num_metrics, num_layers)\n",
    "    all_metrics = []\n",
    "    for key in metrics_to_compare:\n",
    "        all_metrics.append(np.stack(metric_data[key]))  # shape: (samples, layers)\n",
    "    all_metrics = np.stack(all_metrics, axis=1)  # shape: (samples, metrics, layers)\n",
    "\n",
    "    # Compute correlation per-layer and average across layers\n",
    "    correlations = np.full((len(metrics_to_compare), len(metrics_to_compare)), np.nan)\n",
    "    for i in range(len(metrics_to_compare)):\n",
    "        for j in range(len(metrics_to_compare)):\n",
    "            values_i = all_metrics[:, i, :].flatten()\n",
    "            values_j = all_metrics[:, j, :].flatten()\n",
    "            if np.std(values_i) > 0 and np.std(values_j) > 0:\n",
    "                correlations[i, j] = np.corrcoef(values_i, values_j)[0, 1]\n",
    "\n",
    "    return pd.DataFrame(\n",
    "        correlations,\n",
    "        index=metrics_to_compare,\n",
    "        columns=metrics_to_compare\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_config = [\n",
    "    #(\"prob_mean\", \"e) Mean Prob of TopK\", \"Mean Prob.\"),\n",
    "    (\"logit_mean\", \"a) Mean Logit Magnitude\", \"Mean Logit\"),\n",
    "    (\"normalized_entropy\", \"b) Normalized Entropy per Layer\", \"Norm. Entropy\"),\n",
    "    (\"layer_kl_divergences\", \"c) KL Divergence per Layer\", \"KL Divergence\"),\n",
    "    (\"correct_1\", \"d) Correct TopK-1\", \"Correct\"),\n",
    "    (\"correct_1_std\", \"e) Correct TopK-1 Std.\", \"Std.\"),\n",
    "    (\"correct_topk\", \"g) Correct TopK-5\", \"Correct\"),\n",
    "    (\"correct_topk_std\", \"h) Correct TopK-5 Std.\", \"Std.\"),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sns.set(style=\"whitegrid\")\n",
    "\n",
    "def plot_correlation_heatmap(corr_matrix, title=\"Metric Correlation Across Layers\"):\n",
    "    plt.rcParams['font.family'] = 'Times New Roman'\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    sns.heatmap(corr_matrix, annot=True, fmt=\".2f\", cmap=\"coolwarm\", square=True,\n",
    "                cbar_kws={\"shrink\": 0.8}, linewidths=0.5, linecolor=\"gray\")\n",
    "    plt.title(title, fontsize=20)\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.yticks(rotation=0)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_keys = [m[0] for m in corr_config]\n",
    "\n",
    "corr_matrix = compute_layerwise_metric_correlations(df_all, metrics_keys)\n",
    "plot_correlation_heatmap(corr_matrix)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datasets for calibrating activations and analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_inputs = [\n",
    "    # Language understanding\n",
    "    \"The quick brown fox jumps over the lazy dog.\",\n",
    "    \"Despite the rain, the event continued as planned.\",\n",
    "    \n",
    "    # Logic/reasoning\n",
    "    \"If all humans are mortal and Socrates is a human, then Socrates is mortal.\",\n",
    "    \"Either the lights are off or the power is out. The lights are on, so the power must be out.\",\n",
    "\n",
    "    # Math/numerical\n",
    "    \"The derivative of sin(x) with respect to x is cos(x).\",\n",
    "    \"What is the sum of the first 100 natural numbers?\",\n",
    "\n",
    "    # Programming\n",
    "    \"In Python, list comprehensions provide a concise way to create lists.\",\n",
    "    \"To define a function in JavaScript, use the 'function' keyword.\",\n",
    "\n",
    "    # Commonsense knowledge\n",
    "    \"You should refrigerate milk after opening it to keep it fresh.\",\n",
    "    \"People usually eat breakfast in the morning before starting their day.\",\n",
    "\n",
    "    # Scientific knowledge\n",
    "    \"Water boils at 100 degrees Celsius under standard atmospheric pressure.\",\n",
    "    \"Photosynthesis is the process by which plants convert sunlight into chemical energy.\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wikitext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = r'D:\\ThesisData\\wikitext'\n",
    "\n",
    "destination_path = str(Path(filepath))\n",
    "dataset = load_dataset(\n",
    "    'wikitext', 'wikitext-103-raw-v1',\n",
    "    split={\n",
    "        'train': 'train[:200]',\n",
    "    },\n",
    "    cache_dir=destination_path,\n",
    "    download_mode=DownloadMode.REUSE_DATASET_IF_EXISTS,\n",
    "    keep_in_memory=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_texts = dataset['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calibration_texts = [t for t in dataset['train'][\"text\"] if isinstance(t, str) and t.strip()]\n",
    "#calibration_texts = [t for t in sub_txts[\"text\"] if isinstance(t, str) and t.strip()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_txts = train_texts.take(200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Natural Questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = r'D:\\ThesisData\\nq'\n",
    "\n",
    "destination_path = str(Path(filepath))\n",
    "nq_dataset = load_dataset(\n",
    "    'sentence-transformers/natural-questions',\n",
    "    split={\n",
    "        'train': 'train[:20]'\n",
    "    },\n",
    "    cache_dir=destination_path,\n",
    "    download_mode=DownloadMode.REUSE_DATASET_IF_EXISTS,\n",
    "    keep_in_memory=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nq_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "nq_queries= nq_dataset['train']['query']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nq_queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "nq_answers = nq_dataset['train']['answer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nq_answers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GSM8K (Math)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = r'D:\\ThesisData\\gsm8k'\n",
    "\n",
    "destination_path = str(Path(filepath))\n",
    "gsm8k_dataset = load_dataset(\n",
    "    'gsm8k', 'main',\n",
    "    split={\n",
    "        'train': 'train[:20]'\n",
    "    },\n",
    "    cache_dir=destination_path,\n",
    "    download_mode=DownloadMode.REUSE_DATASET_IF_EXISTS,\n",
    "    keep_in_memory=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gsm8k_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "gsm8k_questions = gsm8k_dataset['train']['question']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "gsm8k_answers = gsm8k_dataset['train']['answer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gsm8k_answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gsm8k_questions_sae = \"\"\"\n",
    "Natalia sold clips to 48 of her friends in April, and then she sold half as many clips in May. How many clips did Natalia sell altogether in April and May?\n",
    "Weng earns $12 an hour for babysitting. Yesterday, she just did 50 minutes of babysitting. How much did she earn?\n",
    "Betty is saving money for a new wallet which costs $100. Betty has only half of the money she needs. Her parents decided to give her $15 for that purpose, and her grandparents twice as much as her parents. How much more money does Betty need to buy the wallet?\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models and Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_test_model(model_path:str, dtype=torch.dtype) -> AutoModelForCausalLM:\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_path,\n",
    "        return_dict=True,\n",
    "        output_hidden_states=True,\n",
    "        torch_dtype=dtype,\n",
    "        low_cpu_mem_usage=True,\n",
    "        local_files_only=True,\n",
    "        use_safetensors=True,\n",
    "        #trust_remote_code=True\n",
    "    )\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "hfbit1_tokenizer = AutoTokenizer.from_pretrained(FPKey.HFBIT1_TOKENIZER.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hfbit1_tokenizer.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You have loaded a BitNet model on CPU and have a CUDA device available, make sure to set your model on a GPU device in order to run your model.\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  3.19it/s]\n"
     ]
    }
   ],
   "source": [
    "hfbit1_fp32 = load_test_model(FPKey.HFBIT1_8B.value, dtype=torch.float32) # https://huggingface.co/HF1BitLLM/Llama3-8B-1.58-100B-tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access the input embedding layer\n",
    "embed_weight = hfbit1_fp32.model.embed_tokens.weight\n",
    "\n",
    "# Access the output projection layer\n",
    "output_weight = hfbit1_fp32.lm_head.weight\n",
    "\n",
    "# Check if they are tied\n",
    "print(embed_weight.data_ptr() == output_weight.data_ptr())  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hfbit1_fp32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print model layers to inspect their names\n",
    "for name, module in hfbit1_fp32.named_modules():\n",
    "    print(f\"Layer name: {name}, Module: {module}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "llama8b_tokenizer = AutoTokenizer.from_pretrained(FPKey.LINSTRUCT_TOKENIZER.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llama8b_tokenizer.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 7/7 [00:01<00:00,  4.08it/s]\n"
     ]
    }
   ],
   "source": [
    "llama8b_fp32 = load_test_model(FPKey.LINSTRUCT_8B.value, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access the input embedding layer\n",
    "embed_weight = llama8b_fp32.model.embed_tokens.weight\n",
    "\n",
    "# Access the output projection layer\n",
    "output_weight = llama8b_fp32.lm_head.weight\n",
    "\n",
    "# Check if they are tied\n",
    "print(embed_weight.data_ptr() == output_weight.data_ptr())  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_embedding_tie_info(model):\n",
    "    try:\n",
    "        embed = model.model.embed_tokens\n",
    "    except AttributeError:\n",
    "        embed = model.embed_tokens\n",
    "    lm_head = model.lm_head\n",
    "    print(f\"Embed shape: {embed.weight.shape}\")\n",
    "    print(f\"LM head shape: {lm_head.weight.shape}\")\n",
    "    print(\"Tied:\", embed.weight.data_ptr() == lm_head.weight.data_ptr())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_embedding_tie_info(llama8b_fp32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llama8b_fp32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model with 8-bit quantization using BNB\n",
    "llama8b_bnb8_float32 = AutoModelForCausalLM.from_pretrained(\n",
    "    ModelKey.LLINSTRUCT8B.value,           \n",
    "    #torch_dtype=torch.float32,     # Specify the dtype (for 8-bit, use uint8)\n",
    "    #device_map=\"cpu\",           # Automatically map the model to available devices (GPU/CPU)\n",
    "    load_in_8bit=True,\n",
    "    return_dict=True,\n",
    "    output_hidden_states=True,            # Set to True for 8-bit quantization\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llama8b_bnb8_float32.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model with 4-bit quantization using BNB\n",
    "llama8b_bnb4_float32 = AutoModelForCausalLM.from_pretrained(\n",
    "    ModelKey.LLINSTRUCT8B.value,           \n",
    "    #torch_dtype=torch.float32,     # This would still use uint8 or another type based on quantization method\n",
    "    #device_map=\"auto\",           # Automatically map the model to available devices (GPU/CPU)\n",
    "    load_in_4bit=True,            # Set to True for 4-bit quantization (if available)\n",
    "    return_dict=True,\n",
    "    output_hidden_states=True, \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llama8b_ptsq_float32 = applyPTQ(\n",
    "    load_test_model(ModelKey.LLINSTRUCT8B.value, dtype=torch.float32),\n",
    "    tokenizer=llama8b_tokenizer,\n",
    "    #calibration_input=None,\n",
    "    #calibration_input=sub_txts['text'],\n",
    "    calibration_input=Texts.T1.value,\n",
    "    mode='1.58bit',\n",
    "    safer_quant=True,\n",
    "    q_lmhead=True,\n",
    "    model_half=False,\n",
    "    quant_half=False,\n",
    "    layers_to_quant_weights=QuantStyle.BITNET.value,\n",
    "    layers_to_quant_activations=QuantStyle.BITNET.value,\n",
    "    fragile_layers=False,\n",
    "    act_quant=True,\n",
    "    act_bits=8,\n",
    "    torch_backends=False,\n",
    "    debugging=True,\n",
    "    plot_debugging=False,\n",
    "    plot_quantization=False,\n",
    "    freeze_modules=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hfbit1_tokenizer.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llama8b_tokenizer.vocab_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### allenai/OLMo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "olmo1b_tokenizer = AutoTokenizer.from_pretrained(FPKey.OLMO1B_TOKENIZER.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "olmo1b_tokenizer.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "olmo2t_tokenizer = AutoTokenizer.from_pretrained(FPKey.OLMO7B2T_TOKENIZER.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "olmo2t_tokenizer.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "olmo1b_fp32 = load_test_model(FPKey.OLMO1B_FP.value, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.functional as F\n",
    "# Check self-tying of embeddings for training-time hints\n",
    "embedding_weight = olmo1b_fp32.model.embed_tokens\n",
    "first_hidden = olmo1b_fp32.model(...)[0][:, 0, :]         # or use a dummy forward pass\n",
    "cos_sim = torch.nn.functional.cosine_similarity(\n",
    "    first_hidden @ embedding_weight.T,  # approximates logits\n",
    "    embedding_weight, dim=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_embedding_tie_info(olmo1b_fp32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diagnostic to check weight tying at inference\n",
    "print(\"Inference-time tying:\", olmo1b_fp32.model.embed_tokens.weight is olmo1b_fp32.model.lm_head.weight)\n",
    "\n",
    "# Diagnostic for training-time behavior: check cosine similarity\n",
    "cos_sim = F.cosine_similarity(\n",
    "    olmo1b_fp32.model.embed_tokens.weight, model.lm_head.weight, dim=1\n",
    ")\n",
    "print(\"Cosine similarity (mean):\", cos_sim.mean().item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access the input embedding layer\n",
    "embed_weight = llama8b_fp32.model.embed_tokens.weight\n",
    "\n",
    "# Access the output projection layer\n",
    "output_weight = llama8b_fp32.lm_head.weight\n",
    "\n",
    "# Check if they are tied\n",
    "print(embed_weight.data_ptr() == output_weight.data_ptr())  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "olmo1b_fp32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "olmo2t_fp32 = load_test_model(FPKey.OLMO7B2T_FP.value, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "olmo2t_fp32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def are_embeddings_tied(model):\n",
    "    # Access correctly for various subclasses\n",
    "    try:\n",
    "        embed = model.model.embed_tokens\n",
    "    except AttributeError:\n",
    "        embed = model.embed_tokens\n",
    "\n",
    "    try:\n",
    "        lm_head = model.lm_head\n",
    "    except AttributeError:\n",
    "        raise ValueError(\"Model has no `lm_head`.\")\n",
    "\n",
    "    return embed.weight.data_ptr() == lm_head.weight.data_ptr()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "are_embeddings_tied(olmo1b_fp32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "olmo1b_bitnet_fp32_ptsq_safety = applyPTQ(\n",
    "    load_test_model(FPKey.OLMO1B_FP.value, dtype=torch.float32),\n",
    "    tokenizer=olmo1b_tokenizer,\n",
    "    #calibration_input=None,\n",
    "    #calibration_input=sub_txts['text'],\n",
    "    calibration_input=Texts.T1.value,\n",
    "    mode='1.58bit',\n",
    "    safer_quant=True,\n",
    "    model_half=False,\n",
    "    quant_half=False,\n",
    "    layers_to_quant_weights=QuantStyle.BITNET.value,\n",
    "    layers_to_quant_activations=QuantStyle.BITNET.value,\n",
    "    fragile_layers=False,\n",
    "    act_quant=True,\n",
    "    act_bits=8,\n",
    "    torch_backends=True,\n",
    "    debugging=True,\n",
    "    plot_debugging=False,\n",
    "    plot_quantization=False,\n",
    "    freeze_modules=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|| Quant Configs: 1.58bit | BitNet-style PTQ as: PTSQ ||\n",
      ">> [STEP 1] Wrapping Linear layers (no weight quant yet)...\n",
      "[SKIP] lm_head | Policy: Skip weight quantization!\n",
      ">> [STEP 2] Running activation calibration forward pass...\n",
      "[model.layers.0.self_attn.q_proj] input.mean(): 8.87666828930378e-10, input.std(): 0.997725784778595\n",
      "[model.layers.0.self_attn.k_proj] input.mean(): 8.87666828930378e-10, input.std(): 0.997725784778595\n",
      "[model.layers.0.self_attn.v_proj] input.mean(): 8.87666828930378e-10, input.std(): 0.997725784778595\n",
      "[model.layers.0.self_attn.o_proj] input.mean(): -0.0011488111922517419, input.std(): 0.14034488797187805\n",
      "[model.layers.0.mlp.gate_proj] input.mean(): 7.639755494892597e-10, input.std(): 0.9984983801841736\n",
      "[model.layers.0.mlp.up_proj] input.mean(): 7.639755494892597e-10, input.std(): 0.9984983801841736\n",
      "[model.layers.0.mlp.down_proj] input.mean(): 2.7965479603153653e-05, input.std(): 0.062258560210466385\n",
      "[model.layers.1.self_attn.q_proj] input.mean(): -1.5861587598919868e-09, input.std(): 0.9986528158187866\n",
      "[model.layers.1.self_attn.k_proj] input.mean(): -1.5861587598919868e-09, input.std(): 0.9986528158187866\n",
      "[model.layers.1.self_attn.v_proj] input.mean(): -1.5861587598919868e-09, input.std(): 0.9986528158187866\n",
      "[model.layers.1.self_attn.o_proj] input.mean(): 0.0022282390855252743, input.std(): 0.10513869673013687\n",
      "[model.layers.1.mlp.gate_proj] input.mean(): -6.111804395914078e-10, input.std(): 0.998729944229126\n",
      "[model.layers.1.mlp.up_proj] input.mean(): -6.111804395914078e-10, input.std(): 0.998729944229126\n",
      "[model.layers.1.mlp.down_proj] input.mean(): 0.00015827080642338842, input.std(): 0.16190233826637268\n",
      "[model.layers.2.self_attn.q_proj] input.mean(): 3.346940502524376e-10, input.std(): 0.9988783001899719\n",
      "[model.layers.2.self_attn.k_proj] input.mean(): 3.346940502524376e-10, input.std(): 0.9988783001899719\n",
      "[model.layers.2.self_attn.v_proj] input.mean(): 3.346940502524376e-10, input.std(): 0.9988783001899719\n",
      "[model.layers.2.self_attn.o_proj] input.mean(): 0.0002892923657782376, input.std(): 0.07233059406280518\n",
      "[model.layers.2.mlp.gate_proj] input.mean(): -1.7462298274040222e-10, input.std(): 0.998977541923523\n",
      "[model.layers.2.mlp.up_proj] input.mean(): -1.7462298274040222e-10, input.std(): 0.998977541923523\n",
      "[model.layers.2.mlp.down_proj] input.mean(): 0.00019596150377765298, input.std(): 0.08723480254411697\n",
      "[model.layers.3.self_attn.q_proj] input.mean(): 1.2878444977104664e-09, input.std(): 0.9989968538284302\n",
      "[model.layers.3.self_attn.k_proj] input.mean(): 1.2878444977104664e-09, input.std(): 0.9989968538284302\n",
      "[model.layers.3.self_attn.v_proj] input.mean(): 1.2878444977104664e-09, input.std(): 0.9989968538284302\n",
      "[model.layers.3.self_attn.o_proj] input.mean(): -0.002831411547958851, input.std(): 0.06374410539865494\n",
      "[model.layers.3.mlp.gate_proj] input.mean(): 2.4010660126805305e-10, input.std(): 0.9990004897117615\n",
      "[model.layers.3.mlp.up_proj] input.mean(): 2.4010660126805305e-10, input.std(): 0.9990004897117615\n",
      "[model.layers.3.mlp.down_proj] input.mean(): 0.00021323279361240566, input.std(): 0.050574783235788345\n",
      "[model.layers.4.self_attn.q_proj] input.mean(): -6.693881005048752e-10, input.std(): 0.999005913734436\n",
      "[model.layers.4.self_attn.k_proj] input.mean(): -6.693881005048752e-10, input.std(): 0.999005913734436\n",
      "[model.layers.4.self_attn.v_proj] input.mean(): -6.693881005048752e-10, input.std(): 0.999005913734436\n",
      "[model.layers.4.self_attn.o_proj] input.mean(): 0.00025702675338834524, input.std(): 0.062454983592033386\n",
      "[model.layers.4.mlp.gate_proj] input.mean(): -4.94765117764473e-10, input.std(): 0.9990076422691345\n",
      "[model.layers.4.mlp.up_proj] input.mean(): -4.94765117764473e-10, input.std(): 0.9990076422691345\n",
      "[model.layers.4.mlp.down_proj] input.mean(): -0.0001371003600070253, input.std(): 0.05195312574505806\n",
      "[model.layers.5.self_attn.q_proj] input.mean(): 1.0477378964424133e-09, input.std(): 0.9989944100379944\n",
      "[model.layers.5.self_attn.k_proj] input.mean(): 1.0477378964424133e-09, input.std(): 0.9989944100379944\n",
      "[model.layers.5.self_attn.v_proj] input.mean(): 1.0477378964424133e-09, input.std(): 0.9989944100379944\n",
      "[model.layers.5.self_attn.o_proj] input.mean(): 0.0013972552260383964, input.std(): 0.06405643373727798\n",
      "[model.layers.5.mlp.gate_proj] input.mean(): 1.0622898116707802e-09, input.std(): 0.998995304107666\n",
      "[model.layers.5.mlp.up_proj] input.mean(): 1.0622898116707802e-09, input.std(): 0.998995304107666\n",
      "[model.layers.5.mlp.down_proj] input.mean(): -0.0003018259012605995, input.std(): 0.05673985555768013\n",
      "[model.layers.6.self_attn.q_proj] input.mean(): 1.0477378964424133e-09, input.std(): 0.9990253448486328\n",
      "[model.layers.6.self_attn.k_proj] input.mean(): 1.0477378964424133e-09, input.std(): 0.9990253448486328\n",
      "[model.layers.6.self_attn.v_proj] input.mean(): 1.0477378964424133e-09, input.std(): 0.9990253448486328\n",
      "[model.layers.6.self_attn.o_proj] input.mean(): -0.0007478609913960099, input.std(): 0.08205685764551163\n",
      "[model.layers.6.mlp.gate_proj] input.mean(): 6.257323548197746e-10, input.std(): 0.9990831017494202\n",
      "[model.layers.6.mlp.up_proj] input.mean(): 6.257323548197746e-10, input.std(): 0.9990831017494202\n",
      "[model.layers.6.mlp.down_proj] input.mean(): 0.0002366862609051168, input.std(): 0.1736021786928177\n",
      "[model.layers.7.self_attn.q_proj] input.mean(): -5.748006515204906e-10, input.std(): 0.9990827441215515\n",
      "[model.layers.7.self_attn.k_proj] input.mean(): -5.748006515204906e-10, input.std(): 0.9990827441215515\n",
      "[model.layers.7.self_attn.v_proj] input.mean(): -5.748006515204906e-10, input.std(): 0.9990827441215515\n",
      "[model.layers.7.self_attn.o_proj] input.mean(): -0.0010269536869600415, input.std(): 0.079875648021698\n",
      "[model.layers.7.mlp.gate_proj] input.mean(): 6.111804395914078e-10, input.std(): 0.9991617798805237\n",
      "[model.layers.7.mlp.up_proj] input.mean(): 6.111804395914078e-10, input.std(): 0.9991617798805237\n",
      "[model.layers.7.mlp.down_proj] input.mean(): 8.65115798660554e-05, input.std(): 0.056493695825338364\n",
      "[model.layers.8.self_attn.q_proj] input.mean(): 1.6589183360338211e-09, input.std(): 0.9991868138313293\n",
      "[model.layers.8.self_attn.k_proj] input.mean(): 1.6589183360338211e-09, input.std(): 0.9991868138313293\n",
      "[model.layers.8.self_attn.v_proj] input.mean(): 1.6589183360338211e-09, input.std(): 0.9991868138313293\n",
      "[model.layers.8.self_attn.o_proj] input.mean(): -0.0009135137079283595, input.std(): 0.06495142728090286\n",
      "[model.layers.8.mlp.gate_proj] input.mean(): 3.637978807091713e-10, input.std(): 0.9992341995239258\n",
      "[model.layers.8.mlp.up_proj] input.mean(): 3.637978807091713e-10, input.std(): 0.9992341995239258\n",
      "[model.layers.8.mlp.down_proj] input.mean(): 0.00018601640476845205, input.std(): 0.17690427601337433\n",
      "[model.layers.9.self_attn.q_proj] input.mean(): -1.3242242857813835e-09, input.std(): 0.9992427229881287\n",
      "[model.layers.9.self_attn.k_proj] input.mean(): -1.3242242857813835e-09, input.std(): 0.9992427229881287\n",
      "[model.layers.9.self_attn.v_proj] input.mean(): -1.3242242857813835e-09, input.std(): 0.9992427229881287\n",
      "[model.layers.9.self_attn.o_proj] input.mean(): -0.0003377934917807579, input.std(): 0.20922324061393738\n",
      "[model.layers.9.mlp.gate_proj] input.mean(): -1.6007106751203537e-10, input.std(): 0.9994356036186218\n",
      "[model.layers.9.mlp.up_proj] input.mean(): -1.6007106751203537e-10, input.std(): 0.9994356036186218\n",
      "[model.layers.9.mlp.down_proj] input.mean(): -0.0003582020872272551, input.std(): 0.21292388439178467\n",
      "[model.layers.10.self_attn.q_proj] input.mean(): -2.0372681319713593e-10, input.std(): 0.9994268417358398\n",
      "[model.layers.10.self_attn.k_proj] input.mean(): -2.0372681319713593e-10, input.std(): 0.9994268417358398\n",
      "[model.layers.10.self_attn.v_proj] input.mean(): -2.0372681319713593e-10, input.std(): 0.9994268417358398\n",
      "[model.layers.10.self_attn.o_proj] input.mean(): 0.0008150784415192902, input.std(): 0.12055893242359161\n",
      "[model.layers.10.mlp.gate_proj] input.mean(): 3.7834979593753815e-10, input.std(): 0.9994478225708008\n",
      "[model.layers.10.mlp.up_proj] input.mean(): 3.7834979593753815e-10, input.std(): 0.9994478225708008\n",
      "[model.layers.10.mlp.down_proj] input.mean(): 0.000966481224168092, input.std(): 0.16079097986221313\n",
      "[model.layers.11.self_attn.q_proj] input.mean(): -3.2014213502407074e-10, input.std(): 0.9995402097702026\n",
      "[model.layers.11.self_attn.k_proj] input.mean(): -3.2014213502407074e-10, input.std(): 0.9995402097702026\n",
      "[model.layers.11.self_attn.v_proj] input.mean(): -3.2014213502407074e-10, input.std(): 0.9995402097702026\n",
      "[model.layers.11.self_attn.o_proj] input.mean(): 5.196294659981504e-05, input.std(): 0.10366171598434448\n",
      "[model.layers.11.mlp.gate_proj] input.mean(): 1.0913936421275139e-09, input.std(): 0.9995684623718262\n",
      "[model.layers.11.mlp.up_proj] input.mean(): 1.0913936421275139e-09, input.std(): 0.9995684623718262\n",
      "[model.layers.11.mlp.down_proj] input.mean(): -0.00024113710969686508, input.std(): 0.06567740440368652\n",
      "[model.layers.12.self_attn.q_proj] input.mean(): 1.3096723705530167e-10, input.std(): 0.9996072053909302\n",
      "[model.layers.12.self_attn.k_proj] input.mean(): 1.3096723705530167e-10, input.std(): 0.9996072053909302\n",
      "[model.layers.12.self_attn.v_proj] input.mean(): 1.3096723705530167e-10, input.std(): 0.9996072053909302\n",
      "[model.layers.12.self_attn.o_proj] input.mean(): -0.00040222806273959577, input.std(): 0.09510469436645508\n",
      "[model.layers.12.mlp.gate_proj] input.mean(): 1.1059455573558807e-09, input.std(): 0.999618649482727\n",
      "[model.layers.12.mlp.up_proj] input.mean(): 1.1059455573558807e-09, input.std(): 0.999618649482727\n",
      "[model.layers.12.mlp.down_proj] input.mean(): 0.00040862971218302846, input.std(): 0.077773816883564\n",
      "[model.layers.13.self_attn.q_proj] input.mean(): -6.83940015733242e-10, input.std(): 0.9996604323387146\n",
      "[model.layers.13.self_attn.k_proj] input.mean(): -6.83940015733242e-10, input.std(): 0.9996604323387146\n",
      "[model.layers.13.self_attn.v_proj] input.mean(): -6.83940015733242e-10, input.std(): 0.9996604323387146\n",
      "[model.layers.13.self_attn.o_proj] input.mean(): -0.001347450539469719, input.std(): 0.08987832069396973\n",
      "[model.layers.13.mlp.gate_proj] input.mean(): -2.3283064365386963e-10, input.std(): 0.9996665716171265\n",
      "[model.layers.13.mlp.up_proj] input.mean(): -2.3283064365386963e-10, input.std(): 0.9996665716171265\n",
      "[model.layers.13.mlp.down_proj] input.mean(): 0.00024800654500722885, input.std(): 0.09018371254205704\n",
      "[model.layers.14.self_attn.q_proj] input.mean(): -6.402842700481415e-10, input.std(): 0.9996910095214844\n",
      "[model.layers.14.self_attn.k_proj] input.mean(): -6.402842700481415e-10, input.std(): 0.9996910095214844\n",
      "[model.layers.14.self_attn.v_proj] input.mean(): -6.402842700481415e-10, input.std(): 0.9996910095214844\n",
      "[model.layers.14.self_attn.o_proj] input.mean(): 0.0020981580018997192, input.std(): 0.10136201232671738\n",
      "[model.layers.14.mlp.gate_proj] input.mean(): -2.0372681319713593e-10, input.std(): 0.9996914267539978\n",
      "[model.layers.14.mlp.up_proj] input.mean(): -2.0372681319713593e-10, input.std(): 0.9996914267539978\n",
      "[model.layers.14.mlp.down_proj] input.mean(): 0.0001642597489990294, input.std(): 0.1135561391711235\n",
      "[model.layers.15.self_attn.q_proj] input.mean(): -1.0477378964424133e-09, input.std(): 0.9997469186782837\n",
      "[model.layers.15.self_attn.k_proj] input.mean(): -1.0477378964424133e-09, input.std(): 0.9997469186782837\n",
      "[model.layers.15.self_attn.v_proj] input.mean(): -1.0477378964424133e-09, input.std(): 0.9997469186782837\n",
      "[model.layers.15.self_attn.o_proj] input.mean(): -0.0010582030517980456, input.std(): 0.16151627898216248\n",
      "[model.layers.15.mlp.gate_proj] input.mean(): 6.257323548197746e-10, input.std(): 0.9997705817222595\n",
      "[model.layers.15.mlp.up_proj] input.mean(): 6.257323548197746e-10, input.std(): 0.9997705817222595\n",
      "[model.layers.15.mlp.down_proj] input.mean(): 0.001395554980263114, input.std(): 0.3546150028705597\n",
      "[Calibrated] model.layers.0.self_attn.q_proj                    | act_scale: 1.000000\n",
      "[Calibrated] model.layers.0.self_attn.k_proj                    | act_scale: 1.000000\n",
      "[Calibrated] model.layers.0.self_attn.v_proj                    | act_scale: 1.000000\n",
      "[Calibrated] model.layers.0.self_attn.o_proj                    | act_scale: 1.000000\n",
      "[Calibrated] model.layers.0.mlp.gate_proj                       | act_scale: 1.000000\n",
      "[Calibrated] model.layers.0.mlp.up_proj                         | act_scale: 1.000000\n",
      "[Calibrated] model.layers.0.mlp.down_proj                       | act_scale: 1.000000\n",
      "[Calibrated] model.layers.1.self_attn.q_proj                    | act_scale: 1.000000\n",
      "[Calibrated] model.layers.1.self_attn.k_proj                    | act_scale: 1.000000\n",
      "[Calibrated] model.layers.1.self_attn.v_proj                    | act_scale: 1.000000\n",
      "[Calibrated] model.layers.1.self_attn.o_proj                    | act_scale: 1.000000\n",
      "[Calibrated] model.layers.1.mlp.gate_proj                       | act_scale: 1.000000\n",
      "[Calibrated] model.layers.1.mlp.up_proj                         | act_scale: 1.000000\n",
      "[Calibrated] model.layers.1.mlp.down_proj                       | act_scale: 1.000000\n",
      "[Calibrated] model.layers.2.self_attn.q_proj                    | act_scale: 1.000000\n",
      "[Calibrated] model.layers.2.self_attn.k_proj                    | act_scale: 1.000000\n",
      "[Calibrated] model.layers.2.self_attn.v_proj                    | act_scale: 1.000000\n",
      "[Calibrated] model.layers.2.self_attn.o_proj                    | act_scale: 1.000000\n",
      "[Calibrated] model.layers.2.mlp.gate_proj                       | act_scale: 1.000000\n",
      "[Calibrated] model.layers.2.mlp.up_proj                         | act_scale: 1.000000\n",
      "[Calibrated] model.layers.2.mlp.down_proj                       | act_scale: 1.000000\n",
      "[Calibrated] model.layers.3.self_attn.q_proj                    | act_scale: 1.000000\n",
      "[Calibrated] model.layers.3.self_attn.k_proj                    | act_scale: 1.000000\n",
      "[Calibrated] model.layers.3.self_attn.v_proj                    | act_scale: 1.000000\n",
      "[Calibrated] model.layers.3.self_attn.o_proj                    | act_scale: 1.000000\n",
      "[Calibrated] model.layers.3.mlp.gate_proj                       | act_scale: 1.000000\n",
      "[Calibrated] model.layers.3.mlp.up_proj                         | act_scale: 1.000000\n",
      "[Calibrated] model.layers.3.mlp.down_proj                       | act_scale: 1.000000\n",
      "[Calibrated] model.layers.4.self_attn.q_proj                    | act_scale: 1.000000\n",
      "[Calibrated] model.layers.4.self_attn.k_proj                    | act_scale: 1.000000\n",
      "[Calibrated] model.layers.4.self_attn.v_proj                    | act_scale: 1.000000\n",
      "[Calibrated] model.layers.4.self_attn.o_proj                    | act_scale: 1.000000\n",
      "[Calibrated] model.layers.4.mlp.gate_proj                       | act_scale: 1.000000\n",
      "[Calibrated] model.layers.4.mlp.up_proj                         | act_scale: 1.000000\n",
      "[Calibrated] model.layers.4.mlp.down_proj                       | act_scale: 1.000000\n",
      "[Calibrated] model.layers.5.self_attn.q_proj                    | act_scale: 1.000000\n",
      "[Calibrated] model.layers.5.self_attn.k_proj                    | act_scale: 1.000000\n",
      "[Calibrated] model.layers.5.self_attn.v_proj                    | act_scale: 1.000000\n",
      "[Calibrated] model.layers.5.self_attn.o_proj                    | act_scale: 1.000000\n",
      "[Calibrated] model.layers.5.mlp.gate_proj                       | act_scale: 1.000000\n",
      "[Calibrated] model.layers.5.mlp.up_proj                         | act_scale: 1.000000\n",
      "[Calibrated] model.layers.5.mlp.down_proj                       | act_scale: 1.000000\n",
      "[Calibrated] model.layers.6.self_attn.q_proj                    | act_scale: 1.000000\n",
      "[Calibrated] model.layers.6.self_attn.k_proj                    | act_scale: 1.000000\n",
      "[Calibrated] model.layers.6.self_attn.v_proj                    | act_scale: 1.000000\n",
      "[Calibrated] model.layers.6.self_attn.o_proj                    | act_scale: 1.000000\n",
      "[Calibrated] model.layers.6.mlp.gate_proj                       | act_scale: 1.000000\n",
      "[Calibrated] model.layers.6.mlp.up_proj                         | act_scale: 1.000000\n",
      "[Calibrated] model.layers.6.mlp.down_proj                       | act_scale: 1.000000\n",
      "[Calibrated] model.layers.7.self_attn.q_proj                    | act_scale: 1.000000\n",
      "[Calibrated] model.layers.7.self_attn.k_proj                    | act_scale: 1.000000\n",
      "[Calibrated] model.layers.7.self_attn.v_proj                    | act_scale: 1.000000\n",
      "[Calibrated] model.layers.7.self_attn.o_proj                    | act_scale: 1.000000\n",
      "[Calibrated] model.layers.7.mlp.gate_proj                       | act_scale: 1.000000\n",
      "[Calibrated] model.layers.7.mlp.up_proj                         | act_scale: 1.000000\n",
      "[Calibrated] model.layers.7.mlp.down_proj                       | act_scale: 1.000000\n",
      "[Calibrated] model.layers.8.self_attn.q_proj                    | act_scale: 1.000000\n",
      "[Calibrated] model.layers.8.self_attn.k_proj                    | act_scale: 1.000000\n",
      "[Calibrated] model.layers.8.self_attn.v_proj                    | act_scale: 1.000000\n",
      "[Calibrated] model.layers.8.self_attn.o_proj                    | act_scale: 1.000000\n",
      "[Calibrated] model.layers.8.mlp.gate_proj                       | act_scale: 1.000000\n",
      "[Calibrated] model.layers.8.mlp.up_proj                         | act_scale: 1.000000\n",
      "[Calibrated] model.layers.8.mlp.down_proj                       | act_scale: 1.000000\n",
      "[Calibrated] model.layers.9.self_attn.q_proj                    | act_scale: 1.000000\n",
      "[Calibrated] model.layers.9.self_attn.k_proj                    | act_scale: 1.000000\n",
      "[Calibrated] model.layers.9.self_attn.v_proj                    | act_scale: 1.000000\n",
      "[Calibrated] model.layers.9.self_attn.o_proj                    | act_scale: 1.000000\n",
      "[Calibrated] model.layers.9.mlp.gate_proj                       | act_scale: 1.000000\n",
      "[Calibrated] model.layers.9.mlp.up_proj                         | act_scale: 1.000000\n",
      "[Calibrated] model.layers.9.mlp.down_proj                       | act_scale: 1.000000\n",
      "[Calibrated] model.layers.10.self_attn.q_proj                   | act_scale: 1.000000\n",
      "[Calibrated] model.layers.10.self_attn.k_proj                   | act_scale: 1.000000\n",
      "[Calibrated] model.layers.10.self_attn.v_proj                   | act_scale: 1.000000\n",
      "[Calibrated] model.layers.10.self_attn.o_proj                   | act_scale: 1.000000\n",
      "[Calibrated] model.layers.10.mlp.gate_proj                      | act_scale: 1.000000\n",
      "[Calibrated] model.layers.10.mlp.up_proj                        | act_scale: 1.000000\n",
      "[Calibrated] model.layers.10.mlp.down_proj                      | act_scale: 1.000000\n",
      "[Calibrated] model.layers.11.self_attn.q_proj                   | act_scale: 1.000000\n",
      "[Calibrated] model.layers.11.self_attn.k_proj                   | act_scale: 1.000000\n",
      "[Calibrated] model.layers.11.self_attn.v_proj                   | act_scale: 1.000000\n",
      "[Calibrated] model.layers.11.self_attn.o_proj                   | act_scale: 1.000000\n",
      "[Calibrated] model.layers.11.mlp.gate_proj                      | act_scale: 1.000000\n",
      "[Calibrated] model.layers.11.mlp.up_proj                        | act_scale: 1.000000\n",
      "[Calibrated] model.layers.11.mlp.down_proj                      | act_scale: 1.000000\n",
      "[Calibrated] model.layers.12.self_attn.q_proj                   | act_scale: 1.000000\n",
      "[Calibrated] model.layers.12.self_attn.k_proj                   | act_scale: 1.000000\n",
      "[Calibrated] model.layers.12.self_attn.v_proj                   | act_scale: 1.000000\n",
      "[Calibrated] model.layers.12.self_attn.o_proj                   | act_scale: 1.000000\n",
      "[Calibrated] model.layers.12.mlp.gate_proj                      | act_scale: 1.000000\n",
      "[Calibrated] model.layers.12.mlp.up_proj                        | act_scale: 1.000000\n",
      "[Calibrated] model.layers.12.mlp.down_proj                      | act_scale: 1.000000\n",
      "[Calibrated] model.layers.13.self_attn.q_proj                   | act_scale: 1.000000\n",
      "[Calibrated] model.layers.13.self_attn.k_proj                   | act_scale: 1.000000\n",
      "[Calibrated] model.layers.13.self_attn.v_proj                   | act_scale: 1.000000\n",
      "[Calibrated] model.layers.13.self_attn.o_proj                   | act_scale: 1.000000\n",
      "[Calibrated] model.layers.13.mlp.gate_proj                      | act_scale: 1.000000\n",
      "[Calibrated] model.layers.13.mlp.up_proj                        | act_scale: 1.000000\n",
      "[Calibrated] model.layers.13.mlp.down_proj                      | act_scale: 1.000000\n",
      "[Calibrated] model.layers.14.self_attn.q_proj                   | act_scale: 1.000000\n",
      "[Calibrated] model.layers.14.self_attn.k_proj                   | act_scale: 1.000000\n",
      "[Calibrated] model.layers.14.self_attn.v_proj                   | act_scale: 1.000000\n",
      "[Calibrated] model.layers.14.self_attn.o_proj                   | act_scale: 1.000000\n",
      "[Calibrated] model.layers.14.mlp.gate_proj                      | act_scale: 1.000000\n",
      "[Calibrated] model.layers.14.mlp.up_proj                        | act_scale: 1.000000\n",
      "[Calibrated] model.layers.14.mlp.down_proj                      | act_scale: 1.000000\n",
      "[Calibrated] model.layers.15.self_attn.q_proj                   | act_scale: 1.000000\n",
      "[Calibrated] model.layers.15.self_attn.k_proj                   | act_scale: 1.000000\n",
      "[Calibrated] model.layers.15.self_attn.v_proj                   | act_scale: 1.000000\n",
      "[Calibrated] model.layers.15.self_attn.o_proj                   | act_scale: 1.000000\n",
      "[Calibrated] model.layers.15.mlp.gate_proj                      | act_scale: 1.000000\n",
      "[Calibrated] model.layers.15.mlp.up_proj                        | act_scale: 1.000000\n",
      "[Calibrated] model.layers.15.mlp.down_proj                      | act_scale: 1.000000\n",
      ">> [STEP 3] Applying weight quantization...\n",
      "[DEBUG] Quantized: tensor([[-0.,  0., -1.,  ..., -1., -1., -1.],\n",
      "        [-1., -0.,  1.,  ..., -0., -1., -1.],\n",
      "        [ 1.,  1.,  1.,  ..., -0., -1.,  1.],\n",
      "        ...,\n",
      "        [ 0., -0.,  1.,  ..., -1., -1., -1.],\n",
      "        [ 1.,  1., -0.,  ..., -1.,  1., -1.],\n",
      "        [ 0., -0.,  0.,  ...,  0.,  1.,  0.]])| \n",
      "[Quantized] model.layers.0.self_attn.q_proj | Ternary weights applied.\n",
      "[DEBUG] Quantized: tensor([[-1.,  1., -1.,  ..., -1.,  1., -0.],\n",
      "        [ 1.,  1.,  1.,  ...,  1.,  0.,  1.],\n",
      "        [-1., -1.,  0.,  ...,  1.,  1.,  1.],\n",
      "        ...,\n",
      "        [-1., -1., -1.,  ...,  0.,  0.,  0.],\n",
      "        [ 1.,  1., -0.,  ..., -1., -0., -1.],\n",
      "        [-1.,  0., -0.,  ...,  1.,  0.,  1.]])| \n",
      "[Quantized] model.layers.0.self_attn.k_proj | Ternary weights applied.\n",
      "[DEBUG] Quantized: tensor([[-1.,  1.,  0.,  ..., -1., -1., -1.],\n",
      "        [-1., -1., -1.,  ..., -0.,  1.,  0.],\n",
      "        [ 1., -1., -1.,  ..., -1.,  1.,  1.],\n",
      "        ...,\n",
      "        [-0., -0., -1.,  ...,  0.,  0., -0.],\n",
      "        [-0., -0.,  1.,  ..., -0., -1.,  0.],\n",
      "        [ 0., -0., -1.,  ..., -0.,  1., -1.]])| \n",
      "[Quantized] model.layers.0.self_attn.v_proj | Ternary weights applied.\n",
      "[DEBUG] Quantized: tensor([[-1.,  1.,  1.,  ..., -1., -1., -1.],\n",
      "        [ 1., -1.,  1.,  ..., -1.,  1.,  1.],\n",
      "        [-1.,  1.,  1.,  ...,  0., -1.,  0.],\n",
      "        ...,\n",
      "        [-0.,  0., -1.,  ..., -1.,  1., -0.],\n",
      "        [-1.,  0., -0.,  ..., -1., -1., -0.],\n",
      "        [ 0.,  1., -1.,  ..., -1.,  0., -1.]])| \n",
      "[Quantized] model.layers.0.self_attn.o_proj | Ternary weights applied.\n",
      "[DEBUG] Quantized: tensor([[ 1.,  1., -1.,  ...,  1.,  1.,  0.],\n",
      "        [-1., -1., -0.,  ...,  0.,  0., -1.],\n",
      "        [ 0., -1., -1.,  ...,  1., -1., -1.],\n",
      "        ...,\n",
      "        [-0.,  1.,  1.,  ..., -1., -1., -0.],\n",
      "        [ 0., -0., -1.,  ..., -0., -1., -1.],\n",
      "        [ 1., -1., -1.,  ...,  1.,  1., -0.]])| \n",
      "[Quantized] model.layers.0.mlp.gate_proj | Ternary weights applied.\n",
      "[DEBUG] Quantized: tensor([[-0., -0.,  1.,  ..., -1., -0.,  1.],\n",
      "        [-1.,  0., -1.,  ..., -1., -1.,  1.],\n",
      "        [-1., -1., -0.,  ..., -1., -1.,  1.],\n",
      "        ...,\n",
      "        [-1.,  1., -1.,  ..., -0.,  1., -1.],\n",
      "        [-1.,  1., -0.,  ...,  1., -1., -1.],\n",
      "        [ 1.,  1., -1.,  ...,  1., -1., -1.]])| \n",
      "[Quantized] model.layers.0.mlp.up_proj | Ternary weights applied.\n",
      "[DEBUG] Quantized: tensor([[-0.,  1., -0.,  ...,  1., -0.,  1.],\n",
      "        [-1., -1., -0.,  ...,  0.,  1., -1.],\n",
      "        [ 0.,  0., -0.,  ..., -1.,  0.,  0.],\n",
      "        ...,\n",
      "        [-1.,  1., -1.,  ..., -0.,  0., -0.],\n",
      "        [-1.,  0., -0.,  ..., -1., -0., -1.],\n",
      "        [ 0., -0.,  1.,  ..., -1.,  0., -1.]])| \n",
      "[Quantized] model.layers.0.mlp.down_proj | Ternary weights applied.\n",
      "[DEBUG] Quantized: tensor([[-0., -1.,  1.,  ..., -1., -1., -0.],\n",
      "        [ 1.,  0., -0.,  ...,  1.,  1.,  1.],\n",
      "        [ 1., -1.,  1.,  ...,  1.,  1.,  0.],\n",
      "        ...,\n",
      "        [-1., -1., -1.,  ..., -0., -1., -0.],\n",
      "        [-1.,  0., -0.,  ...,  1., -1.,  1.],\n",
      "        [-0., -1.,  1.,  ..., -0.,  0.,  1.]])| \n",
      "[Quantized] model.layers.1.self_attn.q_proj | Ternary weights applied.\n",
      "[DEBUG] Quantized: tensor([[-1.,  1.,  1.,  ..., -1., -1., -1.],\n",
      "        [ 1.,  1., -1.,  ..., -1.,  0., -1.],\n",
      "        [ 1.,  0., -1.,  ..., -0., -1., -1.],\n",
      "        ...,\n",
      "        [-1., -1., -1.,  ...,  1., -1., -0.],\n",
      "        [-1., -0., -0.,  ...,  1., -1.,  1.],\n",
      "        [ 0., -0.,  1.,  ...,  1.,  1.,  1.]])| \n",
      "[Quantized] model.layers.1.self_attn.k_proj | Ternary weights applied.\n",
      "[DEBUG] Quantized: tensor([[ 0.,  0.,  1.,  ...,  1., -0., -1.],\n",
      "        [ 1.,  0., -1.,  ..., -1., -0., -1.],\n",
      "        [-0., -0., -1.,  ...,  1.,  1.,  1.],\n",
      "        ...,\n",
      "        [-1., -1.,  1.,  ..., -0., -0., -0.],\n",
      "        [ 1., -1.,  1.,  ...,  0., -1., -0.],\n",
      "        [ 0., -1., -0.,  ..., -0., -1., -0.]])| \n",
      "[Quantized] model.layers.1.self_attn.v_proj | Ternary weights applied.\n",
      "[DEBUG] Quantized: tensor([[-0., -1., -0.,  ...,  1.,  1., -1.],\n",
      "        [ 1., -1., -0.,  ...,  1., -0.,  1.],\n",
      "        [ 1.,  1.,  1.,  ...,  0., -1., -0.],\n",
      "        ...,\n",
      "        [-1.,  1., -1.,  ..., -0.,  0.,  0.],\n",
      "        [ 1.,  1.,  1.,  ..., -1., -1.,  0.],\n",
      "        [-0., -1.,  1.,  ..., -1.,  0.,  1.]])| \n",
      "[Quantized] model.layers.1.self_attn.o_proj | Ternary weights applied.\n",
      "[DEBUG] Quantized: tensor([[-1.,  0.,  1.,  ..., -1.,  1.,  1.],\n",
      "        [ 1., -1.,  0.,  ..., -0., -0., -1.],\n",
      "        [-1., -1., -1.,  ...,  0., -0.,  1.],\n",
      "        ...,\n",
      "        [ 1.,  1., -1.,  ...,  1.,  1.,  1.],\n",
      "        [-1., -1., -1.,  ...,  1., -1.,  0.],\n",
      "        [-1., -1., -1.,  ...,  0.,  0.,  0.]])| \n",
      "[Quantized] model.layers.1.mlp.gate_proj | Ternary weights applied.\n",
      "[DEBUG] Quantized: tensor([[ 1.,  0.,  1.,  ..., -0., -0., -1.],\n",
      "        [ 1.,  1., -1.,  ...,  1.,  1.,  0.],\n",
      "        [-0.,  1., -1.,  ...,  0.,  1., -1.],\n",
      "        ...,\n",
      "        [ 0.,  0.,  1.,  ...,  1.,  0., -1.],\n",
      "        [ 0., -0.,  0.,  ..., -1., -1., -1.],\n",
      "        [ 1.,  1.,  1.,  ..., -1.,  0.,  1.]])| \n",
      "[Quantized] model.layers.1.mlp.up_proj | Ternary weights applied.\n",
      "[DEBUG] Quantized: tensor([[ 1.,  1.,  1.,  ..., -1.,  0.,  1.],\n",
      "        [ 1.,  1.,  1.,  ..., -1.,  0.,  1.],\n",
      "        [ 1.,  0., -1.,  ..., -0.,  1.,  0.],\n",
      "        ...,\n",
      "        [ 0.,  1.,  0.,  ...,  0., -1., -1.],\n",
      "        [ 0., -0.,  0.,  ...,  1., -0., -1.],\n",
      "        [-0., -1., -1.,  ..., -0.,  0.,  0.]])| \n",
      "[Quantized] model.layers.1.mlp.down_proj | Ternary weights applied.\n",
      "[DEBUG] Quantized: tensor([[-1.,  1., -0.,  ...,  0.,  1., -0.],\n",
      "        [ 1.,  1.,  0.,  ..., -1., -1.,  0.],\n",
      "        [ 0.,  1.,  0.,  ..., -1.,  1.,  1.],\n",
      "        ...,\n",
      "        [-1., -1., -0.,  ..., -1., -1.,  0.],\n",
      "        [-0.,  0., -1.,  ..., -0., -1.,  0.],\n",
      "        [-0.,  1., -1.,  ...,  1., -1.,  0.]])| \n",
      "[Quantized] model.layers.2.self_attn.q_proj | Ternary weights applied.\n",
      "[DEBUG] Quantized: tensor([[-1., -0.,  0.,  ...,  1.,  0.,  0.],\n",
      "        [-0.,  1., -0.,  ...,  0., -0., -1.],\n",
      "        [ 1.,  1., -0.,  ..., -1.,  1.,  1.],\n",
      "        ...,\n",
      "        [ 1., -1., -1.,  ...,  1.,  1., -0.],\n",
      "        [ 1., -0., -1.,  ..., -0.,  1.,  1.],\n",
      "        [ 1.,  0.,  1.,  ...,  0.,  1.,  1.]])| \n",
      "[Quantized] model.layers.2.self_attn.k_proj | Ternary weights applied.\n",
      "[DEBUG] Quantized: tensor([[ 0., -1., -1.,  ...,  1., -0., -1.],\n",
      "        [ 0.,  1., -1.,  ..., -1.,  1.,  0.],\n",
      "        [ 1., -1., -0.,  ..., -0.,  1., -0.],\n",
      "        ...,\n",
      "        [ 1., -1., -1.,  ...,  1., -1., -0.],\n",
      "        [ 0., -1.,  0.,  ..., -1.,  1.,  0.],\n",
      "        [ 1., -1.,  0.,  ..., -0., -0.,  0.]])| \n",
      "[Quantized] model.layers.2.self_attn.v_proj | Ternary weights applied.\n",
      "[DEBUG] Quantized: tensor([[-1., -0., -1.,  ..., -1., -1., -0.],\n",
      "        [-0., -0.,  1.,  ..., -1., -1., -0.],\n",
      "        [ 1.,  1.,  1.,  ...,  1.,  1.,  1.],\n",
      "        ...,\n",
      "        [-1.,  1.,  1.,  ...,  1.,  1., -1.],\n",
      "        [ 0., -1., -1.,  ..., -1.,  0., -1.],\n",
      "        [ 1., -1.,  0.,  ..., -1., -1.,  0.]])| \n",
      "[Quantized] model.layers.2.self_attn.o_proj | Ternary weights applied.\n",
      "[DEBUG] Quantized: tensor([[ 1.,  1., -1.,  ...,  0., -1., -1.],\n",
      "        [-1., -1., -1.,  ..., -1., -1.,  1.],\n",
      "        [-1., -1., -1.,  ...,  0., -1.,  0.],\n",
      "        ...,\n",
      "        [ 1.,  1., -1.,  ...,  1., -1., -1.],\n",
      "        [-1.,  1.,  1.,  ..., -0.,  0., -1.],\n",
      "        [-1.,  0.,  1.,  ...,  0.,  1., -1.]])| \n",
      "[Quantized] model.layers.2.mlp.gate_proj | Ternary weights applied.\n",
      "[DEBUG] Quantized: tensor([[ 0.,  1., -1.,  ..., -0., -1., -1.],\n",
      "        [-0., -1., -0.,  ..., -1., -1., -1.],\n",
      "        [-1., -1.,  0.,  ...,  0., -0.,  1.],\n",
      "        ...,\n",
      "        [ 1., -1., -1.,  ...,  1.,  1.,  0.],\n",
      "        [-1., -0., -1.,  ...,  1., -0., -1.],\n",
      "        [-1., -0.,  1.,  ..., -1., -1., -0.]])| \n",
      "[Quantized] model.layers.2.mlp.up_proj | Ternary weights applied.\n",
      "[DEBUG] Quantized: tensor([[ 1.,  1.,  1.,  ...,  1.,  1., -1.],\n",
      "        [ 1., -1.,  1.,  ..., -1.,  1., -1.],\n",
      "        [-1., -1., -0.,  ..., -1.,  1., -1.],\n",
      "        ...,\n",
      "        [-1.,  0.,  0.,  ..., -0., -1.,  1.],\n",
      "        [-1., -1., -1.,  ...,  0.,  1., -1.],\n",
      "        [-1., -0., -1.,  ..., -1., -1., -1.]])| \n",
      "[Quantized] model.layers.2.mlp.down_proj | Ternary weights applied.\n",
      "[DEBUG] Quantized: tensor([[-1.,  0.,  1.,  ..., -1.,  1., -1.],\n",
      "        [ 1., -0.,  0.,  ...,  0., -1.,  1.],\n",
      "        [ 1.,  0.,  0.,  ..., -0., -1.,  1.],\n",
      "        ...,\n",
      "        [-1., -1.,  1.,  ..., -1.,  1., -1.],\n",
      "        [-0.,  0., -1.,  ...,  1., -1.,  0.],\n",
      "        [-1., -1., -1.,  ...,  1., -1.,  1.]])| \n",
      "[Quantized] model.layers.3.self_attn.q_proj | Ternary weights applied.\n",
      "[DEBUG] Quantized: tensor([[ 0.,  1., -0.,  ..., -0., -0.,  1.],\n",
      "        [-0., -1., -1.,  ...,  0., -0.,  0.],\n",
      "        [ 0.,  0., -0.,  ..., -0., -0., -0.],\n",
      "        ...,\n",
      "        [ 0., -1., -0.,  ..., -1.,  1., -1.],\n",
      "        [-1.,  1., -1.,  ...,  1., -1., -1.],\n",
      "        [-1., -1., -1.,  ...,  1., -1.,  1.]])| \n",
      "[Quantized] model.layers.3.self_attn.k_proj | Ternary weights applied.\n",
      "[DEBUG] Quantized: tensor([[ 1.,  1.,  0.,  ..., -1., -0., -1.],\n",
      "        [ 1., -1., -0.,  ..., -1., -0.,  1.],\n",
      "        [ 1.,  1., -1.,  ..., -1.,  0., -1.],\n",
      "        ...,\n",
      "        [ 1., -1.,  1.,  ...,  0., -1.,  0.],\n",
      "        [ 1.,  0., -1.,  ..., -1., -1., -1.],\n",
      "        [-0.,  1.,  1.,  ...,  1., -1.,  1.]])| \n",
      "[Quantized] model.layers.3.self_attn.v_proj | Ternary weights applied.\n",
      "[DEBUG] Quantized: tensor([[ 1., -0.,  0.,  ..., -1., -0.,  1.],\n",
      "        [-0.,  1.,  1.,  ...,  1., -1., -1.],\n",
      "        [-1.,  0.,  1.,  ..., -0., -0.,  1.],\n",
      "        ...,\n",
      "        [-1.,  1.,  0.,  ..., -1.,  1., -0.],\n",
      "        [ 0., -1., -1.,  ..., -1.,  1., -0.],\n",
      "        [ 1.,  1., -0.,  ..., -0., -1., -0.]])| \n",
      "[Quantized] model.layers.3.self_attn.o_proj | Ternary weights applied.\n",
      "[DEBUG] Quantized: tensor([[ 1.,  1., -0.,  ..., -1., -0., -1.],\n",
      "        [ 0.,  0., -0.,  ..., -1.,  0., -1.],\n",
      "        [ 1., -1., -1.,  ..., -1.,  1., -0.],\n",
      "        ...,\n",
      "        [-0.,  1.,  0.,  ..., -1.,  1.,  1.],\n",
      "        [ 0., -0., -0.,  ..., -0., -1., -1.],\n",
      "        [-1., -1., -0.,  ..., -1., -1.,  0.]])| \n",
      "[Quantized] model.layers.3.mlp.gate_proj | Ternary weights applied.\n",
      "[DEBUG] Quantized: tensor([[-1., -1.,  1.,  ...,  1.,  1.,  1.],\n",
      "        [ 0., -1., -1.,  ...,  1., -1., -1.],\n",
      "        [ 1.,  1., -1.,  ..., -1.,  1.,  0.],\n",
      "        ...,\n",
      "        [ 1.,  0.,  0.,  ..., -1.,  1., -0.],\n",
      "        [-0., -1.,  1.,  ..., -1., -1.,  0.],\n",
      "        [ 1.,  1.,  1.,  ...,  0.,  1., -1.]])| \n",
      "[Quantized] model.layers.3.mlp.up_proj | Ternary weights applied.\n",
      "[DEBUG] Quantized: tensor([[ 1.,  1., -1.,  ..., -1.,  0., -1.],\n",
      "        [-1., -1., -1.,  ...,  0.,  0., -1.],\n",
      "        [ 1., -1., -1.,  ..., -1., -1.,  1.],\n",
      "        ...,\n",
      "        [ 1.,  1.,  0.,  ...,  1., -0.,  0.],\n",
      "        [ 1., -0.,  1.,  ..., -1.,  1.,  0.],\n",
      "        [ 1., -0., -0.,  ...,  0., -1.,  1.]])| \n",
      "[Quantized] model.layers.3.mlp.down_proj | Ternary weights applied.\n",
      "[DEBUG] Quantized: tensor([[-1., -0., -0.,  ..., -1.,  0., -1.],\n",
      "        [-0.,  1., -0.,  ...,  0., -0.,  1.],\n",
      "        [-1.,  1., -1.,  ..., -1., -1., -1.],\n",
      "        ...,\n",
      "        [-0.,  0., -1.,  ..., -1., -1.,  1.],\n",
      "        [ 0., -1.,  1.,  ...,  1., -1.,  1.],\n",
      "        [-1.,  0.,  0.,  ...,  0.,  1., -0.]])| \n",
      "[Quantized] model.layers.4.self_attn.q_proj | Ternary weights applied.\n",
      "[DEBUG] Quantized: tensor([[-1., -1.,  0.,  ..., -1.,  0., -1.],\n",
      "        [-1., -1.,  1.,  ..., -1.,  0., -1.],\n",
      "        [-1.,  0., -0.,  ..., -1., -0.,  1.],\n",
      "        ...,\n",
      "        [ 1., -1., -1.,  ...,  1., -1., -1.],\n",
      "        [-1., -1.,  0.,  ...,  1.,  0., -1.],\n",
      "        [ 0.,  1., -1.,  ...,  1.,  1.,  0.]])| \n",
      "[Quantized] model.layers.4.self_attn.k_proj | Ternary weights applied.\n",
      "[DEBUG] Quantized: tensor([[ 1.,  1., -1.,  ..., -1.,  1., -0.],\n",
      "        [ 0., -1.,  1.,  ...,  1.,  1., -0.],\n",
      "        [-1.,  0., -1.,  ..., -1.,  1., -1.],\n",
      "        ...,\n",
      "        [ 1., -0.,  1.,  ..., -0.,  1.,  0.],\n",
      "        [-1., -0., -1.,  ..., -1.,  1.,  1.],\n",
      "        [-1., -0., -1.,  ..., -1.,  0., -1.]])| \n",
      "[Quantized] model.layers.4.self_attn.v_proj | Ternary weights applied.\n",
      "[DEBUG] Quantized: tensor([[ 1.,  0., -1.,  ...,  1., -1., -1.],\n",
      "        [-1.,  1.,  0.,  ..., -0.,  1., -1.],\n",
      "        [ 0., -1., -1.,  ..., -0., -1., -1.],\n",
      "        ...,\n",
      "        [-1.,  1., -1.,  ...,  1., -1., -1.],\n",
      "        [ 0.,  1.,  1.,  ...,  1.,  0.,  0.],\n",
      "        [ 1.,  1.,  0.,  ...,  0., -1., -1.]])| \n",
      "[Quantized] model.layers.4.self_attn.o_proj | Ternary weights applied.\n",
      "[DEBUG] Quantized: tensor([[ 1.,  0.,  1.,  ...,  1., -0., -1.],\n",
      "        [ 1., -1., -1.,  ..., -1., -1.,  0.],\n",
      "        [ 1., -1.,  1.,  ..., -1., -1., -1.],\n",
      "        ...,\n",
      "        [ 0., -1.,  0.,  ..., -1., -1.,  1.],\n",
      "        [-1.,  1.,  0.,  ..., -0., -1., -0.],\n",
      "        [-1.,  1.,  1.,  ..., -0.,  1., -1.]])| \n",
      "[Quantized] model.layers.4.mlp.gate_proj | Ternary weights applied.\n",
      "[DEBUG] Quantized: tensor([[ 1.,  1., -0.,  ..., -1., -0.,  1.],\n",
      "        [-1., -0.,  1.,  ..., -0., -1.,  1.],\n",
      "        [ 1.,  1., -0.,  ...,  1., -0.,  0.],\n",
      "        ...,\n",
      "        [-0., -0., -0.,  ..., -1., -1.,  0.],\n",
      "        [ 1.,  1., -1.,  ...,  1.,  1.,  0.],\n",
      "        [ 1., -0., -1.,  ..., -1., -0.,  1.]])| \n",
      "[Quantized] model.layers.4.mlp.up_proj | Ternary weights applied.\n",
      "[DEBUG] Quantized: tensor([[-1.,  1.,  0.,  ...,  1., -1.,  1.],\n",
      "        [-1., -1.,  1.,  ...,  1., -0.,  1.],\n",
      "        [ 1., -1., -0.,  ..., -1., -1., -1.],\n",
      "        ...,\n",
      "        [-1.,  0., -0.,  ..., -1.,  0.,  1.],\n",
      "        [-1., -1., -0.,  ...,  1.,  1., -1.],\n",
      "        [ 0.,  0.,  0.,  ..., -1., -1.,  1.]])| \n",
      "[Quantized] model.layers.4.mlp.down_proj | Ternary weights applied.\n",
      "[DEBUG] Quantized: tensor([[ 1.,  1., -1.,  ...,  1.,  1., -0.],\n",
      "        [ 0., -1., -0.,  ...,  0.,  1., -1.],\n",
      "        [ 1.,  1., -1.,  ...,  1.,  1., -0.],\n",
      "        ...,\n",
      "        [ 0., -0.,  1.,  ..., -0., -1., -1.],\n",
      "        [-0.,  0.,  1.,  ..., -1.,  0., -1.],\n",
      "        [-1., -1.,  1.,  ..., -0., -1.,  1.]])| \n",
      "[Quantized] model.layers.5.self_attn.q_proj | Ternary weights applied.\n",
      "[DEBUG] Quantized: tensor([[-0., -1., -0.,  ...,  0.,  0., -1.],\n",
      "        [-0.,  1.,  1.,  ..., -0., -0.,  1.],\n",
      "        [-0., -0.,  1.,  ...,  0.,  1., -0.],\n",
      "        ...,\n",
      "        [-1., -0., -0.,  ..., -1., -1., -1.],\n",
      "        [ 1.,  1.,  1.,  ..., -1.,  1.,  1.],\n",
      "        [ 1.,  0., -1.,  ...,  1., -1., -1.]])| \n",
      "[Quantized] model.layers.5.self_attn.k_proj | Ternary weights applied.\n",
      "[DEBUG] Quantized: tensor([[ 1., -0., -1.,  ...,  1., -1.,  0.],\n",
      "        [-1.,  1., -1.,  ..., -1.,  0., -1.],\n",
      "        [ 0., -1.,  0.,  ..., -1., -1.,  0.],\n",
      "        ...,\n",
      "        [ 1., -1.,  1.,  ..., -0.,  1., -1.],\n",
      "        [-1., -1.,  0.,  ...,  1., -1., -0.],\n",
      "        [ 1.,  1.,  1.,  ...,  0.,  1.,  0.]])| \n",
      "[Quantized] model.layers.5.self_attn.v_proj | Ternary weights applied.\n",
      "[DEBUG] Quantized: tensor([[-0., -1., -1.,  ...,  1., -0.,  0.],\n",
      "        [ 1.,  0.,  1.,  ..., -0., -1.,  1.],\n",
      "        [-1., -1.,  1.,  ..., -0., -1.,  0.],\n",
      "        ...,\n",
      "        [-0.,  0., -1.,  ...,  1.,  1.,  1.],\n",
      "        [ 0., -0., -0.,  ..., -0.,  1., -1.],\n",
      "        [ 0., -1.,  0.,  ..., -1., -1., -1.]])| \n",
      "[Quantized] model.layers.5.self_attn.o_proj | Ternary weights applied.\n",
      "[DEBUG] Quantized: tensor([[ 1.,  1.,  1.,  ...,  1.,  0.,  1.],\n",
      "        [-0.,  1., -0.,  ...,  1.,  0.,  1.],\n",
      "        [-1., -1.,  1.,  ...,  0.,  0., -1.],\n",
      "        ...,\n",
      "        [ 0.,  1., -0.,  ...,  1., -1., -1.],\n",
      "        [ 1., -0.,  1.,  ..., -1., -0., -1.],\n",
      "        [ 0.,  1., -1.,  ...,  1.,  1., -1.]])| \n",
      "[Quantized] model.layers.5.mlp.gate_proj | Ternary weights applied.\n",
      "[DEBUG] Quantized: tensor([[ 0., -1., -1.,  ..., -0., -1.,  0.],\n",
      "        [-0., -1., -1.,  ...,  0., -1., -0.],\n",
      "        [-0.,  0., -1.,  ...,  0.,  0., -1.],\n",
      "        ...,\n",
      "        [ 1., -1., -0.,  ..., -1.,  0.,  0.],\n",
      "        [ 0., -0., -1.,  ..., -0., -0., -0.],\n",
      "        [ 1.,  0.,  0.,  ...,  1., -1.,  0.]])| \n",
      "[Quantized] model.layers.5.mlp.up_proj | Ternary weights applied.\n",
      "[DEBUG] Quantized: tensor([[ 1., -1.,  1.,  ...,  0.,  1.,  1.],\n",
      "        [-0.,  1.,  1.,  ..., -1.,  1., -0.],\n",
      "        [-0., -0., -0.,  ..., -1.,  0., -1.],\n",
      "        ...,\n",
      "        [-1., -0., -1.,  ...,  1., -1.,  0.],\n",
      "        [ 1.,  1.,  1.,  ...,  1., -1.,  1.],\n",
      "        [ 0.,  1., -1.,  ...,  1.,  0.,  1.]])| \n",
      "[Quantized] model.layers.5.mlp.down_proj | Ternary weights applied.\n",
      "[DEBUG] Quantized: tensor([[-0., -0., -0.,  ..., -1., -0., -1.],\n",
      "        [-1., -1., -1.,  ..., -1.,  1., -0.],\n",
      "        [ 0.,  1.,  0.,  ..., -0., -1.,  1.],\n",
      "        ...,\n",
      "        [ 1.,  1.,  1.,  ...,  1.,  1.,  0.],\n",
      "        [-1.,  1.,  0.,  ..., -1.,  1.,  1.],\n",
      "        [ 1., -1., -0.,  ...,  1., -1.,  0.]])| \n",
      "[Quantized] model.layers.6.self_attn.q_proj | Ternary weights applied.\n",
      "[DEBUG] Quantized: tensor([[-1.,  0.,  1.,  ...,  1.,  1.,  1.],\n",
      "        [ 1.,  1.,  0.,  ...,  0.,  1.,  1.],\n",
      "        [-0.,  1.,  1.,  ..., -0., -0.,  0.],\n",
      "        ...,\n",
      "        [ 1.,  1., -1.,  ...,  1.,  1., -0.],\n",
      "        [-1., -1., -1.,  ..., -0., -0., -1.],\n",
      "        [-1., -1.,  1.,  ..., -1.,  1., -1.]])| \n",
      "[Quantized] model.layers.6.self_attn.k_proj | Ternary weights applied.\n",
      "[DEBUG] Quantized: tensor([[ 1.,  1.,  1.,  ...,  1., -0., -1.],\n",
      "        [ 1., -0.,  1.,  ..., -1.,  1.,  0.],\n",
      "        [-1.,  0.,  0.,  ..., -1., -0., -1.],\n",
      "        ...,\n",
      "        [-1., -1., -1.,  ..., -1., -1.,  1.],\n",
      "        [ 1.,  1., -1.,  ...,  1.,  1., -1.],\n",
      "        [-0., -0.,  0.,  ...,  1.,  1.,  1.]])| \n",
      "[Quantized] model.layers.6.self_attn.v_proj | Ternary weights applied.\n",
      "[DEBUG] Quantized: tensor([[-1., -1.,  1.,  ...,  1.,  1.,  0.],\n",
      "        [-1.,  0.,  0.,  ..., -0.,  0., -1.],\n",
      "        [-1., -0.,  0.,  ..., -1.,  1., -1.],\n",
      "        ...,\n",
      "        [-1.,  1.,  1.,  ...,  1.,  1.,  1.],\n",
      "        [-1., -0.,  0.,  ...,  0.,  0., -1.],\n",
      "        [ 1., -1.,  0.,  ...,  1.,  0.,  0.]])| \n",
      "[Quantized] model.layers.6.self_attn.o_proj | Ternary weights applied.\n",
      "[DEBUG] Quantized: tensor([[-1.,  1.,  1.,  ..., -1.,  1.,  0.],\n",
      "        [-1.,  1., -1.,  ...,  0.,  1., -0.],\n",
      "        [-0., -0., -1.,  ..., -1.,  1., -1.],\n",
      "        ...,\n",
      "        [-1., -0., -1.,  ..., -1.,  1., -1.],\n",
      "        [ 1., -0.,  1.,  ...,  1., -1., -0.],\n",
      "        [-0., -1.,  1.,  ..., -1.,  1.,  1.]])| \n",
      "[Quantized] model.layers.6.mlp.gate_proj | Ternary weights applied.\n",
      "[DEBUG] Quantized: tensor([[-0., -1.,  1.,  ...,  1.,  1., -1.],\n",
      "        [ 1., -1.,  0.,  ..., -1.,  1., -1.],\n",
      "        [-0., -1.,  1.,  ...,  1.,  1.,  0.],\n",
      "        ...,\n",
      "        [-1., -1., -1.,  ..., -0., -0.,  1.],\n",
      "        [-1.,  1., -0.,  ...,  1.,  1., -1.],\n",
      "        [ 1., -0.,  0.,  ..., -1.,  1.,  0.]])| \n",
      "[Quantized] model.layers.6.mlp.up_proj | Ternary weights applied.\n",
      "[DEBUG] Quantized: tensor([[ 1., -0.,  1.,  ...,  1.,  0., -1.],\n",
      "        [ 1.,  1., -0.,  ...,  1., -0.,  1.],\n",
      "        [ 1., -1., -0.,  ...,  1.,  1., -1.],\n",
      "        ...,\n",
      "        [-0.,  1.,  1.,  ..., -1.,  0.,  1.],\n",
      "        [-1.,  1.,  0.,  ..., -1.,  0.,  1.],\n",
      "        [ 1.,  1., -0.,  ..., -1.,  1., -1.]])| \n",
      "[Quantized] model.layers.6.mlp.down_proj | Ternary weights applied.\n",
      "[DEBUG] Quantized: tensor([[ 1., -1.,  0.,  ...,  0.,  1.,  1.],\n",
      "        [-1., -1.,  0.,  ..., -1., -1., -1.],\n",
      "        [-1.,  0., -1.,  ..., -1., -1.,  0.],\n",
      "        ...,\n",
      "        [ 1., -1.,  1.,  ..., -1.,  1., -1.],\n",
      "        [-1.,  1.,  1.,  ..., -1.,  1.,  1.],\n",
      "        [-0., -0., -0.,  ...,  1., -1.,  1.]])| \n",
      "[Quantized] model.layers.7.self_attn.q_proj | Ternary weights applied.\n",
      "[DEBUG] Quantized: tensor([[ 0.,  1., -1.,  ...,  0., -1.,  1.],\n",
      "        [-0.,  0.,  1.,  ...,  0., -1., -0.],\n",
      "        [ 0., -1.,  1.,  ..., -1., -1., -1.],\n",
      "        ...,\n",
      "        [-1.,  1.,  1.,  ...,  1.,  1.,  1.],\n",
      "        [-0.,  1., -1.,  ..., -0., -1.,  0.],\n",
      "        [-1., -1., -1.,  ..., -0., -0.,  1.]])| \n",
      "[Quantized] model.layers.7.self_attn.k_proj | Ternary weights applied.\n",
      "[DEBUG] Quantized: tensor([[ 1.,  1., -0.,  ...,  1., -0.,  1.],\n",
      "        [ 1.,  1.,  1.,  ...,  1.,  1.,  0.],\n",
      "        [ 1.,  1.,  0.,  ..., -1., -0.,  1.],\n",
      "        ...,\n",
      "        [ 1.,  1., -1.,  ..., -1., -0.,  1.],\n",
      "        [-1.,  0.,  1.,  ..., -0.,  0., -0.],\n",
      "        [-1., -0.,  0.,  ..., -0.,  1.,  1.]])| \n",
      "[Quantized] model.layers.7.self_attn.v_proj | Ternary weights applied.\n",
      "[DEBUG] Quantized: tensor([[-1., -0., -0.,  ...,  1., -1.,  0.],\n",
      "        [ 1., -1., -1.,  ..., -1.,  1., -1.],\n",
      "        [ 0., -0.,  0.,  ..., -1., -0.,  1.],\n",
      "        ...,\n",
      "        [ 1., -0.,  0.,  ..., -1.,  0., -1.],\n",
      "        [ 1., -1.,  1.,  ..., -1., -0.,  1.],\n",
      "        [ 0.,  1., -0.,  ..., -1., -1.,  1.]])| \n",
      "[Quantized] model.layers.7.self_attn.o_proj | Ternary weights applied.\n",
      "[DEBUG] Quantized: tensor([[-1., -0., -1.,  ...,  1.,  1.,  1.],\n",
      "        [ 1.,  0., -0.,  ..., -0., -1., -1.],\n",
      "        [ 1., -1.,  1.,  ...,  0.,  1., -1.],\n",
      "        ...,\n",
      "        [-1.,  0., -1.,  ..., -1.,  0., -0.],\n",
      "        [-1., -1., -0.,  ..., -0.,  1., -1.],\n",
      "        [-1., -1., -1.,  ...,  0.,  0.,  0.]])| \n",
      "[Quantized] model.layers.7.mlp.gate_proj | Ternary weights applied.\n",
      "[DEBUG] Quantized: tensor([[-0., -1., -1.,  ...,  1.,  1., -1.],\n",
      "        [-0., -0.,  0.,  ..., -1.,  0.,  0.],\n",
      "        [-1.,  1., -1.,  ...,  1.,  1.,  1.],\n",
      "        ...,\n",
      "        [-1., -1.,  1.,  ...,  1., -1., -1.],\n",
      "        [-0.,  1.,  1.,  ...,  1., -1.,  1.],\n",
      "        [ 0., -1., -1.,  ..., -1., -1., -1.]])| \n",
      "[Quantized] model.layers.7.mlp.up_proj | Ternary weights applied.\n",
      "[DEBUG] Quantized: tensor([[-1.,  1., -1.,  ...,  1.,  0.,  1.],\n",
      "        [ 1., -1., -1.,  ..., -1., -0., -1.],\n",
      "        [-0., -1., -0.,  ...,  0., -1., -0.],\n",
      "        ...,\n",
      "        [-1.,  1.,  0.,  ..., -0.,  1.,  0.],\n",
      "        [-1., -1., -1.,  ...,  1., -0.,  0.],\n",
      "        [-0., -1., -1.,  ...,  0., -0.,  1.]])| \n",
      "[Quantized] model.layers.7.mlp.down_proj | Ternary weights applied.\n",
      "[DEBUG] Quantized: tensor([[ 0.,  1.,  1.,  ..., -0.,  1., -1.],\n",
      "        [ 1., -1.,  1.,  ...,  1.,  1., -1.],\n",
      "        [ 1., -0., -0.,  ..., -1., -1.,  1.],\n",
      "        ...,\n",
      "        [-1., -1., -0.,  ..., -1.,  1.,  0.],\n",
      "        [-1.,  1.,  1.,  ..., -1.,  1., -1.],\n",
      "        [ 1.,  0.,  1.,  ..., -0., -1., -1.]])| \n",
      "[Quantized] model.layers.8.self_attn.q_proj | Ternary weights applied.\n",
      "[DEBUG] Quantized: tensor([[ 0., -1.,  1.,  ..., -0.,  1.,  0.],\n",
      "        [-1., -1.,  0.,  ..., -1.,  1.,  0.],\n",
      "        [-1.,  1., -1.,  ...,  1.,  1.,  1.],\n",
      "        ...,\n",
      "        [ 0., -0.,  1.,  ...,  1., -0., -1.],\n",
      "        [-0., -0.,  0.,  ..., -1.,  1., -1.],\n",
      "        [ 0., -0.,  1.,  ...,  0., -0., -1.]])| \n",
      "[Quantized] model.layers.8.self_attn.k_proj | Ternary weights applied.\n",
      "[DEBUG] Quantized: tensor([[ 1., -0., -1.,  ...,  1.,  1.,  1.],\n",
      "        [ 1., -1.,  1.,  ..., -1.,  1., -1.],\n",
      "        [-1.,  1., -1.,  ..., -1.,  1.,  1.],\n",
      "        ...,\n",
      "        [ 1.,  1.,  1.,  ..., -1.,  1., -1.],\n",
      "        [-0.,  1.,  0.,  ..., -1.,  1., -1.],\n",
      "        [ 1.,  0.,  1.,  ..., -1.,  1., -1.]])| \n",
      "[Quantized] model.layers.8.self_attn.v_proj | Ternary weights applied.\n",
      "[DEBUG] Quantized: tensor([[ 1.,  1., -1.,  ..., -1., -1., -1.],\n",
      "        [-1., -0.,  1.,  ..., -0., -1., -1.],\n",
      "        [-1., -1.,  1.,  ..., -1., -1., -0.],\n",
      "        ...,\n",
      "        [ 1., -1., -1.,  ..., -0., -0.,  1.],\n",
      "        [ 0.,  0.,  1.,  ..., -1., -1., -0.],\n",
      "        [ 1.,  1.,  0.,  ...,  1.,  1.,  0.]])| \n",
      "[Quantized] model.layers.8.self_attn.o_proj | Ternary weights applied.\n",
      "[DEBUG] Quantized: tensor([[ 0.,  1.,  1.,  ...,  1., -1., -0.],\n",
      "        [ 1., -0., -0.,  ...,  1., -1.,  1.],\n",
      "        [ 1.,  1.,  1.,  ..., -1., -1.,  1.],\n",
      "        ...,\n",
      "        [-1., -1., -1.,  ..., -1., -1., -0.],\n",
      "        [-0.,  1., -1.,  ...,  0.,  1., -0.],\n",
      "        [ 1., -1.,  1.,  ..., -0., -0.,  1.]])| \n",
      "[Quantized] model.layers.8.mlp.gate_proj | Ternary weights applied.\n",
      "[DEBUG] Quantized: tensor([[ 1., -1., -1.,  ..., -1., -1.,  1.],\n",
      "        [ 0.,  1., -1.,  ..., -0.,  1., -1.],\n",
      "        [ 0., -0.,  1.,  ..., -1., -1.,  1.],\n",
      "        ...,\n",
      "        [ 1., -1.,  1.,  ...,  1.,  1., -1.],\n",
      "        [-0.,  0., -0.,  ..., -1.,  0., -1.],\n",
      "        [-1.,  1., -0.,  ...,  1., -1.,  0.]])| \n",
      "[Quantized] model.layers.8.mlp.up_proj | Ternary weights applied.\n",
      "[DEBUG] Quantized: tensor([[-1., -0., -1.,  ...,  1.,  1., -0.],\n",
      "        [ 1., -1., -1.,  ..., -1.,  1.,  0.],\n",
      "        [ 1.,  0.,  1.,  ...,  0., -1., -1.],\n",
      "        ...,\n",
      "        [ 1.,  0.,  0.,  ...,  1.,  1., -1.],\n",
      "        [ 1.,  0., -1.,  ...,  1.,  1., -0.],\n",
      "        [-1.,  0.,  1.,  ..., -1., -1.,  0.]])| \n",
      "[Quantized] model.layers.8.mlp.down_proj | Ternary weights applied.\n",
      "[DEBUG] Quantized: tensor([[-0., -0.,  1.,  ...,  0., -1., -0.],\n",
      "        [-1., -0.,  1.,  ...,  1.,  1., -0.],\n",
      "        [-0.,  0., -0.,  ...,  0.,  1.,  1.],\n",
      "        ...,\n",
      "        [-1.,  1., -1.,  ..., -1.,  1., -1.],\n",
      "        [-1.,  0.,  1.,  ...,  0.,  1., -0.],\n",
      "        [ 1., -1., -1.,  ..., -1., -0.,  1.]])| \n",
      "[Quantized] model.layers.9.self_attn.q_proj | Ternary weights applied.\n",
      "[DEBUG] Quantized: tensor([[-0.,  1.,  0.,  ...,  0.,  0.,  1.],\n",
      "        [-0.,  0.,  0.,  ..., -0.,  1.,  1.],\n",
      "        [-0.,  0.,  0.,  ..., -0., -0.,  0.],\n",
      "        ...,\n",
      "        [ 1.,  1., -0.,  ...,  1., -1.,  1.],\n",
      "        [-1., -1.,  1.,  ..., -1.,  1.,  1.],\n",
      "        [-1.,  1.,  1.,  ..., -0., -0.,  1.]])| \n",
      "[Quantized] model.layers.9.self_attn.k_proj | Ternary weights applied.\n",
      "[DEBUG] Quantized: tensor([[ 1.,  0.,  1.,  ..., -0., -1., -0.],\n",
      "        [-1.,  0., -0.,  ..., -0.,  0., -0.],\n",
      "        [-0., -0., -0.,  ...,  0.,  0., -1.],\n",
      "        ...,\n",
      "        [ 0.,  1.,  1.,  ..., -1., -1., -0.],\n",
      "        [-1., -1., -1.,  ...,  0.,  0., -0.],\n",
      "        [ 1., -0., -0.,  ..., -1.,  1.,  0.]])| \n",
      "[Quantized] model.layers.9.self_attn.v_proj | Ternary weights applied.\n",
      "[DEBUG] Quantized: tensor([[-0.,  0.,  1.,  ...,  0.,  0., -0.],\n",
      "        [-1.,  1.,  0.,  ..., -1.,  1., -0.],\n",
      "        [ 0., -0.,  0.,  ..., -0.,  0.,  1.],\n",
      "        ...,\n",
      "        [-1., -0., -1.,  ...,  1., -1.,  1.],\n",
      "        [ 0.,  0.,  1.,  ...,  0., -1., -1.],\n",
      "        [-1., -1.,  0.,  ...,  1.,  1., -0.]])| \n",
      "[Quantized] model.layers.9.self_attn.o_proj | Ternary weights applied.\n",
      "[DEBUG] Quantized: tensor([[ 1., -1., -1.,  ..., -1., -0.,  1.],\n",
      "        [-1.,  1.,  1.,  ...,  1.,  0.,  0.],\n",
      "        [ 1., -1.,  1.,  ..., -0., -1., -1.],\n",
      "        ...,\n",
      "        [-1.,  1.,  1.,  ..., -1.,  1., -1.],\n",
      "        [-0., -1.,  0.,  ..., -0.,  1., -1.],\n",
      "        [ 1.,  1.,  1.,  ..., -1., -1.,  1.]])| \n",
      "[Quantized] model.layers.9.mlp.gate_proj | Ternary weights applied.\n",
      "[DEBUG] Quantized: tensor([[ 1., -0., -1.,  ...,  0., -1.,  1.],\n",
      "        [-1., -0.,  0.,  ...,  1.,  0., -0.],\n",
      "        [ 0.,  1., -1.,  ..., -1., -0.,  0.],\n",
      "        ...,\n",
      "        [-1., -1., -1.,  ..., -1.,  1.,  1.],\n",
      "        [ 1.,  1., -1.,  ...,  1., -1., -1.],\n",
      "        [ 1.,  0.,  1.,  ..., -0.,  1.,  0.]])| \n",
      "[Quantized] model.layers.9.mlp.up_proj | Ternary weights applied.\n",
      "[DEBUG] Quantized: tensor([[ 0., -0.,  0.,  ..., -1., -0., -1.],\n",
      "        [-1.,  1.,  1.,  ...,  0.,  0., -1.],\n",
      "        [ 1., -0., -1.,  ..., -1., -1., -1.],\n",
      "        ...,\n",
      "        [-1., -1., -0.,  ...,  1., -1.,  0.],\n",
      "        [ 1.,  1., -0.,  ...,  1., -0., -1.],\n",
      "        [-1.,  1., -1.,  ..., -1., -0., -1.]])| \n",
      "[Quantized] model.layers.9.mlp.down_proj | Ternary weights applied.\n",
      "[DEBUG] Quantized: tensor([[ 0.,  1.,  1.,  ..., -1.,  1.,  0.],\n",
      "        [-0.,  0., -1.,  ..., -0.,  1., -1.],\n",
      "        [-1., -0., -0.,  ...,  0., -1.,  1.],\n",
      "        ...,\n",
      "        [-1.,  1.,  1.,  ...,  1., -0.,  1.],\n",
      "        [ 0.,  0.,  0.,  ...,  1.,  1., -1.],\n",
      "        [ 1.,  1.,  1.,  ..., -1.,  1.,  0.]])| \n",
      "[Quantized] model.layers.10.self_attn.q_proj | Ternary weights applied.\n",
      "[DEBUG] Quantized: tensor([[ 0., -0., -1.,  ..., -0.,  1., -1.],\n",
      "        [ 1.,  1., -0.,  ..., -0., -0., -1.],\n",
      "        [ 0., -0., -0.,  ...,  0., -1.,  1.],\n",
      "        ...,\n",
      "        [-1.,  1., -0.,  ..., -1., -1., -1.],\n",
      "        [-1., -0.,  1.,  ..., -1.,  1., -1.],\n",
      "        [-1., -1.,  0.,  ..., -1., -1.,  1.]])| \n",
      "[Quantized] model.layers.10.self_attn.k_proj | Ternary weights applied.\n",
      "[DEBUG] Quantized: tensor([[-1.,  0., -1.,  ...,  0., -1., -1.],\n",
      "        [-0.,  1.,  1.,  ...,  1.,  1., -1.],\n",
      "        [-0.,  1.,  0.,  ...,  1.,  1.,  1.],\n",
      "        ...,\n",
      "        [ 1., -0., -1.,  ...,  0.,  1., -1.],\n",
      "        [-1., -1.,  1.,  ..., -0.,  0., -0.],\n",
      "        [ 1.,  0.,  0.,  ...,  1.,  1., -1.]])| \n",
      "[Quantized] model.layers.10.self_attn.v_proj | Ternary weights applied.\n",
      "[DEBUG] Quantized: tensor([[-1.,  1., -1.,  ...,  1., -1.,  1.],\n",
      "        [-1.,  0., -1.,  ..., -0., -1.,  1.],\n",
      "        [ 0., -0.,  0.,  ..., -1.,  0.,  1.],\n",
      "        ...,\n",
      "        [ 0.,  1., -1.,  ..., -1., -0.,  1.],\n",
      "        [-1.,  1.,  0.,  ...,  0.,  0.,  0.],\n",
      "        [-0., -1.,  1.,  ...,  1., -1., -1.]])| \n",
      "[Quantized] model.layers.10.self_attn.o_proj | Ternary weights applied.\n",
      "[DEBUG] Quantized: tensor([[-0., -1.,  1.,  ..., -1., -0., -0.],\n",
      "        [ 1.,  1., -0.,  ...,  0.,  0., -1.],\n",
      "        [ 0.,  1., -1.,  ..., -1., -0.,  1.],\n",
      "        ...,\n",
      "        [ 1., -1., -1.,  ...,  0.,  0., -0.],\n",
      "        [ 1., -1.,  1.,  ..., -1., -1.,  1.],\n",
      "        [ 0.,  1., -1.,  ..., -1., -1.,  0.]])| \n",
      "[Quantized] model.layers.10.mlp.gate_proj | Ternary weights applied.\n",
      "[DEBUG] Quantized: tensor([[ 0., -0., -1.,  ..., -1.,  0., -0.],\n",
      "        [ 0.,  0.,  1.,  ..., -0., -1., -1.],\n",
      "        [-0.,  0., -1.,  ..., -1., -0., -0.],\n",
      "        ...,\n",
      "        [ 1.,  0., -1.,  ...,  1., -1.,  1.],\n",
      "        [ 1., -1.,  1.,  ...,  1.,  0., -1.],\n",
      "        [-0., -1., -1.,  ..., -0., -1., -0.]])| \n",
      "[Quantized] model.layers.10.mlp.up_proj | Ternary weights applied.\n",
      "[DEBUG] Quantized: tensor([[ 0., -1., -0.,  ...,  1., -1.,  0.],\n",
      "        [ 1.,  1.,  1.,  ..., -1.,  0.,  1.],\n",
      "        [ 1., -1., -1.,  ..., -0.,  1., -1.],\n",
      "        ...,\n",
      "        [ 1.,  1.,  1.,  ...,  1.,  1., -0.],\n",
      "        [-0.,  1., -1.,  ...,  1.,  0.,  0.],\n",
      "        [ 1.,  1.,  1.,  ...,  1., -0.,  0.]])| \n",
      "[Quantized] model.layers.10.mlp.down_proj | Ternary weights applied.\n",
      "[DEBUG] Quantized: tensor([[ 1.,  1.,  1.,  ...,  1., -0., -0.],\n",
      "        [ 0.,  0.,  1.,  ...,  1.,  1., -0.],\n",
      "        [ 1.,  0.,  1.,  ...,  0., -0.,  0.],\n",
      "        ...,\n",
      "        [ 1., -1., -1.,  ..., -0.,  1., -1.],\n",
      "        [-1.,  1.,  1.,  ..., -1., -1., -0.],\n",
      "        [-1., -0., -1.,  ...,  1., -1., -0.]])| \n",
      "[Quantized] model.layers.11.self_attn.q_proj | Ternary weights applied.\n",
      "[DEBUG] Quantized: tensor([[-0.,  1.,  1.,  ...,  0., -0.,  0.],\n",
      "        [-0.,  1.,  1.,  ...,  1.,  1., -0.],\n",
      "        [-0.,  0.,  0.,  ...,  1.,  1., -1.],\n",
      "        ...,\n",
      "        [ 1.,  1.,  0.,  ...,  0., -1., -1.],\n",
      "        [-0.,  1., -0.,  ...,  1., -1., -0.],\n",
      "        [-0.,  1.,  1.,  ...,  1., -1.,  1.]])| \n",
      "[Quantized] model.layers.11.self_attn.k_proj | Ternary weights applied.\n",
      "[DEBUG] Quantized: tensor([[-1.,  1.,  1.,  ..., -1.,  1.,  0.],\n",
      "        [-1., -1.,  0.,  ...,  0.,  1.,  0.],\n",
      "        [ 1.,  1.,  1.,  ...,  0., -0.,  1.],\n",
      "        ...,\n",
      "        [ 1., -1.,  0.,  ..., -1., -1., -1.],\n",
      "        [-1., -1., -1.,  ...,  0.,  1.,  0.],\n",
      "        [-1., -1., -1.,  ..., -0., -1., -0.]])| \n",
      "[Quantized] model.layers.11.self_attn.v_proj | Ternary weights applied.\n",
      "[DEBUG] Quantized: tensor([[-0.,  0., -1.,  ..., -1.,  1.,  1.],\n",
      "        [ 1.,  0.,  0.,  ...,  1.,  1., -0.],\n",
      "        [ 1., -0.,  1.,  ..., -1.,  1.,  1.],\n",
      "        ...,\n",
      "        [-0.,  1.,  1.,  ..., -0., -1., -1.],\n",
      "        [-0.,  1., -1.,  ...,  1., -1.,  1.],\n",
      "        [ 1.,  0.,  1.,  ...,  1.,  1., -1.]])| \n",
      "[Quantized] model.layers.11.self_attn.o_proj | Ternary weights applied.\n",
      "[DEBUG] Quantized: tensor([[ 1.,  1., -1.,  ...,  1., -1.,  0.],\n",
      "        [ 1.,  1.,  1.,  ...,  1., -1., -1.],\n",
      "        [-0., -0.,  0.,  ...,  1.,  1.,  1.],\n",
      "        ...,\n",
      "        [ 1.,  1., -1.,  ..., -1., -0., -1.],\n",
      "        [-0., -1.,  1.,  ...,  1., -1.,  0.],\n",
      "        [ 0., -1., -1.,  ..., -0., -1., -0.]])| \n",
      "[Quantized] model.layers.11.mlp.gate_proj | Ternary weights applied.\n",
      "[DEBUG] Quantized: tensor([[ 1.,  1.,  1.,  ...,  1.,  0.,  1.],\n",
      "        [-1., -1., -0.,  ...,  1.,  1.,  0.],\n",
      "        [-0.,  0., -1.,  ...,  1.,  1.,  1.],\n",
      "        ...,\n",
      "        [-1., -0., -1.,  ..., -0., -1.,  1.],\n",
      "        [ 0.,  0.,  1.,  ...,  0., -0.,  0.],\n",
      "        [ 0.,  1.,  1.,  ...,  1., -0.,  1.]])| \n",
      "[Quantized] model.layers.11.mlp.up_proj | Ternary weights applied.\n",
      "[DEBUG] Quantized: tensor([[-0.,  0.,  1.,  ...,  1., -1., -1.],\n",
      "        [ 0.,  0.,  0.,  ...,  1.,  0.,  0.],\n",
      "        [-0.,  1., -0.,  ...,  1., -1., -1.],\n",
      "        ...,\n",
      "        [ 1., -0., -0.,  ...,  1.,  0.,  1.],\n",
      "        [-0.,  1., -1.,  ..., -1., -0., -1.],\n",
      "        [-0., -1., -1.,  ..., -1., -1., -1.]])| \n",
      "[Quantized] model.layers.11.mlp.down_proj | Ternary weights applied.\n",
      "[DEBUG] Quantized: tensor([[ 1.,  0.,  0.,  ..., -0.,  0., -0.],\n",
      "        [ 1.,  1., -0.,  ...,  1., -1.,  1.],\n",
      "        [-0., -0., -0.,  ...,  1.,  1.,  0.],\n",
      "        ...,\n",
      "        [ 1.,  1.,  1.,  ...,  1.,  1., -0.],\n",
      "        [ 0.,  1.,  0.,  ..., -0.,  0., -1.],\n",
      "        [-0., -0.,  0.,  ...,  0.,  1.,  1.]])| \n",
      "[Quantized] model.layers.12.self_attn.q_proj | Ternary weights applied.\n",
      "[DEBUG] Quantized: tensor([[-0.,  1.,  0.,  ..., -0., -0.,  1.],\n",
      "        [-1.,  1.,  1.,  ..., -0., -0.,  1.],\n",
      "        [ 1., -0., -1.,  ..., -1.,  1., -1.],\n",
      "        ...,\n",
      "        [ 0.,  1., -1.,  ...,  0.,  1., -1.],\n",
      "        [ 1.,  1.,  1.,  ..., -1.,  0., -1.],\n",
      "        [-1., -1.,  0.,  ..., -0.,  1., -1.]])| \n",
      "[Quantized] model.layers.12.self_attn.k_proj | Ternary weights applied.\n",
      "[DEBUG] Quantized: tensor([[ 1.,  1., -1.,  ..., -1.,  1.,  1.],\n",
      "        [-1., -1.,  1.,  ...,  1.,  1.,  1.],\n",
      "        [-1.,  1., -1.,  ..., -1., -0.,  1.],\n",
      "        ...,\n",
      "        [ 1.,  0., -1.,  ...,  1., -1.,  0.],\n",
      "        [ 1., -0., -0.,  ...,  1.,  0.,  1.],\n",
      "        [ 1.,  0., -0.,  ..., -1.,  1., -0.]])| \n",
      "[Quantized] model.layers.12.self_attn.v_proj | Ternary weights applied.\n",
      "[DEBUG] Quantized: tensor([[-0.,  1.,  1.,  ...,  1.,  0., -0.],\n",
      "        [-0., -1., -1.,  ..., -0.,  1.,  0.],\n",
      "        [ 1.,  1., -1.,  ...,  0., -0., -0.],\n",
      "        ...,\n",
      "        [-1., -1.,  1.,  ..., -0., -0., -1.],\n",
      "        [ 1., -1.,  1.,  ...,  1., -1.,  0.],\n",
      "        [-0., -1.,  0.,  ..., -0., -1., -1.]])| \n",
      "[Quantized] model.layers.12.self_attn.o_proj | Ternary weights applied.\n",
      "[DEBUG] Quantized: tensor([[ 1., -0., -0.,  ...,  1., -1., -0.],\n",
      "        [-1.,  1., -1.,  ..., -1.,  0.,  0.],\n",
      "        [-1.,  1., -1.,  ..., -1., -1.,  0.],\n",
      "        ...,\n",
      "        [ 1.,  1., -1.,  ..., -1.,  1.,  0.],\n",
      "        [-1., -1., -1.,  ...,  0., -0., -1.],\n",
      "        [ 1., -0., -1.,  ..., -1.,  0.,  0.]])| \n",
      "[Quantized] model.layers.12.mlp.gate_proj | Ternary weights applied.\n",
      "[DEBUG] Quantized: tensor([[-0., -1.,  0.,  ...,  1., -1.,  0.],\n",
      "        [-1.,  1., -1.,  ..., -1., -1., -0.],\n",
      "        [ 0.,  1.,  0.,  ..., -1.,  1., -1.],\n",
      "        ...,\n",
      "        [ 0., -1.,  1.,  ...,  0.,  1., -1.],\n",
      "        [-1., -1., -1.,  ..., -1.,  1., -1.],\n",
      "        [ 1., -1., -1.,  ..., -1.,  1., -1.]])| \n",
      "[Quantized] model.layers.12.mlp.up_proj | Ternary weights applied.\n",
      "[DEBUG] Quantized: tensor([[-0.,  1.,  0.,  ..., -0., -1., -1.],\n",
      "        [ 0., -0.,  1.,  ..., -1., -0.,  1.],\n",
      "        [ 1., -1., -1.,  ...,  1.,  1.,  1.],\n",
      "        ...,\n",
      "        [ 0., -1.,  0.,  ...,  1., -0., -0.],\n",
      "        [ 0., -0., -1.,  ...,  1.,  0.,  0.],\n",
      "        [ 0.,  1.,  1.,  ..., -1., -1., -1.]])| \n",
      "[Quantized] model.layers.12.mlp.down_proj | Ternary weights applied.\n",
      "[DEBUG] Quantized: tensor([[-0., -1.,  1.,  ...,  0., -0., -1.],\n",
      "        [ 0., -1., -1.,  ..., -0., -1., -1.],\n",
      "        [-0., -1., -1.,  ..., -1., -1.,  0.],\n",
      "        ...,\n",
      "        [ 1.,  1.,  1.,  ...,  0.,  1., -0.],\n",
      "        [ 1.,  1., -0.,  ...,  1.,  1., -1.],\n",
      "        [-1.,  0.,  0.,  ...,  0., -1.,  0.]])| \n",
      "[Quantized] model.layers.13.self_attn.q_proj | Ternary weights applied.\n",
      "[DEBUG] Quantized: tensor([[ 0., -1.,  1.,  ..., -1.,  0.,  0.],\n",
      "        [ 0., -0.,  1.,  ...,  1., -1.,  0.],\n",
      "        [-1., -1., -0.,  ...,  0., -1.,  1.],\n",
      "        ...,\n",
      "        [-1., -1.,  0.,  ...,  1.,  0., -0.],\n",
      "        [ 1.,  1.,  1.,  ...,  1., -1., -1.],\n",
      "        [-1., -1., -1.,  ...,  1., -1., -0.]])| \n",
      "[Quantized] model.layers.13.self_attn.k_proj | Ternary weights applied.\n",
      "[DEBUG] Quantized: tensor([[-1., -0.,  1.,  ...,  0.,  1., -0.],\n",
      "        [-1., -0.,  1.,  ...,  0., -0., -1.],\n",
      "        [-1., -1.,  1.,  ...,  0., -0., -1.],\n",
      "        ...,\n",
      "        [-1.,  0.,  1.,  ..., -1.,  1.,  1.],\n",
      "        [-0., -1., -0.,  ...,  0.,  0.,  0.],\n",
      "        [-0.,  1., -0.,  ...,  0., -0., -0.]])| \n",
      "[Quantized] model.layers.13.self_attn.v_proj | Ternary weights applied.\n",
      "[DEBUG] Quantized: tensor([[ 1.,  1., -0.,  ...,  1., -1.,  1.],\n",
      "        [-0.,  1.,  1.,  ...,  0., -1., -0.],\n",
      "        [ 0., -1., -0.,  ...,  1., -1., -0.],\n",
      "        ...,\n",
      "        [-1., -0.,  1.,  ..., -1., -0., -1.],\n",
      "        [ 1.,  1., -1.,  ..., -0., -1., -0.],\n",
      "        [-0., -0., -1.,  ..., -1.,  1., -1.]])| \n",
      "[Quantized] model.layers.13.self_attn.o_proj | Ternary weights applied.\n",
      "[DEBUG] Quantized: tensor([[-0., -1.,  1.,  ...,  1.,  0., -1.],\n",
      "        [-1.,  0., -1.,  ...,  1., -0.,  1.],\n",
      "        [ 0.,  0.,  1.,  ...,  0.,  0.,  1.],\n",
      "        ...,\n",
      "        [ 1.,  1.,  0.,  ...,  1., -0., -0.],\n",
      "        [-0., -0.,  0.,  ...,  1.,  1.,  1.],\n",
      "        [ 1., -1., -0.,  ...,  0.,  1.,  0.]])| \n",
      "[Quantized] model.layers.13.mlp.gate_proj | Ternary weights applied.\n",
      "[DEBUG] Quantized: tensor([[-1., -1., -1.,  ...,  1., -1.,  0.],\n",
      "        [-1.,  0., -0.,  ..., -1.,  0.,  1.],\n",
      "        [ 1., -0., -1.,  ...,  1.,  0.,  0.],\n",
      "        ...,\n",
      "        [ 1.,  1., -0.,  ...,  1., -0.,  1.],\n",
      "        [ 0.,  0., -1.,  ...,  1.,  1.,  0.],\n",
      "        [ 1.,  1., -1.,  ..., -1., -1.,  1.]])| \n",
      "[Quantized] model.layers.13.mlp.up_proj | Ternary weights applied.\n",
      "[DEBUG] Quantized: tensor([[-0., -0., -0.,  ..., -1.,  0., -1.],\n",
      "        [ 0.,  1.,  0.,  ...,  0.,  1.,  1.],\n",
      "        [-0., -1.,  0.,  ...,  0.,  0.,  1.],\n",
      "        ...,\n",
      "        [-1.,  1., -1.,  ..., -1., -1., -1.],\n",
      "        [ 1.,  1., -0.,  ..., -0.,  0., -0.],\n",
      "        [ 1., -0., -1.,  ...,  1.,  0., -1.]])| \n",
      "[Quantized] model.layers.13.mlp.down_proj | Ternary weights applied.\n",
      "[DEBUG] Quantized: tensor([[-0.,  0., -0.,  ...,  0., -0.,  0.],\n",
      "        [-0.,  0.,  0.,  ..., -0., -0.,  0.],\n",
      "        [ 1.,  0.,  0.,  ...,  1., -0., -0.],\n",
      "        ...,\n",
      "        [ 1.,  0.,  0.,  ..., -1.,  1.,  0.],\n",
      "        [-1.,  1.,  0.,  ...,  1., -1., -1.],\n",
      "        [-0.,  0., -1.,  ..., -0.,  0.,  1.]])| \n",
      "[Quantized] model.layers.14.self_attn.q_proj | Ternary weights applied.\n",
      "[DEBUG] Quantized: tensor([[ 0.,  0.,  0.,  ...,  1., -0., -0.],\n",
      "        [-0.,  0., -0.,  ..., -0.,  0.,  0.],\n",
      "        [ 0.,  0., -0.,  ..., -1., -0., -0.],\n",
      "        ...,\n",
      "        [ 1., -1., -1.,  ..., -1., -1., -0.],\n",
      "        [ 1.,  1.,  1.,  ...,  1., -1., -1.],\n",
      "        [-1.,  1., -1.,  ..., -0.,  1., -0.]])| \n",
      "[Quantized] model.layers.14.self_attn.k_proj | Ternary weights applied.\n",
      "[DEBUG] Quantized: tensor([[-1.,  1.,  0.,  ...,  1., -0.,  1.],\n",
      "        [-0.,  0., -0.,  ...,  0., -1., -1.],\n",
      "        [ 1.,  1.,  1.,  ..., -0., -1., -1.],\n",
      "        ...,\n",
      "        [ 1.,  0., -0.,  ..., -1., -1.,  1.],\n",
      "        [ 1.,  1., -0.,  ..., -1., -1., -0.],\n",
      "        [ 0., -0., -1.,  ..., -0.,  1., -1.]])| \n",
      "[Quantized] model.layers.14.self_attn.v_proj | Ternary weights applied.\n",
      "[DEBUG] Quantized: tensor([[-1.,  1.,  1.,  ...,  1., -1., -0.],\n",
      "        [ 1., -0., -0.,  ..., -0.,  1., -1.],\n",
      "        [ 1., -1.,  1.,  ...,  1., -0., -1.],\n",
      "        ...,\n",
      "        [-0.,  1.,  1.,  ...,  0.,  1.,  1.],\n",
      "        [-0.,  0.,  1.,  ...,  1., -1.,  1.],\n",
      "        [ 1., -1., -0.,  ...,  1., -1., -1.]])| \n",
      "[Quantized] model.layers.14.self_attn.o_proj | Ternary weights applied.\n",
      "[DEBUG] Quantized: tensor([[-1., -0.,  1.,  ...,  1., -0., -0.],\n",
      "        [-1.,  0., -0.,  ...,  1., -1.,  1.],\n",
      "        [-1.,  1.,  1.,  ...,  0., -1.,  1.],\n",
      "        ...,\n",
      "        [ 1.,  0.,  0.,  ...,  0., -1., -0.],\n",
      "        [-1.,  0.,  1.,  ...,  1., -1.,  1.],\n",
      "        [ 1., -1.,  0.,  ..., -0., -1., -1.]])| \n",
      "[Quantized] model.layers.14.mlp.gate_proj | Ternary weights applied.\n",
      "[DEBUG] Quantized: tensor([[-1.,  1.,  1.,  ..., -0., -0., -0.],\n",
      "        [-0.,  1., -0.,  ...,  1., -1., -1.],\n",
      "        [-1.,  0., -0.,  ..., -1., -0., -1.],\n",
      "        ...,\n",
      "        [ 1.,  1., -1.,  ...,  0.,  1., -0.],\n",
      "        [-0., -1., -1.,  ...,  1., -1.,  1.],\n",
      "        [-0., -0.,  1.,  ...,  1., -0., -1.]])| \n",
      "[Quantized] model.layers.14.mlp.up_proj | Ternary weights applied.\n",
      "[DEBUG] Quantized: tensor([[ 0.,  0.,  0.,  ..., -1.,  0., -0.],\n",
      "        [ 0.,  0., -1.,  ..., -0.,  1.,  1.],\n",
      "        [ 1., -1., -1.,  ...,  1.,  0.,  0.],\n",
      "        ...,\n",
      "        [-0.,  0., -1.,  ...,  1., -1., -1.],\n",
      "        [ 1.,  1.,  1.,  ..., -0.,  1.,  0.],\n",
      "        [ 1., -0., -0.,  ...,  0., -1., -0.]])| \n",
      "[Quantized] model.layers.14.mlp.down_proj | Ternary weights applied.\n",
      "[DEBUG] Quantized: tensor([[-0., -0., -1.,  ..., -1., -0., -1.],\n",
      "        [ 1.,  0.,  1.,  ...,  1.,  0.,  1.],\n",
      "        [ 0.,  0.,  0.,  ...,  0.,  0., -0.],\n",
      "        ...,\n",
      "        [ 1.,  1., -1.,  ...,  1., -1.,  1.],\n",
      "        [-1., -1.,  1.,  ...,  0.,  0.,  1.],\n",
      "        [ 0., -1., -1.,  ...,  1.,  0.,  1.]])| \n",
      "[Quantized] model.layers.15.self_attn.q_proj | Ternary weights applied.\n",
      "[DEBUG] Quantized: tensor([[-0.,  0., -0.,  ..., -0., -0.,  0.],\n",
      "        [ 0., -0.,  0.,  ...,  0., -0., -0.],\n",
      "        [-1.,  0., -0.,  ..., -0., -1.,  0.],\n",
      "        ...,\n",
      "        [-1.,  1.,  1.,  ...,  1.,  1.,  1.],\n",
      "        [-0.,  1.,  1.,  ...,  1.,  1.,  1.],\n",
      "        [-0.,  1., -1.,  ...,  1.,  1.,  1.]])| \n",
      "[Quantized] model.layers.15.self_attn.k_proj | Ternary weights applied.\n",
      "[DEBUG] Quantized: tensor([[ 0.,  1.,  1.,  ...,  1.,  1., -0.],\n",
      "        [-1.,  1.,  0.,  ...,  1., -1.,  0.],\n",
      "        [-1., -1., -0.,  ...,  1., -1.,  1.],\n",
      "        ...,\n",
      "        [-1.,  1., -0.,  ..., -1.,  1.,  1.],\n",
      "        [-1.,  1.,  0.,  ...,  0.,  0., -0.],\n",
      "        [ 1.,  1., -1.,  ...,  0.,  0., -1.]])| \n",
      "[Quantized] model.layers.15.self_attn.v_proj | Ternary weights applied.\n",
      "[DEBUG] Quantized: tensor([[ 1., -0., -1.,  ..., -1., -1.,  0.],\n",
      "        [ 0.,  1., -1.,  ...,  1., -0., -0.],\n",
      "        [-1., -1., -0.,  ...,  0.,  0., -1.],\n",
      "        ...,\n",
      "        [ 1.,  1.,  0.,  ..., -0., -0., -0.],\n",
      "        [ 0.,  1.,  0.,  ...,  1.,  0.,  1.],\n",
      "        [ 1.,  1., -0.,  ...,  1., -0., -1.]])| \n",
      "[Quantized] model.layers.15.self_attn.o_proj | Ternary weights applied.\n",
      "[DEBUG] Quantized: tensor([[ 0.,  0., -0.,  ..., -0.,  1., -0.],\n",
      "        [-0.,  0.,  0.,  ..., -0.,  1.,  0.],\n",
      "        [-0.,  0.,  0.,  ..., -1., -1., -1.],\n",
      "        ...,\n",
      "        [-1.,  0., -1.,  ..., -1., -1.,  0.],\n",
      "        [-1.,  1.,  1.,  ..., -0., -1.,  1.],\n",
      "        [-0.,  1., -1.,  ..., -0., -1., -1.]])| \n",
      "[Quantized] model.layers.15.mlp.gate_proj | Ternary weights applied.\n",
      "[DEBUG] Quantized: tensor([[ 1., -1.,  0.,  ..., -1., -1.,  1.],\n",
      "        [-1., -1., -1.,  ..., -1.,  1., -1.],\n",
      "        [-1.,  1., -0.,  ..., -1.,  1., -1.],\n",
      "        ...,\n",
      "        [-0., -1., -1.,  ...,  1.,  1.,  0.],\n",
      "        [-0., -0.,  0.,  ...,  1., -0., -1.],\n",
      "        [-0., -1.,  1.,  ..., -0., -0.,  1.]])| \n",
      "[Quantized] model.layers.15.mlp.up_proj | Ternary weights applied.\n",
      "[DEBUG] Quantized: tensor([[-1.,  1.,  0.,  ...,  0.,  0.,  1.],\n",
      "        [ 0.,  0., -1.,  ...,  0.,  1.,  1.],\n",
      "        [ 0., -1.,  1.,  ...,  1.,  1., -0.],\n",
      "        ...,\n",
      "        [ 1.,  1.,  1.,  ..., -1.,  1.,  1.],\n",
      "        [ 0., -1.,  0.,  ..., -1., -0.,  1.],\n",
      "        [-1., -1.,  1.,  ...,  1., -0.,  0.]])| \n",
      "[Quantized] model.layers.15.mlp.down_proj | Ternary weights applied.\n",
      ">> [STEP 4]: Freezing modules...\n",
      "model.layers.0.self_attn.q_proj mean act: 8.87666828930378e-10\n",
      "model.layers.0.self_attn.k_proj mean act: 8.87666828930378e-10\n",
      "model.layers.0.self_attn.v_proj mean act: 8.87666828930378e-10\n",
      "model.layers.0.self_attn.o_proj mean act: -0.0011488111922517419\n",
      "model.layers.0.mlp.gate_proj mean act: 7.639755494892597e-10\n",
      "model.layers.0.mlp.up_proj mean act: 7.639755494892597e-10\n",
      "model.layers.0.mlp.down_proj mean act: 2.7965479603153653e-05\n",
      "model.layers.1.self_attn.q_proj mean act: -1.5861587598919868e-09\n",
      "model.layers.1.self_attn.k_proj mean act: -1.5861587598919868e-09\n",
      "model.layers.1.self_attn.v_proj mean act: -1.5861587598919868e-09\n",
      "model.layers.1.self_attn.o_proj mean act: 0.0022282390855252743\n",
      "model.layers.1.mlp.gate_proj mean act: -6.111804395914078e-10\n",
      "model.layers.1.mlp.up_proj mean act: -6.111804395914078e-10\n",
      "model.layers.1.mlp.down_proj mean act: 0.00015827080642338842\n",
      "model.layers.2.self_attn.q_proj mean act: 3.346940502524376e-10\n",
      "model.layers.2.self_attn.k_proj mean act: 3.346940502524376e-10\n",
      "model.layers.2.self_attn.v_proj mean act: 3.346940502524376e-10\n",
      "model.layers.2.self_attn.o_proj mean act: 0.0002892923657782376\n",
      "model.layers.2.mlp.gate_proj mean act: -1.7462298274040222e-10\n",
      "model.layers.2.mlp.up_proj mean act: -1.7462298274040222e-10\n",
      "model.layers.2.mlp.down_proj mean act: 0.00019596150377765298\n",
      "model.layers.3.self_attn.q_proj mean act: 1.2878444977104664e-09\n",
      "model.layers.3.self_attn.k_proj mean act: 1.2878444977104664e-09\n",
      "model.layers.3.self_attn.v_proj mean act: 1.2878444977104664e-09\n",
      "model.layers.3.self_attn.o_proj mean act: -0.002831411547958851\n",
      "model.layers.3.mlp.gate_proj mean act: 2.4010660126805305e-10\n",
      "model.layers.3.mlp.up_proj mean act: 2.4010660126805305e-10\n",
      "model.layers.3.mlp.down_proj mean act: 0.00021323279361240566\n",
      "model.layers.4.self_attn.q_proj mean act: -6.693881005048752e-10\n",
      "model.layers.4.self_attn.k_proj mean act: -6.693881005048752e-10\n",
      "model.layers.4.self_attn.v_proj mean act: -6.693881005048752e-10\n",
      "model.layers.4.self_attn.o_proj mean act: 0.00025702675338834524\n",
      "model.layers.4.mlp.gate_proj mean act: -4.94765117764473e-10\n",
      "model.layers.4.mlp.up_proj mean act: -4.94765117764473e-10\n",
      "model.layers.4.mlp.down_proj mean act: -0.0001371003600070253\n",
      "model.layers.5.self_attn.q_proj mean act: 1.0477378964424133e-09\n",
      "model.layers.5.self_attn.k_proj mean act: 1.0477378964424133e-09\n",
      "model.layers.5.self_attn.v_proj mean act: 1.0477378964424133e-09\n",
      "model.layers.5.self_attn.o_proj mean act: 0.0013972552260383964\n",
      "model.layers.5.mlp.gate_proj mean act: 1.0622898116707802e-09\n",
      "model.layers.5.mlp.up_proj mean act: 1.0622898116707802e-09\n",
      "model.layers.5.mlp.down_proj mean act: -0.0003018259012605995\n",
      "model.layers.6.self_attn.q_proj mean act: 1.0477378964424133e-09\n",
      "model.layers.6.self_attn.k_proj mean act: 1.0477378964424133e-09\n",
      "model.layers.6.self_attn.v_proj mean act: 1.0477378964424133e-09\n",
      "model.layers.6.self_attn.o_proj mean act: -0.0007478609913960099\n",
      "model.layers.6.mlp.gate_proj mean act: 6.257323548197746e-10\n",
      "model.layers.6.mlp.up_proj mean act: 6.257323548197746e-10\n",
      "model.layers.6.mlp.down_proj mean act: 0.0002366862609051168\n",
      "model.layers.7.self_attn.q_proj mean act: -5.748006515204906e-10\n",
      "model.layers.7.self_attn.k_proj mean act: -5.748006515204906e-10\n",
      "model.layers.7.self_attn.v_proj mean act: -5.748006515204906e-10\n",
      "model.layers.7.self_attn.o_proj mean act: -0.0010269536869600415\n",
      "model.layers.7.mlp.gate_proj mean act: 6.111804395914078e-10\n",
      "model.layers.7.mlp.up_proj mean act: 6.111804395914078e-10\n",
      "model.layers.7.mlp.down_proj mean act: 8.65115798660554e-05\n",
      "model.layers.8.self_attn.q_proj mean act: 1.6589183360338211e-09\n",
      "model.layers.8.self_attn.k_proj mean act: 1.6589183360338211e-09\n",
      "model.layers.8.self_attn.v_proj mean act: 1.6589183360338211e-09\n",
      "model.layers.8.self_attn.o_proj mean act: -0.0009135137079283595\n",
      "model.layers.8.mlp.gate_proj mean act: 3.637978807091713e-10\n",
      "model.layers.8.mlp.up_proj mean act: 3.637978807091713e-10\n",
      "model.layers.8.mlp.down_proj mean act: 0.00018601640476845205\n",
      "model.layers.9.self_attn.q_proj mean act: -1.3242242857813835e-09\n",
      "model.layers.9.self_attn.k_proj mean act: -1.3242242857813835e-09\n",
      "model.layers.9.self_attn.v_proj mean act: -1.3242242857813835e-09\n",
      "model.layers.9.self_attn.o_proj mean act: -0.0003377934917807579\n",
      "model.layers.9.mlp.gate_proj mean act: -1.6007106751203537e-10\n",
      "model.layers.9.mlp.up_proj mean act: -1.6007106751203537e-10\n",
      "model.layers.9.mlp.down_proj mean act: -0.0003582020872272551\n",
      "model.layers.10.self_attn.q_proj mean act: -2.0372681319713593e-10\n",
      "model.layers.10.self_attn.k_proj mean act: -2.0372681319713593e-10\n",
      "model.layers.10.self_attn.v_proj mean act: -2.0372681319713593e-10\n",
      "model.layers.10.self_attn.o_proj mean act: 0.0008150784415192902\n",
      "model.layers.10.mlp.gate_proj mean act: 3.7834979593753815e-10\n",
      "model.layers.10.mlp.up_proj mean act: 3.7834979593753815e-10\n",
      "model.layers.10.mlp.down_proj mean act: 0.000966481224168092\n",
      "model.layers.11.self_attn.q_proj mean act: -3.2014213502407074e-10\n",
      "model.layers.11.self_attn.k_proj mean act: -3.2014213502407074e-10\n",
      "model.layers.11.self_attn.v_proj mean act: -3.2014213502407074e-10\n",
      "model.layers.11.self_attn.o_proj mean act: 5.196294659981504e-05\n",
      "model.layers.11.mlp.gate_proj mean act: 1.0913936421275139e-09\n",
      "model.layers.11.mlp.up_proj mean act: 1.0913936421275139e-09\n",
      "model.layers.11.mlp.down_proj mean act: -0.00024113710969686508\n",
      "model.layers.12.self_attn.q_proj mean act: 1.3096723705530167e-10\n",
      "model.layers.12.self_attn.k_proj mean act: 1.3096723705530167e-10\n",
      "model.layers.12.self_attn.v_proj mean act: 1.3096723705530167e-10\n",
      "model.layers.12.self_attn.o_proj mean act: -0.00040222806273959577\n",
      "model.layers.12.mlp.gate_proj mean act: 1.1059455573558807e-09\n",
      "model.layers.12.mlp.up_proj mean act: 1.1059455573558807e-09\n",
      "model.layers.12.mlp.down_proj mean act: 0.00040862971218302846\n",
      "model.layers.13.self_attn.q_proj mean act: -6.83940015733242e-10\n",
      "model.layers.13.self_attn.k_proj mean act: -6.83940015733242e-10\n",
      "model.layers.13.self_attn.v_proj mean act: -6.83940015733242e-10\n",
      "model.layers.13.self_attn.o_proj mean act: -0.001347450539469719\n",
      "model.layers.13.mlp.gate_proj mean act: -2.3283064365386963e-10\n",
      "model.layers.13.mlp.up_proj mean act: -2.3283064365386963e-10\n",
      "model.layers.13.mlp.down_proj mean act: 0.00024800654500722885\n",
      "model.layers.14.self_attn.q_proj mean act: -6.402842700481415e-10\n",
      "model.layers.14.self_attn.k_proj mean act: -6.402842700481415e-10\n",
      "model.layers.14.self_attn.v_proj mean act: -6.402842700481415e-10\n",
      "model.layers.14.self_attn.o_proj mean act: 0.0020981580018997192\n",
      "model.layers.14.mlp.gate_proj mean act: -2.0372681319713593e-10\n",
      "model.layers.14.mlp.up_proj mean act: -2.0372681319713593e-10\n",
      "model.layers.14.mlp.down_proj mean act: 0.0001642597489990294\n",
      "model.layers.15.self_attn.q_proj mean act: -1.0477378964424133e-09\n",
      "model.layers.15.self_attn.k_proj mean act: -1.0477378964424133e-09\n",
      "model.layers.15.self_attn.v_proj mean act: -1.0477378964424133e-09\n",
      "model.layers.15.self_attn.o_proj mean act: -0.0010582030517980456\n",
      "model.layers.15.mlp.gate_proj mean act: 6.257323548197746e-10\n",
      "model.layers.15.mlp.up_proj mean act: 6.257323548197746e-10\n",
      "model.layers.15.mlp.down_proj mean act: 0.001395554980263114\n",
      "model.layers.0.self_attn.q_proj                    | act_scale: 1.000000\n",
      "model.layers.0.self_attn.k_proj                    | act_scale: 1.000000\n",
      "model.layers.0.self_attn.v_proj                    | act_scale: 1.000000\n",
      "model.layers.0.self_attn.o_proj                    | act_scale: 1.000000\n",
      "model.layers.0.mlp.gate_proj                       | act_scale: 1.000000\n",
      "model.layers.0.mlp.up_proj                         | act_scale: 1.000000\n",
      "model.layers.0.mlp.down_proj                       | act_scale: 1.000000\n",
      "model.layers.1.self_attn.q_proj                    | act_scale: 1.000000\n",
      "model.layers.1.self_attn.k_proj                    | act_scale: 1.000000\n",
      "model.layers.1.self_attn.v_proj                    | act_scale: 1.000000\n",
      "model.layers.1.self_attn.o_proj                    | act_scale: 1.000000\n",
      "model.layers.1.mlp.gate_proj                       | act_scale: 1.000000\n",
      "model.layers.1.mlp.up_proj                         | act_scale: 1.000000\n",
      "model.layers.1.mlp.down_proj                       | act_scale: 1.000000\n",
      "model.layers.2.self_attn.q_proj                    | act_scale: 1.000000\n",
      "model.layers.2.self_attn.k_proj                    | act_scale: 1.000000\n",
      "model.layers.2.self_attn.v_proj                    | act_scale: 1.000000\n",
      "model.layers.2.self_attn.o_proj                    | act_scale: 1.000000\n",
      "model.layers.2.mlp.gate_proj                       | act_scale: 1.000000\n",
      "model.layers.2.mlp.up_proj                         | act_scale: 1.000000\n",
      "model.layers.2.mlp.down_proj                       | act_scale: 1.000000\n",
      "model.layers.3.self_attn.q_proj                    | act_scale: 1.000000\n",
      "model.layers.3.self_attn.k_proj                    | act_scale: 1.000000\n",
      "model.layers.3.self_attn.v_proj                    | act_scale: 1.000000\n",
      "model.layers.3.self_attn.o_proj                    | act_scale: 1.000000\n",
      "model.layers.3.mlp.gate_proj                       | act_scale: 1.000000\n",
      "model.layers.3.mlp.up_proj                         | act_scale: 1.000000\n",
      "model.layers.3.mlp.down_proj                       | act_scale: 1.000000\n",
      "model.layers.4.self_attn.q_proj                    | act_scale: 1.000000\n",
      "model.layers.4.self_attn.k_proj                    | act_scale: 1.000000\n",
      "model.layers.4.self_attn.v_proj                    | act_scale: 1.000000\n",
      "model.layers.4.self_attn.o_proj                    | act_scale: 1.000000\n",
      "model.layers.4.mlp.gate_proj                       | act_scale: 1.000000\n",
      "model.layers.4.mlp.up_proj                         | act_scale: 1.000000\n",
      "model.layers.4.mlp.down_proj                       | act_scale: 1.000000\n",
      "model.layers.5.self_attn.q_proj                    | act_scale: 1.000000\n",
      "model.layers.5.self_attn.k_proj                    | act_scale: 1.000000\n",
      "model.layers.5.self_attn.v_proj                    | act_scale: 1.000000\n",
      "model.layers.5.self_attn.o_proj                    | act_scale: 1.000000\n",
      "model.layers.5.mlp.gate_proj                       | act_scale: 1.000000\n",
      "model.layers.5.mlp.up_proj                         | act_scale: 1.000000\n",
      "model.layers.5.mlp.down_proj                       | act_scale: 1.000000\n",
      "model.layers.6.self_attn.q_proj                    | act_scale: 1.000000\n",
      "model.layers.6.self_attn.k_proj                    | act_scale: 1.000000\n",
      "model.layers.6.self_attn.v_proj                    | act_scale: 1.000000\n",
      "model.layers.6.self_attn.o_proj                    | act_scale: 1.000000\n",
      "model.layers.6.mlp.gate_proj                       | act_scale: 1.000000\n",
      "model.layers.6.mlp.up_proj                         | act_scale: 1.000000\n",
      "model.layers.6.mlp.down_proj                       | act_scale: 1.000000\n",
      "model.layers.7.self_attn.q_proj                    | act_scale: 1.000000\n",
      "model.layers.7.self_attn.k_proj                    | act_scale: 1.000000\n",
      "model.layers.7.self_attn.v_proj                    | act_scale: 1.000000\n",
      "model.layers.7.self_attn.o_proj                    | act_scale: 1.000000\n",
      "model.layers.7.mlp.gate_proj                       | act_scale: 1.000000\n",
      "model.layers.7.mlp.up_proj                         | act_scale: 1.000000\n",
      "model.layers.7.mlp.down_proj                       | act_scale: 1.000000\n",
      "model.layers.8.self_attn.q_proj                    | act_scale: 1.000000\n",
      "model.layers.8.self_attn.k_proj                    | act_scale: 1.000000\n",
      "model.layers.8.self_attn.v_proj                    | act_scale: 1.000000\n",
      "model.layers.8.self_attn.o_proj                    | act_scale: 1.000000\n",
      "model.layers.8.mlp.gate_proj                       | act_scale: 1.000000\n",
      "model.layers.8.mlp.up_proj                         | act_scale: 1.000000\n",
      "model.layers.8.mlp.down_proj                       | act_scale: 1.000000\n",
      "model.layers.9.self_attn.q_proj                    | act_scale: 1.000000\n",
      "model.layers.9.self_attn.k_proj                    | act_scale: 1.000000\n",
      "model.layers.9.self_attn.v_proj                    | act_scale: 1.000000\n",
      "model.layers.9.self_attn.o_proj                    | act_scale: 1.000000\n",
      "model.layers.9.mlp.gate_proj                       | act_scale: 1.000000\n",
      "model.layers.9.mlp.up_proj                         | act_scale: 1.000000\n",
      "model.layers.9.mlp.down_proj                       | act_scale: 1.000000\n",
      "model.layers.10.self_attn.q_proj                   | act_scale: 1.000000\n",
      "model.layers.10.self_attn.k_proj                   | act_scale: 1.000000\n",
      "model.layers.10.self_attn.v_proj                   | act_scale: 1.000000\n",
      "model.layers.10.self_attn.o_proj                   | act_scale: 1.000000\n",
      "model.layers.10.mlp.gate_proj                      | act_scale: 1.000000\n",
      "model.layers.10.mlp.up_proj                        | act_scale: 1.000000\n",
      "model.layers.10.mlp.down_proj                      | act_scale: 1.000000\n",
      "model.layers.11.self_attn.q_proj                   | act_scale: 1.000000\n",
      "model.layers.11.self_attn.k_proj                   | act_scale: 1.000000\n",
      "model.layers.11.self_attn.v_proj                   | act_scale: 1.000000\n",
      "model.layers.11.self_attn.o_proj                   | act_scale: 1.000000\n",
      "model.layers.11.mlp.gate_proj                      | act_scale: 1.000000\n",
      "model.layers.11.mlp.up_proj                        | act_scale: 1.000000\n",
      "model.layers.11.mlp.down_proj                      | act_scale: 1.000000\n",
      "model.layers.12.self_attn.q_proj                   | act_scale: 1.000000\n",
      "model.layers.12.self_attn.k_proj                   | act_scale: 1.000000\n",
      "model.layers.12.self_attn.v_proj                   | act_scale: 1.000000\n",
      "model.layers.12.self_attn.o_proj                   | act_scale: 1.000000\n",
      "model.layers.12.mlp.gate_proj                      | act_scale: 1.000000\n",
      "model.layers.12.mlp.up_proj                        | act_scale: 1.000000\n",
      "model.layers.12.mlp.down_proj                      | act_scale: 1.000000\n",
      "model.layers.13.self_attn.q_proj                   | act_scale: 1.000000\n",
      "model.layers.13.self_attn.k_proj                   | act_scale: 1.000000\n",
      "model.layers.13.self_attn.v_proj                   | act_scale: 1.000000\n",
      "model.layers.13.self_attn.o_proj                   | act_scale: 1.000000\n",
      "model.layers.13.mlp.gate_proj                      | act_scale: 1.000000\n",
      "model.layers.13.mlp.up_proj                        | act_scale: 1.000000\n",
      "model.layers.13.mlp.down_proj                      | act_scale: 1.000000\n",
      "model.layers.14.self_attn.q_proj                   | act_scale: 1.000000\n",
      "model.layers.14.self_attn.k_proj                   | act_scale: 1.000000\n",
      "model.layers.14.self_attn.v_proj                   | act_scale: 1.000000\n",
      "model.layers.14.self_attn.o_proj                   | act_scale: 1.000000\n",
      "model.layers.14.mlp.gate_proj                      | act_scale: 1.000000\n",
      "model.layers.14.mlp.up_proj                        | act_scale: 1.000000\n",
      "model.layers.14.mlp.down_proj                      | act_scale: 1.000000\n",
      "model.layers.15.self_attn.q_proj                   | act_scale: 1.000000\n",
      "model.layers.15.self_attn.k_proj                   | act_scale: 1.000000\n",
      "model.layers.15.self_attn.v_proj                   | act_scale: 1.000000\n",
      "model.layers.15.self_attn.o_proj                   | act_scale: 1.000000\n",
      "model.layers.15.mlp.gate_proj                      | act_scale: 1.000000\n",
      "model.layers.15.mlp.up_proj                        | act_scale: 1.000000\n",
      "model.layers.15.mlp.down_proj                      | act_scale: 1.000000\n"
     ]
    }
   ],
   "source": [
    "olmo1b_bitnet_fp32_ptsq = applyPTQ(\n",
    "    load_test_model(FPKey.OLMO1B_FP.value, dtype=torch.float32),\n",
    "    tokenizer=olmo1b_tokenizer,\n",
    "    #calibration_input=None,\n",
    "    #calibration_input=sub_txts['text'],\n",
    "    calibration_input=Texts.T1.value,\n",
    "    mode='1.58bit',\n",
    "    safer_quant=False,\n",
    "    q_safety_layers=None,\n",
    "    model_half=False,\n",
    "    quant_half=False,\n",
    "    layers_to_quant_weights=QuantStyle.BITNET.value,\n",
    "    layers_to_quant_activations=QuantStyle.BITNET.value,\n",
    "    fragile_layers=False,\n",
    "    act_quant=True,\n",
    "    act_bits=8,\n",
    "    torch_backends=False,\n",
    "    debugging=True,\n",
    "    plot_debugging=False,\n",
    "    plot_quantization=False,\n",
    "    freeze_modules=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "olmo2t_bitnet_fp32_ptsq = applyPTQ(\n",
    "    load_test_model(FPKey.OLMO7B2T_FP.value, dtype=torch.float32),\n",
    "    tokenizer=olmo2t_tokenizer,\n",
    "    #calibration_input=None,\n",
    "    #calibration_input=sub_txts['text'],\n",
    "    calibration_input=Texts.T1.value,\n",
    "    mode='1.58bit',\n",
    "    safer_quant=True,\n",
    "    q_lmhead=True,\n",
    "    model_half=False,\n",
    "    quant_half=False,\n",
    "    layers_to_quant_weights=QuantStyle.BITNET.value,\n",
    "    layers_to_quant_activations=QuantStyle.BITNET.value,\n",
    "    fragile_layers=False,\n",
    "    act_quant=True,\n",
    "    act_bits=8,\n",
    "    torch_backends=False,\n",
    "    debugging=True,\n",
    "    plot_debugging=False,\n",
    "    plot_quantization=False,\n",
    "    freeze_modules=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model with 8-bit quantization using BNB\n",
    "olmo1b_bnb8_float32 = AutoModelForCausalLM.from_pretrained(\n",
    "    FPKey.OLMO1B_FP.value,           \n",
    "    #torch_dtype=torch.uint8,     # Specify the dtype (for 8-bit, use uint8)\n",
    "    #device_map=\"cpu\",           # Automatically map the model to available devices (GPU/CPU)\n",
    "    load_in_8bit=True,\n",
    "    return_dict=True,\n",
    "    output_hidden_states=True,            # Set to True for 8-bit quantization\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "olmo1b_bnb8_float32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.check_model_dtypes(olmo1b_bnb8_float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model with 4-bit quantization using BNB\n",
    "olmo1b_bnb4_float32 = AutoModelForCausalLM.from_pretrained(\n",
    "    FPKey.OLMO1B_FP.value,           \n",
    "    #torch_dtype=torch.float32,     # This would still use uint8 or another type based on quantization method\n",
    "    #device_map=\"auto\",           # Automatically map the model to available devices (GPU/CPU)\n",
    "    load_in_4bit=True,            # Set to True for 4-bit quantization (if available)\n",
    "    return_dict=True,\n",
    "    output_hidden_states=True, \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "olmo1b_bnb4_float32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.check_model_dtypes(olmo1b_bnb4_float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.check_model_dtypes(olmo1b_bnb4_float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NousResearch/DeepHermes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dh3b_tokenizer = AutoTokenizer.from_pretrained(FPKey.TOKENIZER_3B.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dh3b_tokenizer.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dh8b_tokenizer = AutoTokenizer.from_pretrained(FPKey.TOKENIZER_8B.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dh8b_tokenizer.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.04s/it]\n"
     ]
    }
   ],
   "source": [
    "dh3b_fp32 = load_test_model(FPKey.FP_3B.value, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "are_embeddings_tied(dh3b_fp32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "are_embeddings_tied(llama8b_fp32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "are_embeddings_tied(hfbit1_fp32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access the input embedding layer\n",
    "embed_weight = llama8b_fp32.model.embed_tokens.weight\n",
    "\n",
    "# Access the output projection layer\n",
    "output_weight = llama8b_fp32.lm_head.weight\n",
    "\n",
    "# Check if they are tied\n",
    "print(embed_weight.data_ptr() == output_weight.data_ptr())  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dh3b_fp32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dh8b_fp32 = load_test_model(FPKey.FP_8B.value, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dh8b_fp32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.29s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|| Quant Configs: 1.58bit | BitNet-style PTQ as: PTSQ ||\n",
      ">> [STEP 1] Wrapping Linear layers (no weight quant yet)...\n",
      "[SKIP] lm_head | Policy: Skip weight quantization!\n",
      ">> [STEP 2] Running activation calibration forward pass...\n",
      "[model.layers.0.self_attn.q_proj] input.mean(): 0.00042038538958877325, input.std(): 0.1273992657661438\n",
      "[model.layers.0.self_attn.k_proj] input.mean(): 0.00042038538958877325, input.std(): 0.1273992657661438\n",
      "[model.layers.0.self_attn.v_proj] input.mean(): 0.00042038538958877325, input.std(): 0.1273992657661438\n",
      "[model.layers.0.self_attn.o_proj] input.mean(): -0.00013043987564742565, input.std(): 0.011427002027630806\n",
      "[model.layers.0.mlp.gate_proj] input.mean(): 0.00023862022499088198, input.std(): 0.14004600048065186\n",
      "[model.layers.0.mlp.up_proj] input.mean(): 0.00023862022499088198, input.std(): 0.14004600048065186\n",
      "[model.layers.0.mlp.down_proj] input.mean(): -9.107174264499918e-06, input.std(): 0.0038633914664387703\n",
      "[model.layers.1.self_attn.q_proj] input.mean(): 0.0005290800472721457, input.std(): 0.157298743724823\n",
      "[model.layers.1.self_attn.k_proj] input.mean(): 0.0005290800472721457, input.std(): 0.157298743724823\n",
      "[model.layers.1.self_attn.v_proj] input.mean(): 0.0005290800472721457, input.std(): 0.157298743724823\n",
      "[model.layers.1.self_attn.o_proj] input.mean(): -0.00025867999647744, input.std(): 0.012285135686397552\n",
      "[model.layers.1.mlp.gate_proj] input.mean(): 8.554317901143804e-05, input.std(): 0.17512817680835724\n",
      "[model.layers.1.mlp.up_proj] input.mean(): 8.554317901143804e-05, input.std(): 0.17512817680835724\n",
      "[model.layers.1.mlp.down_proj] input.mean(): -0.0007557441131211817, input.std(): 0.6793496608734131\n",
      "[model.layers.2.self_attn.q_proj] input.mean(): 0.00048162610619328916, input.std(): 0.3214634358882904\n",
      "[model.layers.2.self_attn.k_proj] input.mean(): 0.00048162610619328916, input.std(): 0.3214634358882904\n",
      "[model.layers.2.self_attn.v_proj] input.mean(): 0.00048162610619328916, input.std(): 0.3214634358882904\n",
      "[model.layers.2.self_attn.o_proj] input.mean(): -0.000687955878674984, input.std(): 0.04817629233002663\n",
      "[model.layers.2.mlp.gate_proj] input.mean(): -0.0003481012536212802, input.std(): 0.2036902755498886\n",
      "[model.layers.2.mlp.up_proj] input.mean(): -0.0003481012536212802, input.std(): 0.2036902755498886\n",
      "[model.layers.2.mlp.down_proj] input.mean(): -2.3580882952956017e-06, input.std(): 0.00938556157052517\n",
      "[model.layers.3.self_attn.q_proj] input.mean(): 0.00015327522123698145, input.std(): 0.3235785961151123\n",
      "[model.layers.3.self_attn.k_proj] input.mean(): 0.00015327522123698145, input.std(): 0.3235785961151123\n",
      "[model.layers.3.self_attn.v_proj] input.mean(): 0.00015327522123698145, input.std(): 0.3235785961151123\n",
      "[model.layers.3.self_attn.o_proj] input.mean(): -0.0008714402210898697, input.std(): 0.06281422823667526\n",
      "[model.layers.3.mlp.gate_proj] input.mean(): -0.0004613345954567194, input.std(): 0.22355404496192932\n",
      "[model.layers.3.mlp.up_proj] input.mean(): -0.0004613345954567194, input.std(): 0.22355404496192932\n",
      "[model.layers.3.mlp.down_proj] input.mean(): -4.163149424130097e-05, input.std(): 0.014809302985668182\n",
      "[model.layers.4.self_attn.q_proj] input.mean(): 2.9187203836045228e-05, input.std(): 0.2899114191532135\n",
      "[model.layers.4.self_attn.k_proj] input.mean(): 2.9187203836045228e-05, input.std(): 0.2899114191532135\n",
      "[model.layers.4.self_attn.v_proj] input.mean(): 2.9187203836045228e-05, input.std(): 0.2899114191532135\n",
      "[model.layers.4.self_attn.o_proj] input.mean(): -0.000521469977684319, input.std(): 0.05381884425878525\n",
      "[model.layers.4.mlp.gate_proj] input.mean(): -0.00040825732867233455, input.std(): 0.2385437786579132\n",
      "[model.layers.4.mlp.up_proj] input.mean(): -0.00040825732867233455, input.std(): 0.2385437786579132\n",
      "[model.layers.4.mlp.down_proj] input.mean(): 2.1927626221440732e-05, input.std(): 0.02229779213666916\n",
      "[model.layers.5.self_attn.q_proj] input.mean(): 0.00011689870007103309, input.std(): 0.3355952799320221\n",
      "[model.layers.5.self_attn.k_proj] input.mean(): 0.00011689870007103309, input.std(): 0.3355952799320221\n",
      "[model.layers.5.self_attn.v_proj] input.mean(): 0.00011689870007103309, input.std(): 0.3355952799320221\n",
      "[model.layers.5.self_attn.o_proj] input.mean(): 0.0014180844882503152, input.std(): 0.05891520902514458\n",
      "[model.layers.5.mlp.gate_proj] input.mean(): -0.00035769594251178205, input.std(): 0.2511763572692871\n",
      "[model.layers.5.mlp.up_proj] input.mean(): -0.00035769594251178205, input.std(): 0.2511763572692871\n",
      "[model.layers.5.mlp.down_proj] input.mean(): -4.010583143099211e-05, input.std(): 0.02717946656048298\n",
      "[model.layers.6.self_attn.q_proj] input.mean(): 7.089276186889037e-05, input.std(): 0.32971861958503723\n",
      "[model.layers.6.self_attn.k_proj] input.mean(): 7.089276186889037e-05, input.std(): 0.32971861958503723\n",
      "[model.layers.6.self_attn.v_proj] input.mean(): 7.089276186889037e-05, input.std(): 0.32971861958503723\n",
      "[model.layers.6.self_attn.o_proj] input.mean(): 0.00020766448869835585, input.std(): 0.06527499109506607\n",
      "[model.layers.6.mlp.gate_proj] input.mean(): -0.00031325698364526033, input.std(): 0.2558828294277191\n",
      "[model.layers.6.mlp.up_proj] input.mean(): -0.00031325698364526033, input.std(): 0.2558828294277191\n",
      "[model.layers.6.mlp.down_proj] input.mean(): 8.12420421425486e-06, input.std(): 0.02908831089735031\n",
      "[model.layers.7.self_attn.q_proj] input.mean(): 0.0003699113440234214, input.std(): 0.33348965644836426\n",
      "[model.layers.7.self_attn.k_proj] input.mean(): 0.0003699113440234214, input.std(): 0.33348965644836426\n",
      "[model.layers.7.self_attn.v_proj] input.mean(): 0.0003699113440234214, input.std(): 0.33348965644836426\n",
      "[model.layers.7.self_attn.o_proj] input.mean(): 0.0001842249184846878, input.std(): 0.07593797147274017\n",
      "[model.layers.7.mlp.gate_proj] input.mean(): -2.6312927730032243e-05, input.std(): 0.25533396005630493\n",
      "[model.layers.7.mlp.up_proj] input.mean(): -2.6312927730032243e-05, input.std(): 0.25533396005630493\n",
      "[model.layers.7.mlp.down_proj] input.mean(): 9.947695070877671e-05, input.std(): 0.028986291959881783\n",
      "[model.layers.8.self_attn.q_proj] input.mean(): 0.0004397847515065223, input.std(): 0.360604852437973\n",
      "[model.layers.8.self_attn.k_proj] input.mean(): 0.0004397847515065223, input.std(): 0.360604852437973\n",
      "[model.layers.8.self_attn.v_proj] input.mean(): 0.0004397847515065223, input.std(): 0.360604852437973\n",
      "[model.layers.8.self_attn.o_proj] input.mean(): -0.0030318843200802803, input.std(): 0.09433171898126602\n",
      "[model.layers.8.mlp.gate_proj] input.mean(): -0.00016497356409672648, input.std(): 0.25934654474258423\n",
      "[model.layers.8.mlp.up_proj] input.mean(): -0.00016497356409672648, input.std(): 0.25934654474258423\n",
      "[model.layers.8.mlp.down_proj] input.mean(): 7.853782153688371e-05, input.std(): 0.03219760209321976\n",
      "[model.layers.9.self_attn.q_proj] input.mean(): 0.0008135871030390263, input.std(): 0.37443476915359497\n",
      "[model.layers.9.self_attn.k_proj] input.mean(): 0.0008135871030390263, input.std(): 0.37443476915359497\n",
      "[model.layers.9.self_attn.v_proj] input.mean(): 0.0008135871030390263, input.std(): 0.37443476915359497\n",
      "[model.layers.9.self_attn.o_proj] input.mean(): -0.003389206947758794, input.std(): 0.11782548576593399\n",
      "[model.layers.9.mlp.gate_proj] input.mean(): 0.0004989822045899928, input.std(): 0.25821205973625183\n",
      "[model.layers.9.mlp.up_proj] input.mean(): 0.0004989822045899928, input.std(): 0.25821205973625183\n",
      "[model.layers.9.mlp.down_proj] input.mean(): 5.651613901136443e-05, input.std(): 0.03375563025474548\n",
      "[model.layers.10.self_attn.q_proj] input.mean(): 0.0012058926513418555, input.std(): 0.3849044740200043\n",
      "[model.layers.10.self_attn.k_proj] input.mean(): 0.0012058926513418555, input.std(): 0.3849044740200043\n",
      "[model.layers.10.self_attn.v_proj] input.mean(): 0.0012058926513418555, input.std(): 0.3849044740200043\n",
      "[model.layers.10.self_attn.o_proj] input.mean(): 0.00020072999177500606, input.std(): 0.12087235599756241\n",
      "[model.layers.10.mlp.gate_proj] input.mean(): 0.0005271640256978571, input.std(): 0.26213258504867554\n",
      "[model.layers.10.mlp.up_proj] input.mean(): 0.0005271640256978571, input.std(): 0.26213258504867554\n",
      "[model.layers.10.mlp.down_proj] input.mean(): 4.5064185542287305e-05, input.std(): 0.037509724497795105\n",
      "[model.layers.11.self_attn.q_proj] input.mean(): 0.0008661610190756619, input.std(): 0.3662598431110382\n",
      "[model.layers.11.self_attn.k_proj] input.mean(): 0.0008661610190756619, input.std(): 0.3662598431110382\n",
      "[model.layers.11.self_attn.v_proj] input.mean(): 0.0008661610190756619, input.std(): 0.3662598431110382\n",
      "[model.layers.11.self_attn.o_proj] input.mean(): 0.0004222040588501841, input.std(): 0.1138526052236557\n",
      "[model.layers.11.mlp.gate_proj] input.mean(): 0.00018184301734436303, input.std(): 0.26548340916633606\n",
      "[model.layers.11.mlp.up_proj] input.mean(): 0.00018184301734436303, input.std(): 0.26548340916633606\n",
      "[model.layers.11.mlp.down_proj] input.mean(): -4.86893804918509e-05, input.std(): 0.034829337149858475\n",
      "[model.layers.12.self_attn.q_proj] input.mean(): 0.0006846648757345974, input.std(): 0.42584028840065\n",
      "[model.layers.12.self_attn.k_proj] input.mean(): 0.0006846648757345974, input.std(): 0.42584028840065\n",
      "[model.layers.12.self_attn.v_proj] input.mean(): 0.0006846648757345974, input.std(): 0.42584028840065\n",
      "[model.layers.12.self_attn.o_proj] input.mean(): 0.0020182207226753235, input.std(): 0.13765087723731995\n",
      "[model.layers.12.mlp.gate_proj] input.mean(): 3.798257603193633e-05, input.std(): 0.2713591456413269\n",
      "[model.layers.12.mlp.up_proj] input.mean(): 3.798257603193633e-05, input.std(): 0.2713591456413269\n",
      "[model.layers.12.mlp.down_proj] input.mean(): 6.883998867124319e-05, input.std(): 0.041563596576452255\n",
      "[model.layers.13.self_attn.q_proj] input.mean(): 0.0008302386268042028, input.std(): 0.42078205943107605\n",
      "[model.layers.13.self_attn.k_proj] input.mean(): 0.0008302386268042028, input.std(): 0.42078205943107605\n",
      "[model.layers.13.self_attn.v_proj] input.mean(): 0.0008302386268042028, input.std(): 0.42078205943107605\n",
      "[model.layers.13.self_attn.o_proj] input.mean(): 0.000121789809782058, input.std(): 0.12208887189626694\n",
      "[model.layers.13.mlp.gate_proj] input.mean(): 4.395512860355666e-06, input.std(): 0.2873159945011139\n",
      "[model.layers.13.mlp.up_proj] input.mean(): 4.395512860355666e-06, input.std(): 0.2873159945011139\n",
      "[model.layers.13.mlp.down_proj] input.mean(): -4.473925946513191e-05, input.std(): 0.05181271955370903\n",
      "[model.layers.14.self_attn.q_proj] input.mean(): 0.0015524978516623378, input.std(): 0.4223807454109192\n",
      "[model.layers.14.self_attn.k_proj] input.mean(): 0.0015524978516623378, input.std(): 0.4223807454109192\n",
      "[model.layers.14.self_attn.v_proj] input.mean(): 0.0015524978516623378, input.std(): 0.4223807454109192\n",
      "[model.layers.14.self_attn.o_proj] input.mean(): -0.0028085371013730764, input.std(): 0.10810291022062302\n",
      "[model.layers.14.mlp.gate_proj] input.mean(): 0.0007876343443058431, input.std(): 0.3016882538795471\n",
      "[model.layers.14.mlp.up_proj] input.mean(): 0.0007876343443058431, input.std(): 0.3016882538795471\n",
      "[model.layers.14.mlp.down_proj] input.mean(): 0.00015559745952486992, input.std(): 0.06458260864019394\n",
      "[model.layers.15.self_attn.q_proj] input.mean(): 0.0030726538971066475, input.std(): 0.43533965945243835\n",
      "[model.layers.15.self_attn.k_proj] input.mean(): 0.0030726538971066475, input.std(): 0.43533965945243835\n",
      "[model.layers.15.self_attn.v_proj] input.mean(): 0.0030726538971066475, input.std(): 0.43533965945243835\n",
      "[model.layers.15.self_attn.o_proj] input.mean(): 0.0002883984998334199, input.std(): 0.09907513111829758\n",
      "[model.layers.15.mlp.gate_proj] input.mean(): 0.0015023117884993553, input.std(): 0.31442633271217346\n",
      "[model.layers.15.mlp.up_proj] input.mean(): 0.0015023117884993553, input.std(): 0.31442633271217346\n",
      "[model.layers.15.mlp.down_proj] input.mean(): -1.8776416254695505e-05, input.std(): 0.06804341822862625\n",
      "[model.layers.16.self_attn.q_proj] input.mean(): 0.002225817646831274, input.std(): 0.4368653893470764\n",
      "[model.layers.16.self_attn.k_proj] input.mean(): 0.002225817646831274, input.std(): 0.4368653893470764\n",
      "[model.layers.16.self_attn.v_proj] input.mean(): 0.002225817646831274, input.std(): 0.4368653893470764\n",
      "[model.layers.16.self_attn.o_proj] input.mean(): 0.0009095338173210621, input.std(): 0.08975491672754288\n",
      "[model.layers.16.mlp.gate_proj] input.mean(): 0.0012011093785986304, input.std(): 0.3278385400772095\n",
      "[model.layers.16.mlp.up_proj] input.mean(): 0.0012011093785986304, input.std(): 0.3278385400772095\n",
      "[model.layers.16.mlp.down_proj] input.mean(): 5.736786988563836e-06, input.std(): 0.08404070883989334\n",
      "[model.layers.17.self_attn.q_proj] input.mean(): 0.0023741235490888357, input.std(): 0.43792206048965454\n",
      "[model.layers.17.self_attn.k_proj] input.mean(): 0.0023741235490888357, input.std(): 0.43792206048965454\n",
      "[model.layers.17.self_attn.v_proj] input.mean(): 0.0023741235490888357, input.std(): 0.43792206048965454\n",
      "[model.layers.17.self_attn.o_proj] input.mean(): 0.002512869192287326, input.std(): 0.09578251838684082\n",
      "[model.layers.17.mlp.gate_proj] input.mean(): 0.0012561356415972114, input.std(): 0.3374541401863098\n",
      "[model.layers.17.mlp.up_proj] input.mean(): 0.0012561356415972114, input.std(): 0.3374541401863098\n",
      "[model.layers.17.mlp.down_proj] input.mean(): 0.00013324141036719084, input.std(): 0.08309026062488556\n",
      "[model.layers.18.self_attn.q_proj] input.mean(): 0.001825351850129664, input.std(): 0.44787904620170593\n",
      "[model.layers.18.self_attn.k_proj] input.mean(): 0.001825351850129664, input.std(): 0.44787904620170593\n",
      "[model.layers.18.self_attn.v_proj] input.mean(): 0.001825351850129664, input.std(): 0.44787904620170593\n",
      "[model.layers.18.self_attn.o_proj] input.mean(): -0.003386454423889518, input.std(): 0.09629828482866287\n",
      "[model.layers.18.mlp.gate_proj] input.mean(): 0.0010612548794597387, input.std(): 0.34465277194976807\n",
      "[model.layers.18.mlp.up_proj] input.mean(): 0.0010612548794597387, input.std(): 0.34465277194976807\n",
      "[model.layers.18.mlp.down_proj] input.mean(): -9.098320879274979e-05, input.std(): 0.08857323229312897\n",
      "[model.layers.19.self_attn.q_proj] input.mean(): 0.002173997927457094, input.std(): 0.44153693318367004\n",
      "[model.layers.19.self_attn.k_proj] input.mean(): 0.002173997927457094, input.std(): 0.44153693318367004\n",
      "[model.layers.19.self_attn.v_proj] input.mean(): 0.002173997927457094, input.std(): 0.44153693318367004\n",
      "[model.layers.19.self_attn.o_proj] input.mean(): -0.001121343346312642, input.std(): 0.1022348403930664\n",
      "[model.layers.19.mlp.gate_proj] input.mean(): 0.0011030397145077586, input.std(): 0.36361223459243774\n",
      "[model.layers.19.mlp.up_proj] input.mean(): 0.0011030397145077586, input.std(): 0.36361223459243774\n",
      "[model.layers.19.mlp.down_proj] input.mean(): -0.0001872347347671166, input.std(): 0.10008004307746887\n",
      "[model.layers.20.self_attn.q_proj] input.mean(): 0.002066150074824691, input.std(): 0.4589891731739044\n",
      "[model.layers.20.self_attn.k_proj] input.mean(): 0.002066150074824691, input.std(): 0.4589891731739044\n",
      "[model.layers.20.self_attn.v_proj] input.mean(): 0.002066150074824691, input.std(): 0.4589891731739044\n",
      "[model.layers.20.self_attn.o_proj] input.mean(): -0.000616338278632611, input.std(): 0.10865896195173264\n",
      "[model.layers.20.mlp.gate_proj] input.mean(): 0.0011945520527660847, input.std(): 0.36887314915657043\n",
      "[model.layers.20.mlp.up_proj] input.mean(): 0.0011945520527660847, input.std(): 0.36887314915657043\n",
      "[model.layers.20.mlp.down_proj] input.mean(): 4.58206377516035e-05, input.std(): 0.10723236203193665\n",
      "[model.layers.21.self_attn.q_proj] input.mean(): 0.002815560670569539, input.std(): 0.4611795246601105\n",
      "[model.layers.21.self_attn.k_proj] input.mean(): 0.002815560670569539, input.std(): 0.4611795246601105\n",
      "[model.layers.21.self_attn.v_proj] input.mean(): 0.002815560670569539, input.std(): 0.4611795246601105\n",
      "[model.layers.21.self_attn.o_proj] input.mean(): 0.0051920609548687935, input.std(): 0.11618544906377792\n",
      "[model.layers.21.mlp.gate_proj] input.mean(): 0.0018110187957063317, input.std(): 0.37931665778160095\n",
      "[model.layers.21.mlp.up_proj] input.mean(): 0.0018110187957063317, input.std(): 0.37931665778160095\n",
      "[model.layers.21.mlp.down_proj] input.mean(): 0.0003248743596486747, input.std(): 0.11537862569093704\n",
      "[model.layers.22.self_attn.q_proj] input.mean(): 0.003054417669773102, input.std(): 0.46558159589767456\n",
      "[model.layers.22.self_attn.k_proj] input.mean(): 0.003054417669773102, input.std(): 0.46558159589767456\n",
      "[model.layers.22.self_attn.v_proj] input.mean(): 0.003054417669773102, input.std(): 0.46558159589767456\n",
      "[model.layers.22.self_attn.o_proj] input.mean(): -0.0006346137379296124, input.std(): 0.1256600320339203\n",
      "[model.layers.22.mlp.gate_proj] input.mean(): 0.002069491194561124, input.std(): 0.393459290266037\n",
      "[model.layers.22.mlp.up_proj] input.mean(): 0.002069491194561124, input.std(): 0.393459290266037\n",
      "[model.layers.22.mlp.down_proj] input.mean(): -0.00015862476720940322, input.std(): 0.12865965068340302\n",
      "[model.layers.23.self_attn.q_proj] input.mean(): 0.0028670737519860268, input.std(): 0.4486681818962097\n",
      "[model.layers.23.self_attn.k_proj] input.mean(): 0.0028670737519860268, input.std(): 0.4486681818962097\n",
      "[model.layers.23.self_attn.v_proj] input.mean(): 0.0028670737519860268, input.std(): 0.4486681818962097\n",
      "[model.layers.23.self_attn.o_proj] input.mean(): -0.0024459140840917826, input.std(): 0.12535755336284637\n",
      "[model.layers.23.mlp.gate_proj] input.mean(): 0.00241486681625247, input.std(): 0.4115617573261261\n",
      "[model.layers.23.mlp.up_proj] input.mean(): 0.00241486681625247, input.std(): 0.4115617573261261\n",
      "[model.layers.23.mlp.down_proj] input.mean(): -0.00021605982328765094, input.std(): 0.14606741070747375\n",
      "[model.layers.24.self_attn.q_proj] input.mean(): 0.003120503621175885, input.std(): 0.4810261130332947\n",
      "[model.layers.24.self_attn.k_proj] input.mean(): 0.003120503621175885, input.std(): 0.4810261130332947\n",
      "[model.layers.24.self_attn.v_proj] input.mean(): 0.003120503621175885, input.std(): 0.4810261130332947\n",
      "[model.layers.24.self_attn.o_proj] input.mean(): -0.005469045136123896, input.std(): 0.14488883316516876\n",
      "[model.layers.24.mlp.gate_proj] input.mean(): 0.0028786060865968466, input.std(): 0.43282753229141235\n",
      "[model.layers.24.mlp.up_proj] input.mean(): 0.0028786060865968466, input.std(): 0.43282753229141235\n",
      "[model.layers.24.mlp.down_proj] input.mean(): 0.0001372200349578634, input.std(): 0.16741381585597992\n",
      "[model.layers.25.self_attn.q_proj] input.mean(): 0.000858222832903266, input.std(): 0.4951912462711334\n",
      "[model.layers.25.self_attn.k_proj] input.mean(): 0.000858222832903266, input.std(): 0.4951912462711334\n",
      "[model.layers.25.self_attn.v_proj] input.mean(): 0.000858222832903266, input.std(): 0.4951912462711334\n",
      "[model.layers.25.self_attn.o_proj] input.mean(): 0.001235505798831582, input.std(): 0.190803661942482\n",
      "[model.layers.25.mlp.gate_proj] input.mean(): 0.0014367069816216826, input.std(): 0.4583243429660797\n",
      "[model.layers.25.mlp.up_proj] input.mean(): 0.0014367069816216826, input.std(): 0.4583243429660797\n",
      "[model.layers.25.mlp.down_proj] input.mean(): -0.00016930668789427727, input.std(): 0.22462108731269836\n",
      "[model.layers.26.self_attn.q_proj] input.mean(): 0.00037832395173609257, input.std(): 0.45882952213287354\n",
      "[model.layers.26.self_attn.k_proj] input.mean(): 0.00037832395173609257, input.std(): 0.45882952213287354\n",
      "[model.layers.26.self_attn.v_proj] input.mean(): 0.00037832395173609257, input.std(): 0.45882952213287354\n",
      "[model.layers.26.self_attn.o_proj] input.mean(): -0.0038202020805329084, input.std(): 0.16427306830883026\n",
      "[model.layers.26.mlp.gate_proj] input.mean(): 0.0006286646239459515, input.std(): 0.4736802279949188\n",
      "[model.layers.26.mlp.up_proj] input.mean(): 0.0006286646239459515, input.std(): 0.4736802279949188\n",
      "[model.layers.26.mlp.down_proj] input.mean(): -0.0005125975585542619, input.std(): 0.24465782940387726\n",
      "[model.layers.27.self_attn.q_proj] input.mean(): -0.0005207746871747077, input.std(): 0.4283064305782318\n",
      "[model.layers.27.self_attn.k_proj] input.mean(): -0.0005207746871747077, input.std(): 0.4283064305782318\n",
      "[model.layers.27.self_attn.v_proj] input.mean(): -0.0005207746871747077, input.std(): 0.4283064305782318\n",
      "[model.layers.27.self_attn.o_proj] input.mean(): 0.0011967048048973083, input.std(): 0.1582898199558258\n",
      "[model.layers.27.mlp.gate_proj] input.mean(): 4.7990080929594114e-05, input.std(): 0.44005799293518066\n",
      "[model.layers.27.mlp.up_proj] input.mean(): 4.7990080929594114e-05, input.std(): 0.44005799293518066\n",
      "[model.layers.27.mlp.down_proj] input.mean(): 0.04193215072154999, input.std(): 13.25027084350586\n",
      "[Calibrated] model.layers.0.self_attn.q_proj                    | act_scale: 1.000000\n",
      "[Calibrated] model.layers.0.self_attn.k_proj                    | act_scale: 1.000000\n",
      "[Calibrated] model.layers.0.self_attn.v_proj                    | act_scale: 1.000000\n",
      "[Calibrated] model.layers.0.self_attn.o_proj                    | act_scale: 1.000000\n",
      "[Calibrated] model.layers.0.mlp.gate_proj                       | act_scale: 1.000000\n",
      "[Calibrated] model.layers.0.mlp.up_proj                         | act_scale: 1.000000\n",
      "[Calibrated] model.layers.0.mlp.down_proj                       | act_scale: 1.000000\n",
      "[Calibrated] model.layers.1.self_attn.q_proj                    | act_scale: 1.000000\n",
      "[Calibrated] model.layers.1.self_attn.k_proj                    | act_scale: 1.000000\n",
      "[Calibrated] model.layers.1.self_attn.v_proj                    | act_scale: 1.000000\n",
      "[Calibrated] model.layers.1.self_attn.o_proj                    | act_scale: 1.000000\n",
      "[Calibrated] model.layers.1.mlp.gate_proj                       | act_scale: 1.000000\n",
      "[Calibrated] model.layers.1.mlp.up_proj                         | act_scale: 1.000000\n",
      "[Calibrated] model.layers.1.mlp.down_proj                       | act_scale: 1.000000\n",
      "[Calibrated] model.layers.2.self_attn.q_proj                    | act_scale: 1.000000\n",
      "[Calibrated] model.layers.2.self_attn.k_proj                    | act_scale: 1.000000\n",
      "[Calibrated] model.layers.2.self_attn.v_proj                    | act_scale: 1.000000\n",
      "[Calibrated] model.layers.2.self_attn.o_proj                    | act_scale: 1.000000\n",
      "[Calibrated] model.layers.2.mlp.gate_proj                       | act_scale: 1.000000\n",
      "[Calibrated] model.layers.2.mlp.up_proj                         | act_scale: 1.000000\n",
      "[Calibrated] model.layers.2.mlp.down_proj                       | act_scale: 1.000000\n",
      "[Calibrated] model.layers.3.self_attn.q_proj                    | act_scale: 1.000000\n",
      "[Calibrated] model.layers.3.self_attn.k_proj                    | act_scale: 1.000000\n",
      "[Calibrated] model.layers.3.self_attn.v_proj                    | act_scale: 1.000000\n",
      "[Calibrated] model.layers.3.self_attn.o_proj                    | act_scale: 1.000000\n",
      "[Calibrated] model.layers.3.mlp.gate_proj                       | act_scale: 1.000000\n",
      "[Calibrated] model.layers.3.mlp.up_proj                         | act_scale: 1.000000\n",
      "[Calibrated] model.layers.3.mlp.down_proj                       | act_scale: 1.000000\n",
      "[Calibrated] model.layers.4.self_attn.q_proj                    | act_scale: 1.000000\n",
      "[Calibrated] model.layers.4.self_attn.k_proj                    | act_scale: 1.000000\n",
      "[Calibrated] model.layers.4.self_attn.v_proj                    | act_scale: 1.000000\n",
      "[Calibrated] model.layers.4.self_attn.o_proj                    | act_scale: 1.000000\n",
      "[Calibrated] model.layers.4.mlp.gate_proj                       | act_scale: 1.000000\n",
      "[Calibrated] model.layers.4.mlp.up_proj                         | act_scale: 1.000000\n",
      "[Calibrated] model.layers.4.mlp.down_proj                       | act_scale: 1.000000\n",
      "[Calibrated] model.layers.5.self_attn.q_proj                    | act_scale: 1.000000\n",
      "[Calibrated] model.layers.5.self_attn.k_proj                    | act_scale: 1.000000\n",
      "[Calibrated] model.layers.5.self_attn.v_proj                    | act_scale: 1.000000\n",
      "[Calibrated] model.layers.5.self_attn.o_proj                    | act_scale: 1.000000\n",
      "[Calibrated] model.layers.5.mlp.gate_proj                       | act_scale: 1.000000\n",
      "[Calibrated] model.layers.5.mlp.up_proj                         | act_scale: 1.000000\n",
      "[Calibrated] model.layers.5.mlp.down_proj                       | act_scale: 1.000000\n",
      "[Calibrated] model.layers.6.self_attn.q_proj                    | act_scale: 1.000000\n",
      "[Calibrated] model.layers.6.self_attn.k_proj                    | act_scale: 1.000000\n",
      "[Calibrated] model.layers.6.self_attn.v_proj                    | act_scale: 1.000000\n",
      "[Calibrated] model.layers.6.self_attn.o_proj                    | act_scale: 1.000000\n",
      "[Calibrated] model.layers.6.mlp.gate_proj                       | act_scale: 1.000000\n",
      "[Calibrated] model.layers.6.mlp.up_proj                         | act_scale: 1.000000\n",
      "[Calibrated] model.layers.6.mlp.down_proj                       | act_scale: 1.000000\n",
      "[Calibrated] model.layers.7.self_attn.q_proj                    | act_scale: 1.000000\n",
      "[Calibrated] model.layers.7.self_attn.k_proj                    | act_scale: 1.000000\n",
      "[Calibrated] model.layers.7.self_attn.v_proj                    | act_scale: 1.000000\n",
      "[Calibrated] model.layers.7.self_attn.o_proj                    | act_scale: 1.000000\n",
      "[Calibrated] model.layers.7.mlp.gate_proj                       | act_scale: 1.000000\n",
      "[Calibrated] model.layers.7.mlp.up_proj                         | act_scale: 1.000000\n",
      "[Calibrated] model.layers.7.mlp.down_proj                       | act_scale: 1.000000\n",
      "[Calibrated] model.layers.8.self_attn.q_proj                    | act_scale: 1.000000\n",
      "[Calibrated] model.layers.8.self_attn.k_proj                    | act_scale: 1.000000\n",
      "[Calibrated] model.layers.8.self_attn.v_proj                    | act_scale: 1.000000\n",
      "[Calibrated] model.layers.8.self_attn.o_proj                    | act_scale: 1.000000\n",
      "[Calibrated] model.layers.8.mlp.gate_proj                       | act_scale: 1.000000\n",
      "[Calibrated] model.layers.8.mlp.up_proj                         | act_scale: 1.000000\n",
      "[Calibrated] model.layers.8.mlp.down_proj                       | act_scale: 1.000000\n",
      "[Calibrated] model.layers.9.self_attn.q_proj                    | act_scale: 1.000000\n",
      "[Calibrated] model.layers.9.self_attn.k_proj                    | act_scale: 1.000000\n",
      "[Calibrated] model.layers.9.self_attn.v_proj                    | act_scale: 1.000000\n",
      "[Calibrated] model.layers.9.self_attn.o_proj                    | act_scale: 1.000000\n",
      "[Calibrated] model.layers.9.mlp.gate_proj                       | act_scale: 1.000000\n",
      "[Calibrated] model.layers.9.mlp.up_proj                         | act_scale: 1.000000\n",
      "[Calibrated] model.layers.9.mlp.down_proj                       | act_scale: 1.000000\n",
      "[Calibrated] model.layers.10.self_attn.q_proj                   | act_scale: 1.000000\n",
      "[Calibrated] model.layers.10.self_attn.k_proj                   | act_scale: 1.000000\n",
      "[Calibrated] model.layers.10.self_attn.v_proj                   | act_scale: 1.000000\n",
      "[Calibrated] model.layers.10.self_attn.o_proj                   | act_scale: 1.000000\n",
      "[Calibrated] model.layers.10.mlp.gate_proj                      | act_scale: 1.000000\n",
      "[Calibrated] model.layers.10.mlp.up_proj                        | act_scale: 1.000000\n",
      "[Calibrated] model.layers.10.mlp.down_proj                      | act_scale: 1.000000\n",
      "[Calibrated] model.layers.11.self_attn.q_proj                   | act_scale: 1.000000\n",
      "[Calibrated] model.layers.11.self_attn.k_proj                   | act_scale: 1.000000\n",
      "[Calibrated] model.layers.11.self_attn.v_proj                   | act_scale: 1.000000\n",
      "[Calibrated] model.layers.11.self_attn.o_proj                   | act_scale: 1.000000\n",
      "[Calibrated] model.layers.11.mlp.gate_proj                      | act_scale: 1.000000\n",
      "[Calibrated] model.layers.11.mlp.up_proj                        | act_scale: 1.000000\n",
      "[Calibrated] model.layers.11.mlp.down_proj                      | act_scale: 1.000000\n",
      "[Calibrated] model.layers.12.self_attn.q_proj                   | act_scale: 1.000000\n",
      "[Calibrated] model.layers.12.self_attn.k_proj                   | act_scale: 1.000000\n",
      "[Calibrated] model.layers.12.self_attn.v_proj                   | act_scale: 1.000000\n",
      "[Calibrated] model.layers.12.self_attn.o_proj                   | act_scale: 1.000000\n",
      "[Calibrated] model.layers.12.mlp.gate_proj                      | act_scale: 1.000000\n",
      "[Calibrated] model.layers.12.mlp.up_proj                        | act_scale: 1.000000\n",
      "[Calibrated] model.layers.12.mlp.down_proj                      | act_scale: 1.000000\n",
      "[Calibrated] model.layers.13.self_attn.q_proj                   | act_scale: 1.000000\n",
      "[Calibrated] model.layers.13.self_attn.k_proj                   | act_scale: 1.000000\n",
      "[Calibrated] model.layers.13.self_attn.v_proj                   | act_scale: 1.000000\n",
      "[Calibrated] model.layers.13.self_attn.o_proj                   | act_scale: 1.000000\n",
      "[Calibrated] model.layers.13.mlp.gate_proj                      | act_scale: 1.000000\n",
      "[Calibrated] model.layers.13.mlp.up_proj                        | act_scale: 1.000000\n",
      "[Calibrated] model.layers.13.mlp.down_proj                      | act_scale: 1.000000\n",
      "[Calibrated] model.layers.14.self_attn.q_proj                   | act_scale: 1.000000\n",
      "[Calibrated] model.layers.14.self_attn.k_proj                   | act_scale: 1.000000\n",
      "[Calibrated] model.layers.14.self_attn.v_proj                   | act_scale: 1.000000\n",
      "[Calibrated] model.layers.14.self_attn.o_proj                   | act_scale: 1.000000\n",
      "[Calibrated] model.layers.14.mlp.gate_proj                      | act_scale: 1.000000\n",
      "[Calibrated] model.layers.14.mlp.up_proj                        | act_scale: 1.000000\n",
      "[Calibrated] model.layers.14.mlp.down_proj                      | act_scale: 1.000000\n",
      "[Calibrated] model.layers.15.self_attn.q_proj                   | act_scale: 1.000000\n",
      "[Calibrated] model.layers.15.self_attn.k_proj                   | act_scale: 1.000000\n",
      "[Calibrated] model.layers.15.self_attn.v_proj                   | act_scale: 1.000000\n",
      "[Calibrated] model.layers.15.self_attn.o_proj                   | act_scale: 1.000000\n",
      "[Calibrated] model.layers.15.mlp.gate_proj                      | act_scale: 1.000000\n",
      "[Calibrated] model.layers.15.mlp.up_proj                        | act_scale: 1.000000\n",
      "[Calibrated] model.layers.15.mlp.down_proj                      | act_scale: 1.000000\n",
      "[Calibrated] model.layers.16.self_attn.q_proj                   | act_scale: 1.000000\n",
      "[Calibrated] model.layers.16.self_attn.k_proj                   | act_scale: 1.000000\n",
      "[Calibrated] model.layers.16.self_attn.v_proj                   | act_scale: 1.000000\n",
      "[Calibrated] model.layers.16.self_attn.o_proj                   | act_scale: 1.000000\n",
      "[Calibrated] model.layers.16.mlp.gate_proj                      | act_scale: 1.000000\n",
      "[Calibrated] model.layers.16.mlp.up_proj                        | act_scale: 1.000000\n",
      "[Calibrated] model.layers.16.mlp.down_proj                      | act_scale: 1.000000\n",
      "[Calibrated] model.layers.17.self_attn.q_proj                   | act_scale: 1.000000\n",
      "[Calibrated] model.layers.17.self_attn.k_proj                   | act_scale: 1.000000\n",
      "[Calibrated] model.layers.17.self_attn.v_proj                   | act_scale: 1.000000\n",
      "[Calibrated] model.layers.17.self_attn.o_proj                   | act_scale: 1.000000\n",
      "[Calibrated] model.layers.17.mlp.gate_proj                      | act_scale: 1.000000\n",
      "[Calibrated] model.layers.17.mlp.up_proj                        | act_scale: 1.000000\n",
      "[Calibrated] model.layers.17.mlp.down_proj                      | act_scale: 1.000000\n",
      "[Calibrated] model.layers.18.self_attn.q_proj                   | act_scale: 1.000000\n",
      "[Calibrated] model.layers.18.self_attn.k_proj                   | act_scale: 1.000000\n",
      "[Calibrated] model.layers.18.self_attn.v_proj                   | act_scale: 1.000000\n",
      "[Calibrated] model.layers.18.self_attn.o_proj                   | act_scale: 1.000000\n",
      "[Calibrated] model.layers.18.mlp.gate_proj                      | act_scale: 1.000000\n",
      "[Calibrated] model.layers.18.mlp.up_proj                        | act_scale: 1.000000\n",
      "[Calibrated] model.layers.18.mlp.down_proj                      | act_scale: 1.000000\n",
      "[Calibrated] model.layers.19.self_attn.q_proj                   | act_scale: 1.000000\n",
      "[Calibrated] model.layers.19.self_attn.k_proj                   | act_scale: 1.000000\n",
      "[Calibrated] model.layers.19.self_attn.v_proj                   | act_scale: 1.000000\n",
      "[Calibrated] model.layers.19.self_attn.o_proj                   | act_scale: 1.000000\n",
      "[Calibrated] model.layers.19.mlp.gate_proj                      | act_scale: 1.000000\n",
      "[Calibrated] model.layers.19.mlp.up_proj                        | act_scale: 1.000000\n",
      "[Calibrated] model.layers.19.mlp.down_proj                      | act_scale: 1.000000\n",
      "[Calibrated] model.layers.20.self_attn.q_proj                   | act_scale: 1.000000\n",
      "[Calibrated] model.layers.20.self_attn.k_proj                   | act_scale: 1.000000\n",
      "[Calibrated] model.layers.20.self_attn.v_proj                   | act_scale: 1.000000\n",
      "[Calibrated] model.layers.20.self_attn.o_proj                   | act_scale: 1.000000\n",
      "[Calibrated] model.layers.20.mlp.gate_proj                      | act_scale: 1.000000\n",
      "[Calibrated] model.layers.20.mlp.up_proj                        | act_scale: 1.000000\n",
      "[Calibrated] model.layers.20.mlp.down_proj                      | act_scale: 1.000000\n",
      "[Calibrated] model.layers.21.self_attn.q_proj                   | act_scale: 1.000000\n",
      "[Calibrated] model.layers.21.self_attn.k_proj                   | act_scale: 1.000000\n",
      "[Calibrated] model.layers.21.self_attn.v_proj                   | act_scale: 1.000000\n",
      "[Calibrated] model.layers.21.self_attn.o_proj                   | act_scale: 1.000000\n",
      "[Calibrated] model.layers.21.mlp.gate_proj                      | act_scale: 1.000000\n",
      "[Calibrated] model.layers.21.mlp.up_proj                        | act_scale: 1.000000\n",
      "[Calibrated] model.layers.21.mlp.down_proj                      | act_scale: 1.000000\n",
      "[Calibrated] model.layers.22.self_attn.q_proj                   | act_scale: 1.000000\n",
      "[Calibrated] model.layers.22.self_attn.k_proj                   | act_scale: 1.000000\n",
      "[Calibrated] model.layers.22.self_attn.v_proj                   | act_scale: 1.000000\n",
      "[Calibrated] model.layers.22.self_attn.o_proj                   | act_scale: 1.000000\n",
      "[Calibrated] model.layers.22.mlp.gate_proj                      | act_scale: 1.000000\n",
      "[Calibrated] model.layers.22.mlp.up_proj                        | act_scale: 1.000000\n",
      "[Calibrated] model.layers.22.mlp.down_proj                      | act_scale: 1.000000\n",
      "[Calibrated] model.layers.23.self_attn.q_proj                   | act_scale: 1.000000\n",
      "[Calibrated] model.layers.23.self_attn.k_proj                   | act_scale: 1.000000\n",
      "[Calibrated] model.layers.23.self_attn.v_proj                   | act_scale: 1.000000\n",
      "[Calibrated] model.layers.23.self_attn.o_proj                   | act_scale: 1.000000\n",
      "[Calibrated] model.layers.23.mlp.gate_proj                      | act_scale: 1.000000\n",
      "[Calibrated] model.layers.23.mlp.up_proj                        | act_scale: 1.000000\n",
      "[Calibrated] model.layers.23.mlp.down_proj                      | act_scale: 1.000000\n",
      "[Calibrated] model.layers.24.self_attn.q_proj                   | act_scale: 1.000000\n",
      "[Calibrated] model.layers.24.self_attn.k_proj                   | act_scale: 1.000000\n",
      "[Calibrated] model.layers.24.self_attn.v_proj                   | act_scale: 1.000000\n",
      "[Calibrated] model.layers.24.self_attn.o_proj                   | act_scale: 1.000000\n",
      "[Calibrated] model.layers.24.mlp.gate_proj                      | act_scale: 1.000000\n",
      "[Calibrated] model.layers.24.mlp.up_proj                        | act_scale: 1.000000\n",
      "[Calibrated] model.layers.24.mlp.down_proj                      | act_scale: 1.000000\n",
      "[Calibrated] model.layers.25.self_attn.q_proj                   | act_scale: 1.000000\n",
      "[Calibrated] model.layers.25.self_attn.k_proj                   | act_scale: 1.000000\n",
      "[Calibrated] model.layers.25.self_attn.v_proj                   | act_scale: 1.000000\n",
      "[Calibrated] model.layers.25.self_attn.o_proj                   | act_scale: 1.000000\n",
      "[Calibrated] model.layers.25.mlp.gate_proj                      | act_scale: 1.000000\n",
      "[Calibrated] model.layers.25.mlp.up_proj                        | act_scale: 1.000000\n",
      "[Calibrated] model.layers.25.mlp.down_proj                      | act_scale: 1.000000\n",
      "[Calibrated] model.layers.26.self_attn.q_proj                   | act_scale: 1.000000\n",
      "[Calibrated] model.layers.26.self_attn.k_proj                   | act_scale: 1.000000\n",
      "[Calibrated] model.layers.26.self_attn.v_proj                   | act_scale: 1.000000\n",
      "[Calibrated] model.layers.26.self_attn.o_proj                   | act_scale: 1.000000\n",
      "[Calibrated] model.layers.26.mlp.gate_proj                      | act_scale: 1.000000\n",
      "[Calibrated] model.layers.26.mlp.up_proj                        | act_scale: 1.000000\n",
      "[Calibrated] model.layers.26.mlp.down_proj                      | act_scale: 1.000000\n",
      "[Calibrated] model.layers.27.self_attn.q_proj                   | act_scale: 1.000000\n",
      "[Calibrated] model.layers.27.self_attn.k_proj                   | act_scale: 1.000000\n",
      "[Calibrated] model.layers.27.self_attn.v_proj                   | act_scale: 1.000000\n",
      "[Calibrated] model.layers.27.self_attn.o_proj                   | act_scale: 1.000000\n",
      "[Calibrated] model.layers.27.mlp.gate_proj                      | act_scale: 1.000000\n",
      "[Calibrated] model.layers.27.mlp.up_proj                        | act_scale: 1.000000\n",
      "[Calibrated] model.layers.27.mlp.down_proj                      | act_scale: 1.000000\n",
      ">> [STEP 3] Applying weight quantization...\n",
      "[DEBUG] Quantized: tensor([[-1.,  1.,  1.,  ...,  1., -1., -1.],\n",
      "        [-0., -1.,  1.,  ...,  1.,  0., -1.],\n",
      "        [ 1.,  1.,  1.,  ...,  1., -1., -1.],\n",
      "        ...,\n",
      "        [ 0., -0.,  1.,  ..., -0.,  0.,  0.],\n",
      "        [ 0., -1.,  1.,  ..., -0., -1.,  0.],\n",
      "        [-0.,  1., -0.,  ...,  0.,  0., -0.]])| \n",
      "[Quantized] model.layers.0.self_attn.q_proj | Ternary weights applied.\n",
      "[DEBUG] Quantized: tensor([[ 0., -1.,  1.,  ..., -1., -1.,  0.],\n",
      "        [ 0., -1.,  1.,  ..., -1., -1., -1.],\n",
      "        [-0., -1.,  1.,  ..., -1., -1.,  1.],\n",
      "        ...,\n",
      "        [ 0.,  1., -1.,  ..., -1.,  0.,  0.],\n",
      "        [ 0.,  1.,  1.,  ...,  1.,  1.,  1.],\n",
      "        [-0., -0.,  1.,  ...,  1., -1.,  0.]])| \n",
      "[Quantized] model.layers.0.self_attn.k_proj | Ternary weights applied.\n",
      "[DEBUG] Quantized: tensor([[ 1.,  1.,  0.,  ..., -1., -1.,  1.],\n",
      "        [ 1., -0., -0.,  ..., -1., -1.,  1.],\n",
      "        [ 1., -1.,  1.,  ..., -1., -1., -1.],\n",
      "        ...,\n",
      "        [ 0.,  1.,  0.,  ..., -1.,  0.,  1.],\n",
      "        [ 1.,  1.,  0.,  ..., -1.,  0.,  0.],\n",
      "        [ 1.,  0.,  0.,  ...,  1., -0., -1.]])| \n",
      "[Quantized] model.layers.0.self_attn.v_proj | Ternary weights applied.\n",
      "[DEBUG] Quantized: tensor([[ 1.,  0.,  1.,  ..., -0.,  1.,  1.],\n",
      "        [ 1.,  1.,  1.,  ...,  1., -0.,  0.],\n",
      "        [ 0., -1., -0.,  ...,  1., -0., -0.],\n",
      "        ...,\n",
      "        [ 1., -1.,  0.,  ...,  0.,  1., -1.],\n",
      "        [ 1., -1.,  1.,  ..., -0., -0., -1.],\n",
      "        [ 1., -0.,  1.,  ...,  1.,  1.,  1.]])| \n",
      "[Quantized] model.layers.0.self_attn.o_proj | Ternary weights applied.\n",
      "[DEBUG] Quantized: tensor([[-0., -0.,  1.,  ...,  1., -1.,  1.],\n",
      "        [ 0.,  1.,  1.,  ...,  1., -0., -0.],\n",
      "        [ 1., -1.,  1.,  ..., -1., -1.,  0.],\n",
      "        ...,\n",
      "        [ 0., -1., -1.,  ...,  1.,  1.,  1.],\n",
      "        [ 1., -1.,  0.,  ..., -0.,  1.,  0.],\n",
      "        [ 1.,  0.,  0.,  ..., -0.,  1., -1.]])| \n",
      "[Quantized] model.layers.0.mlp.gate_proj | Ternary weights applied.\n",
      "[DEBUG] Quantized: tensor([[-1.,  1., -0.,  ...,  1., -0.,  0.],\n",
      "        [-0.,  1., -1.,  ..., -1., -1.,  0.],\n",
      "        [-1., -1.,  1.,  ...,  0., -1., -1.],\n",
      "        ...,\n",
      "        [-1.,  0.,  0.,  ..., -1.,  0., -1.],\n",
      "        [ 1.,  1.,  0.,  ...,  1.,  1., -1.],\n",
      "        [-1.,  1., -0.,  ..., -1., -1., -1.]])| \n",
      "[Quantized] model.layers.0.mlp.up_proj | Ternary weights applied.\n",
      "[DEBUG] Quantized: tensor([[ 1., -1., -1.,  ...,  0.,  1.,  1.],\n",
      "        [ 1., -0., -0.,  ..., -0., -1.,  1.],\n",
      "        [ 1., -1.,  1.,  ...,  1.,  1., -1.],\n",
      "        ...,\n",
      "        [-1., -1.,  1.,  ..., -0., -1., -1.],\n",
      "        [-0., -1., -1.,  ...,  1.,  1., -1.],\n",
      "        [-1.,  0., -1.,  ...,  1., -1., -1.]])| \n",
      "[Quantized] model.layers.0.mlp.down_proj | Ternary weights applied.\n",
      "[DEBUG] Quantized: tensor([[-1., -1., -1.,  ..., -1.,  1., -0.],\n",
      "        [ 0., -0.,  1.,  ..., -1.,  1.,  0.],\n",
      "        [-1.,  1.,  1.,  ..., -0.,  1.,  1.],\n",
      "        ...,\n",
      "        [ 1., -0., -0.,  ...,  1.,  1., -1.],\n",
      "        [ 1., -1.,  1.,  ...,  1.,  0.,  1.],\n",
      "        [ 1.,  1.,  1.,  ..., -1.,  1., -1.]])| \n",
      "[Quantized] model.layers.1.self_attn.q_proj | Ternary weights applied.\n",
      "[DEBUG] Quantized: tensor([[-1., -1.,  0.,  ..., -1., -1., -0.],\n",
      "        [-0.,  1.,  1.,  ..., -1., -1., -1.],\n",
      "        [-0., -1.,  0.,  ..., -1., -1., -0.],\n",
      "        ...,\n",
      "        [ 1.,  0., -1.,  ..., -1.,  0., -1.],\n",
      "        [ 1., -1.,  1.,  ..., -0., -1.,  1.],\n",
      "        [ 1.,  1.,  1.,  ..., -1., -0.,  0.]])| \n",
      "[Quantized] model.layers.1.self_attn.k_proj | Ternary weights applied.\n",
      "[DEBUG] Quantized: tensor([[-1., -1., -0.,  ...,  0., -1.,  1.],\n",
      "        [ 1., -0.,  0.,  ..., -0., -0.,  0.],\n",
      "        [-1., -1., -1.,  ..., -1., -1., -1.],\n",
      "        ...,\n",
      "        [ 1.,  1.,  0.,  ..., -0.,  1., -1.],\n",
      "        [ 1.,  1.,  0.,  ..., -1.,  1., -1.],\n",
      "        [ 0.,  1.,  1.,  ..., -1., -0.,  1.]])| \n",
      "[Quantized] model.layers.1.self_attn.v_proj | Ternary weights applied.\n",
      "[DEBUG] Quantized: tensor([[ 0.,  1.,  1.,  ..., -1.,  1.,  0.],\n",
      "        [ 1.,  1., -0.,  ..., -1., -1.,  1.],\n",
      "        [-0., -0.,  1.,  ...,  0.,  1.,  0.],\n",
      "        ...,\n",
      "        [ 1., -1., -1.,  ...,  1.,  0.,  1.],\n",
      "        [ 1.,  0., -1.,  ..., -1., -1.,  0.],\n",
      "        [ 1., -1.,  1.,  ...,  0.,  1., -1.]])| \n",
      "[Quantized] model.layers.1.self_attn.o_proj | Ternary weights applied.\n",
      "[DEBUG] Quantized: tensor([[-1.,  1.,  1.,  ..., -1.,  1., -1.],\n",
      "        [-1.,  0., -0.,  ..., -1., -1., -1.],\n",
      "        [-1., -1., -0.,  ...,  1.,  1., -1.],\n",
      "        ...,\n",
      "        [-1.,  1., -0.,  ..., -0.,  0., -1.],\n",
      "        [-1.,  1.,  1.,  ...,  1.,  1.,  1.],\n",
      "        [ 1., -1.,  1.,  ...,  0., -1., -1.]])| \n",
      "[Quantized] model.layers.1.mlp.gate_proj | Ternary weights applied.\n",
      "[DEBUG] Quantized: tensor([[ 1.,  1.,  1.,  ..., -1.,  1.,  1.],\n",
      "        [-1.,  0.,  1.,  ...,  0., -1., -1.],\n",
      "        [ 1.,  1.,  1.,  ...,  0., -1.,  0.],\n",
      "        ...,\n",
      "        [ 0.,  1.,  1.,  ..., -0., -0.,  1.],\n",
      "        [ 1., -1., -0.,  ...,  0., -0.,  1.],\n",
      "        [-1.,  1., -1.,  ..., -1., -1., -1.]])| \n",
      "[Quantized] model.layers.1.mlp.up_proj | Ternary weights applied.\n",
      "[DEBUG] Quantized: tensor([[-1., -1., -0.,  ..., -1., -1.,  0.],\n",
      "        [ 0., -0.,  1.,  ...,  1., -1., -0.],\n",
      "        [ 1., -0., -1.,  ...,  1.,  1.,  1.],\n",
      "        ...,\n",
      "        [-1.,  0.,  0.,  ...,  0.,  1.,  1.],\n",
      "        [-1.,  1.,  1.,  ..., -1.,  0., -0.],\n",
      "        [-1., -0., -1.,  ..., -1., -0.,  1.]])| \n",
      "[Quantized] model.layers.1.mlp.down_proj | Ternary weights applied.\n",
      "[DEBUG] Quantized: tensor([[-1.,  1., -0.,  ...,  1.,  1., -0.],\n",
      "        [-1., -1., -1.,  ...,  0.,  1.,  1.],\n",
      "        [ 1., -1., -1.,  ...,  1., -0., -0.],\n",
      "        ...,\n",
      "        [ 0.,  1., -0.,  ...,  1., -1.,  1.],\n",
      "        [-1., -1., -0.,  ..., -0., -0., -1.],\n",
      "        [ 1.,  1., -0.,  ...,  0., -1., -1.]])| \n",
      "[Quantized] model.layers.2.self_attn.q_proj | Ternary weights applied.\n",
      "[DEBUG] Quantized: tensor([[ 1., -0.,  1.,  ...,  1.,  1.,  0.],\n",
      "        [ 1.,  1.,  1.,  ..., -1.,  0.,  0.],\n",
      "        [ 0.,  0., -1.,  ...,  1.,  0., -1.],\n",
      "        ...,\n",
      "        [-1., -0., -1.,  ...,  1.,  1., -1.],\n",
      "        [ 1.,  1., -1.,  ...,  1.,  1.,  1.],\n",
      "        [ 0., -1., -0.,  ...,  1., -1.,  1.]])| \n",
      "[Quantized] model.layers.2.self_attn.k_proj | Ternary weights applied.\n",
      "[DEBUG] Quantized: tensor([[ 1., -1.,  1.,  ...,  0.,  0.,  1.],\n",
      "        [ 0., -0.,  1.,  ...,  1.,  1., -1.],\n",
      "        [-1., -1.,  0.,  ..., -1.,  1., -0.],\n",
      "        ...,\n",
      "        [ 1.,  1.,  0.,  ..., -1., -1., -0.],\n",
      "        [ 1.,  1., -1.,  ..., -1., -1., -1.],\n",
      "        [ 0.,  1.,  0.,  ...,  0., -1.,  1.]])| \n",
      "[Quantized] model.layers.2.self_attn.v_proj | Ternary weights applied.\n",
      "[DEBUG] Quantized: tensor([[ 1.,  0.,  1.,  ..., -1.,  0.,  1.],\n",
      "        [-0., -1., -1.,  ..., -1., -1., -1.],\n",
      "        [-1., -1., -1.,  ..., -1., -0.,  1.],\n",
      "        ...,\n",
      "        [-0.,  1.,  1.,  ...,  1., -1., -0.],\n",
      "        [-1., -1., -1.,  ...,  0., -0.,  1.],\n",
      "        [-0.,  1.,  1.,  ..., -0.,  1.,  1.]])| \n",
      "[Quantized] model.layers.2.self_attn.o_proj | Ternary weights applied.\n",
      "[DEBUG] Quantized: tensor([[-0., -1.,  0.,  ..., -0.,  0.,  1.],\n",
      "        [ 1., -0., -1.,  ..., -1., -1.,  0.],\n",
      "        [-1.,  1.,  0.,  ..., -1., -0., -1.],\n",
      "        ...,\n",
      "        [-1.,  1.,  0.,  ...,  1., -1.,  1.],\n",
      "        [-1., -0., -1.,  ..., -1., -1., -1.],\n",
      "        [ 0.,  1.,  1.,  ...,  0.,  1., -1.]])| \n",
      "[Quantized] model.layers.2.mlp.gate_proj | Ternary weights applied.\n",
      "[DEBUG] Quantized: tensor([[-1.,  1., -0.,  ..., -0.,  1., -1.],\n",
      "        [ 1., -1., -0.,  ..., -1.,  1.,  1.],\n",
      "        [ 1.,  1.,  1.,  ..., -1.,  1., -1.],\n",
      "        ...,\n",
      "        [ 0., -0.,  1.,  ...,  1., -1., -0.],\n",
      "        [ 1.,  1.,  0.,  ...,  1.,  1., -0.],\n",
      "        [-1.,  1., -1.,  ..., -1.,  1., -1.]])| \n",
      "[Quantized] model.layers.2.mlp.up_proj | Ternary weights applied.\n",
      "[DEBUG] Quantized: tensor([[ 1., -1.,  1.,  ...,  1.,  1., -1.],\n",
      "        [-0., -1., -0.,  ...,  0., -1., -1.],\n",
      "        [ 0., -1., -1.,  ...,  1., -1., -1.],\n",
      "        ...,\n",
      "        [-1.,  0., -1.,  ...,  1., -1.,  1.],\n",
      "        [ 1.,  1., -1.,  ..., -0.,  1.,  1.],\n",
      "        [-1., -1., -1.,  ..., -1., -0., -1.]])| \n",
      "[Quantized] model.layers.2.mlp.down_proj | Ternary weights applied.\n",
      "[DEBUG] Quantized: tensor([[-0., -0.,  1.,  ...,  0., -0.,  0.],\n",
      "        [ 1., -0.,  0.,  ...,  0., -1.,  0.],\n",
      "        [ 0., -0.,  0.,  ...,  0., -1.,  0.],\n",
      "        ...,\n",
      "        [-1.,  1.,  1.,  ...,  1.,  1., -0.],\n",
      "        [-1., -1.,  1.,  ...,  1.,  1.,  1.],\n",
      "        [-0., -0.,  1.,  ..., -1.,  1., -1.]])| \n",
      "[Quantized] model.layers.3.self_attn.q_proj | Ternary weights applied.\n",
      "[DEBUG] Quantized: tensor([[-1.,  0., -0.,  ...,  1.,  1.,  1.],\n",
      "        [ 0., -1., -1.,  ...,  0., -0.,  1.],\n",
      "        [-1.,  1.,  1.,  ...,  0.,  0., -1.],\n",
      "        ...,\n",
      "        [ 1., -0.,  1.,  ...,  1.,  1., -1.],\n",
      "        [-1.,  1.,  0.,  ..., -1., -1., -1.],\n",
      "        [ 0.,  0.,  1.,  ..., -1.,  1.,  1.]])| \n",
      "[Quantized] model.layers.3.self_attn.k_proj | Ternary weights applied.\n",
      "[DEBUG] Quantized: tensor([[-0., -0.,  0.,  ..., -0., -1., -0.],\n",
      "        [-1.,  1.,  1.,  ...,  0., -1.,  1.],\n",
      "        [ 1.,  0., -0.,  ..., -1., -1.,  1.],\n",
      "        ...,\n",
      "        [ 1.,  1., -0.,  ..., -1.,  0.,  0.],\n",
      "        [ 1., -0.,  1.,  ..., -1.,  1.,  1.],\n",
      "        [-1.,  0., -0.,  ...,  0., -1., -1.]])| \n",
      "[Quantized] model.layers.3.self_attn.v_proj | Ternary weights applied.\n",
      "[DEBUG] Quantized: tensor([[ 1., -0.,  0.,  ...,  1.,  0.,  1.],\n",
      "        [ 1.,  0., -1.,  ..., -1., -1.,  0.],\n",
      "        [-1.,  0., -0.,  ..., -0.,  1., -1.],\n",
      "        ...,\n",
      "        [-1., -1., -1.,  ..., -1., -0.,  1.],\n",
      "        [ 1., -0., -1.,  ...,  1.,  1., -1.],\n",
      "        [-1.,  1.,  1.,  ..., -0.,  1., -0.]])| \n",
      "[Quantized] model.layers.3.self_attn.o_proj | Ternary weights applied.\n",
      "[DEBUG] Quantized: tensor([[ 1., -0., -1.,  ..., -1., -0., -1.],\n",
      "        [ 1., -1.,  1.,  ..., -1., -1.,  0.],\n",
      "        [-0., -1., -0.,  ...,  1., -1.,  1.],\n",
      "        ...,\n",
      "        [ 1., -1.,  1.,  ...,  0.,  0.,  0.],\n",
      "        [ 1.,  0.,  0.,  ...,  0.,  1., -0.],\n",
      "        [ 0., -1.,  0.,  ...,  0.,  1., -0.]])| \n",
      "[Quantized] model.layers.3.mlp.gate_proj | Ternary weights applied.\n",
      "[DEBUG] Quantized: tensor([[ 1.,  0.,  0.,  ...,  0., -1., -1.],\n",
      "        [ 1., -0., -1.,  ..., -1., -1.,  1.],\n",
      "        [-1., -1., -0.,  ...,  1., -1., -0.],\n",
      "        ...,\n",
      "        [-1., -1., -1.,  ...,  0., -1., -0.],\n",
      "        [-1., -0.,  0.,  ..., -1.,  1.,  0.],\n",
      "        [ 1.,  1.,  1.,  ..., -1., -1., -0.]])| \n",
      "[Quantized] model.layers.3.mlp.up_proj | Ternary weights applied.\n",
      "[DEBUG] Quantized: tensor([[ 1., -0.,  1.,  ...,  1.,  1., -1.],\n",
      "        [ 0., -1.,  1.,  ..., -0., -1.,  1.],\n",
      "        [-1.,  0.,  0.,  ..., -1., -0.,  1.],\n",
      "        ...,\n",
      "        [ 1., -1.,  1.,  ...,  0., -1., -1.],\n",
      "        [-1., -1., -1.,  ...,  1.,  0.,  1.],\n",
      "        [ 1.,  1.,  0.,  ..., -1.,  1., -0.]])| \n",
      "[Quantized] model.layers.3.mlp.down_proj | Ternary weights applied.\n",
      "[DEBUG] Quantized: tensor([[ 1., -1., -1.,  ..., -0., -0.,  1.],\n",
      "        [-1.,  0.,  0.,  ...,  0.,  0., -0.],\n",
      "        [ 1.,  0., -0.,  ..., -0.,  1., -1.],\n",
      "        ...,\n",
      "        [-0.,  0.,  1.,  ...,  0.,  0., -1.],\n",
      "        [ 1.,  1., -1.,  ..., -1., -1.,  0.],\n",
      "        [-0., -1.,  1.,  ...,  0., -1.,  1.]])| \n",
      "[Quantized] model.layers.4.self_attn.q_proj | Ternary weights applied.\n",
      "[DEBUG] Quantized: tensor([[ 1., -1.,  1.,  ...,  1., -0.,  0.],\n",
      "        [ 0., -0., -1.,  ..., -1., -1., -0.],\n",
      "        [-0., -1.,  1.,  ...,  1., -1.,  1.],\n",
      "        ...,\n",
      "        [ 1.,  1.,  1.,  ..., -1., -1., -1.],\n",
      "        [ 0., -1., -1.,  ...,  1.,  0.,  1.],\n",
      "        [ 1., -0.,  1.,  ..., -1.,  1., -0.]])| \n",
      "[Quantized] model.layers.4.self_attn.k_proj | Ternary weights applied.\n",
      "[DEBUG] Quantized: tensor([[-1., -1.,  0.,  ...,  1.,  1., -1.],\n",
      "        [ 1., -1., -0.,  ...,  1.,  1., -0.],\n",
      "        [ 1., -0.,  0.,  ...,  0., -1., -1.],\n",
      "        ...,\n",
      "        [ 1.,  1., -1.,  ..., -0., -0., -1.],\n",
      "        [-1.,  0., -0.,  ..., -0.,  0.,  1.],\n",
      "        [ 0., -0., -0.,  ..., -0.,  1., -1.]])| \n",
      "[Quantized] model.layers.4.self_attn.v_proj | Ternary weights applied.\n",
      "[DEBUG] Quantized: tensor([[ 0., -1.,  0.,  ...,  1., -1.,  0.],\n",
      "        [ 1., -1., -1.,  ...,  1.,  0., -1.],\n",
      "        [ 0.,  0.,  1.,  ...,  0., -0.,  1.],\n",
      "        ...,\n",
      "        [-0., -0., -1.,  ...,  1., -1., -0.],\n",
      "        [ 1.,  1.,  1.,  ...,  1.,  1.,  1.],\n",
      "        [ 1., -0.,  1.,  ..., -0.,  1.,  1.]])| \n",
      "[Quantized] model.layers.4.self_attn.o_proj | Ternary weights applied.\n",
      "[DEBUG] Quantized: tensor([[ 0.,  0.,  0.,  ..., -0.,  0., -1.],\n",
      "        [-1.,  0.,  0.,  ...,  1., -0., -0.],\n",
      "        [ 1., -0., -0.,  ..., -0., -1., -0.],\n",
      "        ...,\n",
      "        [ 0.,  1.,  1.,  ..., -0.,  1.,  0.],\n",
      "        [-0.,  1.,  1.,  ...,  1., -1., -0.],\n",
      "        [ 1.,  1., -1.,  ..., -0., -1., -1.]])| \n",
      "[Quantized] model.layers.4.mlp.gate_proj | Ternary weights applied.\n",
      "[DEBUG] Quantized: tensor([[ 1., -1., -0.,  ...,  0., -1., -0.],\n",
      "        [ 1., -1., -1.,  ...,  0.,  1.,  1.],\n",
      "        [ 1.,  1., -0.,  ...,  1., -1., -1.],\n",
      "        ...,\n",
      "        [ 1.,  0.,  0.,  ..., -0.,  0.,  1.],\n",
      "        [-0.,  1., -1.,  ...,  1., -1.,  1.],\n",
      "        [-1., -0.,  0.,  ...,  1., -1.,  0.]])| \n",
      "[Quantized] model.layers.4.mlp.up_proj | Ternary weights applied.\n",
      "[DEBUG] Quantized: tensor([[ 0., -1., -1.,  ..., -1., -0., -1.],\n",
      "        [ 1., -1.,  1.,  ..., -0.,  1.,  0.],\n",
      "        [ 0., -0.,  0.,  ..., -1., -0.,  0.],\n",
      "        ...,\n",
      "        [-0.,  0.,  0.,  ..., -0.,  1.,  1.],\n",
      "        [ 1.,  1., -1.,  ..., -0., -0., -0.],\n",
      "        [ 1.,  1.,  0.,  ..., -1.,  1.,  1.]])| \n",
      "[Quantized] model.layers.4.mlp.down_proj | Ternary weights applied.\n",
      "[DEBUG] Quantized: tensor([[-0., -1., -1.,  ..., -0., -0., -0.],\n",
      "        [-1.,  1., -1.,  ..., -0.,  1., -1.],\n",
      "        [-1., -0., -1.,  ...,  0.,  0.,  1.],\n",
      "        ...,\n",
      "        [ 1.,  0., -1.,  ..., -1., -1., -0.],\n",
      "        [-1.,  1., -1.,  ..., -1., -1.,  1.],\n",
      "        [ 1., -0., -1.,  ..., -0.,  1., -1.]])| \n",
      "[Quantized] model.layers.5.self_attn.q_proj | Ternary weights applied.\n",
      "[DEBUG] Quantized: tensor([[ 0.,  1., -0.,  ...,  0.,  0.,  1.],\n",
      "        [ 1.,  1.,  1.,  ..., -1.,  1.,  1.],\n",
      "        [ 0., -0.,  1.,  ...,  0.,  1.,  0.],\n",
      "        ...,\n",
      "        [-1., -1.,  1.,  ...,  1.,  1., -1.],\n",
      "        [-0.,  1.,  0.,  ..., -1., -1.,  1.],\n",
      "        [ 1.,  1., -1.,  ...,  0., -1.,  0.]])| \n",
      "[Quantized] model.layers.5.self_attn.k_proj | Ternary weights applied.\n",
      "[DEBUG] Quantized: tensor([[-1.,  1., -0.,  ...,  0.,  1.,  0.],\n",
      "        [ 1.,  0., -0.,  ..., -0.,  0., -1.],\n",
      "        [ 0., -1., -0.,  ..., -1.,  1., -1.],\n",
      "        ...,\n",
      "        [-1.,  0., -0.,  ...,  0.,  1.,  1.],\n",
      "        [-1.,  1., -1.,  ...,  0.,  1., -0.],\n",
      "        [-0., -0., -0.,  ..., -0.,  0.,  1.]])| \n",
      "[Quantized] model.layers.5.self_attn.v_proj | Ternary weights applied.\n",
      "[DEBUG] Quantized: tensor([[-1., -1.,  1.,  ..., -1., -0.,  1.],\n",
      "        [-0.,  1., -1.,  ..., -1.,  1.,  1.],\n",
      "        [ 0., -0., -0.,  ..., -0.,  0., -1.],\n",
      "        ...,\n",
      "        [ 0., -1.,  1.,  ...,  1., -1., -1.],\n",
      "        [-0., -1., -1.,  ..., -1., -1.,  1.],\n",
      "        [ 1.,  1., -0.,  ...,  1.,  1., -1.]])| \n",
      "[Quantized] model.layers.5.self_attn.o_proj | Ternary weights applied.\n",
      "[DEBUG] Quantized: tensor([[-0.,  0., -0.,  ...,  1.,  1.,  0.],\n",
      "        [-0.,  1.,  0.,  ...,  1.,  1., -1.],\n",
      "        [-1.,  1.,  1.,  ...,  1., -1., -1.],\n",
      "        ...,\n",
      "        [-1., -1., -1.,  ..., -1., -1.,  1.],\n",
      "        [-1., -1.,  1.,  ...,  1., -1.,  1.],\n",
      "        [ 1., -1., -1.,  ...,  1.,  1., -1.]])| \n",
      "[Quantized] model.layers.5.mlp.gate_proj | Ternary weights applied.\n",
      "[DEBUG] Quantized: tensor([[ 1.,  1., -1.,  ...,  0.,  0., -1.],\n",
      "        [ 0.,  0.,  1.,  ...,  1., -1.,  1.],\n",
      "        [-1., -1., -1.,  ...,  1.,  1.,  1.],\n",
      "        ...,\n",
      "        [-1., -1.,  1.,  ...,  1.,  1., -0.],\n",
      "        [-1.,  1., -1.,  ...,  1., -1.,  0.],\n",
      "        [-1., -1.,  1.,  ..., -1.,  1., -1.]])| \n",
      "[Quantized] model.layers.5.mlp.up_proj | Ternary weights applied.\n",
      "[DEBUG] Quantized: tensor([[ 1.,  0., -0.,  ..., -1., -1., -0.],\n",
      "        [ 0., -1., -0.,  ..., -1., -1., -1.],\n",
      "        [-1.,  0., -1.,  ...,  0.,  0.,  0.],\n",
      "        ...,\n",
      "        [ 1.,  1.,  0.,  ...,  1., -1., -0.],\n",
      "        [ 1., -1.,  1.,  ...,  1., -1.,  1.],\n",
      "        [-0.,  1.,  1.,  ..., -1.,  1., -1.]])| \n",
      "[Quantized] model.layers.5.mlp.down_proj | Ternary weights applied.\n",
      "[DEBUG] Quantized: tensor([[-0.,  1.,  1.,  ...,  0.,  1., -1.],\n",
      "        [-1., -1., -0.,  ..., -0., -1.,  0.],\n",
      "        [ 1., -1., -0.,  ..., -1.,  1., -0.],\n",
      "        ...,\n",
      "        [-0.,  1., -1.,  ..., -1., -0., -1.],\n",
      "        [-1.,  0.,  0.,  ..., -1.,  1.,  1.],\n",
      "        [-0.,  0., -1.,  ...,  0., -1., -0.]])| \n",
      "[Quantized] model.layers.6.self_attn.q_proj | Ternary weights applied.\n",
      "[DEBUG] Quantized: tensor([[-1., -1.,  1.,  ..., -0., -1.,  1.],\n",
      "        [ 1., -1., -0.,  ...,  0.,  0., -0.],\n",
      "        [ 1.,  1.,  0.,  ..., -0.,  1.,  0.],\n",
      "        ...,\n",
      "        [ 1.,  1., -1.,  ..., -1.,  0., -0.],\n",
      "        [-1.,  0., -1.,  ..., -0., -1., -1.],\n",
      "        [ 1.,  0.,  1.,  ...,  1., -1.,  0.]])| \n",
      "[Quantized] model.layers.6.self_attn.k_proj | Ternary weights applied.\n",
      "[DEBUG] Quantized: tensor([[-1.,  1.,  1.,  ...,  0., -1.,  1.],\n",
      "        [ 1., -1., -1.,  ...,  1., -1.,  1.],\n",
      "        [ 0.,  1., -1.,  ...,  0.,  1., -1.],\n",
      "        ...,\n",
      "        [-1.,  1.,  0.,  ...,  1., -1.,  1.],\n",
      "        [-1.,  1., -1.,  ..., -0.,  1.,  1.],\n",
      "        [-1.,  0., -1.,  ..., -0., -0.,  1.]])| \n",
      "[Quantized] model.layers.6.self_attn.v_proj | Ternary weights applied.\n",
      "[DEBUG] Quantized: tensor([[ 1.,  1., -1.,  ..., -1., -0., -1.],\n",
      "        [ 1., -1.,  1.,  ...,  1.,  0.,  0.],\n",
      "        [-0.,  0., -1.,  ..., -1., -1.,  0.],\n",
      "        ...,\n",
      "        [-0., -1.,  1.,  ...,  1., -1.,  1.],\n",
      "        [ 1.,  0.,  1.,  ..., -1., -1., -1.],\n",
      "        [ 1.,  1.,  1.,  ...,  1.,  0., -1.]])| \n",
      "[Quantized] model.layers.6.self_attn.o_proj | Ternary weights applied.\n",
      "[DEBUG] Quantized: tensor([[-0.,  1.,  1.,  ..., -1.,  0.,  1.],\n",
      "        [ 1., -1.,  1.,  ..., -1.,  0., -1.],\n",
      "        [-1., -0., -1.,  ...,  1.,  1., -1.],\n",
      "        ...,\n",
      "        [-1.,  0.,  1.,  ...,  0., -0., -1.],\n",
      "        [-1., -1., -1.,  ...,  0., -1., -0.],\n",
      "        [-1., -1.,  0.,  ..., -1.,  1., -1.]])| \n",
      "[Quantized] model.layers.6.mlp.gate_proj | Ternary weights applied.\n",
      "[DEBUG] Quantized: tensor([[ 0.,  1.,  0.,  ...,  1.,  1.,  0.],\n",
      "        [ 1.,  1., -1.,  ..., -0., -1.,  0.],\n",
      "        [ 0., -0., -1.,  ...,  1., -1., -1.],\n",
      "        ...,\n",
      "        [ 0.,  0., -1.,  ...,  1., -1., -0.],\n",
      "        [ 1., -0., -1.,  ...,  1.,  1.,  0.],\n",
      "        [-1.,  0.,  1.,  ..., -1.,  1.,  1.]])| \n",
      "[Quantized] model.layers.6.mlp.up_proj | Ternary weights applied.\n",
      "[DEBUG] Quantized: tensor([[-1.,  0.,  1.,  ..., -1.,  0.,  1.],\n",
      "        [ 1.,  0.,  1.,  ..., -1., -1.,  0.],\n",
      "        [ 1.,  1.,  0.,  ...,  0., -0.,  0.],\n",
      "        ...,\n",
      "        [ 1.,  0., -1.,  ..., -0.,  1., -1.],\n",
      "        [ 1.,  0., -1.,  ...,  1., -0.,  1.],\n",
      "        [ 1.,  1., -1.,  ...,  0., -1.,  1.]])| \n",
      "[Quantized] model.layers.6.mlp.down_proj | Ternary weights applied.\n",
      "[DEBUG] Quantized: tensor([[ 1., -0.,  0.,  ..., -1., -1., -0.],\n",
      "        [ 0.,  0.,  0.,  ..., -1., -0., -1.],\n",
      "        [-0., -1., -1.,  ..., -1., -1.,  1.],\n",
      "        ...,\n",
      "        [-1.,  1., -0.,  ...,  1., -0.,  1.],\n",
      "        [-1., -1.,  1.,  ..., -1., -1.,  1.],\n",
      "        [-1., -1.,  1.,  ...,  0., -0.,  1.]])| \n",
      "[Quantized] model.layers.7.self_attn.q_proj | Ternary weights applied.\n",
      "[DEBUG] Quantized: tensor([[-1.,  1., -1.,  ..., -0., -1.,  0.],\n",
      "        [-0.,  1., -1.,  ..., -1., -1.,  0.],\n",
      "        [-1.,  0., -1.,  ..., -0., -1., -1.],\n",
      "        ...,\n",
      "        [ 1.,  1.,  1.,  ..., -0.,  1., -1.],\n",
      "        [-1., -1., -1.,  ...,  1.,  1., -1.],\n",
      "        [ 0.,  1.,  1.,  ..., -0., -0.,  1.]])| \n",
      "[Quantized] model.layers.7.self_attn.k_proj | Ternary weights applied.\n",
      "[DEBUG] Quantized: tensor([[ 1., -0.,  0.,  ..., -1.,  1., -0.],\n",
      "        [ 0.,  1.,  0.,  ..., -1., -0.,  1.],\n",
      "        [-1., -0.,  1.,  ...,  0., -1.,  0.],\n",
      "        ...,\n",
      "        [-1.,  1.,  1.,  ...,  1., -0., -0.],\n",
      "        [ 1., -1., -1.,  ..., -1., -1.,  1.],\n",
      "        [-1.,  0.,  0.,  ...,  1.,  1.,  0.]])| \n",
      "[Quantized] model.layers.7.self_attn.v_proj | Ternary weights applied.\n",
      "[DEBUG] Quantized: tensor([[ 1.,  1.,  1.,  ..., -1., -1.,  0.],\n",
      "        [ 0., -1., -0.,  ...,  0., -1., -0.],\n",
      "        [ 1.,  0., -1.,  ..., -0., -0.,  0.],\n",
      "        ...,\n",
      "        [-0., -1., -1.,  ..., -1., -0., -0.],\n",
      "        [ 1.,  1., -0.,  ...,  1., -1.,  1.],\n",
      "        [-1., -1.,  1.,  ...,  0., -0.,  1.]])| \n",
      "[Quantized] model.layers.7.self_attn.o_proj | Ternary weights applied.\n",
      "[DEBUG] Quantized: tensor([[-1., -1., -0.,  ...,  0.,  1.,  0.],\n",
      "        [-1., -1.,  1.,  ..., -0., -0.,  1.],\n",
      "        [ 1.,  1.,  1.,  ...,  1., -1., -1.],\n",
      "        ...,\n",
      "        [-1.,  1.,  0.,  ..., -0., -0., -0.],\n",
      "        [ 1., -1., -1.,  ..., -0., -1.,  1.],\n",
      "        [ 0., -0.,  1.,  ...,  1., -0.,  1.]])| \n",
      "[Quantized] model.layers.7.mlp.gate_proj | Ternary weights applied.\n",
      "[DEBUG] Quantized: tensor([[ 1., -0., -1.,  ..., -1.,  1.,  0.],\n",
      "        [ 0.,  1., -0.,  ...,  0.,  1.,  1.],\n",
      "        [ 1.,  1.,  1.,  ..., -1.,  1.,  0.],\n",
      "        ...,\n",
      "        [ 1.,  1., -0.,  ...,  1., -0., -1.],\n",
      "        [-1., -1.,  1.,  ..., -0., -0.,  1.],\n",
      "        [-1.,  1., -1.,  ...,  1., -0., -0.]])| \n",
      "[Quantized] model.layers.7.mlp.up_proj | Ternary weights applied.\n",
      "[DEBUG] Quantized: tensor([[ 1.,  1.,  1.,  ..., -1., -0., -1.],\n",
      "        [ 1.,  1., -1.,  ...,  1., -0.,  1.],\n",
      "        [ 1.,  0., -0.,  ...,  1.,  1.,  0.],\n",
      "        ...,\n",
      "        [-0., -0., -1.,  ...,  1., -1.,  1.],\n",
      "        [-1., -0.,  0.,  ..., -1.,  1., -0.],\n",
      "        [ 0.,  1., -0.,  ...,  0., -0.,  1.]])| \n",
      "[Quantized] model.layers.7.mlp.down_proj | Ternary weights applied.\n",
      "[DEBUG] Quantized: tensor([[-1., -1.,  1.,  ...,  1., -0.,  0.],\n",
      "        [-0., -1.,  1.,  ...,  1.,  1.,  0.],\n",
      "        [-0., -1., -0.,  ...,  0.,  0., -1.],\n",
      "        ...,\n",
      "        [-0.,  1., -1.,  ..., -1., -1.,  1.],\n",
      "        [-1., -1., -0.,  ...,  0., -1., -0.],\n",
      "        [-0., -1., -1.,  ...,  0., -1.,  1.]])| \n",
      "[Quantized] model.layers.8.self_attn.q_proj | Ternary weights applied.\n",
      "[DEBUG] Quantized: tensor([[-0., -0., -1.,  ..., -0.,  0.,  0.],\n",
      "        [ 1., -0., -0.,  ...,  1.,  0.,  1.],\n",
      "        [ 0.,  1.,  0.,  ..., -0., -0., -0.],\n",
      "        ...,\n",
      "        [-1.,  1.,  0.,  ...,  1.,  1.,  1.],\n",
      "        [-0.,  0., -1.,  ...,  0.,  1.,  0.],\n",
      "        [-1., -0., -1.,  ..., -1., -1., -1.]])| \n",
      "[Quantized] model.layers.8.self_attn.k_proj | Ternary weights applied.\n",
      "[DEBUG] Quantized: tensor([[ 1., -0., -1.,  ..., -0.,  0., -1.],\n",
      "        [-0., -0.,  1.,  ..., -1., -1., -1.],\n",
      "        [ 0., -0., -0.,  ..., -1., -1., -1.],\n",
      "        ...,\n",
      "        [ 1.,  1., -0.,  ...,  0.,  1.,  1.],\n",
      "        [ 0.,  1., -0.,  ...,  0., -0., -1.],\n",
      "        [ 1., -1., -1.,  ..., -0., -0., -1.]])| \n",
      "[Quantized] model.layers.8.self_attn.v_proj | Ternary weights applied.\n",
      "[DEBUG] Quantized: tensor([[ 1.,  1.,  1.,  ..., -1., -1., -1.],\n",
      "        [-1., -1., -1.,  ...,  1., -1., -0.],\n",
      "        [ 0.,  0.,  0.,  ...,  1.,  0.,  0.],\n",
      "        ...,\n",
      "        [ 1., -1.,  0.,  ...,  1.,  0., -0.],\n",
      "        [-1.,  0.,  1.,  ..., -0.,  1.,  1.],\n",
      "        [ 1., -1., -0.,  ..., -1.,  1., -0.]])| \n",
      "[Quantized] model.layers.8.self_attn.o_proj | Ternary weights applied.\n",
      "[DEBUG] Quantized: tensor([[ 0.,  1., -1.,  ...,  0., -1.,  0.],\n",
      "        [-0., -0., -0.,  ...,  0.,  0., -0.],\n",
      "        [-1.,  0.,  1.,  ..., -1., -1., -1.],\n",
      "        ...,\n",
      "        [ 1., -1.,  1.,  ...,  1., -0.,  1.],\n",
      "        [-1., -1.,  0.,  ...,  1., -1., -1.],\n",
      "        [-1.,  0., -0.,  ..., -1.,  1., -0.]])| \n",
      "[Quantized] model.layers.8.mlp.gate_proj | Ternary weights applied.\n",
      "[DEBUG] Quantized: tensor([[ 1., -1., -1.,  ..., -1., -0.,  1.],\n",
      "        [-0., -1.,  1.,  ..., -1.,  0., -0.],\n",
      "        [ 1.,  1.,  0.,  ..., -1., -1., -1.],\n",
      "        ...,\n",
      "        [ 1., -0., -0.,  ..., -1., -1., -1.],\n",
      "        [ 0.,  0., -0.,  ..., -1., -1.,  1.],\n",
      "        [ 0.,  1.,  0.,  ...,  1., -1., -0.]])| \n",
      "[Quantized] model.layers.8.mlp.up_proj | Ternary weights applied.\n",
      "[DEBUG] Quantized: tensor([[ 1.,  1.,  0.,  ...,  1.,  1., -1.],\n",
      "        [-1., -1.,  1.,  ..., -1., -0.,  1.],\n",
      "        [ 0.,  0., -0.,  ...,  1., -0.,  0.],\n",
      "        ...,\n",
      "        [-1., -1.,  1.,  ..., -1.,  0.,  0.],\n",
      "        [-1.,  1., -0.,  ...,  0., -1., -1.],\n",
      "        [-0.,  0., -1.,  ..., -1.,  0., -1.]])| \n",
      "[Quantized] model.layers.8.mlp.down_proj | Ternary weights applied.\n",
      "[DEBUG] Quantized: tensor([[-0.,  0.,  1.,  ...,  1.,  1., -0.],\n",
      "        [-1.,  1.,  1.,  ..., -1., -0., -1.],\n",
      "        [-1., -0., -0.,  ..., -1., -1., -0.],\n",
      "        ...,\n",
      "        [-1.,  0., -1.,  ..., -1.,  1.,  0.],\n",
      "        [-1.,  1.,  0.,  ...,  1., -1.,  1.],\n",
      "        [ 0.,  0., -1.,  ..., -1., -1., -1.]])| \n",
      "[Quantized] model.layers.9.self_attn.q_proj | Ternary weights applied.\n",
      "[DEBUG] Quantized: tensor([[-0.,  0., -0.,  ...,  0.,  1.,  1.],\n",
      "        [ 1.,  0., -1.,  ..., -0., -0.,  0.],\n",
      "        [ 0., -1., -0.,  ..., -1.,  0., -1.],\n",
      "        ...,\n",
      "        [-1.,  1., -1.,  ..., -0., -1.,  1.],\n",
      "        [-1.,  1., -0.,  ..., -1., -1., -1.],\n",
      "        [ 0., -0.,  1.,  ...,  1., -0.,  0.]])| \n",
      "[Quantized] model.layers.9.self_attn.k_proj | Ternary weights applied.\n",
      "[DEBUG] Quantized: tensor([[-0., -0.,  0.,  ..., -1., -1., -1.],\n",
      "        [ 1., -1., -0.,  ..., -0.,  1.,  0.],\n",
      "        [-0.,  1., -0.,  ..., -0., -1., -0.],\n",
      "        ...,\n",
      "        [ 1.,  0.,  0.,  ...,  1., -1., -1.],\n",
      "        [-1.,  1., -1.,  ..., -0., -1.,  1.],\n",
      "        [-0.,  1.,  1.,  ..., -0., -1.,  1.]])| \n",
      "[Quantized] model.layers.9.self_attn.v_proj | Ternary weights applied.\n",
      "[DEBUG] Quantized: tensor([[-1.,  1., -1.,  ...,  1.,  1.,  1.],\n",
      "        [ 0.,  1., -0.,  ...,  1.,  1.,  0.],\n",
      "        [-1.,  0., -0.,  ...,  0.,  0.,  0.],\n",
      "        ...,\n",
      "        [-1.,  1., -1.,  ...,  1.,  1., -1.],\n",
      "        [-1., -1., -0.,  ...,  0., -1., -0.],\n",
      "        [ 1.,  1., -1.,  ..., -1., -0.,  1.]])| \n",
      "[Quantized] model.layers.9.self_attn.o_proj | Ternary weights applied.\n",
      "[DEBUG] Quantized: tensor([[ 1.,  1.,  0.,  ..., -0., -0.,  1.],\n",
      "        [ 1., -1.,  1.,  ...,  0.,  1., -1.],\n",
      "        [ 1.,  0.,  0.,  ...,  1., -0., -1.],\n",
      "        ...,\n",
      "        [ 0., -0., -0.,  ..., -1.,  0., -1.],\n",
      "        [ 0.,  1., -1.,  ..., -0.,  1.,  1.],\n",
      "        [ 0., -0., -1.,  ..., -1.,  0.,  1.]])| \n",
      "[Quantized] model.layers.9.mlp.gate_proj | Ternary weights applied.\n",
      "[DEBUG] Quantized: tensor([[-0., -1., -1.,  ..., -1., -0.,  1.],\n",
      "        [ 1., -1.,  0.,  ...,  0., -1.,  1.],\n",
      "        [-1., -0.,  0.,  ...,  0.,  1.,  1.],\n",
      "        ...,\n",
      "        [ 0., -0., -0.,  ...,  1., -1., -1.],\n",
      "        [-1., -1., -1.,  ...,  0., -1.,  1.],\n",
      "        [-1., -1., -1.,  ..., -1.,  0.,  1.]])| \n",
      "[Quantized] model.layers.9.mlp.up_proj | Ternary weights applied.\n",
      "[DEBUG] Quantized: tensor([[ 1.,  1., -1.,  ...,  1., -1.,  0.],\n",
      "        [ 0., -1., -1.,  ..., -0., -1.,  1.],\n",
      "        [ 0.,  0., -1.,  ..., -0.,  0.,  1.],\n",
      "        ...,\n",
      "        [-1.,  0.,  1.,  ...,  1.,  0.,  1.],\n",
      "        [-1., -0., -1.,  ..., -1., -1.,  1.],\n",
      "        [ 1.,  1.,  0.,  ..., -1.,  1., -1.]])| \n",
      "[Quantized] model.layers.9.mlp.down_proj | Ternary weights applied.\n",
      "[DEBUG] Quantized: tensor([[ 0.,  1.,  1.,  ..., -1., -1., -1.],\n",
      "        [-1., -0., -1.,  ..., -1., -1., -1.],\n",
      "        [ 0.,  1., -1.,  ...,  1., -1.,  0.],\n",
      "        ...,\n",
      "        [-1., -1., -0.,  ...,  1., -0., -1.],\n",
      "        [-1.,  1.,  1.,  ..., -1., -1.,  1.],\n",
      "        [ 1.,  1.,  1.,  ..., -1.,  1., -0.]])| \n",
      "[Quantized] model.layers.10.self_attn.q_proj | Ternary weights applied.\n",
      "[DEBUG] Quantized: tensor([[-0.,  0., -1.,  ..., -1., -1., -0.],\n",
      "        [ 1.,  0., -1.,  ..., -0., -0., -0.],\n",
      "        [ 0., -0.,  0.,  ..., -0., -0.,  0.],\n",
      "        ...,\n",
      "        [-1., -0.,  1.,  ...,  1.,  0., -1.],\n",
      "        [-1., -1., -0.,  ..., -1.,  1.,  0.],\n",
      "        [ 1.,  1., -1.,  ..., -1.,  1.,  1.]])| \n",
      "[Quantized] model.layers.10.self_attn.k_proj | Ternary weights applied.\n",
      "[DEBUG] Quantized: tensor([[-1., -0.,  1.,  ..., -1.,  0.,  1.],\n",
      "        [-0., -1.,  0.,  ..., -1.,  1., -1.],\n",
      "        [-1., -1., -1.,  ..., -1.,  1., -1.],\n",
      "        ...,\n",
      "        [ 1.,  1., -1.,  ...,  0.,  1.,  0.],\n",
      "        [ 1., -1.,  1.,  ..., -0., -1., -1.],\n",
      "        [-0.,  1., -1.,  ..., -1., -0.,  1.]])| \n",
      "[Quantized] model.layers.10.self_attn.v_proj | Ternary weights applied.\n",
      "[DEBUG] Quantized: tensor([[-1.,  1.,  1.,  ..., -0.,  1., -1.],\n",
      "        [ 0.,  1.,  1.,  ...,  1.,  0., -1.],\n",
      "        [ 0.,  0., -0.,  ...,  0.,  1., -0.],\n",
      "        ...,\n",
      "        [-1.,  1., -0.,  ..., -0.,  1., -0.],\n",
      "        [-0., -1., -1.,  ..., -1.,  1., -0.],\n",
      "        [ 1.,  1., -0.,  ..., -0.,  1.,  1.]])| \n",
      "[Quantized] model.layers.10.self_attn.o_proj | Ternary weights applied.\n",
      "[DEBUG] Quantized: tensor([[-1.,  1., -1.,  ..., -1.,  1.,  1.],\n",
      "        [ 1.,  0., -0.,  ..., -1., -1.,  1.],\n",
      "        [ 1.,  1., -1.,  ...,  1., -1., -1.],\n",
      "        ...,\n",
      "        [-0., -1.,  1.,  ..., -0., -0., -0.],\n",
      "        [-1.,  1., -0.,  ...,  1., -0.,  1.],\n",
      "        [-1.,  1., -1.,  ..., -1.,  1., -1.]])| \n",
      "[Quantized] model.layers.10.mlp.gate_proj | Ternary weights applied.\n",
      "[DEBUG] Quantized: tensor([[ 1.,  1.,  1.,  ..., -1.,  1.,  1.],\n",
      "        [ 0., -1., -1.,  ..., -0., -1.,  1.],\n",
      "        [ 1., -1.,  0.,  ...,  0.,  0., -0.],\n",
      "        ...,\n",
      "        [-1., -1., -1.,  ..., -0., -0., -0.],\n",
      "        [-1.,  1., -1.,  ...,  0., -1., -1.],\n",
      "        [ 1., -1., -1.,  ...,  0., -1., -1.]])| \n",
      "[Quantized] model.layers.10.mlp.up_proj | Ternary weights applied.\n",
      "[DEBUG] Quantized: tensor([[ 0., -1.,  1.,  ...,  1., -0.,  0.],\n",
      "        [ 1., -1.,  1.,  ...,  1.,  1.,  1.],\n",
      "        [ 1.,  0., -0.,  ...,  1., -1.,  0.],\n",
      "        ...,\n",
      "        [ 1.,  1.,  1.,  ...,  0.,  1., -1.],\n",
      "        [ 1., -1., -1.,  ..., -0., -0., -1.],\n",
      "        [ 1., -1., -1.,  ...,  1.,  0., -1.]])| \n",
      "[Quantized] model.layers.10.mlp.down_proj | Ternary weights applied.\n",
      "[DEBUG] Quantized: tensor([[ 1.,  1., -1.,  ...,  1., -0., -1.],\n",
      "        [ 1., -0.,  1.,  ...,  1.,  1.,  1.],\n",
      "        [-0., -1.,  0.,  ...,  1.,  1.,  1.],\n",
      "        ...,\n",
      "        [-0., -1., -1.,  ..., -1., -1., -1.],\n",
      "        [ 1., -0., -1.,  ...,  1.,  1.,  0.],\n",
      "        [-1., -1.,  1.,  ..., -1., -1.,  1.]])| \n",
      "[Quantized] model.layers.11.self_attn.q_proj | Ternary weights applied.\n",
      "[DEBUG] Quantized: tensor([[-0., -0.,  0.,  ..., -1., -0., -0.],\n",
      "        [-1.,  0.,  1.,  ...,  0., -0.,  0.],\n",
      "        [-1.,  0., -1.,  ..., -1., -0., -1.],\n",
      "        ...,\n",
      "        [ 0., -0.,  0.,  ...,  0., -1., -1.],\n",
      "        [ 1.,  1.,  1.,  ..., -1.,  1., -1.],\n",
      "        [-1.,  1., -0.,  ...,  0., -0.,  0.]])| \n",
      "[Quantized] model.layers.11.self_attn.k_proj | Ternary weights applied.\n",
      "[DEBUG] Quantized: tensor([[-1.,  0.,  0.,  ...,  0., -1., -1.],\n",
      "        [ 1.,  1., -0.,  ..., -1.,  0., -1.],\n",
      "        [-0.,  1., -0.,  ...,  1.,  0., -1.],\n",
      "        ...,\n",
      "        [ 0., -1., -0.,  ..., -0.,  0., -1.],\n",
      "        [-1.,  0., -1.,  ..., -1.,  0., -1.],\n",
      "        [-0.,  1., -0.,  ..., -1.,  0., -1.]])| \n",
      "[Quantized] model.layers.11.self_attn.v_proj | Ternary weights applied.\n",
      "[DEBUG] Quantized: tensor([[ 0., -1.,  1.,  ..., -1.,  1., -1.],\n",
      "        [ 1., -0.,  1.,  ...,  1., -1., -1.],\n",
      "        [-1.,  1., -0.,  ..., -1., -0.,  0.],\n",
      "        ...,\n",
      "        [-0., -1., -0.,  ...,  0., -1., -1.],\n",
      "        [ 1., -1., -0.,  ..., -0.,  1., -1.],\n",
      "        [-1.,  0., -0.,  ..., -0., -0., -0.]])| \n",
      "[Quantized] model.layers.11.self_attn.o_proj | Ternary weights applied.\n",
      "[DEBUG] Quantized: tensor([[ 1.,  1.,  1.,  ...,  0.,  1.,  0.],\n",
      "        [-1., -1.,  1.,  ...,  1.,  1.,  1.],\n",
      "        [-0.,  1., -1.,  ...,  1.,  1., -1.],\n",
      "        ...,\n",
      "        [-0., -1.,  1.,  ..., -1., -1.,  1.],\n",
      "        [-1.,  1.,  1.,  ...,  1.,  1., -0.],\n",
      "        [ 1., -1.,  1.,  ...,  0.,  1., -1.]])| \n",
      "[Quantized] model.layers.11.mlp.gate_proj | Ternary weights applied.\n",
      "[DEBUG] Quantized: tensor([[ 0.,  1.,  0.,  ...,  1.,  1.,  1.],\n",
      "        [ 0.,  1., -0.,  ...,  0., -1., -0.],\n",
      "        [ 0.,  1., -0.,  ...,  1.,  1., -1.],\n",
      "        ...,\n",
      "        [-1.,  1.,  0.,  ..., -1.,  1., -1.],\n",
      "        [-1., -1.,  0.,  ..., -0.,  1., -1.],\n",
      "        [ 1., -1., -0.,  ..., -1., -0.,  1.]])| \n",
      "[Quantized] model.layers.11.mlp.up_proj | Ternary weights applied.\n",
      "[DEBUG] Quantized: tensor([[ 0.,  1.,  0.,  ..., -1., -1., -1.],\n",
      "        [-0.,  1.,  1.,  ..., -1., -1.,  1.],\n",
      "        [-0.,  1.,  0.,  ...,  0.,  1., -0.],\n",
      "        ...,\n",
      "        [ 1., -1.,  1.,  ...,  1.,  0., -1.],\n",
      "        [ 1., -0.,  0.,  ...,  1.,  1.,  0.],\n",
      "        [-0.,  1., -1.,  ..., -1.,  0.,  1.]])| \n",
      "[Quantized] model.layers.11.mlp.down_proj | Ternary weights applied.\n",
      "[DEBUG] Quantized: tensor([[-1., -1.,  1.,  ..., -0.,  0., -1.],\n",
      "        [ 0.,  0.,  1.,  ..., -1., -1.,  0.],\n",
      "        [-1., -0.,  1.,  ...,  0.,  0., -1.],\n",
      "        ...,\n",
      "        [-1.,  0.,  0.,  ..., -0.,  0., -1.],\n",
      "        [ 0., -1.,  1.,  ...,  1.,  1.,  1.],\n",
      "        [-0.,  1., -1.,  ..., -1., -1., -1.]])| \n",
      "[Quantized] model.layers.12.self_attn.q_proj | Ternary weights applied.\n",
      "[DEBUG] Quantized: tensor([[ 1., -0., -1.,  ..., -0.,  1., -0.],\n",
      "        [-1.,  0., -1.,  ...,  1., -0., -0.],\n",
      "        [ 0., -0., -1.,  ...,  0.,  0.,  0.],\n",
      "        ...,\n",
      "        [ 1., -0., -0.,  ...,  1.,  1., -1.],\n",
      "        [-0.,  1.,  1.,  ...,  1.,  1.,  0.],\n",
      "        [-1., -1., -1.,  ..., -1.,  1.,  1.]])| \n",
      "[Quantized] model.layers.12.self_attn.k_proj | Ternary weights applied.\n",
      "[DEBUG] Quantized: tensor([[-1.,  1.,  1.,  ..., -1., -1., -1.],\n",
      "        [-1.,  1., -0.,  ...,  1., -1.,  0.],\n",
      "        [ 0.,  0., -0.,  ..., -1., -0.,  1.],\n",
      "        ...,\n",
      "        [ 1.,  1.,  1.,  ...,  0., -1., -1.],\n",
      "        [-0.,  1., -0.,  ...,  1.,  1., -1.],\n",
      "        [-1., -1., -1.,  ...,  1.,  1.,  1.]])| \n",
      "[Quantized] model.layers.12.self_attn.v_proj | Ternary weights applied.\n",
      "[DEBUG] Quantized: tensor([[ 1.,  1., -1.,  ...,  0., -0., -0.],\n",
      "        [ 0.,  0.,  1.,  ..., -0.,  1., -1.],\n",
      "        [-1., -0., -0.,  ...,  1.,  0., -1.],\n",
      "        ...,\n",
      "        [-0., -1.,  0.,  ..., -0.,  1., -1.],\n",
      "        [ 1.,  1., -1.,  ...,  1.,  1.,  1.],\n",
      "        [-0.,  1.,  0.,  ..., -1., -1.,  1.]])| \n",
      "[Quantized] model.layers.12.self_attn.o_proj | Ternary weights applied.\n",
      "[DEBUG] Quantized: tensor([[ 1.,  1.,  0.,  ..., -0., -1.,  1.],\n",
      "        [ 1.,  1.,  1.,  ..., -1.,  1., -1.],\n",
      "        [-0.,  1., -1.,  ...,  1., -0.,  0.],\n",
      "        ...,\n",
      "        [ 0.,  1., -0.,  ...,  0.,  0., -1.],\n",
      "        [ 1.,  1.,  1.,  ..., -1.,  1.,  0.],\n",
      "        [-0., -1., -1.,  ..., -1.,  1., -1.]])| \n",
      "[Quantized] model.layers.12.mlp.gate_proj | Ternary weights applied.\n",
      "[DEBUG] Quantized: tensor([[-0., -1.,  0.,  ..., -1.,  1., -1.],\n",
      "        [-1.,  0., -1.,  ...,  1., -1., -1.],\n",
      "        [-1., -1., -0.,  ...,  1.,  1.,  1.],\n",
      "        ...,\n",
      "        [ 0., -1., -0.,  ..., -0.,  1., -1.],\n",
      "        [-0.,  1., -0.,  ..., -1., -1.,  1.],\n",
      "        [ 1.,  1.,  0.,  ...,  1., -0.,  0.]])| \n",
      "[Quantized] model.layers.12.mlp.up_proj | Ternary weights applied.\n",
      "[DEBUG] Quantized: tensor([[ 1., -1., -1.,  ..., -1., -1.,  0.],\n",
      "        [-1., -1.,  0.,  ..., -1.,  1.,  0.],\n",
      "        [-0., -0., -0.,  ...,  1., -0.,  0.],\n",
      "        ...,\n",
      "        [-0.,  1.,  1.,  ..., -1., -0.,  1.],\n",
      "        [ 1., -0.,  1.,  ...,  1., -1., -0.],\n",
      "        [-0.,  0.,  0.,  ..., -1., -0.,  0.]])| \n",
      "[Quantized] model.layers.12.mlp.down_proj | Ternary weights applied.\n",
      "[DEBUG] Quantized: tensor([[ 1.,  1.,  1.,  ..., -0., -0., -1.],\n",
      "        [-1., -1., -0.,  ...,  1.,  0.,  0.],\n",
      "        [ 1.,  1., -0.,  ...,  0.,  1., -0.],\n",
      "        ...,\n",
      "        [-1., -1., -0.,  ..., -1., -1.,  1.],\n",
      "        [ 1., -0., -0.,  ...,  1.,  1.,  1.],\n",
      "        [-1., -1.,  1.,  ...,  0.,  1., -1.]])| \n",
      "[Quantized] model.layers.13.self_attn.q_proj | Ternary weights applied.\n",
      "[DEBUG] Quantized: tensor([[ 1., -0., -1.,  ...,  0.,  0., -1.],\n",
      "        [-0., -0.,  1.,  ..., -1., -1.,  0.],\n",
      "        [-0.,  1.,  1.,  ..., -1.,  1., -0.],\n",
      "        ...,\n",
      "        [ 1.,  1., -1.,  ...,  1.,  0.,  1.],\n",
      "        [-1.,  1., -1.,  ...,  1.,  0.,  1.],\n",
      "        [-1., -1.,  0.,  ...,  1., -0., -1.]])| \n",
      "[Quantized] model.layers.13.self_attn.k_proj | Ternary weights applied.\n",
      "[DEBUG] Quantized: tensor([[ 1.,  1.,  1.,  ..., -1.,  1., -1.],\n",
      "        [-1., -1.,  1.,  ...,  1., -1.,  1.],\n",
      "        [ 0.,  0.,  1.,  ...,  0.,  1.,  1.],\n",
      "        ...,\n",
      "        [ 1.,  1., -0.,  ...,  1., -1.,  1.],\n",
      "        [-0., -1., -1.,  ...,  1.,  0., -0.],\n",
      "        [-0.,  1., -0.,  ..., -0., -1.,  1.]])| \n",
      "[Quantized] model.layers.13.self_attn.v_proj | Ternary weights applied.\n",
      "[DEBUG] Quantized: tensor([[-0.,  1., -1.,  ...,  1.,  1., -0.],\n",
      "        [ 1.,  1.,  0.,  ..., -0., -0.,  1.],\n",
      "        [ 0., -1., -1.,  ...,  0., -0.,  0.],\n",
      "        ...,\n",
      "        [ 1.,  1., -1.,  ...,  1., -0., -1.],\n",
      "        [-0., -1., -0.,  ..., -1.,  1., -1.],\n",
      "        [ 1.,  1.,  1.,  ...,  1., -1., -1.]])| \n",
      "[Quantized] model.layers.13.self_attn.o_proj | Ternary weights applied.\n",
      "[DEBUG] Quantized: tensor([[ 1.,  1.,  1.,  ..., -1.,  1.,  1.],\n",
      "        [ 1.,  1.,  1.,  ...,  1.,  1., -1.],\n",
      "        [ 1.,  1., -1.,  ...,  1.,  1.,  0.],\n",
      "        ...,\n",
      "        [-1.,  1., -0.,  ...,  1., -0., -0.],\n",
      "        [-0., -0.,  0.,  ..., -1.,  1.,  1.],\n",
      "        [-1., -0.,  0.,  ...,  1.,  1.,  0.]])| \n",
      "[Quantized] model.layers.13.mlp.gate_proj | Ternary weights applied.\n",
      "[DEBUG] Quantized: tensor([[-1., -0.,  1.,  ...,  1.,  1.,  1.],\n",
      "        [-0., -1.,  1.,  ..., -1.,  0., -1.],\n",
      "        [ 1., -0., -1.,  ...,  1.,  0.,  0.],\n",
      "        ...,\n",
      "        [ 1., -1., -0.,  ..., -1., -1.,  1.],\n",
      "        [ 1.,  1.,  0.,  ..., -1.,  0., -1.],\n",
      "        [-1., -1.,  0.,  ...,  1.,  1., -1.]])| \n",
      "[Quantized] model.layers.13.mlp.up_proj | Ternary weights applied.\n",
      "[DEBUG] Quantized: tensor([[-1.,  1., -0.,  ...,  1., -1., -1.],\n",
      "        [-1., -1., -1.,  ..., -1.,  1., -0.],\n",
      "        [ 0.,  1.,  1.,  ..., -0.,  1.,  1.],\n",
      "        ...,\n",
      "        [ 1., -1.,  1.,  ..., -1., -1.,  0.],\n",
      "        [ 1.,  1.,  1.,  ..., -1.,  0.,  1.],\n",
      "        [ 1., -1., -1.,  ...,  1., -1., -1.]])| \n",
      "[Quantized] model.layers.13.mlp.down_proj | Ternary weights applied.\n",
      "[DEBUG] Quantized: tensor([[ 0.,  1., -1.,  ...,  0., -1.,  1.],\n",
      "        [ 0.,  1., -0.,  ..., -1.,  1., -1.],\n",
      "        [ 1.,  1.,  0.,  ...,  1.,  0.,  1.],\n",
      "        ...,\n",
      "        [-1.,  0.,  1.,  ...,  1.,  1.,  0.],\n",
      "        [-1., -0., -1.,  ...,  0., -1., -1.],\n",
      "        [ 0.,  1.,  1.,  ...,  1.,  1., -1.]])| \n",
      "[Quantized] model.layers.14.self_attn.q_proj | Ternary weights applied.\n",
      "[DEBUG] Quantized: tensor([[ 1., -1., -1.,  ...,  0.,  1.,  0.],\n",
      "        [ 0.,  1., -1.,  ..., -1.,  1., -1.],\n",
      "        [ 1., -1.,  1.,  ...,  1.,  1.,  0.],\n",
      "        ...,\n",
      "        [-1.,  1.,  0.,  ..., -0., -1.,  0.],\n",
      "        [-1., -1.,  0.,  ...,  1., -0., -1.],\n",
      "        [ 1.,  1., -1.,  ..., -1.,  0., -0.]])| \n",
      "[Quantized] model.layers.14.self_attn.k_proj | Ternary weights applied.\n",
      "[DEBUG] Quantized: tensor([[-0.,  1., -1.,  ..., -0., -0.,  1.],\n",
      "        [-0.,  0.,  1.,  ...,  1., -1.,  0.],\n",
      "        [-1.,  1.,  1.,  ...,  1., -1.,  1.],\n",
      "        ...,\n",
      "        [-1.,  1., -1.,  ..., -1., -0., -1.],\n",
      "        [-1., -1., -1.,  ..., -0.,  1., -1.],\n",
      "        [-1., -0., -1.,  ..., -0.,  0., -0.]])| \n",
      "[Quantized] model.layers.14.self_attn.v_proj | Ternary weights applied.\n",
      "[DEBUG] Quantized: tensor([[ 1.,  1.,  1.,  ..., -1., -1., -1.],\n",
      "        [-1.,  1., -0.,  ...,  1.,  1., -1.],\n",
      "        [-1., -1.,  1.,  ...,  0., -0.,  0.],\n",
      "        ...,\n",
      "        [-1., -0., -0.,  ..., -1., -0., -0.],\n",
      "        [-1., -1.,  0.,  ...,  1., -1., -1.],\n",
      "        [-1.,  1., -1.,  ..., -1., -1.,  1.]])| \n",
      "[Quantized] model.layers.14.self_attn.o_proj | Ternary weights applied.\n",
      "[DEBUG] Quantized: tensor([[-0., -0.,  1.,  ...,  1., -1.,  1.],\n",
      "        [-1.,  1., -1.,  ...,  1.,  1.,  1.],\n",
      "        [ 1., -0.,  0.,  ..., -0., -0.,  1.],\n",
      "        ...,\n",
      "        [ 1., -1., -0.,  ..., -1.,  1.,  0.],\n",
      "        [ 1., -0.,  1.,  ..., -1.,  0., -1.],\n",
      "        [ 1.,  1.,  0.,  ...,  0., -0., -1.]])| \n",
      "[Quantized] model.layers.14.mlp.gate_proj | Ternary weights applied.\n",
      "[DEBUG] Quantized: tensor([[-0.,  1., -0.,  ..., -0., -0.,  1.],\n",
      "        [-0.,  0.,  0.,  ..., -0., -1.,  1.],\n",
      "        [-1.,  1., -1.,  ..., -0.,  1., -1.],\n",
      "        ...,\n",
      "        [-1., -1., -1.,  ..., -0., -0.,  1.],\n",
      "        [ 1.,  1., -0.,  ..., -0., -0.,  1.],\n",
      "        [-0.,  1., -1.,  ...,  1., -1., -1.]])| \n",
      "[Quantized] model.layers.14.mlp.up_proj | Ternary weights applied.\n",
      "[DEBUG] Quantized: tensor([[-1., -1., -0.,  ...,  1.,  1.,  0.],\n",
      "        [-1.,  0., -1.,  ..., -0.,  1., -0.],\n",
      "        [ 1., -0.,  1.,  ...,  0., -1.,  0.],\n",
      "        ...,\n",
      "        [ 0., -0., -1.,  ...,  0., -1., -1.],\n",
      "        [ 1., -0.,  1.,  ..., -0., -1., -1.],\n",
      "        [ 1.,  1., -1.,  ..., -0.,  1., -0.]])| \n",
      "[Quantized] model.layers.14.mlp.down_proj | Ternary weights applied.\n",
      "[DEBUG] Quantized: tensor([[ 1., -1.,  1.,  ..., -0.,  0., -1.],\n",
      "        [ 1.,  0.,  1.,  ...,  0.,  1., -0.],\n",
      "        [ 1.,  1.,  1.,  ..., -1.,  0.,  1.],\n",
      "        ...,\n",
      "        [-1., -1., -1.,  ...,  1., -1.,  1.],\n",
      "        [ 1., -0.,  0.,  ..., -1., -1., -1.],\n",
      "        [-1.,  1., -0.,  ..., -1.,  0.,  1.]])| \n",
      "[Quantized] model.layers.15.self_attn.q_proj | Ternary weights applied.\n",
      "[DEBUG] Quantized: tensor([[-0.,  1., -1.,  ...,  0., -0., -0.],\n",
      "        [ 1., -1., -1.,  ..., -0., -1., -1.],\n",
      "        [ 0., -0., -1.,  ..., -1., -1.,  1.],\n",
      "        ...,\n",
      "        [-0.,  1., -0.,  ..., -1., -0., -1.],\n",
      "        [-1.,  1.,  1.,  ..., -1.,  1., -1.],\n",
      "        [-0.,  0.,  0.,  ...,  1.,  1., -1.]])| \n",
      "[Quantized] model.layers.15.self_attn.k_proj | Ternary weights applied.\n",
      "[DEBUG] Quantized: tensor([[-0.,  1.,  0.,  ...,  1., -0., -0.],\n",
      "        [ 1.,  0.,  1.,  ...,  1.,  0., -1.],\n",
      "        [ 1., -1., -0.,  ..., -1.,  1.,  1.],\n",
      "        ...,\n",
      "        [-0., -0., -0.,  ..., -0., -0.,  1.],\n",
      "        [ 1.,  1., -1.,  ...,  0.,  1., -1.],\n",
      "        [-0., -0.,  0.,  ..., -1., -1., -1.]])| \n",
      "[Quantized] model.layers.15.self_attn.v_proj | Ternary weights applied.\n",
      "[DEBUG] Quantized: tensor([[ 1., -1., -0.,  ..., -1.,  1., -1.],\n",
      "        [-1.,  1.,  1.,  ...,  0.,  0., -1.],\n",
      "        [ 1., -1.,  1.,  ..., -1.,  0.,  0.],\n",
      "        ...,\n",
      "        [-0., -1.,  0.,  ..., -1., -0.,  1.],\n",
      "        [-1., -1.,  1.,  ..., -1., -0., -0.],\n",
      "        [ 1.,  0., -1.,  ...,  0., -1., -0.]])| \n",
      "[Quantized] model.layers.15.self_attn.o_proj | Ternary weights applied.\n",
      "[DEBUG] Quantized: tensor([[-1., -1.,  1.,  ..., -1.,  1.,  1.],\n",
      "        [ 1.,  1.,  1.,  ...,  0.,  1., -1.],\n",
      "        [ 0.,  1., -1.,  ..., -0.,  1., -1.],\n",
      "        ...,\n",
      "        [ 1., -1., -1.,  ..., -1., -0.,  1.],\n",
      "        [-1.,  1., -1.,  ..., -1., -0.,  0.],\n",
      "        [-1.,  1.,  1.,  ...,  1.,  1., -0.]])| \n",
      "[Quantized] model.layers.15.mlp.gate_proj | Ternary weights applied.\n",
      "[DEBUG] Quantized: tensor([[-1.,  0.,  0.,  ...,  1., -1., -1.],\n",
      "        [ 1., -1., -0.,  ..., -1.,  1.,  1.],\n",
      "        [-1., -1.,  1.,  ...,  1., -1., -1.],\n",
      "        ...,\n",
      "        [ 0., -0.,  1.,  ...,  0.,  1., -1.],\n",
      "        [-1.,  1.,  0.,  ...,  0.,  1.,  0.],\n",
      "        [ 1.,  1.,  1.,  ...,  1., -1.,  1.]])| \n",
      "[Quantized] model.layers.15.mlp.up_proj | Ternary weights applied.\n",
      "[DEBUG] Quantized: tensor([[ 0.,  1., -1.,  ...,  1., -1.,  1.],\n",
      "        [-0.,  0., -1.,  ...,  1., -0.,  1.],\n",
      "        [-1., -1.,  0.,  ...,  0., -0., -1.],\n",
      "        ...,\n",
      "        [ 1., -1.,  1.,  ..., -0., -1., -0.],\n",
      "        [-1.,  1., -1.,  ...,  1.,  0., -1.],\n",
      "        [-1.,  0., -1.,  ..., -0.,  1.,  1.]])| \n",
      "[Quantized] model.layers.15.mlp.down_proj | Ternary weights applied.\n",
      "[DEBUG] Quantized: tensor([[-1.,  0., -0.,  ...,  1.,  1.,  0.],\n",
      "        [ 1., -1.,  0.,  ...,  0., -0., -1.],\n",
      "        [ 0., -1.,  1.,  ...,  1.,  1.,  1.],\n",
      "        ...,\n",
      "        [ 1.,  1.,  0.,  ...,  1.,  0.,  1.],\n",
      "        [-1.,  1.,  1.,  ...,  0.,  1., -1.],\n",
      "        [ 0.,  1., -1.,  ..., -0., -1., -1.]])| \n",
      "[Quantized] model.layers.16.self_attn.q_proj | Ternary weights applied.\n",
      "[DEBUG] Quantized: tensor([[-0.,  1., -1.,  ...,  1.,  0., -1.],\n",
      "        [-1.,  1., -0.,  ..., -1., -0., -1.],\n",
      "        [-1., -0.,  1.,  ..., -1.,  0.,  1.],\n",
      "        ...,\n",
      "        [-0., -0., -1.,  ..., -1.,  1., -1.],\n",
      "        [-1.,  1.,  1.,  ..., -0., -1., -1.],\n",
      "        [ 1.,  0., -1.,  ..., -0.,  0.,  0.]])| \n",
      "[Quantized] model.layers.16.self_attn.k_proj | Ternary weights applied.\n",
      "[DEBUG] Quantized: tensor([[ 1.,  1., -0.,  ..., -0., -1., -1.],\n",
      "        [ 1., -1.,  0.,  ...,  0., -0.,  1.],\n",
      "        [-1., -1., -0.,  ...,  0.,  1., -0.],\n",
      "        ...,\n",
      "        [-0., -1., -1.,  ...,  0., -1.,  0.],\n",
      "        [ 1.,  0., -1.,  ...,  0., -0., -1.],\n",
      "        [-0.,  0., -1.,  ...,  0., -0.,  0.]])| \n",
      "[Quantized] model.layers.16.self_attn.v_proj | Ternary weights applied.\n",
      "[DEBUG] Quantized: tensor([[ 0.,  1., -0.,  ...,  1.,  1., -1.],\n",
      "        [ 1.,  0., -0.,  ...,  0.,  1., -1.],\n",
      "        [-0., -0., -1.,  ...,  0., -1.,  1.],\n",
      "        ...,\n",
      "        [ 0.,  0., -1.,  ..., -1.,  1.,  0.],\n",
      "        [ 1., -1.,  1.,  ...,  1., -1.,  1.],\n",
      "        [ 1., -1.,  1.,  ..., -0., -0., -1.]])| \n",
      "[Quantized] model.layers.16.self_attn.o_proj | Ternary weights applied.\n",
      "[DEBUG] Quantized: tensor([[-0., -0., -1.,  ...,  0.,  1., -1.],\n",
      "        [ 1.,  1.,  0.,  ...,  1.,  1., -1.],\n",
      "        [-0.,  0.,  0.,  ..., -1.,  1.,  1.],\n",
      "        ...,\n",
      "        [-1., -1., -1.,  ...,  1.,  1.,  1.],\n",
      "        [ 1.,  1., -1.,  ..., -1., -0.,  1.],\n",
      "        [ 1., -1.,  0.,  ...,  1., -1.,  0.]])| \n",
      "[Quantized] model.layers.16.mlp.gate_proj | Ternary weights applied.\n",
      "[DEBUG] Quantized: tensor([[ 1.,  0.,  0.,  ..., -0.,  0., -1.],\n",
      "        [ 1.,  0.,  0.,  ...,  1., -1.,  1.],\n",
      "        [ 1.,  1.,  0.,  ...,  1.,  1., -1.],\n",
      "        ...,\n",
      "        [-1.,  1., -1.,  ..., -0., -0., -1.],\n",
      "        [-0.,  1., -1.,  ...,  1.,  1., -1.],\n",
      "        [-1.,  0., -1.,  ..., -1., -1., -1.]])| \n",
      "[Quantized] model.layers.16.mlp.up_proj | Ternary weights applied.\n",
      "[DEBUG] Quantized: tensor([[ 0., -0.,  1.,  ...,  0.,  0., -1.],\n",
      "        [-1.,  1.,  1.,  ...,  1.,  1.,  1.],\n",
      "        [-1.,  0., -0.,  ..., -0.,  1.,  0.],\n",
      "        ...,\n",
      "        [ 1., -0.,  1.,  ...,  0.,  1.,  1.],\n",
      "        [ 1., -1.,  1.,  ..., -0., -0., -1.],\n",
      "        [ 0.,  1.,  1.,  ..., -1., -1., -1.]])| \n",
      "[Quantized] model.layers.16.mlp.down_proj | Ternary weights applied.\n",
      "[DEBUG] Quantized: tensor([[ 1., -1., -0.,  ..., -1.,  0.,  0.],\n",
      "        [ 1., -0.,  1.,  ..., -1., -0., -1.],\n",
      "        [-1.,  1., -1.,  ...,  1.,  1.,  0.],\n",
      "        ...,\n",
      "        [-0.,  1.,  1.,  ...,  1.,  1.,  0.],\n",
      "        [ 0., -1.,  0.,  ...,  1., -1.,  1.],\n",
      "        [ 1., -0., -1.,  ..., -1.,  1.,  1.]])| \n",
      "[Quantized] model.layers.17.self_attn.q_proj | Ternary weights applied.\n",
      "[DEBUG] Quantized: tensor([[-1., -1.,  0.,  ..., -1.,  0., -0.],\n",
      "        [ 1., -1., -0.,  ..., -1., -1., -0.],\n",
      "        [ 0.,  1.,  1.,  ...,  1.,  1.,  1.],\n",
      "        ...,\n",
      "        [-1., -1.,  0.,  ...,  0., -1.,  0.],\n",
      "        [-1., -1., -1.,  ...,  1.,  1.,  1.],\n",
      "        [-1., -0., -1.,  ...,  1., -1., -1.]])| \n",
      "[Quantized] model.layers.17.self_attn.k_proj | Ternary weights applied.\n",
      "[DEBUG] Quantized: tensor([[ 0., -0.,  1.,  ...,  1., -0.,  0.],\n",
      "        [ 1.,  1.,  1.,  ...,  0., -0., -1.],\n",
      "        [-0., -1.,  1.,  ..., -1.,  0.,  0.],\n",
      "        ...,\n",
      "        [-0., -1.,  1.,  ..., -1., -0.,  0.],\n",
      "        [-1., -0., -1.,  ...,  0., -1.,  1.],\n",
      "        [-1.,  1., -0.,  ...,  1.,  1., -0.]])| \n",
      "[Quantized] model.layers.17.self_attn.v_proj | Ternary weights applied.\n",
      "[DEBUG] Quantized: tensor([[ 1., -0.,  1.,  ..., -1.,  1., -1.],\n",
      "        [ 1.,  0., -1.,  ..., -1.,  1.,  1.],\n",
      "        [ 0., -0., -1.,  ..., -0., -1.,  1.],\n",
      "        ...,\n",
      "        [ 1.,  1., -0.,  ..., -0.,  1., -0.],\n",
      "        [-1.,  1.,  1.,  ..., -1., -0.,  0.],\n",
      "        [ 0., -1.,  1.,  ..., -0.,  0.,  1.]])| \n",
      "[Quantized] model.layers.17.self_attn.o_proj | Ternary weights applied.\n",
      "[DEBUG] Quantized: tensor([[ 1., -0.,  1.,  ..., -0., -1.,  1.],\n",
      "        [-1.,  0., -1.,  ..., -1., -1.,  1.],\n",
      "        [ 1., -1.,  1.,  ...,  1.,  1., -1.],\n",
      "        ...,\n",
      "        [-0.,  1.,  1.,  ...,  1.,  1., -1.],\n",
      "        [-0.,  0.,  0.,  ..., -0., -1.,  1.],\n",
      "        [-1., -1.,  1.,  ...,  1., -0., -0.]])| \n",
      "[Quantized] model.layers.17.mlp.gate_proj | Ternary weights applied.\n",
      "[DEBUG] Quantized: tensor([[-0.,  0., -0.,  ...,  1., -1.,  1.],\n",
      "        [ 1.,  0.,  1.,  ..., -1.,  1., -1.],\n",
      "        [ 0., -1., -0.,  ...,  0., -0.,  0.],\n",
      "        ...,\n",
      "        [-0., -0., -1.,  ..., -1.,  0.,  1.],\n",
      "        [ 1., -1., -0.,  ..., -1., -1.,  0.],\n",
      "        [-0.,  1.,  1.,  ...,  1.,  1., -0.]])| \n",
      "[Quantized] model.layers.17.mlp.up_proj | Ternary weights applied.\n",
      "[DEBUG] Quantized: tensor([[-1.,  1., -1.,  ..., -0.,  0., -1.],\n",
      "        [-1.,  0., -1.,  ...,  1.,  1.,  1.],\n",
      "        [-1.,  1.,  1.,  ...,  0., -1., -0.],\n",
      "        ...,\n",
      "        [ 1.,  0.,  1.,  ...,  1., -0.,  1.],\n",
      "        [ 1.,  1.,  0.,  ...,  1., -1.,  0.],\n",
      "        [-1., -1.,  1.,  ...,  1.,  1., -0.]])| \n",
      "[Quantized] model.layers.17.mlp.down_proj | Ternary weights applied.\n",
      "[DEBUG] Quantized: tensor([[ 1., -1., -1.,  ..., -0., -1., -1.],\n",
      "        [-1., -1.,  0.,  ...,  1., -1.,  0.],\n",
      "        [ 1., -1., -1.,  ..., -1., -0., -1.],\n",
      "        ...,\n",
      "        [-1.,  1.,  1.,  ..., -1., -1., -1.],\n",
      "        [-1.,  0., -1.,  ...,  1., -0.,  1.],\n",
      "        [ 1.,  1.,  1.,  ..., -1.,  1.,  1.]])| \n",
      "[Quantized] model.layers.18.self_attn.q_proj | Ternary weights applied.\n",
      "[DEBUG] Quantized: tensor([[-1.,  0.,  1.,  ..., -1.,  1.,  0.],\n",
      "        [-0.,  1., -1.,  ..., -0.,  1.,  0.],\n",
      "        [ 0.,  1., -1.,  ...,  1.,  1., -0.],\n",
      "        ...,\n",
      "        [ 0., -0., -0.,  ...,  0.,  1., -1.],\n",
      "        [-0.,  1., -1.,  ...,  1.,  0., -0.],\n",
      "        [ 1., -1.,  1.,  ..., -1.,  1., -1.]])| \n",
      "[Quantized] model.layers.18.self_attn.k_proj | Ternary weights applied.\n",
      "[DEBUG] Quantized: tensor([[ 0.,  1., -1.,  ...,  1., -0.,  0.],\n",
      "        [-1.,  1., -1.,  ...,  1.,  0., -1.],\n",
      "        [-1., -0., -1.,  ...,  1.,  1.,  1.],\n",
      "        ...,\n",
      "        [-1.,  1., -1.,  ...,  1., -0., -1.],\n",
      "        [ 1.,  1.,  0.,  ...,  1., -1.,  1.],\n",
      "        [-0.,  1., -0.,  ..., -1., -1.,  1.]])| \n",
      "[Quantized] model.layers.18.self_attn.v_proj | Ternary weights applied.\n",
      "[DEBUG] Quantized: tensor([[ 1.,  1.,  1.,  ...,  1.,  0., -0.],\n",
      "        [ 0.,  0., -1.,  ...,  1.,  1., -0.],\n",
      "        [ 1.,  1.,  0.,  ..., -1., -1., -1.],\n",
      "        ...,\n",
      "        [-1.,  0.,  0.,  ..., -1.,  0.,  0.],\n",
      "        [-0.,  1., -0.,  ..., -1.,  1.,  1.],\n",
      "        [-1.,  1.,  1.,  ...,  1., -1., -1.]])| \n",
      "[Quantized] model.layers.18.self_attn.o_proj | Ternary weights applied.\n",
      "[DEBUG] Quantized: tensor([[ 0., -0., -1.,  ..., -0., -0.,  1.],\n",
      "        [ 1., -1., -0.,  ..., -1.,  1.,  1.],\n",
      "        [ 1., -0., -0.,  ..., -0.,  1.,  1.],\n",
      "        ...,\n",
      "        [-1., -1.,  1.,  ..., -1.,  1.,  1.],\n",
      "        [-1.,  0.,  1.,  ...,  0., -0., -1.],\n",
      "        [-1., -0., -1.,  ...,  1., -0.,  0.]])| \n",
      "[Quantized] model.layers.18.mlp.gate_proj | Ternary weights applied.\n",
      "[DEBUG] Quantized: tensor([[ 1.,  1., -0.,  ...,  1.,  0.,  1.],\n",
      "        [ 1., -1.,  0.,  ...,  0., -1.,  1.],\n",
      "        [-0.,  1.,  1.,  ...,  1.,  1., -0.],\n",
      "        ...,\n",
      "        [ 0.,  1.,  1.,  ...,  1.,  0.,  1.],\n",
      "        [-1., -1.,  0.,  ...,  0.,  0.,  1.],\n",
      "        [ 0., -1.,  0.,  ..., -0.,  1., -1.]])| \n",
      "[Quantized] model.layers.18.mlp.up_proj | Ternary weights applied.\n",
      "[DEBUG] Quantized: tensor([[ 1.,  1., -1.,  ...,  1., -1.,  1.],\n",
      "        [ 1., -1.,  0.,  ..., -0., -0.,  1.],\n",
      "        [-1., -1.,  0.,  ...,  1., -1.,  0.],\n",
      "        ...,\n",
      "        [ 0.,  1., -1.,  ..., -0., -1.,  1.],\n",
      "        [ 0., -0.,  1.,  ...,  1., -1., -1.],\n",
      "        [ 0.,  1.,  1.,  ..., -0., -1.,  0.]])| \n",
      "[Quantized] model.layers.18.mlp.down_proj | Ternary weights applied.\n",
      "[DEBUG] Quantized: tensor([[-1., -0.,  0.,  ..., -0.,  0., -0.],\n",
      "        [-1.,  1., -1.,  ...,  0., -0., -1.],\n",
      "        [ 1.,  1.,  0.,  ..., -1., -0.,  1.],\n",
      "        ...,\n",
      "        [-1., -1.,  1.,  ..., -0.,  1., -1.],\n",
      "        [-1.,  1., -1.,  ...,  0.,  0., -1.],\n",
      "        [ 1., -1., -1.,  ..., -1., -1., -1.]])| \n",
      "[Quantized] model.layers.19.self_attn.q_proj | Ternary weights applied.\n",
      "[DEBUG] Quantized: tensor([[ 1., -0.,  1.,  ..., -0.,  1.,  1.],\n",
      "        [ 1.,  1.,  1.,  ...,  1.,  1.,  0.],\n",
      "        [-0., -1., -0.,  ...,  0.,  1., -0.],\n",
      "        ...,\n",
      "        [-1., -1.,  1.,  ..., -1.,  1., -1.],\n",
      "        [ 0.,  1.,  0.,  ...,  1.,  1.,  0.],\n",
      "        [ 0.,  0., -1.,  ...,  1., -0.,  0.]])| \n",
      "[Quantized] model.layers.19.self_attn.k_proj | Ternary weights applied.\n",
      "[DEBUG] Quantized: tensor([[-0., -1.,  1.,  ...,  0., -0., -0.],\n",
      "        [ 1.,  1., -0.,  ..., -0.,  0., -1.],\n",
      "        [-1.,  1., -1.,  ..., -0., -1.,  1.],\n",
      "        ...,\n",
      "        [-0.,  1.,  1.,  ..., -1.,  0., -1.],\n",
      "        [ 1., -1.,  1.,  ...,  0.,  0.,  1.],\n",
      "        [ 1.,  0.,  1.,  ..., -1.,  0., -1.]])| \n",
      "[Quantized] model.layers.19.self_attn.v_proj | Ternary weights applied.\n",
      "[DEBUG] Quantized: tensor([[-1., -0., -1.,  ...,  1.,  0.,  1.],\n",
      "        [ 0., -1., -1.,  ...,  1., -1., -1.],\n",
      "        [ 1.,  0.,  0.,  ...,  0.,  1., -0.],\n",
      "        ...,\n",
      "        [ 0.,  1., -0.,  ...,  1., -0., -1.],\n",
      "        [-0.,  0., -1.,  ...,  1., -1., -1.],\n",
      "        [ 1., -0.,  1.,  ...,  1., -1.,  1.]])| \n",
      "[Quantized] model.layers.19.self_attn.o_proj | Ternary weights applied.\n",
      "[DEBUG] Quantized: tensor([[ 1.,  1., -1.,  ...,  0., -1.,  1.],\n",
      "        [-0., -0., -0.,  ..., -1., -1., -1.],\n",
      "        [ 0.,  1.,  1.,  ..., -1., -1.,  1.],\n",
      "        ...,\n",
      "        [-0.,  0.,  1.,  ...,  1.,  1.,  0.],\n",
      "        [ 1.,  1., -1.,  ...,  0., -0.,  1.],\n",
      "        [ 1., -1., -0.,  ..., -0., -0.,  0.]])| \n",
      "[Quantized] model.layers.19.mlp.gate_proj | Ternary weights applied.\n",
      "[DEBUG] Quantized: tensor([[ 0., -1., -0.,  ..., -0., -1.,  1.],\n",
      "        [-1.,  0.,  1.,  ..., -1.,  0., -1.],\n",
      "        [ 1., -0.,  1.,  ...,  1., -1., -0.],\n",
      "        ...,\n",
      "        [ 1., -1.,  1.,  ..., -1., -0., -1.],\n",
      "        [-1.,  1.,  0.,  ..., -1., -1., -1.],\n",
      "        [ 1.,  1.,  0.,  ...,  0., -1., -1.]])| \n",
      "[Quantized] model.layers.19.mlp.up_proj | Ternary weights applied.\n",
      "[DEBUG] Quantized: tensor([[-0., -0.,  1.,  ...,  1., -1.,  1.],\n",
      "        [-1., -0., -0.,  ..., -1.,  1., -1.],\n",
      "        [ 1.,  0., -1.,  ...,  0., -0.,  1.],\n",
      "        ...,\n",
      "        [ 1., -0.,  1.,  ..., -1., -1., -1.],\n",
      "        [ 1.,  1., -0.,  ...,  1., -1., -1.],\n",
      "        [ 0., -1., -1.,  ...,  1., -1.,  1.]])| \n",
      "[Quantized] model.layers.19.mlp.down_proj | Ternary weights applied.\n",
      "[DEBUG] Quantized: tensor([[ 1., -0.,  0.,  ...,  1.,  1.,  1.],\n",
      "        [ 1.,  0.,  0.,  ..., -0., -1., -1.],\n",
      "        [ 1., -0.,  0.,  ..., -1., -1., -1.],\n",
      "        ...,\n",
      "        [-1.,  1., -1.,  ..., -1., -0.,  1.],\n",
      "        [-1., -1.,  1.,  ...,  1., -1., -1.],\n",
      "        [ 0., -1.,  1.,  ...,  0., -1., -1.]])| \n",
      "[Quantized] model.layers.20.self_attn.q_proj | Ternary weights applied.\n",
      "[DEBUG] Quantized: tensor([[ 1.,  1.,  1.,  ..., -1.,  1., -0.],\n",
      "        [-1.,  1.,  1.,  ..., -1., -1.,  0.],\n",
      "        [-0., -1., -1.,  ..., -1.,  1.,  0.],\n",
      "        ...,\n",
      "        [ 0., -1.,  0.,  ..., -0., -1.,  1.],\n",
      "        [-1.,  1., -1.,  ...,  0., -1., -1.],\n",
      "        [ 1., -1., -0.,  ...,  1., -1., -0.]])| \n",
      "[Quantized] model.layers.20.self_attn.k_proj | Ternary weights applied.\n",
      "[DEBUG] Quantized: tensor([[ 1.,  1.,  0.,  ...,  1., -0., -1.],\n",
      "        [ 0.,  0., -1.,  ..., -1.,  1.,  1.],\n",
      "        [ 0., -1.,  0.,  ...,  1.,  1., -0.],\n",
      "        ...,\n",
      "        [-1.,  0.,  1.,  ..., -0.,  1.,  0.],\n",
      "        [ 1., -0., -1.,  ..., -0., -1., -1.],\n",
      "        [ 1., -0., -1.,  ...,  1.,  0., -1.]])| \n",
      "[Quantized] model.layers.20.self_attn.v_proj | Ternary weights applied.\n",
      "[DEBUG] Quantized: tensor([[ 1., -1., -1.,  ..., -1., -1.,  1.],\n",
      "        [ 0.,  1.,  1.,  ...,  0., -0., -1.],\n",
      "        [-0., -1., -0.,  ..., -0., -1., -0.],\n",
      "        ...,\n",
      "        [ 1., -1.,  1.,  ..., -1.,  0.,  0.],\n",
      "        [ 1., -1.,  1.,  ...,  0., -0., -1.],\n",
      "        [-1., -1.,  1.,  ..., -1.,  1.,  1.]])| \n",
      "[Quantized] model.layers.20.self_attn.o_proj | Ternary weights applied.\n",
      "[DEBUG] Quantized: tensor([[-1.,  0.,  1.,  ..., -0.,  0., -1.],\n",
      "        [-0., -1.,  0.,  ..., -1.,  1.,  0.],\n",
      "        [-1.,  1., -0.,  ..., -0., -1.,  1.],\n",
      "        ...,\n",
      "        [ 1.,  0., -0.,  ...,  1., -1., -1.],\n",
      "        [ 1.,  1., -0.,  ..., -0.,  0., -0.],\n",
      "        [ 1., -1.,  1.,  ..., -1.,  1., -0.]])| \n",
      "[Quantized] model.layers.20.mlp.gate_proj | Ternary weights applied.\n",
      "[DEBUG] Quantized: tensor([[ 1., -1.,  1.,  ...,  0., -1.,  1.],\n",
      "        [-0.,  1., -1.,  ...,  1., -1.,  0.],\n",
      "        [ 1., -1., -1.,  ...,  1.,  1.,  1.],\n",
      "        ...,\n",
      "        [-0.,  1.,  1.,  ...,  0., -0.,  0.],\n",
      "        [ 0., -0., -0.,  ...,  0., -0.,  0.],\n",
      "        [ 1.,  0., -1.,  ...,  1., -0.,  0.]])| \n",
      "[Quantized] model.layers.20.mlp.up_proj | Ternary weights applied.\n",
      "[DEBUG] Quantized: tensor([[ 1.,  1.,  1.,  ...,  1., -1.,  1.],\n",
      "        [-1.,  1., -1.,  ..., -0.,  1., -0.],\n",
      "        [ 1.,  1., -0.,  ..., -1., -1.,  1.],\n",
      "        ...,\n",
      "        [ 1.,  1.,  1.,  ..., -1., -0., -0.],\n",
      "        [-1., -1., -1.,  ..., -0., -1.,  0.],\n",
      "        [-1.,  1., -0.,  ..., -1.,  1.,  1.]])| \n",
      "[Quantized] model.layers.20.mlp.down_proj | Ternary weights applied.\n",
      "[DEBUG] Quantized: tensor([[ 1.,  0., -1.,  ...,  0., -1., -1.],\n",
      "        [-0.,  0.,  1.,  ...,  0., -1.,  1.],\n",
      "        [-0.,  1.,  1.,  ..., -1., -1.,  1.],\n",
      "        ...,\n",
      "        [-1., -1.,  0.,  ...,  1., -1., -1.],\n",
      "        [-0., -1., -0.,  ...,  1.,  1., -1.],\n",
      "        [-1.,  1.,  1.,  ..., -0., -1., -0.]])| \n",
      "[Quantized] model.layers.21.self_attn.q_proj | Ternary weights applied.\n",
      "[DEBUG] Quantized: tensor([[ 1.,  0.,  1.,  ...,  1., -0.,  1.],\n",
      "        [-1.,  1.,  0.,  ..., -1.,  0.,  1.],\n",
      "        [ 0.,  1.,  0.,  ..., -0., -1., -1.],\n",
      "        ...,\n",
      "        [-1., -1.,  0.,  ...,  1., -1.,  1.],\n",
      "        [ 0.,  0.,  1.,  ..., -1., -1., -1.],\n",
      "        [-1., -0.,  1.,  ...,  1.,  1., -0.]])| \n",
      "[Quantized] model.layers.21.self_attn.k_proj | Ternary weights applied.\n",
      "[DEBUG] Quantized: tensor([[ 0., -1., -1.,  ..., -1., -1., -1.],\n",
      "        [ 1.,  1.,  0.,  ..., -1.,  1., -1.],\n",
      "        [-1.,  1., -0.,  ...,  0., -1., -0.],\n",
      "        ...,\n",
      "        [-0.,  1., -0.,  ...,  0., -0.,  0.],\n",
      "        [ 1.,  0.,  0.,  ..., -1., -1.,  1.],\n",
      "        [-1., -0.,  0.,  ...,  0.,  0., -1.]])| \n",
      "[Quantized] model.layers.21.self_attn.v_proj | Ternary weights applied.\n",
      "[DEBUG] Quantized: tensor([[ 1.,  0., -1.,  ..., -1.,  0., -0.],\n",
      "        [-1.,  0.,  1.,  ...,  1.,  1., -0.],\n",
      "        [-0.,  0.,  1.,  ..., -0., -0.,  1.],\n",
      "        ...,\n",
      "        [-0., -1., -0.,  ...,  0., -0.,  1.],\n",
      "        [ 0., -0., -1.,  ..., -1., -1., -0.],\n",
      "        [-0., -1., -0.,  ..., -0., -1., -0.]])| \n",
      "[Quantized] model.layers.21.self_attn.o_proj | Ternary weights applied.\n",
      "[DEBUG] Quantized: tensor([[-1., -0., -1.,  ...,  1.,  0., -1.],\n",
      "        [-1., -1.,  0.,  ...,  1.,  1.,  0.],\n",
      "        [ 1.,  1., -1.,  ..., -1., -1., -1.],\n",
      "        ...,\n",
      "        [ 1., -0., -1.,  ...,  1.,  1.,  1.],\n",
      "        [ 1., -1.,  1.,  ...,  1.,  1.,  1.],\n",
      "        [ 1.,  1.,  1.,  ..., -1.,  0., -0.]])| \n",
      "[Quantized] model.layers.21.mlp.gate_proj | Ternary weights applied.\n",
      "[DEBUG] Quantized: tensor([[ 1.,  1.,  0.,  ..., -0.,  0.,  1.],\n",
      "        [-1., -1.,  1.,  ..., -1.,  1., -0.],\n",
      "        [ 1., -1., -0.,  ...,  0., -1., -1.],\n",
      "        ...,\n",
      "        [-1.,  0., -1.,  ..., -0., -1.,  1.],\n",
      "        [ 0., -1.,  1.,  ..., -0., -1.,  0.],\n",
      "        [-0.,  1.,  1.,  ...,  0., -1.,  0.]])| \n",
      "[Quantized] model.layers.21.mlp.up_proj | Ternary weights applied.\n",
      "[DEBUG] Quantized: tensor([[ 1., -1., -1.,  ...,  1.,  1., -1.],\n",
      "        [ 0.,  0., -1.,  ...,  1.,  1., -1.],\n",
      "        [-1.,  0., -0.,  ..., -1., -1.,  1.],\n",
      "        ...,\n",
      "        [ 1., -1.,  0.,  ...,  1., -1.,  0.],\n",
      "        [ 1., -1.,  1.,  ..., -1., -0.,  1.],\n",
      "        [-1.,  1., -1.,  ...,  1.,  1.,  0.]])| \n",
      "[Quantized] model.layers.21.mlp.down_proj | Ternary weights applied.\n",
      "[DEBUG] Quantized: tensor([[-1.,  0., -1.,  ..., -1.,  1.,  1.],\n",
      "        [ 0., -1., -1.,  ...,  1., -1.,  1.],\n",
      "        [-0., -1.,  1.,  ...,  1.,  1., -1.],\n",
      "        ...,\n",
      "        [ 0.,  1., -1.,  ...,  1., -0.,  1.],\n",
      "        [-1.,  0., -1.,  ...,  0.,  1., -1.],\n",
      "        [ 1., -0.,  1.,  ...,  1., -0.,  0.]])| \n",
      "[Quantized] model.layers.22.self_attn.q_proj | Ternary weights applied.\n",
      "[DEBUG] Quantized: tensor([[ 0.,  1., -1.,  ..., -1., -1., -1.],\n",
      "        [ 0.,  1., -1.,  ...,  0.,  1., -1.],\n",
      "        [ 1., -1.,  1.,  ..., -1., -1.,  0.],\n",
      "        ...,\n",
      "        [-0., -1., -0.,  ...,  1.,  1.,  1.],\n",
      "        [ 1., -1., -1.,  ..., -0.,  1.,  1.],\n",
      "        [ 1.,  1., -0.,  ...,  1., -1.,  1.]])| \n",
      "[Quantized] model.layers.22.self_attn.k_proj | Ternary weights applied.\n",
      "[DEBUG] Quantized: tensor([[ 1., -1.,  1.,  ..., -1., -1.,  0.],\n",
      "        [ 0., -0., -0.,  ..., -0., -1.,  0.],\n",
      "        [ 1., -0.,  1.,  ..., -0.,  1., -1.],\n",
      "        ...,\n",
      "        [-1., -1.,  0.,  ...,  1.,  1., -1.],\n",
      "        [-0.,  0.,  1.,  ..., -0.,  1., -1.],\n",
      "        [ 1.,  0.,  0.,  ..., -1., -1., -1.]])| \n",
      "[Quantized] model.layers.22.self_attn.v_proj | Ternary weights applied.\n",
      "[DEBUG] Quantized: tensor([[-0., -1., -0.,  ..., -1., -1., -1.],\n",
      "        [ 1.,  1., -1.,  ..., -1., -1., -1.],\n",
      "        [ 1., -1.,  0.,  ...,  1., -1.,  1.],\n",
      "        ...,\n",
      "        [-1.,  1.,  1.,  ..., -1., -0., -0.],\n",
      "        [ 1.,  1., -0.,  ...,  1.,  1.,  1.],\n",
      "        [-1., -1.,  1.,  ...,  1., -1.,  1.]])| \n",
      "[Quantized] model.layers.22.self_attn.o_proj | Ternary weights applied.\n",
      "[DEBUG] Quantized: tensor([[ 1., -1.,  0.,  ...,  1., -0.,  1.],\n",
      "        [ 1.,  1.,  0.,  ..., -1.,  1.,  1.],\n",
      "        [-0., -1.,  1.,  ...,  1., -1.,  1.],\n",
      "        ...,\n",
      "        [ 0.,  0.,  1.,  ...,  1.,  1.,  1.],\n",
      "        [-0., -1.,  0.,  ...,  1., -1.,  0.],\n",
      "        [ 1., -1.,  1.,  ...,  1.,  0.,  1.]])| \n",
      "[Quantized] model.layers.22.mlp.gate_proj | Ternary weights applied.\n",
      "[DEBUG] Quantized: tensor([[-1.,  0.,  0.,  ...,  1., -0.,  1.],\n",
      "        [ 0.,  1., -0.,  ...,  1.,  1.,  1.],\n",
      "        [ 0., -0., -1.,  ...,  0., -1., -1.],\n",
      "        ...,\n",
      "        [ 1., -1.,  1.,  ...,  1., -0.,  1.],\n",
      "        [-1., -1.,  1.,  ..., -1., -1., -1.],\n",
      "        [-1., -1., -1.,  ...,  1., -1.,  1.]])| \n",
      "[Quantized] model.layers.22.mlp.up_proj | Ternary weights applied.\n",
      "[DEBUG] Quantized: tensor([[-1., -1.,  1.,  ..., -1.,  1., -1.],\n",
      "        [-1.,  0., -0.,  ...,  1., -1., -1.],\n",
      "        [-1., -0.,  1.,  ..., -1., -1.,  1.],\n",
      "        ...,\n",
      "        [ 1., -1.,  1.,  ..., -1., -0., -1.],\n",
      "        [-0., -0.,  1.,  ..., -1.,  1., -0.],\n",
      "        [ 1.,  1.,  0.,  ..., -0.,  1.,  0.]])| \n",
      "[Quantized] model.layers.22.mlp.down_proj | Ternary weights applied.\n",
      "[DEBUG] Quantized: tensor([[ 0., -1.,  1.,  ...,  0., -0.,  1.],\n",
      "        [ 1., -1.,  1.,  ..., -0., -0., -0.],\n",
      "        [-1.,  1., -1.,  ..., -0., -1.,  1.],\n",
      "        ...,\n",
      "        [-0.,  1., -0.,  ..., -1., -1.,  1.],\n",
      "        [-0.,  1., -1.,  ..., -1., -0., -1.],\n",
      "        [-1., -1.,  1.,  ..., -0., -1., -1.]])| \n",
      "[Quantized] model.layers.23.self_attn.q_proj | Ternary weights applied.\n",
      "[DEBUG] Quantized: tensor([[-1.,  1.,  1.,  ..., -0.,  0.,  1.],\n",
      "        [ 1., -1.,  1.,  ..., -0.,  1.,  1.],\n",
      "        [ 0.,  1., -1.,  ...,  0.,  0., -0.],\n",
      "        ...,\n",
      "        [-0.,  0.,  0.,  ..., -0.,  0., -1.],\n",
      "        [-1., -1., -1.,  ...,  1., -0.,  1.],\n",
      "        [-1., -0., -1.,  ...,  1.,  1.,  1.]])| \n",
      "[Quantized] model.layers.23.self_attn.k_proj | Ternary weights applied.\n",
      "[DEBUG] Quantized: tensor([[ 1., -1., -0.,  ...,  0., -1., -0.],\n",
      "        [-1., -1., -0.,  ..., -1., -0.,  1.],\n",
      "        [-0., -0.,  0.,  ..., -1.,  1.,  0.],\n",
      "        ...,\n",
      "        [-1.,  0.,  1.,  ..., -0.,  0.,  1.],\n",
      "        [-1.,  1.,  0.,  ...,  0., -1.,  1.],\n",
      "        [-1.,  1., -1.,  ..., -1., -1.,  1.]])| \n",
      "[Quantized] model.layers.23.self_attn.v_proj | Ternary weights applied.\n",
      "[DEBUG] Quantized: tensor([[-1.,  0.,  0.,  ..., -1., -1., -0.],\n",
      "        [ 1.,  1.,  0.,  ...,  0.,  1.,  1.],\n",
      "        [-1.,  0., -1.,  ...,  0., -0., -1.],\n",
      "        ...,\n",
      "        [ 1.,  1.,  1.,  ..., -1.,  0., -1.],\n",
      "        [ 1.,  1., -1.,  ..., -1., -1., -1.],\n",
      "        [-0., -1., -1.,  ...,  1.,  1., -1.]])| \n",
      "[Quantized] model.layers.23.self_attn.o_proj | Ternary weights applied.\n",
      "[DEBUG] Quantized: tensor([[-1., -1.,  1.,  ...,  0., -1., -1.],\n",
      "        [-0.,  0., -1.,  ..., -1.,  0.,  0.],\n",
      "        [ 1.,  0.,  1.,  ..., -1.,  1.,  1.],\n",
      "        ...,\n",
      "        [-1.,  0., -1.,  ..., -0.,  1.,  1.],\n",
      "        [ 1.,  1., -0.,  ..., -0., -1.,  1.],\n",
      "        [ 0.,  1., -1.,  ..., -1.,  1., -0.]])| \n",
      "[Quantized] model.layers.23.mlp.gate_proj | Ternary weights applied.\n",
      "[DEBUG] Quantized: tensor([[-0.,  0., -1.,  ..., -1.,  1., -1.],\n",
      "        [ 1., -0., -1.,  ...,  1., -1.,  1.],\n",
      "        [ 1., -1., -0.,  ..., -0., -0., -0.],\n",
      "        ...,\n",
      "        [ 0., -0., -0.,  ...,  1., -0.,  1.],\n",
      "        [ 0., -1., -0.,  ..., -1., -1.,  0.],\n",
      "        [ 1.,  0., -1.,  ..., -1., -1.,  1.]])| \n",
      "[Quantized] model.layers.23.mlp.up_proj | Ternary weights applied.\n",
      "[DEBUG] Quantized: tensor([[-1., -0., -0.,  ...,  0., -1., -1.],\n",
      "        [ 1., -0.,  0.,  ...,  0.,  1.,  1.],\n",
      "        [ 0.,  1.,  1.,  ...,  0.,  1.,  1.],\n",
      "        ...,\n",
      "        [ 1., -0., -1.,  ..., -0., -1., -1.],\n",
      "        [-0.,  1.,  1.,  ..., -0.,  1., -0.],\n",
      "        [ 1., -1.,  1.,  ..., -0.,  1., -0.]])| \n",
      "[Quantized] model.layers.23.mlp.down_proj | Ternary weights applied.\n",
      "[DEBUG] Quantized: tensor([[ 0., -0.,  0.,  ...,  1.,  1.,  0.],\n",
      "        [ 0.,  1.,  0.,  ..., -1., -0.,  1.],\n",
      "        [-0.,  1.,  1.,  ...,  1.,  1.,  0.],\n",
      "        ...,\n",
      "        [-1.,  1., -1.,  ...,  1., -1., -1.],\n",
      "        [-1.,  0., -1.,  ...,  1., -1., -0.],\n",
      "        [-0., -1.,  1.,  ...,  0.,  1., -1.]])| \n",
      "[Quantized] model.layers.24.self_attn.q_proj | Ternary weights applied.\n",
      "[DEBUG] Quantized: tensor([[ 1., -1.,  1.,  ...,  0.,  1.,  1.],\n",
      "        [-1., -0.,  0.,  ..., -0., -0.,  0.],\n",
      "        [-1., -1., -0.,  ..., -0., -1.,  0.],\n",
      "        ...,\n",
      "        [ 1., -1., -1.,  ..., -1., -1.,  1.],\n",
      "        [-0.,  1.,  0.,  ..., -1.,  1., -0.],\n",
      "        [ 1.,  0.,  1.,  ..., -0., -1., -1.]])| \n",
      "[Quantized] model.layers.24.self_attn.k_proj | Ternary weights applied.\n",
      "[DEBUG] Quantized: tensor([[ 0.,  0., -1.,  ...,  0., -1., -1.],\n",
      "        [ 0.,  0.,  1.,  ...,  1., -0.,  1.],\n",
      "        [-1., -1., -0.,  ..., -0., -0.,  0.],\n",
      "        ...,\n",
      "        [-0., -1.,  0.,  ..., -0.,  1., -0.],\n",
      "        [ 0., -1.,  0.,  ..., -1.,  1., -0.],\n",
      "        [-1., -1., -1.,  ...,  1.,  1., -1.]])| \n",
      "[Quantized] model.layers.24.self_attn.v_proj | Ternary weights applied.\n",
      "[DEBUG] Quantized: tensor([[ 1.,  1., -1.,  ...,  0.,  1.,  1.],\n",
      "        [ 0.,  0.,  0.,  ...,  1.,  1., -0.],\n",
      "        [ 1., -1.,  1.,  ..., -0., -1., -1.],\n",
      "        ...,\n",
      "        [ 0.,  0.,  0.,  ...,  1., -1.,  0.],\n",
      "        [-0., -1.,  1.,  ..., -0., -1.,  1.],\n",
      "        [-1.,  1.,  1.,  ..., -1.,  0.,  1.]])| \n",
      "[Quantized] model.layers.24.self_attn.o_proj | Ternary weights applied.\n",
      "[DEBUG] Quantized: tensor([[-1., -1.,  0.,  ...,  0., -1.,  1.],\n",
      "        [-1., -1., -0.,  ...,  1., -1.,  1.],\n",
      "        [ 1., -1.,  1.,  ...,  1., -0.,  0.],\n",
      "        ...,\n",
      "        [ 1., -0.,  0.,  ...,  0.,  1.,  1.],\n",
      "        [-0., -0.,  1.,  ...,  1.,  1., -1.],\n",
      "        [ 1., -0., -1.,  ..., -0., -0., -1.]])| \n",
      "[Quantized] model.layers.24.mlp.gate_proj | Ternary weights applied.\n",
      "[DEBUG] Quantized: tensor([[ 0., -1., -0.,  ...,  1.,  1.,  1.],\n",
      "        [-1., -0.,  1.,  ...,  1.,  0.,  0.],\n",
      "        [-1., -0., -0.,  ...,  0.,  0., -1.],\n",
      "        ...,\n",
      "        [ 1.,  0., -0.,  ...,  1., -1., -0.],\n",
      "        [-1., -0.,  1.,  ...,  1., -1.,  1.],\n",
      "        [ 1., -0., -0.,  ...,  1.,  0.,  1.]])| \n",
      "[Quantized] model.layers.24.mlp.up_proj | Ternary weights applied.\n",
      "[DEBUG] Quantized: tensor([[-1., -1.,  0.,  ..., -1., -1.,  1.],\n",
      "        [ 0., -1., -0.,  ..., -1.,  1., -0.],\n",
      "        [ 0.,  1., -1.,  ..., -0., -1.,  1.],\n",
      "        ...,\n",
      "        [ 1.,  1.,  1.,  ..., -1.,  0., -0.],\n",
      "        [ 0.,  0., -1.,  ..., -1.,  1., -0.],\n",
      "        [ 1., -1.,  1.,  ...,  1., -1.,  1.]])| \n",
      "[Quantized] model.layers.24.mlp.down_proj | Ternary weights applied.\n",
      "[DEBUG] Quantized: tensor([[-1., -0.,  0.,  ..., -0., -1.,  0.],\n",
      "        [ 0.,  1., -1.,  ..., -0.,  0.,  1.],\n",
      "        [ 1., -1.,  1.,  ...,  1.,  1.,  1.],\n",
      "        ...,\n",
      "        [ 1., -1.,  1.,  ...,  0., -1., -1.],\n",
      "        [-0.,  0., -0.,  ...,  0.,  1., -1.],\n",
      "        [ 0., -1., -1.,  ...,  1.,  1.,  1.]])| \n",
      "[Quantized] model.layers.25.self_attn.q_proj | Ternary weights applied.\n",
      "[DEBUG] Quantized: tensor([[-1.,  0.,  0.,  ..., -1.,  1.,  1.],\n",
      "        [ 1.,  1.,  0.,  ...,  1.,  0., -1.],\n",
      "        [-1., -0., -1.,  ..., -1., -1., -1.],\n",
      "        ...,\n",
      "        [ 1., -1., -0.,  ..., -1.,  0., -1.],\n",
      "        [ 1.,  1.,  1.,  ..., -0.,  1., -1.],\n",
      "        [ 1.,  1., -1.,  ..., -1.,  0., -1.]])| \n",
      "[Quantized] model.layers.25.self_attn.k_proj | Ternary weights applied.\n",
      "[DEBUG] Quantized: tensor([[ 1.,  0.,  0.,  ...,  1., -1.,  1.],\n",
      "        [-1., -1.,  0.,  ...,  1.,  0., -1.],\n",
      "        [ 0.,  1., -0.,  ...,  1., -1., -1.],\n",
      "        ...,\n",
      "        [-0., -1.,  0.,  ...,  1.,  1.,  0.],\n",
      "        [-1.,  1., -0.,  ..., -1., -1., -1.],\n",
      "        [ 1.,  0.,  0.,  ..., -1.,  1., -1.]])| \n",
      "[Quantized] model.layers.25.self_attn.v_proj | Ternary weights applied.\n",
      "[DEBUG] Quantized: tensor([[-1., -0.,  1.,  ...,  0.,  0.,  1.],\n",
      "        [ 1.,  1., -1.,  ..., -1.,  1.,  0.],\n",
      "        [-1.,  1., -0.,  ...,  0.,  1.,  1.],\n",
      "        ...,\n",
      "        [ 1., -1.,  1.,  ...,  1., -1., -1.],\n",
      "        [ 1., -0., -0.,  ...,  1., -1., -1.],\n",
      "        [ 1., -1., -0.,  ...,  0.,  0., -1.]])| \n",
      "[Quantized] model.layers.25.self_attn.o_proj | Ternary weights applied.\n",
      "[DEBUG] Quantized: tensor([[ 1., -1., -1.,  ..., -1.,  0.,  1.],\n",
      "        [-1.,  0., -0.,  ...,  1.,  1., -1.],\n",
      "        [-0., -1.,  0.,  ...,  0., -1., -1.],\n",
      "        ...,\n",
      "        [ 1., -1.,  0.,  ...,  1., -0.,  1.],\n",
      "        [-0., -1., -1.,  ..., -1.,  1.,  1.],\n",
      "        [-1., -1.,  1.,  ..., -1.,  0.,  1.]])| \n",
      "[Quantized] model.layers.25.mlp.gate_proj | Ternary weights applied.\n",
      "[DEBUG] Quantized: tensor([[ 1., -1., -0.,  ...,  0., -1.,  1.],\n",
      "        [ 0.,  0., -0.,  ..., -1.,  1.,  1.],\n",
      "        [-0., -0., -1.,  ...,  1.,  1.,  1.],\n",
      "        ...,\n",
      "        [ 0.,  0., -1.,  ..., -1., -0., -1.],\n",
      "        [ 1., -1., -0.,  ...,  0.,  0., -1.],\n",
      "        [ 0., -1.,  1.,  ...,  0.,  1., -1.]])| \n",
      "[Quantized] model.layers.25.mlp.up_proj | Ternary weights applied.\n",
      "[DEBUG] Quantized: tensor([[-0.,  1.,  1.,  ...,  0., -0.,  1.],\n",
      "        [ 1.,  1., -0.,  ...,  1.,  0.,  1.],\n",
      "        [ 0., -1., -0.,  ..., -1., -0., -0.],\n",
      "        ...,\n",
      "        [-1., -1., -1.,  ...,  0., -1., -1.],\n",
      "        [ 1., -1., -1.,  ..., -1.,  1.,  1.],\n",
      "        [-1., -0., -1.,  ...,  1., -1.,  1.]])| \n",
      "[Quantized] model.layers.25.mlp.down_proj | Ternary weights applied.\n",
      "[DEBUG] Quantized: tensor([[-1.,  0.,  1.,  ..., -0., -1., -1.],\n",
      "        [ 1., -1., -0.,  ..., -1.,  1., -0.],\n",
      "        [-1., -1., -1.,  ..., -0., -1.,  1.],\n",
      "        ...,\n",
      "        [ 1.,  1., -1.,  ..., -1., -1.,  1.],\n",
      "        [-1.,  0., -1.,  ..., -0., -1., -1.],\n",
      "        [ 0., -1., -0.,  ..., -1.,  0., -1.]])| \n",
      "[Quantized] model.layers.26.self_attn.q_proj | Ternary weights applied.\n",
      "[DEBUG] Quantized: tensor([[-0.,  1., -1.,  ..., -1., -0.,  1.],\n",
      "        [-0., -1.,  1.,  ..., -1., -0.,  1.],\n",
      "        [ 0., -1.,  0.,  ...,  1., -0., -1.],\n",
      "        ...,\n",
      "        [-0., -1.,  1.,  ...,  1.,  0., -0.],\n",
      "        [ 0., -1., -1.,  ...,  1.,  1.,  1.],\n",
      "        [ 1., -1., -0.,  ...,  0., -1.,  1.]])| \n",
      "[Quantized] model.layers.26.self_attn.k_proj | Ternary weights applied.\n",
      "[DEBUG] Quantized: tensor([[-1., -1.,  1.,  ...,  1., -1., -1.],\n",
      "        [ 1.,  0.,  0.,  ...,  1., -1., -1.],\n",
      "        [-0.,  1.,  0.,  ..., -0., -1., -1.],\n",
      "        ...,\n",
      "        [ 1.,  0., -1.,  ..., -1.,  0., -1.],\n",
      "        [-0., -1.,  1.,  ...,  1.,  1., -1.],\n",
      "        [-1., -0.,  1.,  ...,  1.,  0.,  1.]])| \n",
      "[Quantized] model.layers.26.self_attn.v_proj | Ternary weights applied.\n",
      "[DEBUG] Quantized: tensor([[ 1., -1., -0.,  ...,  1., -0., -0.],\n",
      "        [-0., -1.,  0.,  ..., -1., -1., -0.],\n",
      "        [-1.,  0., -1.,  ..., -1., -1.,  1.],\n",
      "        ...,\n",
      "        [-0., -1., -0.,  ..., -1.,  0.,  1.],\n",
      "        [ 1.,  1.,  1.,  ...,  0.,  1.,  1.],\n",
      "        [ 0., -0.,  1.,  ..., -1., -0., -1.]])| \n",
      "[Quantized] model.layers.26.self_attn.o_proj | Ternary weights applied.\n",
      "[DEBUG] Quantized: tensor([[ 0.,  0., -0.,  ...,  1., -1., -0.],\n",
      "        [-1.,  1., -1.,  ..., -1., -1., -1.],\n",
      "        [ 0., -1.,  0.,  ..., -1., -1., -0.],\n",
      "        ...,\n",
      "        [-1., -0., -0.,  ...,  1.,  1., -0.],\n",
      "        [-1.,  1.,  1.,  ...,  1., -1., -1.],\n",
      "        [-1.,  0., -1.,  ...,  1., -1.,  1.]])| \n",
      "[Quantized] model.layers.26.mlp.gate_proj | Ternary weights applied.\n",
      "[DEBUG] Quantized: tensor([[ 1.,  1.,  1.,  ..., -1., -1., -0.],\n",
      "        [-1., -1.,  1.,  ...,  1., -0.,  1.],\n",
      "        [-0.,  1., -1.,  ..., -1., -0.,  1.],\n",
      "        ...,\n",
      "        [-0., -1., -1.,  ..., -1., -1., -1.],\n",
      "        [-0., -1., -0.,  ...,  1., -0., -1.],\n",
      "        [-1.,  1.,  1.,  ...,  1.,  1.,  1.]])| \n",
      "[Quantized] model.layers.26.mlp.up_proj | Ternary weights applied.\n",
      "[DEBUG] Quantized: tensor([[-1.,  1., -1.,  ..., -1.,  0.,  1.],\n",
      "        [-0., -0., -0.,  ..., -1.,  1.,  0.],\n",
      "        [-1., -0.,  1.,  ..., -1., -1., -1.],\n",
      "        ...,\n",
      "        [ 1.,  1., -1.,  ..., -1., -1.,  1.],\n",
      "        [-0., -0.,  0.,  ..., -0.,  1.,  0.],\n",
      "        [ 1., -0., -0.,  ..., -1., -1., -1.]])| \n",
      "[Quantized] model.layers.26.mlp.down_proj | Ternary weights applied.\n",
      "[DEBUG] Quantized: tensor([[-0.,  0.,  1.,  ...,  1., -1., -0.],\n",
      "        [ 0.,  1., -0.,  ..., -1.,  0.,  0.],\n",
      "        [ 1.,  1.,  1.,  ..., -0., -1.,  1.],\n",
      "        ...,\n",
      "        [ 1.,  0.,  1.,  ..., -1., -0.,  1.],\n",
      "        [-1., -1., -1.,  ...,  1., -1., -0.],\n",
      "        [ 1., -1., -0.,  ...,  1., -1.,  1.]])| \n",
      "[Quantized] model.layers.27.self_attn.q_proj | Ternary weights applied.\n",
      "[DEBUG] Quantized: tensor([[-1., -0., -1.,  ...,  0.,  1.,  1.],\n",
      "        [-0., -0., -1.,  ..., -0., -0.,  0.],\n",
      "        [ 1., -0.,  1.,  ...,  1.,  1., -0.],\n",
      "        ...,\n",
      "        [-1.,  1.,  1.,  ..., -1., -0., -0.],\n",
      "        [ 1.,  0., -1.,  ..., -0., -1., -1.],\n",
      "        [ 0.,  0., -1.,  ..., -0., -1., -1.]])| \n",
      "[Quantized] model.layers.27.self_attn.k_proj | Ternary weights applied.\n",
      "[DEBUG] Quantized: tensor([[ 1., -0.,  0.,  ..., -1., -1.,  0.],\n",
      "        [-1.,  1.,  0.,  ...,  0.,  1., -1.],\n",
      "        [-1., -1.,  1.,  ..., -1.,  0., -0.],\n",
      "        ...,\n",
      "        [-0.,  1., -1.,  ...,  0., -0.,  1.],\n",
      "        [ 0.,  1., -1.,  ..., -1., -1., -1.],\n",
      "        [-1.,  1.,  1.,  ...,  0., -1.,  0.]])| \n",
      "[Quantized] model.layers.27.self_attn.v_proj | Ternary weights applied.\n",
      "[DEBUG] Quantized: tensor([[ 1.,  0., -1.,  ..., -1.,  0.,  1.],\n",
      "        [-1., -0., -1.,  ...,  0., -1., -1.],\n",
      "        [ 0.,  1., -0.,  ...,  1.,  1.,  1.],\n",
      "        ...,\n",
      "        [ 1.,  1.,  0.,  ..., -1., -1., -1.],\n",
      "        [-1.,  1.,  1.,  ..., -1.,  1.,  1.],\n",
      "        [ 1., -1.,  0.,  ..., -1., -0.,  1.]])| \n",
      "[Quantized] model.layers.27.self_attn.o_proj | Ternary weights applied.\n",
      "[DEBUG] Quantized: tensor([[-1.,  1., -1.,  ...,  1.,  1., -1.],\n",
      "        [ 1., -0.,  1.,  ...,  1., -1.,  1.],\n",
      "        [-1.,  1.,  1.,  ..., -0., -0., -1.],\n",
      "        ...,\n",
      "        [-0., -1., -1.,  ..., -1.,  0.,  1.],\n",
      "        [ 1.,  1.,  0.,  ...,  1.,  1., -1.],\n",
      "        [ 0., -1.,  1.,  ...,  0., -0., -0.]])| \n",
      "[Quantized] model.layers.27.mlp.gate_proj | Ternary weights applied.\n",
      "[DEBUG] Quantized: tensor([[-1., -1.,  1.,  ...,  1., -0.,  1.],\n",
      "        [-1., -1., -1.,  ..., -0.,  1., -1.],\n",
      "        [ 0.,  1., -1.,  ...,  0.,  1.,  1.],\n",
      "        ...,\n",
      "        [ 1.,  1., -1.,  ...,  1., -0., -1.],\n",
      "        [ 0., -1., -1.,  ...,  0., -1.,  0.],\n",
      "        [-0.,  1.,  0.,  ..., -1.,  1., -1.]])| \n",
      "[Quantized] model.layers.27.mlp.up_proj | Ternary weights applied.\n",
      "[DEBUG] Quantized: tensor([[-1.,  1.,  1.,  ..., -0.,  1.,  0.],\n",
      "        [-1.,  0., -0.,  ..., -1.,  1.,  0.],\n",
      "        [-0., -1.,  0.,  ..., -1., -0.,  0.],\n",
      "        ...,\n",
      "        [ 0.,  1.,  1.,  ...,  1., -1.,  1.],\n",
      "        [ 1., -1.,  1.,  ..., -1.,  1., -1.],\n",
      "        [-1.,  1., -0.,  ..., -1., -1.,  1.]])| \n",
      "[Quantized] model.layers.27.mlp.down_proj | Ternary weights applied.\n",
      ">> [STEP 4]: Freezing modules...\n",
      "model.layers.0.self_attn.q_proj mean act: 0.00042038538958877325\n",
      "model.layers.0.self_attn.k_proj mean act: 0.00042038538958877325\n",
      "model.layers.0.self_attn.v_proj mean act: 0.00042038538958877325\n",
      "model.layers.0.self_attn.o_proj mean act: -0.00013043987564742565\n",
      "model.layers.0.mlp.gate_proj mean act: 0.00023862022499088198\n",
      "model.layers.0.mlp.up_proj mean act: 0.00023862022499088198\n",
      "model.layers.0.mlp.down_proj mean act: -9.107174264499918e-06\n",
      "model.layers.1.self_attn.q_proj mean act: 0.0005290800472721457\n",
      "model.layers.1.self_attn.k_proj mean act: 0.0005290800472721457\n",
      "model.layers.1.self_attn.v_proj mean act: 0.0005290800472721457\n",
      "model.layers.1.self_attn.o_proj mean act: -0.00025867999647744\n",
      "model.layers.1.mlp.gate_proj mean act: 8.554317901143804e-05\n",
      "model.layers.1.mlp.up_proj mean act: 8.554317901143804e-05\n",
      "model.layers.1.mlp.down_proj mean act: -0.0007557441131211817\n",
      "model.layers.2.self_attn.q_proj mean act: 0.00048162610619328916\n",
      "model.layers.2.self_attn.k_proj mean act: 0.00048162610619328916\n",
      "model.layers.2.self_attn.v_proj mean act: 0.00048162610619328916\n",
      "model.layers.2.self_attn.o_proj mean act: -0.000687955878674984\n",
      "model.layers.2.mlp.gate_proj mean act: -0.0003481012536212802\n",
      "model.layers.2.mlp.up_proj mean act: -0.0003481012536212802\n",
      "model.layers.2.mlp.down_proj mean act: -2.3580882952956017e-06\n",
      "model.layers.3.self_attn.q_proj mean act: 0.00015327522123698145\n",
      "model.layers.3.self_attn.k_proj mean act: 0.00015327522123698145\n",
      "model.layers.3.self_attn.v_proj mean act: 0.00015327522123698145\n",
      "model.layers.3.self_attn.o_proj mean act: -0.0008714402210898697\n",
      "model.layers.3.mlp.gate_proj mean act: -0.0004613345954567194\n",
      "model.layers.3.mlp.up_proj mean act: -0.0004613345954567194\n",
      "model.layers.3.mlp.down_proj mean act: -4.163149424130097e-05\n",
      "model.layers.4.self_attn.q_proj mean act: 2.9187203836045228e-05\n",
      "model.layers.4.self_attn.k_proj mean act: 2.9187203836045228e-05\n",
      "model.layers.4.self_attn.v_proj mean act: 2.9187203836045228e-05\n",
      "model.layers.4.self_attn.o_proj mean act: -0.000521469977684319\n",
      "model.layers.4.mlp.gate_proj mean act: -0.00040825732867233455\n",
      "model.layers.4.mlp.up_proj mean act: -0.00040825732867233455\n",
      "model.layers.4.mlp.down_proj mean act: 2.1927626221440732e-05\n",
      "model.layers.5.self_attn.q_proj mean act: 0.00011689870007103309\n",
      "model.layers.5.self_attn.k_proj mean act: 0.00011689870007103309\n",
      "model.layers.5.self_attn.v_proj mean act: 0.00011689870007103309\n",
      "model.layers.5.self_attn.o_proj mean act: 0.0014180844882503152\n",
      "model.layers.5.mlp.gate_proj mean act: -0.00035769594251178205\n",
      "model.layers.5.mlp.up_proj mean act: -0.00035769594251178205\n",
      "model.layers.5.mlp.down_proj mean act: -4.010583143099211e-05\n",
      "model.layers.6.self_attn.q_proj mean act: 7.089276186889037e-05\n",
      "model.layers.6.self_attn.k_proj mean act: 7.089276186889037e-05\n",
      "model.layers.6.self_attn.v_proj mean act: 7.089276186889037e-05\n",
      "model.layers.6.self_attn.o_proj mean act: 0.00020766448869835585\n",
      "model.layers.6.mlp.gate_proj mean act: -0.00031325698364526033\n",
      "model.layers.6.mlp.up_proj mean act: -0.00031325698364526033\n",
      "model.layers.6.mlp.down_proj mean act: 8.12420421425486e-06\n",
      "model.layers.7.self_attn.q_proj mean act: 0.0003699113440234214\n",
      "model.layers.7.self_attn.k_proj mean act: 0.0003699113440234214\n",
      "model.layers.7.self_attn.v_proj mean act: 0.0003699113440234214\n",
      "model.layers.7.self_attn.o_proj mean act: 0.0001842249184846878\n",
      "model.layers.7.mlp.gate_proj mean act: -2.6312927730032243e-05\n",
      "model.layers.7.mlp.up_proj mean act: -2.6312927730032243e-05\n",
      "model.layers.7.mlp.down_proj mean act: 9.947695070877671e-05\n",
      "model.layers.8.self_attn.q_proj mean act: 0.0004397847515065223\n",
      "model.layers.8.self_attn.k_proj mean act: 0.0004397847515065223\n",
      "model.layers.8.self_attn.v_proj mean act: 0.0004397847515065223\n",
      "model.layers.8.self_attn.o_proj mean act: -0.0030318843200802803\n",
      "model.layers.8.mlp.gate_proj mean act: -0.00016497356409672648\n",
      "model.layers.8.mlp.up_proj mean act: -0.00016497356409672648\n",
      "model.layers.8.mlp.down_proj mean act: 7.853782153688371e-05\n",
      "model.layers.9.self_attn.q_proj mean act: 0.0008135871030390263\n",
      "model.layers.9.self_attn.k_proj mean act: 0.0008135871030390263\n",
      "model.layers.9.self_attn.v_proj mean act: 0.0008135871030390263\n",
      "model.layers.9.self_attn.o_proj mean act: -0.003389206947758794\n",
      "model.layers.9.mlp.gate_proj mean act: 0.0004989822045899928\n",
      "model.layers.9.mlp.up_proj mean act: 0.0004989822045899928\n",
      "model.layers.9.mlp.down_proj mean act: 5.651613901136443e-05\n",
      "model.layers.10.self_attn.q_proj mean act: 0.0012058926513418555\n",
      "model.layers.10.self_attn.k_proj mean act: 0.0012058926513418555\n",
      "model.layers.10.self_attn.v_proj mean act: 0.0012058926513418555\n",
      "model.layers.10.self_attn.o_proj mean act: 0.00020072999177500606\n",
      "model.layers.10.mlp.gate_proj mean act: 0.0005271640256978571\n",
      "model.layers.10.mlp.up_proj mean act: 0.0005271640256978571\n",
      "model.layers.10.mlp.down_proj mean act: 4.5064185542287305e-05\n",
      "model.layers.11.self_attn.q_proj mean act: 0.0008661610190756619\n",
      "model.layers.11.self_attn.k_proj mean act: 0.0008661610190756619\n",
      "model.layers.11.self_attn.v_proj mean act: 0.0008661610190756619\n",
      "model.layers.11.self_attn.o_proj mean act: 0.0004222040588501841\n",
      "model.layers.11.mlp.gate_proj mean act: 0.00018184301734436303\n",
      "model.layers.11.mlp.up_proj mean act: 0.00018184301734436303\n",
      "model.layers.11.mlp.down_proj mean act: -4.86893804918509e-05\n",
      "model.layers.12.self_attn.q_proj mean act: 0.0006846648757345974\n",
      "model.layers.12.self_attn.k_proj mean act: 0.0006846648757345974\n",
      "model.layers.12.self_attn.v_proj mean act: 0.0006846648757345974\n",
      "model.layers.12.self_attn.o_proj mean act: 0.0020182207226753235\n",
      "model.layers.12.mlp.gate_proj mean act: 3.798257603193633e-05\n",
      "model.layers.12.mlp.up_proj mean act: 3.798257603193633e-05\n",
      "model.layers.12.mlp.down_proj mean act: 6.883998867124319e-05\n",
      "model.layers.13.self_attn.q_proj mean act: 0.0008302386268042028\n",
      "model.layers.13.self_attn.k_proj mean act: 0.0008302386268042028\n",
      "model.layers.13.self_attn.v_proj mean act: 0.0008302386268042028\n",
      "model.layers.13.self_attn.o_proj mean act: 0.000121789809782058\n",
      "model.layers.13.mlp.gate_proj mean act: 4.395512860355666e-06\n",
      "model.layers.13.mlp.up_proj mean act: 4.395512860355666e-06\n",
      "model.layers.13.mlp.down_proj mean act: -4.473925946513191e-05\n",
      "model.layers.14.self_attn.q_proj mean act: 0.0015524978516623378\n",
      "model.layers.14.self_attn.k_proj mean act: 0.0015524978516623378\n",
      "model.layers.14.self_attn.v_proj mean act: 0.0015524978516623378\n",
      "model.layers.14.self_attn.o_proj mean act: -0.0028085371013730764\n",
      "model.layers.14.mlp.gate_proj mean act: 0.0007876343443058431\n",
      "model.layers.14.mlp.up_proj mean act: 0.0007876343443058431\n",
      "model.layers.14.mlp.down_proj mean act: 0.00015559745952486992\n",
      "model.layers.15.self_attn.q_proj mean act: 0.0030726538971066475\n",
      "model.layers.15.self_attn.k_proj mean act: 0.0030726538971066475\n",
      "model.layers.15.self_attn.v_proj mean act: 0.0030726538971066475\n",
      "model.layers.15.self_attn.o_proj mean act: 0.0002883984998334199\n",
      "model.layers.15.mlp.gate_proj mean act: 0.0015023117884993553\n",
      "model.layers.15.mlp.up_proj mean act: 0.0015023117884993553\n",
      "model.layers.15.mlp.down_proj mean act: -1.8776416254695505e-05\n",
      "model.layers.16.self_attn.q_proj mean act: 0.002225817646831274\n",
      "model.layers.16.self_attn.k_proj mean act: 0.002225817646831274\n",
      "model.layers.16.self_attn.v_proj mean act: 0.002225817646831274\n",
      "model.layers.16.self_attn.o_proj mean act: 0.0009095338173210621\n",
      "model.layers.16.mlp.gate_proj mean act: 0.0012011093785986304\n",
      "model.layers.16.mlp.up_proj mean act: 0.0012011093785986304\n",
      "model.layers.16.mlp.down_proj mean act: 5.736786988563836e-06\n",
      "model.layers.17.self_attn.q_proj mean act: 0.0023741235490888357\n",
      "model.layers.17.self_attn.k_proj mean act: 0.0023741235490888357\n",
      "model.layers.17.self_attn.v_proj mean act: 0.0023741235490888357\n",
      "model.layers.17.self_attn.o_proj mean act: 0.002512869192287326\n",
      "model.layers.17.mlp.gate_proj mean act: 0.0012561356415972114\n",
      "model.layers.17.mlp.up_proj mean act: 0.0012561356415972114\n",
      "model.layers.17.mlp.down_proj mean act: 0.00013324141036719084\n",
      "model.layers.18.self_attn.q_proj mean act: 0.001825351850129664\n",
      "model.layers.18.self_attn.k_proj mean act: 0.001825351850129664\n",
      "model.layers.18.self_attn.v_proj mean act: 0.001825351850129664\n",
      "model.layers.18.self_attn.o_proj mean act: -0.003386454423889518\n",
      "model.layers.18.mlp.gate_proj mean act: 0.0010612548794597387\n",
      "model.layers.18.mlp.up_proj mean act: 0.0010612548794597387\n",
      "model.layers.18.mlp.down_proj mean act: -9.098320879274979e-05\n",
      "model.layers.19.self_attn.q_proj mean act: 0.002173997927457094\n",
      "model.layers.19.self_attn.k_proj mean act: 0.002173997927457094\n",
      "model.layers.19.self_attn.v_proj mean act: 0.002173997927457094\n",
      "model.layers.19.self_attn.o_proj mean act: -0.001121343346312642\n",
      "model.layers.19.mlp.gate_proj mean act: 0.0011030397145077586\n",
      "model.layers.19.mlp.up_proj mean act: 0.0011030397145077586\n",
      "model.layers.19.mlp.down_proj mean act: -0.0001872347347671166\n",
      "model.layers.20.self_attn.q_proj mean act: 0.002066150074824691\n",
      "model.layers.20.self_attn.k_proj mean act: 0.002066150074824691\n",
      "model.layers.20.self_attn.v_proj mean act: 0.002066150074824691\n",
      "model.layers.20.self_attn.o_proj mean act: -0.000616338278632611\n",
      "model.layers.20.mlp.gate_proj mean act: 0.0011945520527660847\n",
      "model.layers.20.mlp.up_proj mean act: 0.0011945520527660847\n",
      "model.layers.20.mlp.down_proj mean act: 4.58206377516035e-05\n",
      "model.layers.21.self_attn.q_proj mean act: 0.002815560670569539\n",
      "model.layers.21.self_attn.k_proj mean act: 0.002815560670569539\n",
      "model.layers.21.self_attn.v_proj mean act: 0.002815560670569539\n",
      "model.layers.21.self_attn.o_proj mean act: 0.0051920609548687935\n",
      "model.layers.21.mlp.gate_proj mean act: 0.0018110187957063317\n",
      "model.layers.21.mlp.up_proj mean act: 0.0018110187957063317\n",
      "model.layers.21.mlp.down_proj mean act: 0.0003248743596486747\n",
      "model.layers.22.self_attn.q_proj mean act: 0.003054417669773102\n",
      "model.layers.22.self_attn.k_proj mean act: 0.003054417669773102\n",
      "model.layers.22.self_attn.v_proj mean act: 0.003054417669773102\n",
      "model.layers.22.self_attn.o_proj mean act: -0.0006346137379296124\n",
      "model.layers.22.mlp.gate_proj mean act: 0.002069491194561124\n",
      "model.layers.22.mlp.up_proj mean act: 0.002069491194561124\n",
      "model.layers.22.mlp.down_proj mean act: -0.00015862476720940322\n",
      "model.layers.23.self_attn.q_proj mean act: 0.0028670737519860268\n",
      "model.layers.23.self_attn.k_proj mean act: 0.0028670737519860268\n",
      "model.layers.23.self_attn.v_proj mean act: 0.0028670737519860268\n",
      "model.layers.23.self_attn.o_proj mean act: -0.0024459140840917826\n",
      "model.layers.23.mlp.gate_proj mean act: 0.00241486681625247\n",
      "model.layers.23.mlp.up_proj mean act: 0.00241486681625247\n",
      "model.layers.23.mlp.down_proj mean act: -0.00021605982328765094\n",
      "model.layers.24.self_attn.q_proj mean act: 0.003120503621175885\n",
      "model.layers.24.self_attn.k_proj mean act: 0.003120503621175885\n",
      "model.layers.24.self_attn.v_proj mean act: 0.003120503621175885\n",
      "model.layers.24.self_attn.o_proj mean act: -0.005469045136123896\n",
      "model.layers.24.mlp.gate_proj mean act: 0.0028786060865968466\n",
      "model.layers.24.mlp.up_proj mean act: 0.0028786060865968466\n",
      "model.layers.24.mlp.down_proj mean act: 0.0001372200349578634\n",
      "model.layers.25.self_attn.q_proj mean act: 0.000858222832903266\n",
      "model.layers.25.self_attn.k_proj mean act: 0.000858222832903266\n",
      "model.layers.25.self_attn.v_proj mean act: 0.000858222832903266\n",
      "model.layers.25.self_attn.o_proj mean act: 0.001235505798831582\n",
      "model.layers.25.mlp.gate_proj mean act: 0.0014367069816216826\n",
      "model.layers.25.mlp.up_proj mean act: 0.0014367069816216826\n",
      "model.layers.25.mlp.down_proj mean act: -0.00016930668789427727\n",
      "model.layers.26.self_attn.q_proj mean act: 0.00037832395173609257\n",
      "model.layers.26.self_attn.k_proj mean act: 0.00037832395173609257\n",
      "model.layers.26.self_attn.v_proj mean act: 0.00037832395173609257\n",
      "model.layers.26.self_attn.o_proj mean act: -0.0038202020805329084\n",
      "model.layers.26.mlp.gate_proj mean act: 0.0006286646239459515\n",
      "model.layers.26.mlp.up_proj mean act: 0.0006286646239459515\n",
      "model.layers.26.mlp.down_proj mean act: -0.0005125975585542619\n",
      "model.layers.27.self_attn.q_proj mean act: -0.0005207746871747077\n",
      "model.layers.27.self_attn.k_proj mean act: -0.0005207746871747077\n",
      "model.layers.27.self_attn.v_proj mean act: -0.0005207746871747077\n",
      "model.layers.27.self_attn.o_proj mean act: 0.0011967048048973083\n",
      "model.layers.27.mlp.gate_proj mean act: 4.7990080929594114e-05\n",
      "model.layers.27.mlp.up_proj mean act: 4.7990080929594114e-05\n",
      "model.layers.27.mlp.down_proj mean act: 0.04193215072154999\n",
      "model.layers.0.self_attn.q_proj                    | act_scale: 1.000000\n",
      "model.layers.0.self_attn.k_proj                    | act_scale: 1.000000\n",
      "model.layers.0.self_attn.v_proj                    | act_scale: 1.000000\n",
      "model.layers.0.self_attn.o_proj                    | act_scale: 1.000000\n",
      "model.layers.0.mlp.gate_proj                       | act_scale: 1.000000\n",
      "model.layers.0.mlp.up_proj                         | act_scale: 1.000000\n",
      "model.layers.0.mlp.down_proj                       | act_scale: 1.000000\n",
      "model.layers.1.self_attn.q_proj                    | act_scale: 1.000000\n",
      "model.layers.1.self_attn.k_proj                    | act_scale: 1.000000\n",
      "model.layers.1.self_attn.v_proj                    | act_scale: 1.000000\n",
      "model.layers.1.self_attn.o_proj                    | act_scale: 1.000000\n",
      "model.layers.1.mlp.gate_proj                       | act_scale: 1.000000\n",
      "model.layers.1.mlp.up_proj                         | act_scale: 1.000000\n",
      "model.layers.1.mlp.down_proj                       | act_scale: 1.000000\n",
      "model.layers.2.self_attn.q_proj                    | act_scale: 1.000000\n",
      "model.layers.2.self_attn.k_proj                    | act_scale: 1.000000\n",
      "model.layers.2.self_attn.v_proj                    | act_scale: 1.000000\n",
      "model.layers.2.self_attn.o_proj                    | act_scale: 1.000000\n",
      "model.layers.2.mlp.gate_proj                       | act_scale: 1.000000\n",
      "model.layers.2.mlp.up_proj                         | act_scale: 1.000000\n",
      "model.layers.2.mlp.down_proj                       | act_scale: 1.000000\n",
      "model.layers.3.self_attn.q_proj                    | act_scale: 1.000000\n",
      "model.layers.3.self_attn.k_proj                    | act_scale: 1.000000\n",
      "model.layers.3.self_attn.v_proj                    | act_scale: 1.000000\n",
      "model.layers.3.self_attn.o_proj                    | act_scale: 1.000000\n",
      "model.layers.3.mlp.gate_proj                       | act_scale: 1.000000\n",
      "model.layers.3.mlp.up_proj                         | act_scale: 1.000000\n",
      "model.layers.3.mlp.down_proj                       | act_scale: 1.000000\n",
      "model.layers.4.self_attn.q_proj                    | act_scale: 1.000000\n",
      "model.layers.4.self_attn.k_proj                    | act_scale: 1.000000\n",
      "model.layers.4.self_attn.v_proj                    | act_scale: 1.000000\n",
      "model.layers.4.self_attn.o_proj                    | act_scale: 1.000000\n",
      "model.layers.4.mlp.gate_proj                       | act_scale: 1.000000\n",
      "model.layers.4.mlp.up_proj                         | act_scale: 1.000000\n",
      "model.layers.4.mlp.down_proj                       | act_scale: 1.000000\n",
      "model.layers.5.self_attn.q_proj                    | act_scale: 1.000000\n",
      "model.layers.5.self_attn.k_proj                    | act_scale: 1.000000\n",
      "model.layers.5.self_attn.v_proj                    | act_scale: 1.000000\n",
      "model.layers.5.self_attn.o_proj                    | act_scale: 1.000000\n",
      "model.layers.5.mlp.gate_proj                       | act_scale: 1.000000\n",
      "model.layers.5.mlp.up_proj                         | act_scale: 1.000000\n",
      "model.layers.5.mlp.down_proj                       | act_scale: 1.000000\n",
      "model.layers.6.self_attn.q_proj                    | act_scale: 1.000000\n",
      "model.layers.6.self_attn.k_proj                    | act_scale: 1.000000\n",
      "model.layers.6.self_attn.v_proj                    | act_scale: 1.000000\n",
      "model.layers.6.self_attn.o_proj                    | act_scale: 1.000000\n",
      "model.layers.6.mlp.gate_proj                       | act_scale: 1.000000\n",
      "model.layers.6.mlp.up_proj                         | act_scale: 1.000000\n",
      "model.layers.6.mlp.down_proj                       | act_scale: 1.000000\n",
      "model.layers.7.self_attn.q_proj                    | act_scale: 1.000000\n",
      "model.layers.7.self_attn.k_proj                    | act_scale: 1.000000\n",
      "model.layers.7.self_attn.v_proj                    | act_scale: 1.000000\n",
      "model.layers.7.self_attn.o_proj                    | act_scale: 1.000000\n",
      "model.layers.7.mlp.gate_proj                       | act_scale: 1.000000\n",
      "model.layers.7.mlp.up_proj                         | act_scale: 1.000000\n",
      "model.layers.7.mlp.down_proj                       | act_scale: 1.000000\n",
      "model.layers.8.self_attn.q_proj                    | act_scale: 1.000000\n",
      "model.layers.8.self_attn.k_proj                    | act_scale: 1.000000\n",
      "model.layers.8.self_attn.v_proj                    | act_scale: 1.000000\n",
      "model.layers.8.self_attn.o_proj                    | act_scale: 1.000000\n",
      "model.layers.8.mlp.gate_proj                       | act_scale: 1.000000\n",
      "model.layers.8.mlp.up_proj                         | act_scale: 1.000000\n",
      "model.layers.8.mlp.down_proj                       | act_scale: 1.000000\n",
      "model.layers.9.self_attn.q_proj                    | act_scale: 1.000000\n",
      "model.layers.9.self_attn.k_proj                    | act_scale: 1.000000\n",
      "model.layers.9.self_attn.v_proj                    | act_scale: 1.000000\n",
      "model.layers.9.self_attn.o_proj                    | act_scale: 1.000000\n",
      "model.layers.9.mlp.gate_proj                       | act_scale: 1.000000\n",
      "model.layers.9.mlp.up_proj                         | act_scale: 1.000000\n",
      "model.layers.9.mlp.down_proj                       | act_scale: 1.000000\n",
      "model.layers.10.self_attn.q_proj                   | act_scale: 1.000000\n",
      "model.layers.10.self_attn.k_proj                   | act_scale: 1.000000\n",
      "model.layers.10.self_attn.v_proj                   | act_scale: 1.000000\n",
      "model.layers.10.self_attn.o_proj                   | act_scale: 1.000000\n",
      "model.layers.10.mlp.gate_proj                      | act_scale: 1.000000\n",
      "model.layers.10.mlp.up_proj                        | act_scale: 1.000000\n",
      "model.layers.10.mlp.down_proj                      | act_scale: 1.000000\n",
      "model.layers.11.self_attn.q_proj                   | act_scale: 1.000000\n",
      "model.layers.11.self_attn.k_proj                   | act_scale: 1.000000\n",
      "model.layers.11.self_attn.v_proj                   | act_scale: 1.000000\n",
      "model.layers.11.self_attn.o_proj                   | act_scale: 1.000000\n",
      "model.layers.11.mlp.gate_proj                      | act_scale: 1.000000\n",
      "model.layers.11.mlp.up_proj                        | act_scale: 1.000000\n",
      "model.layers.11.mlp.down_proj                      | act_scale: 1.000000\n",
      "model.layers.12.self_attn.q_proj                   | act_scale: 1.000000\n",
      "model.layers.12.self_attn.k_proj                   | act_scale: 1.000000\n",
      "model.layers.12.self_attn.v_proj                   | act_scale: 1.000000\n",
      "model.layers.12.self_attn.o_proj                   | act_scale: 1.000000\n",
      "model.layers.12.mlp.gate_proj                      | act_scale: 1.000000\n",
      "model.layers.12.mlp.up_proj                        | act_scale: 1.000000\n",
      "model.layers.12.mlp.down_proj                      | act_scale: 1.000000\n",
      "model.layers.13.self_attn.q_proj                   | act_scale: 1.000000\n",
      "model.layers.13.self_attn.k_proj                   | act_scale: 1.000000\n",
      "model.layers.13.self_attn.v_proj                   | act_scale: 1.000000\n",
      "model.layers.13.self_attn.o_proj                   | act_scale: 1.000000\n",
      "model.layers.13.mlp.gate_proj                      | act_scale: 1.000000\n",
      "model.layers.13.mlp.up_proj                        | act_scale: 1.000000\n",
      "model.layers.13.mlp.down_proj                      | act_scale: 1.000000\n",
      "model.layers.14.self_attn.q_proj                   | act_scale: 1.000000\n",
      "model.layers.14.self_attn.k_proj                   | act_scale: 1.000000\n",
      "model.layers.14.self_attn.v_proj                   | act_scale: 1.000000\n",
      "model.layers.14.self_attn.o_proj                   | act_scale: 1.000000\n",
      "model.layers.14.mlp.gate_proj                      | act_scale: 1.000000\n",
      "model.layers.14.mlp.up_proj                        | act_scale: 1.000000\n",
      "model.layers.14.mlp.down_proj                      | act_scale: 1.000000\n",
      "model.layers.15.self_attn.q_proj                   | act_scale: 1.000000\n",
      "model.layers.15.self_attn.k_proj                   | act_scale: 1.000000\n",
      "model.layers.15.self_attn.v_proj                   | act_scale: 1.000000\n",
      "model.layers.15.self_attn.o_proj                   | act_scale: 1.000000\n",
      "model.layers.15.mlp.gate_proj                      | act_scale: 1.000000\n",
      "model.layers.15.mlp.up_proj                        | act_scale: 1.000000\n",
      "model.layers.15.mlp.down_proj                      | act_scale: 1.000000\n",
      "model.layers.16.self_attn.q_proj                   | act_scale: 1.000000\n",
      "model.layers.16.self_attn.k_proj                   | act_scale: 1.000000\n",
      "model.layers.16.self_attn.v_proj                   | act_scale: 1.000000\n",
      "model.layers.16.self_attn.o_proj                   | act_scale: 1.000000\n",
      "model.layers.16.mlp.gate_proj                      | act_scale: 1.000000\n",
      "model.layers.16.mlp.up_proj                        | act_scale: 1.000000\n",
      "model.layers.16.mlp.down_proj                      | act_scale: 1.000000\n",
      "model.layers.17.self_attn.q_proj                   | act_scale: 1.000000\n",
      "model.layers.17.self_attn.k_proj                   | act_scale: 1.000000\n",
      "model.layers.17.self_attn.v_proj                   | act_scale: 1.000000\n",
      "model.layers.17.self_attn.o_proj                   | act_scale: 1.000000\n",
      "model.layers.17.mlp.gate_proj                      | act_scale: 1.000000\n",
      "model.layers.17.mlp.up_proj                        | act_scale: 1.000000\n",
      "model.layers.17.mlp.down_proj                      | act_scale: 1.000000\n",
      "model.layers.18.self_attn.q_proj                   | act_scale: 1.000000\n",
      "model.layers.18.self_attn.k_proj                   | act_scale: 1.000000\n",
      "model.layers.18.self_attn.v_proj                   | act_scale: 1.000000\n",
      "model.layers.18.self_attn.o_proj                   | act_scale: 1.000000\n",
      "model.layers.18.mlp.gate_proj                      | act_scale: 1.000000\n",
      "model.layers.18.mlp.up_proj                        | act_scale: 1.000000\n",
      "model.layers.18.mlp.down_proj                      | act_scale: 1.000000\n",
      "model.layers.19.self_attn.q_proj                   | act_scale: 1.000000\n",
      "model.layers.19.self_attn.k_proj                   | act_scale: 1.000000\n",
      "model.layers.19.self_attn.v_proj                   | act_scale: 1.000000\n",
      "model.layers.19.self_attn.o_proj                   | act_scale: 1.000000\n",
      "model.layers.19.mlp.gate_proj                      | act_scale: 1.000000\n",
      "model.layers.19.mlp.up_proj                        | act_scale: 1.000000\n",
      "model.layers.19.mlp.down_proj                      | act_scale: 1.000000\n",
      "model.layers.20.self_attn.q_proj                   | act_scale: 1.000000\n",
      "model.layers.20.self_attn.k_proj                   | act_scale: 1.000000\n",
      "model.layers.20.self_attn.v_proj                   | act_scale: 1.000000\n",
      "model.layers.20.self_attn.o_proj                   | act_scale: 1.000000\n",
      "model.layers.20.mlp.gate_proj                      | act_scale: 1.000000\n",
      "model.layers.20.mlp.up_proj                        | act_scale: 1.000000\n",
      "model.layers.20.mlp.down_proj                      | act_scale: 1.000000\n",
      "model.layers.21.self_attn.q_proj                   | act_scale: 1.000000\n",
      "model.layers.21.self_attn.k_proj                   | act_scale: 1.000000\n",
      "model.layers.21.self_attn.v_proj                   | act_scale: 1.000000\n",
      "model.layers.21.self_attn.o_proj                   | act_scale: 1.000000\n",
      "model.layers.21.mlp.gate_proj                      | act_scale: 1.000000\n",
      "model.layers.21.mlp.up_proj                        | act_scale: 1.000000\n",
      "model.layers.21.mlp.down_proj                      | act_scale: 1.000000\n",
      "model.layers.22.self_attn.q_proj                   | act_scale: 1.000000\n",
      "model.layers.22.self_attn.k_proj                   | act_scale: 1.000000\n",
      "model.layers.22.self_attn.v_proj                   | act_scale: 1.000000\n",
      "model.layers.22.self_attn.o_proj                   | act_scale: 1.000000\n",
      "model.layers.22.mlp.gate_proj                      | act_scale: 1.000000\n",
      "model.layers.22.mlp.up_proj                        | act_scale: 1.000000\n",
      "model.layers.22.mlp.down_proj                      | act_scale: 1.000000\n",
      "model.layers.23.self_attn.q_proj                   | act_scale: 1.000000\n",
      "model.layers.23.self_attn.k_proj                   | act_scale: 1.000000\n",
      "model.layers.23.self_attn.v_proj                   | act_scale: 1.000000\n",
      "model.layers.23.self_attn.o_proj                   | act_scale: 1.000000\n",
      "model.layers.23.mlp.gate_proj                      | act_scale: 1.000000\n",
      "model.layers.23.mlp.up_proj                        | act_scale: 1.000000\n",
      "model.layers.23.mlp.down_proj                      | act_scale: 1.000000\n",
      "model.layers.24.self_attn.q_proj                   | act_scale: 1.000000\n",
      "model.layers.24.self_attn.k_proj                   | act_scale: 1.000000\n",
      "model.layers.24.self_attn.v_proj                   | act_scale: 1.000000\n",
      "model.layers.24.self_attn.o_proj                   | act_scale: 1.000000\n",
      "model.layers.24.mlp.gate_proj                      | act_scale: 1.000000\n",
      "model.layers.24.mlp.up_proj                        | act_scale: 1.000000\n",
      "model.layers.24.mlp.down_proj                      | act_scale: 1.000000\n",
      "model.layers.25.self_attn.q_proj                   | act_scale: 1.000000\n",
      "model.layers.25.self_attn.k_proj                   | act_scale: 1.000000\n",
      "model.layers.25.self_attn.v_proj                   | act_scale: 1.000000\n",
      "model.layers.25.self_attn.o_proj                   | act_scale: 1.000000\n",
      "model.layers.25.mlp.gate_proj                      | act_scale: 1.000000\n",
      "model.layers.25.mlp.up_proj                        | act_scale: 1.000000\n",
      "model.layers.25.mlp.down_proj                      | act_scale: 1.000000\n",
      "model.layers.26.self_attn.q_proj                   | act_scale: 1.000000\n",
      "model.layers.26.self_attn.k_proj                   | act_scale: 1.000000\n",
      "model.layers.26.self_attn.v_proj                   | act_scale: 1.000000\n",
      "model.layers.26.self_attn.o_proj                   | act_scale: 1.000000\n",
      "model.layers.26.mlp.gate_proj                      | act_scale: 1.000000\n",
      "model.layers.26.mlp.up_proj                        | act_scale: 1.000000\n",
      "model.layers.26.mlp.down_proj                      | act_scale: 1.000000\n",
      "model.layers.27.self_attn.q_proj                   | act_scale: 1.000000\n",
      "model.layers.27.self_attn.k_proj                   | act_scale: 1.000000\n",
      "model.layers.27.self_attn.v_proj                   | act_scale: 1.000000\n",
      "model.layers.27.self_attn.o_proj                   | act_scale: 1.000000\n",
      "model.layers.27.mlp.gate_proj                      | act_scale: 1.000000\n",
      "model.layers.27.mlp.up_proj                        | act_scale: 1.000000\n",
      "model.layers.27.mlp.down_proj                      | act_scale: 1.000000\n"
     ]
    }
   ],
   "source": [
    "dh3b_bitnet_fp32_ptsq = applyPTQ(\n",
    "    load_test_model(FPKey.FP_3B.value, dtype=torch.float32),\n",
    "    tokenizer=dh3b_tokenizer,\n",
    "    #calibration_input=None,\n",
    "    #calibration_input=sub_txts['text'],\n",
    "    calibration_input=Texts.T1.value,\n",
    "    mode='1.58bit',\n",
    "    safer_quant=False,\n",
    "    model_half=False,\n",
    "    q_safety_layers=None,\n",
    "    quant_half=False,\n",
    "    layers_to_quant_weights=QuantStyle.BITNET.value,\n",
    "    layers_to_quant_activations=QuantStyle.BITNET.value,\n",
    "    fragile_layers=False,\n",
    "    act_quant=True,\n",
    "    act_bits=8,\n",
    "    torch_backends=False,\n",
    "    debugging=True,\n",
    "    plot_debugging=False,\n",
    "    plot_quantization=False,\n",
    "    freeze_modules=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model with 8-bit quantization using BNB\n",
    "dh3b_bnb8_float32 = AutoModelForCausalLM.from_pretrained(\n",
    "    FPKey.FP_3B.value,           \n",
    "    #torch_dtype=torch.float32,     # Specify the dtype (for 8-bit, use uint8)\n",
    "    #device_map=\"cpu\",           # Automatically map the model to available devices (GPU/CPU)\n",
    "    load_in_8bit=True,\n",
    "    return_dict=True,\n",
    "    output_hidden_states=True,            # Set to True for 8-bit quantization\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n",
      "`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.51s/it]\n"
     ]
    }
   ],
   "source": [
    "# Load the model with 4-bit quantization using BNB\n",
    "dh3b_bnb4_float32 = AutoModelForCausalLM.from_pretrained(\n",
    "    FPKey.FP_3B.value,           \n",
    "    #torch_dtype=torch.float32,     # This would still use uint8 or another type based on quantization method\n",
    "    #device_map=\"auto\",           # Automatically map the model to available devices (GPU/CPU)\n",
    "    load_in_4bit=True,            # Set to True for 4-bit quantization (if available)\n",
    "    return_dict=True,\n",
    "    output_hidden_states=True, \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model with 8-bit quantization using BNB\n",
    "dh8b_bnb8_float32 = AutoModelForCausalLM.from_pretrained(\n",
    "    FPKey.FP_8B.value,           \n",
    "    #torch_dtype=torch.float32,     # Specify the dtype (for 8-bit, use uint8)\n",
    "    #device_map=\"cpu\",           # Automatically map the model to available devices (GPU/CPU)\n",
    "    load_in_8bit=True,\n",
    "    return_dict=True,\n",
    "    output_hidden_states=True,            # Set to True for 8-bit quantization\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model with 4-bit quantization using BNB\n",
    "dh8b_bnb4_float32 = AutoModelForCausalLM.from_pretrained(\n",
    "    FPKey.FP_8B.value,           \n",
    "    #torch_dtype=torch.float32,     # This would still use uint8 or another type based on quantization method\n",
    "    #device_map=\"auto\",           # Automatically map the model to available devices (GPU/CPU)\n",
    "    load_in_4bit=True,            # Set to True for 4-bit quantization (if available)\n",
    "    return_dict=True,\n",
    "    output_hidden_states=True, \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dh8b_bnb4_float32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logit Lens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gsm8k_questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gsm8k_answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nq_queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch._dynamo\n",
    "torch._dynamo.config.suppress_errors = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "colorscale": [
          [
           0,
           "rgb(247,251,255)"
          ],
          [
           0.125,
           "rgb(222,235,247)"
          ],
          [
           0.25,
           "rgb(198,219,239)"
          ],
          [
           0.375,
           "rgb(158,202,225)"
          ],
          [
           0.5,
           "rgb(107,174,214)"
          ],
          [
           0.625,
           "rgb(66,146,198)"
          ],
          [
           0.75,
           "rgb(33,113,181)"
          ],
          [
           0.875,
           "rgb(8,81,156)"
          ],
          [
           1,
           "rgb(8,48,107)"
          ]
         ],
         "hoverinfo": "text",
         "hovertext": [
          [
           "<b>NWD:</b> 0.352<br><b>Pred:</b>  <br><b>Top-5:</b><br>&nbsp;&nbsp; : 0.181<br>&nbsp;&nbsp; the: 0.030<br>&nbsp;&nbsp; (: 0.015<br>&nbsp;&nbsp; The: 0.014<br>&nbsp;&nbsp; and: 0.012<br>",
           "<b>NWD:</b> 0.035<br><b>Pred:</b>  the<br><b>Top-5:</b><br>&nbsp;&nbsp; the: 0.189<br>&nbsp;&nbsp; a: 0.058<br>&nbsp;&nbsp; : 0.051<br>&nbsp;&nbsp; it: 0.043<br>&nbsp;&nbsp; that: 0.033<br>",
           "<b>NWD:</b> 0.093<br><b>Pred:</b>  the<br><b>Top-5:</b><br>&nbsp;&nbsp; the: 0.174<br>&nbsp;&nbsp; a: 0.118<br>&nbsp;&nbsp; not: 0.069<br>&nbsp;&nbsp; be: 0.044<br>&nbsp;&nbsp; to: 0.024<br>",
           "<b>NWD:</b> 0.053<br><b>Pred:</b>  people<br><b>Top-5:</b><br>&nbsp;&nbsp; people: 0.228<br>&nbsp;&nbsp;ard: 0.096<br>&nbsp;&nbsp; men: 0.061<br>&nbsp;&nbsp; and: 0.056<br>&nbsp;&nbsp;ards: 0.042<br>",
           "<b>NWD:</b> 0.040<br><b>Pred:</b>  county<br><b>Top-5:</b><br>&nbsp;&nbsp; county: 0.106<br>&nbsp;&nbsp;,: 0.033<br>&nbsp;&nbsp; and: 0.030<br>&nbsp;&nbsp;'s: 0.026<br>&nbsp;&nbsp; get: 0.021<br>"
          ],
          [
           "<b>NWD:</b> 0.039<br><b>Pred:</b> 'gc<br><b>Top-5:</b><br>&nbsp;&nbsp;'gc: 0.003<br>&nbsp;&nbsp;﻿using: 0.001<br>&nbsp;&nbsp;actionDate: 0.001<br>&nbsp;&nbsp;GuidId: 0.001<br>&nbsp;&nbsp;��: 0.001<br>",
           "<b>NWD:</b> 0.312<br><b>Pred:</b> 'gc<br><b>Top-5:</b><br>&nbsp;&nbsp;'gc: 0.001<br>&nbsp;&nbsp;﻿using: 0.001<br>&nbsp;&nbsp;字幕: 0.001<br>&nbsp;&nbsp;HeaderCode: 0.000<br>&nbsp;&nbsp;GuidId: 0.000<br>",
           "<b>NWD:</b> 0.217<br><b>Pred:</b> 字幕<br><b>Top-5:</b><br>&nbsp;&nbsp;字幕: 0.001<br>&nbsp;&nbsp;aeda: 0.001<br>&nbsp;&nbsp;페이지: 0.001<br>&nbsp;&nbsp;سال: 0.001<br>&nbsp;&nbsp;نوع: 0.001<br>",
           "<b>NWD:</b> 0.012<br><b>Pred:</b> ard<br><b>Top-5:</b><br>&nbsp;&nbsp;ard: 0.543<br>&nbsp;&nbsp;ards: 0.336<br>&nbsp;&nbsp;mond: 0.048<br>&nbsp;&nbsp;ARD: 0.013<br>&nbsp;&nbsp;enda: 0.007<br>",
           "<b>NWD:</b> 0.057<br><b>Pred:</b> iana<br><b>Top-5:</b><br>&nbsp;&nbsp;iana: 0.119<br>&nbsp;&nbsp;mond: 0.104<br>&nbsp;&nbsp;ians: 0.069<br>&nbsp;&nbsp; county: 0.068<br>&nbsp;&nbsp; County: 0.046<br>"
          ],
          [
           "<b>NWD:</b> 0.035<br><b>Pred:</b> 'gc<br><b>Top-5:</b><br>&nbsp;&nbsp;'gc: 0.002<br>&nbsp;&nbsp;﻿using: 0.001<br>&nbsp;&nbsp;actionDate: 0.001<br>&nbsp;&nbsp;GuidId: 0.001<br>&nbsp;&nbsp;��: 0.001<br>",
           "<b>NWD:</b> 0.177<br><b>Pred:</b> 字幕<br><b>Top-5:</b><br>&nbsp;&nbsp;字幕: 0.001<br>&nbsp;&nbsp;페이지: 0.001<br>&nbsp;&nbsp;﻿using: 0.001<br>&nbsp;&nbsp;'gc: 0.001<br>&nbsp;&nbsp;actionDate: 0.001<br>",
           "<b>NWD:</b> 0.178<br><b>Pred:</b> 字幕<br><b>Top-5:</b><br>&nbsp;&nbsp;字幕: 0.001<br>&nbsp;&nbsp;페이지: 0.001<br>&nbsp;&nbsp;aeda: 0.001<br>&nbsp;&nbsp;نوع: 0.001<br>&nbsp;&nbsp;سال: 0.001<br>",
           "<b>NWD:</b> 0.059<br><b>Pred:</b> mond<br><b>Top-5:</b><br>&nbsp;&nbsp;mond: 0.468<br>&nbsp;&nbsp;ards: 0.259<br>&nbsp;&nbsp;ard: 0.069<br>&nbsp;&nbsp;enda: 0.029<br>&nbsp;&nbsp;ness: 0.008<br>",
           "<b>NWD:</b> 0.141<br><b>Pred:</b> ians<br><b>Top-5:</b><br>&nbsp;&nbsp;ians: 0.186<br>&nbsp;&nbsp;mond: 0.152<br>&nbsp;&nbsp;shire: 0.043<br>&nbsp;&nbsp;ian: 0.027<br>&nbsp;&nbsp; VA: 0.014<br>"
          ],
          [
           "<b>NWD:</b> 0.034<br><b>Pred:</b> 'gc<br><b>Top-5:</b><br>&nbsp;&nbsp;'gc: 0.002<br>&nbsp;&nbsp;﻿using: 0.001<br>&nbsp;&nbsp;actionDate: 0.001<br>&nbsp;&nbsp;GuidId: 0.001<br>&nbsp;&nbsp;��: 0.001<br>",
           "<b>NWD:</b> 0.094<br><b>Pred:</b> 字幕<br><b>Top-5:</b><br>&nbsp;&nbsp;字幕: 0.001<br>&nbsp;&nbsp;페이지: 0.001<br>&nbsp;&nbsp;﻿using: 0.001<br>&nbsp;&nbsp;'gc: 0.001<br>&nbsp;&nbsp;actionDate: 0.000<br>",
           "<b>NWD:</b> 0.103<br><b>Pred:</b> 페이지<br><b>Top-5:</b><br>&nbsp;&nbsp;페이지: 0.002<br>&nbsp;&nbsp;字幕: 0.001<br>&nbsp;&nbsp;aeda: 0.001<br>&nbsp;&nbsp;نوع: 0.001<br>&nbsp;&nbsp;سال: 0.001<br>",
           "<b>NWD:</b> 0.030<br><b>Pred:</b> ards<br><b>Top-5:</b><br>&nbsp;&nbsp;ards: 0.235<br>&nbsp;&nbsp;mond: 0.153<br>&nbsp;&nbsp;rich: 0.056<br>&nbsp;&nbsp;ard: 0.040<br>&nbsp;&nbsp; richer: 0.037<br>",
           "<b>NWD:</b> 0.140<br><b>Pred:</b> mond<br><b>Top-5:</b><br>&nbsp;&nbsp;mond: 0.084<br>&nbsp;&nbsp;ians: 0.026<br>&nbsp;&nbsp;ensis: 0.019<br>&nbsp;&nbsp;iana: 0.015<br>&nbsp;&nbsp;ian: 0.009<br>"
          ],
          [
           "<b>NWD:</b> 0.037<br><b>Pred:</b> 'gc<br><b>Top-5:</b><br>&nbsp;&nbsp;'gc: 0.002<br>&nbsp;&nbsp;﻿using: 0.001<br>&nbsp;&nbsp;actionDate: 0.001<br>&nbsp;&nbsp;��: 0.001<br>&nbsp;&nbsp;GuidId: 0.001<br>",
           "<b>NWD:</b> 0.152<br><b>Pred:</b> 字幕<br><b>Top-5:</b><br>&nbsp;&nbsp;字幕: 0.001<br>&nbsp;&nbsp;페이지: 0.001<br>&nbsp;&nbsp;نوع: 0.001<br>&nbsp;&nbsp;﻿using: 0.001<br>&nbsp;&nbsp;aeda: 0.000<br>",
           "<b>NWD:</b> 0.057<br><b>Pred:</b> 페이지<br><b>Top-5:</b><br>&nbsp;&nbsp;페이지: 0.002<br>&nbsp;&nbsp;字幕: 0.002<br>&nbsp;&nbsp;نوع: 0.001<br>&nbsp;&nbsp;aeda: 0.001<br>&nbsp;&nbsp;سال: 0.001<br>",
           "<b>NWD:</b> 0.020<br><b>Pred:</b> mond<br><b>Top-5:</b><br>&nbsp;&nbsp;mond: 0.032<br>&nbsp;&nbsp;ness: 0.011<br>&nbsp;&nbsp;ens: 0.010<br>&nbsp;&nbsp;ards: 0.007<br>&nbsp;&nbsp;ly: 0.006<br>",
           "<b>NWD:</b> 0.012<br><b>Pred:</b> IPHER<br><b>Top-5:</b><br>&nbsp;&nbsp;IPHER: 0.006<br>&nbsp;&nbsp;raz: 0.005<br>&nbsp;&nbsp;CAPE: 0.004<br>&nbsp;&nbsp;.opend: 0.004<br>&nbsp;&nbsp;令: 0.003<br>"
          ],
          [
           "<b>NWD:</b> 0.037<br><b>Pred:</b> 'gc<br><b>Top-5:</b><br>&nbsp;&nbsp;'gc: 0.002<br>&nbsp;&nbsp;字幕: 0.001<br>&nbsp;&nbsp;��: 0.001<br>&nbsp;&nbsp;﻿using: 0.001<br>&nbsp;&nbsp;페이지: 0.001<br>",
           "<b>NWD:</b> 0.077<br><b>Pred:</b> 페이지<br><b>Top-5:</b><br>&nbsp;&nbsp;페이지: 0.002<br>&nbsp;&nbsp;字幕: 0.002<br>&nbsp;&nbsp;نوع: 0.001<br>&nbsp;&nbsp;Produto: 0.001<br>&nbsp;&nbsp;سال: 0.001<br>",
           "<b>NWD:</b> 0.018<br><b>Pred:</b> 페이지<br><b>Top-5:</b><br>&nbsp;&nbsp;페이지: 0.003<br>&nbsp;&nbsp;字幕: 0.002<br>&nbsp;&nbsp;نوع: 0.001<br>&nbsp;&nbsp;سال: 0.001<br>&nbsp;&nbsp;aeda: 0.001<br>",
           "<b>NWD:</b> 0.070<br><b>Pred:</b>  cop<br><b>Top-5:</b><br>&nbsp;&nbsp; cop: 0.007<br>&nbsp;&nbsp;rob: 0.004<br>&nbsp;&nbsp;clone: 0.004<br>&nbsp;&nbsp; rabbit: 0.003<br>&nbsp;&nbsp;ervo: 0.003<br>",
           "<b>NWD:</b> 0.064<br><b>Pred:</b> aje<br><b>Top-5:</b><br>&nbsp;&nbsp;aje: 0.023<br>&nbsp;&nbsp; quot: 0.013<br>&nbsp;&nbsp;ians: 0.008<br>&nbsp;&nbsp;eted: 0.007<br>&nbsp;&nbsp; dur: 0.005<br>"
          ],
          [
           "<b>NWD:</b> 0.029<br><b>Pred:</b> 페이지<br><b>Top-5:</b><br>&nbsp;&nbsp;페이지: 0.002<br>&nbsp;&nbsp;字幕: 0.001<br>&nbsp;&nbsp;…\": 0.001<br>&nbsp;&nbsp;'gc: 0.001<br>&nbsp;&nbsp;��: 0.001<br>",
           "<b>NWD:</b> 0.043<br><b>Pred:</b> 페이지<br><b>Top-5:</b><br>&nbsp;&nbsp;페이지: 0.003<br>&nbsp;&nbsp;字幕: 0.002<br>&nbsp;&nbsp;نوع: 0.001<br>&nbsp;&nbsp;…\": 0.001<br>&nbsp;&nbsp;سال: 0.001<br>",
           "<b>NWD:</b> 0.011<br><b>Pred:</b> 페이지<br><b>Top-5:</b><br>&nbsp;&nbsp;페이지: 0.003<br>&nbsp;&nbsp;字幕: 0.002<br>&nbsp;&nbsp;نوع: 0.001<br>&nbsp;&nbsp;سال: 0.001<br>&nbsp;&nbsp;…\": 0.001<br>",
           "<b>NWD:</b> 0.176<br><b>Pred:</b> ness<br><b>Top-5:</b><br>&nbsp;&nbsp;ness: 0.008<br>&nbsp;&nbsp;λό: 0.006<br>&nbsp;&nbsp;olas: 0.005<br>&nbsp;&nbsp;ief: 0.004<br>&nbsp;&nbsp;jas: 0.004<br>",
           "<b>NWD:</b> 0.067<br><b>Pred:</b> 員<br><b>Top-5:</b><br>&nbsp;&nbsp;員: 0.020<br>&nbsp;&nbsp;zhou: 0.010<br>&nbsp;&nbsp; dub: 0.007<br>&nbsp;&nbsp;olls: 0.004<br>&nbsp;&nbsp;clc: 0.004<br>"
          ],
          [
           "<b>NWD:</b> 0.027<br><b>Pred:</b> 페이지<br><b>Top-5:</b><br>&nbsp;&nbsp;페이지: 0.004<br>&nbsp;&nbsp;…\": 0.003<br>&nbsp;&nbsp;字幕: 0.002<br>&nbsp;&nbsp;,…: 0.001<br>&nbsp;&nbsp;…and: 0.001<br>",
           "<b>NWD:</b> 0.009<br><b>Pred:</b> 페이지<br><b>Top-5:</b><br>&nbsp;&nbsp;페이지: 0.004<br>&nbsp;&nbsp;字幕: 0.002<br>&nbsp;&nbsp;…\": 0.002<br>&nbsp;&nbsp;نوع: 0.002<br>&nbsp;&nbsp;,…: 0.001<br>",
           "<b>NWD:</b> 0.051<br><b>Pred:</b> 페이지<br><b>Top-5:</b><br>&nbsp;&nbsp;페이지: 0.003<br>&nbsp;&nbsp;字幕: 0.002<br>&nbsp;&nbsp;نوع: 0.002<br>&nbsp;&nbsp;…\": 0.001<br>&nbsp;&nbsp;,…: 0.001<br>",
           "<b>NWD:</b> 0.160<br><b>Pred:</b> ness<br><b>Top-5:</b><br>&nbsp;&nbsp;ness: 0.086<br>&nbsp;&nbsp;nes: 0.016<br>&nbsp;&nbsp;olas: 0.009<br>&nbsp;&nbsp;745: 0.008<br>&nbsp;&nbsp;iddy: 0.006<br>",
           "<b>NWD:</b> 0.010<br><b>Pred:</b> idis<br><b>Top-5:</b><br>&nbsp;&nbsp;idis: 0.005<br>&nbsp;&nbsp;RITE: 0.003<br>&nbsp;&nbsp;omik: 0.003<br>&nbsp;&nbsp; vej: 0.003<br>&nbsp;&nbsp;flen: 0.003<br>"
          ],
          [
           "<b>NWD:</b> 0.069<br><b>Pred:</b> rone<br><b>Top-5:</b><br>&nbsp;&nbsp;rone: 0.007<br>&nbsp;&nbsp;utron: 0.007<br>&nbsp;&nbsp;�: 0.007<br>&nbsp;&nbsp;ardown: 0.004<br>&nbsp;&nbsp;hev: 0.003<br>",
           "<b>NWD:</b> 0.127<br><b>Pred:</b> tim<br><b>Top-5:</b><br>&nbsp;&nbsp;tim: 0.012<br>&nbsp;&nbsp; spare: 0.007<br>&nbsp;&nbsp;,: 0.006<br>&nbsp;&nbsp; Hi: 0.005<br>&nbsp;&nbsp; Wolver: 0.005<br>",
           "<b>NWD:</b> 0.369<br><b>Pred:</b>  not<br><b>Top-5:</b><br>&nbsp;&nbsp; not: 0.736<br>&nbsp;&nbsp;rotch: 0.007<br>&nbsp;&nbsp; go: 0.004<br>&nbsp;&nbsp;$MESS: 0.003<br>&nbsp;&nbsp;enschaft: 0.002<br>",
           "<b>NWD:</b> 0.039<br><b>Pred:</b>  parallel<br><b>Top-5:</b><br>&nbsp;&nbsp; parallel: 0.018<br>&nbsp;&nbsp; Gram: 0.013<br>&nbsp;&nbsp;ly: 0.012<br>&nbsp;&nbsp;ness: 0.007<br>&nbsp;&nbsp; string: 0.005<br>",
           "<b>NWD:</b> 0.020<br><b>Pred:</b> chia<br><b>Top-5:</b><br>&nbsp;&nbsp;chia: 0.020<br>&nbsp;&nbsp;serve: 0.006<br>&nbsp;&nbsp;APT: 0.006<br>&nbsp;&nbsp;�: 0.006<br>&nbsp;&nbsp; Rob: 0.003<br>"
          ],
          [
           "<b>NWD:</b> 0.013<br><b>Pred:</b> greg<br><b>Top-5:</b><br>&nbsp;&nbsp;greg: 0.011<br>&nbsp;&nbsp;ister: 0.004<br>&nbsp;&nbsp;utron: 0.004<br>&nbsp;&nbsp;otron: 0.003<br>&nbsp;&nbsp;Forward: 0.003<br>",
           "<b>NWD:</b> 0.014<br><b>Pred:</b>  Meadows<br><b>Top-5:</b><br>&nbsp;&nbsp; Meadows: 0.006<br>&nbsp;&nbsp; False: 0.003<br>&nbsp;&nbsp;δή: 0.003<br>&nbsp;&nbsp;esse: 0.003<br>&nbsp;&nbsp;ena: 0.002<br>",
           "<b>NWD:</b> 0.206<br><b>Pred:</b>  not<br><b>Top-5:</b><br>&nbsp;&nbsp; not: 0.230<br>&nbsp;&nbsp;umin: 0.011<br>&nbsp;&nbsp; bel: 0.007<br>&nbsp;&nbsp; Bel: 0.007<br>&nbsp;&nbsp; Institution: 0.004<br>",
           "<b>NWD:</b> 0.049<br><b>Pred:</b>  in<br><b>Top-5:</b><br>&nbsp;&nbsp; in: 0.022<br>&nbsp;&nbsp;ly: 0.017<br>&nbsp;&nbsp; Umb: 0.011<br>&nbsp;&nbsp; rich: 0.005<br>&nbsp;&nbsp; set: 0.004<br>",
           "<b>NWD:</b> 0.015<br><b>Pred:</b> essa<br><b>Top-5:</b><br>&nbsp;&nbsp;essa: 0.006<br>&nbsp;&nbsp;ocator: 0.006<br>&nbsp;&nbsp;gest: 0.004<br>&nbsp;&nbsp;elier: 0.003<br>&nbsp;&nbsp; Bowling: 0.003<br>"
          ]
         ],
         "text": [
          [
           " ",
           " the",
           " the",
           " people",
           " county"
          ],
          [
           "'gc",
           "'gc",
           "字幕",
           "ard",
           "iana"
          ],
          [
           "'gc",
           "字幕",
           "字幕",
           "mond",
           "ians"
          ],
          [
           "'gc",
           "字幕",
           "페이지",
           "ards",
           "mond"
          ],
          [
           "'gc",
           "字幕",
           "페이지",
           "mond",
           "IPHER"
          ],
          [
           "'gc",
           "페이지",
           "페이지",
           " cop",
           "aje"
          ],
          [
           "페이지",
           "페이지",
           "페이지",
           "ness",
           "員"
          ],
          [
           "페이지",
           "페이지",
           "페이지",
           "ness",
           "idis"
          ],
          [
           "rone",
           "tim",
           " not",
           " parallel",
           "chia"
          ],
          [
           "greg",
           " Meadows",
           " not",
           " in",
           "essa"
          ]
         ],
         "textfont": {
          "size": 30
         },
         "texttemplate": "%{text}",
         "type": "heatmap",
         "x": [
          0,
          1,
          2,
          3,
          4
         ],
         "y": [
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9
         ],
         "z": {
          "bdata": "wBSjYtKA7j8LwwoO35OyP1xxw2Ci7c0/dUN0Wcpkvz99uvyc1QW2Pw72fhZKULU/NPpCi+Do6j8vlytS0ofiP9SgwLbQ+YI/CY2Kk0xIwT/Y1qIx7aCyP50YWgnc3d0/8jVgEAUt3j8Eh1jh/uzBPxWhGM0zdNc/NUzaKbVJsj9LAVIhQzfOPylAoy3b0dA/3YBYjAAtrj+t8TSjc1PXP4vehyIF27M/5RcIAr9q2T8UOr6HvRHBP9biH2qaKp8/kl6w+LI6gD/N4NFc2DK0Px53SJjUNMg/ozb2uFmAmT9GPDXAEMjFPx6LJrWXl8M/NgRs/wxfrT9GyXb84Ia4P4SZOpOG9Xc/sI8fCB7T3T+tpnAtmZrEP8raIc0nu6o/AAAAAAAAAACaOn8jCti9P040h8pG5do/rDmtPDt8aj8r3k2jF0rFP+vuCoEY8tQ/AAAAAAAA8D8xB/fi4+G1PymOr37SAKA/wb7+6OQphj/LDkRec8qLP/JIT5VVh+E/eKdms1BWvD92mvX4GhaSPw==",
          "dtype": "f8",
          "shape": "10, 5"
         },
         "zmax": 1,
         "zmin": 0
        },
        {
         "hoverinfo": "skip",
         "marker": {
          "opacity": 0
         },
         "mode": "markers",
         "showlegend": false,
         "type": "scatter",
         "x": [
          0,
          1,
          2,
          3,
          4
         ],
         "xaxis": "x2",
         "y": [
          null,
          null,
          null,
          null,
          null
         ]
        }
       ],
       "layout": {
        "font": {
         "family": "DejaVu Sans",
         "size": 14
        },
        "height": 600,
        "margin": {
         "b": 10,
         "l": 20,
         "r": 10,
         "t": 40
        },
        "shapes": [
         {
          "layer": "above",
          "line": {
           "color": "black",
           "width": 2
          },
          "type": "rect",
          "x0": 2.5,
          "x1": 3.5,
          "y0": 3.5,
          "y1": 4.5
         },
         {
          "layer": "above",
          "line": {
           "color": "black",
           "width": 2
          },
          "type": "rect",
          "x0": 2.5,
          "x1": 3.5,
          "y0": 1.5,
          "y1": 2.5
         }
        ],
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermap": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermap"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "width": 1200,
        "xaxis": {
         "anchor": "y",
         "domain": [
          0,
          1
         ],
         "side": "bottom",
         "tickmode": "array",
         "ticktext": [
          "<|begin_of_text|>",
          "when",
          " did",
          " rich",
          "mond"
         ],
         "tickvals": [
          0,
          1,
          2,
          3,
          4
         ],
         "title": {
          "text": "Input Token"
         }
        },
        "xaxis2": {
         "anchor": "free",
         "overlaying": "x",
         "position": 1,
         "showline": true,
         "side": "top",
         "tickmode": "array",
         "ticks": "outside",
         "ticktext": [
          "when",
          " did",
          " rich",
          "mond",
          " last"
         ],
         "tickvals": [
          0,
          1,
          2,
          3,
          4
         ]
        },
        "yaxis": {
         "autorange": "reversed",
         "tickmode": "array",
         "ticktext": [
          "layers.31",
          "layers.28",
          "layers.24",
          "layers.20",
          "layers.16",
          "layers.12",
          "layers.8",
          "layers.4",
          "layers.0",
          "embed_tokens"
         ],
         "tickvals": [
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9
         ],
         "title": {
          "text": "Layer"
         }
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "logit_lens.plot_topk_comparing_lens(\n",
    "    models=(llama8b_fp32, hfbit1_fp32),\n",
    "    tokenizers=(llama8b_tokenizer, hfbit1_tokenizer),\n",
    "    inputs=nq_queries[0],\n",
    "    start_ix=0, end_ix=5,\n",
    "    topk=5,\n",
    "    topk_mean=False,\n",
    "    #js=True,\n",
    "    #js=True,\n",
    "    block_step=2,\n",
    "    token_font_size=30,\n",
    "    #json_log_path=None,\n",
    "    #json_log_path='logs/nq_answers/dh.3b-bnb4bit.fp32', # 20 samples\n",
    "    #save_fig_path=None,\n",
    "    #save_fig_path='Outputs/LogitLens/DH3B/logits_3b_fp32_math.jpg',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logit_lens.plot_topk_comparing_lens(\n",
    "    models=(hfbit1_fp32, llama8b_fp32),\n",
    "    tokenizers=(hfbit1_tokenizer, llama8b_tokenizer),\n",
    "    inputs=MiscPrompts.Q11.value,\n",
    "    start_ix=0, end_ix=5,\n",
    "    topk=5,\n",
    "    topk_mean=True,\n",
    "    #js=True,\n",
    "    block_step=2,\n",
    "    token_font_size=30,\n",
    "    #json_log_path=None,\n",
    "    #json_log_path='logs/nq_answers/dh.3b-bnb4bit.fp32', # 20 samples\n",
    "    #save_fig_path=None,\n",
    "    #save_fig_path='Outputs/LogitLens/DH3B/logits_3b_fp32_math.jpg',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "colorscale": [
          [
           0,
           "rgb(5,48,97)"
          ],
          [
           0.1,
           "rgb(33,102,172)"
          ],
          [
           0.2,
           "rgb(67,147,195)"
          ],
          [
           0.3,
           "rgb(146,197,222)"
          ],
          [
           0.4,
           "rgb(209,229,240)"
          ],
          [
           0.5,
           "rgb(247,247,247)"
          ],
          [
           0.6,
           "rgb(253,219,199)"
          ],
          [
           0.7,
           "rgb(244,165,130)"
          ],
          [
           0.8,
           "rgb(214,96,77)"
          ],
          [
           0.9,
           "rgb(178,24,43)"
          ],
          [
           1,
           "rgb(103,0,31)"
          ]
         ],
         "hoverinfo": "text",
         "hovertext": [
          [
           "<b>Entropy:</b> 0.101<br><b>Pred:</b> umat<br><b>Top-5:</b><br>&nbsp;&nbsp;umat: 0.006<br>&nbsp;&nbsp; slic: 0.003<br>&nbsp;&nbsp;般: 0.003<br>&nbsp;&nbsp;담: 0.003<br>&nbsp;&nbsp;uls: 0.003<br>",
           "<b>Entropy:</b> 0.189<br><b>Pred:</b> osl<br><b>Top-5:</b><br>&nbsp;&nbsp;osl: 0.012<br>&nbsp;&nbsp;insky: 0.012<br>&nbsp;&nbsp; branches: 0.006<br>&nbsp;&nbsp;ụ: 0.006<br>&nbsp;&nbsp; gra: 0.004<br>",
           "<b>Entropy:</b> 0.109<br><b>Pred:</b> afa<br><b>Top-5:</b><br>&nbsp;&nbsp;afa: 0.010<br>&nbsp;&nbsp; Darkness: 0.003<br>&nbsp;&nbsp;erview: 0.003<br>&nbsp;&nbsp; Agency: 0.003<br>&nbsp;&nbsp;acement: 0.002<br>",
           "<b>Entropy:</b> 0.102<br><b>Pred:</b>  Daw<br><b>Top-5:</b><br>&nbsp;&nbsp; Daw: 0.005<br>&nbsp;&nbsp; Placeholder: 0.005<br>&nbsp;&nbsp;awning: 0.003<br>&nbsp;&nbsp; snapshots: 0.003<br>&nbsp;&nbsp; placeholder: 0.002<br>",
           "<b>Entropy:</b> 0.113<br><b>Pred:</b> اید<br><b>Top-5:</b><br>&nbsp;&nbsp;اید: 0.009<br>&nbsp;&nbsp;bee: 0.004<br>&nbsp;&nbsp;gro: 0.003<br>&nbsp;&nbsp; Sachs: 0.003<br>&nbsp;&nbsp;asz: 0.003<br>"
          ],
          [
           "<b>Entropy:</b> 0.160<br><b>Pred:</b> umat<br><b>Top-5:</b><br>&nbsp;&nbsp;umat: 0.012<br>&nbsp;&nbsp; summar: 0.009<br>&nbsp;&nbsp;.pow: 0.005<br>&nbsp;&nbsp;담: 0.003<br>&nbsp;&nbsp;herits: 0.003<br>",
           "<b>Entropy:</b> 0.195<br><b>Pred:</b> ụ<br><b>Top-5:</b><br>&nbsp;&nbsp;ụ: 0.016<br>&nbsp;&nbsp; gra: 0.010<br>&nbsp;&nbsp; branches: 0.006<br>&nbsp;&nbsp;.bank: 0.005<br>&nbsp;&nbsp;osl: 0.005<br>",
           "<b>Entropy:</b> 0.093<br><b>Pred:</b>  Agency<br><b>Top-5:</b><br>&nbsp;&nbsp; Agency: 0.005<br>&nbsp;&nbsp;afa: 0.003<br>&nbsp;&nbsp;gota: 0.003<br>&nbsp;&nbsp; Blend: 0.003<br>&nbsp;&nbsp; reproduce: 0.002<br>",
           "<b>Entropy:</b> 0.131<br><b>Pred:</b>  Daw<br><b>Top-5:</b><br>&nbsp;&nbsp; Daw: 0.013<br>&nbsp;&nbsp;ivas: 0.004<br>&nbsp;&nbsp; Placeholder: 0.003<br>&nbsp;&nbsp; bur: 0.003<br>&nbsp;&nbsp;atham: 0.003<br>",
           "<b>Entropy:</b> 0.115<br><b>Pred:</b> اید<br><b>Top-5:</b><br>&nbsp;&nbsp;اید: 0.008<br>&nbsp;&nbsp; Sachs: 0.007<br>&nbsp;&nbsp;prime: 0.003<br>&nbsp;&nbsp;bee: 0.003<br>&nbsp;&nbsp;asz: 0.002<br>"
          ],
          [
           "<b>Entropy:</b> 0.144<br><b>Pred:</b> umat<br><b>Top-5:</b><br>&nbsp;&nbsp;umat: 0.014<br>&nbsp;&nbsp; summar: 0.005<br>&nbsp;&nbsp;.pow: 0.004<br>&nbsp;&nbsp; multiplier: 0.003<br>&nbsp;&nbsp;iland: 0.003<br>",
           "<b>Entropy:</b> 0.183<br><b>Pred:</b> ụ<br><b>Top-5:</b><br>&nbsp;&nbsp;ụ: 0.018<br>&nbsp;&nbsp; gra: 0.006<br>&nbsp;&nbsp;osl: 0.005<br>&nbsp;&nbsp; branches: 0.005<br>&nbsp;&nbsp;dp: 0.004<br>",
           "<b>Entropy:</b> 0.118<br><b>Pred:</b>  Agency<br><b>Top-5:</b><br>&nbsp;&nbsp; Agency: 0.007<br>&nbsp;&nbsp;afa: 0.005<br>&nbsp;&nbsp;essian: 0.004<br>&nbsp;&nbsp; Blend: 0.003<br>&nbsp;&nbsp;Agency: 0.003<br>",
           "<b>Entropy:</b> 0.125<br><b>Pred:</b>  Placeholder<br><b>Top-5:</b><br>&nbsp;&nbsp; Placeholder: 0.006<br>&nbsp;&nbsp;�: 0.005<br>&nbsp;&nbsp; bur: 0.004<br>&nbsp;&nbsp; polled: 0.004<br>&nbsp;&nbsp; Daw: 0.004<br>",
           "<b>Entropy:</b> 0.128<br><b>Pred:</b> ouch<br><b>Top-5:</b><br>&nbsp;&nbsp;ouch: 0.006<br>&nbsp;&nbsp;asz: 0.006<br>&nbsp;&nbsp;inet: 0.005<br>&nbsp;&nbsp; Cru: 0.005<br>&nbsp;&nbsp;.bukkit: 0.003<br>"
          ],
          [
           "<b>Entropy:</b> 0.095<br><b>Pred:</b> umat<br><b>Top-5:</b><br>&nbsp;&nbsp;umat: 0.005<br>&nbsp;&nbsp;iller: 0.003<br>&nbsp;&nbsp;ive: 0.003<br>&nbsp;&nbsp;kat: 0.003<br>&nbsp;&nbsp;cka: 0.003<br>",
           "<b>Entropy:</b> 0.148<br><b>Pred:</b>  branches<br><b>Top-5:</b><br>&nbsp;&nbsp; branches: 0.011<br>&nbsp;&nbsp;ụ: 0.010<br>&nbsp;&nbsp;osl: 0.003<br>&nbsp;&nbsp; brunch: 0.003<br>&nbsp;&nbsp;午: 0.003<br>",
           "<b>Entropy:</b> 0.113<br><b>Pred:</b>  Agency<br><b>Top-5:</b><br>&nbsp;&nbsp; Agency: 0.008<br>&nbsp;&nbsp; Aging: 0.005<br>&nbsp;&nbsp; Agencies: 0.003<br>&nbsp;&nbsp;-dat: 0.003<br>&nbsp;&nbsp; scramble: 0.003<br>",
           "<b>Entropy:</b> 0.091<br><b>Pred:</b>  polled<br><b>Top-5:</b><br>&nbsp;&nbsp; polled: 0.004<br>&nbsp;&nbsp;�: 0.003<br>&nbsp;&nbsp; Placeholder: 0.003<br>&nbsp;&nbsp; awake: 0.003<br>&nbsp;&nbsp; welcoming: 0.003<br>",
           "<b>Entropy:</b> 0.139<br><b>Pred:</b> asz<br><b>Top-5:</b><br>&nbsp;&nbsp;asz: 0.007<br>&nbsp;&nbsp; Orient: 0.006<br>&nbsp;&nbsp; Wein: 0.005<br>&nbsp;&nbsp;.bukkit: 0.005<br>&nbsp;&nbsp;ouch: 0.004<br>"
          ],
          [
           "<b>Entropy:</b> 0.114<br><b>Pred:</b> ker<br><b>Top-5:</b><br>&nbsp;&nbsp;ker: 0.006<br>&nbsp;&nbsp;散: 0.005<br>&nbsp;&nbsp;aler: 0.004<br>&nbsp;&nbsp; expensive: 0.004<br>&nbsp;&nbsp; Nr: 0.003<br>",
           "<b>Entropy:</b> 0.158<br><b>Pred:</b> ụ<br><b>Top-5:</b><br>&nbsp;&nbsp;ụ: 0.022<br>&nbsp;&nbsp;slt: 0.004<br>&nbsp;&nbsp;-peer: 0.004<br>&nbsp;&nbsp; underrated: 0.003<br>&nbsp;&nbsp; handwriting: 0.003<br>",
           "<b>Entropy:</b> 0.082<br><b>Pred:</b>  Agency<br><b>Top-5:</b><br>&nbsp;&nbsp; Agency: 0.005<br>&nbsp;&nbsp; Agencies: 0.002<br>&nbsp;&nbsp;Agency: 0.002<br>&nbsp;&nbsp; Burb: 0.002<br>&nbsp;&nbsp; agency: 0.002<br>",
           "<b>Entropy:</b> 0.099<br><b>Pred:</b>  lou<br><b>Top-5:</b><br>&nbsp;&nbsp; lou: 0.005<br>&nbsp;&nbsp; awake: 0.005<br>&nbsp;&nbsp; male: 0.004<br>&nbsp;&nbsp;-away: 0.003<br>&nbsp;&nbsp; polled: 0.002<br>",
           "<b>Entropy:</b> 0.121<br><b>Pred:</b> ouch<br><b>Top-5:</b><br>&nbsp;&nbsp;ouch: 0.008<br>&nbsp;&nbsp;asz: 0.006<br>&nbsp;&nbsp;.bukkit: 0.004<br>&nbsp;&nbsp; radioactive: 0.003<br>&nbsp;&nbsp;alent: 0.002<br>"
          ],
          [
           "<b>Entropy:</b> 0.110<br><b>Pred:</b> 般<br><b>Top-5:</b><br>&nbsp;&nbsp;般: 0.006<br>&nbsp;&nbsp; impose: 0.004<br>&nbsp;&nbsp; Nit: 0.004<br>&nbsp;&nbsp; imposition: 0.003<br>&nbsp;&nbsp; Nr: 0.003<br>",
           "<b>Entropy:</b> 0.149<br><b>Pred:</b> ụ<br><b>Top-5:</b><br>&nbsp;&nbsp;ụ: 0.022<br>&nbsp;&nbsp;副: 0.003<br>&nbsp;&nbsp; Odd: 0.003<br>&nbsp;&nbsp;lv: 0.003<br>&nbsp;&nbsp;holm: 0.003<br>",
           "<b>Entropy:</b> 0.117<br><b>Pred:</b> Uploaded<br><b>Top-5:</b><br>&nbsp;&nbsp;Uploaded: 0.007<br>&nbsp;&nbsp; STATS: 0.005<br>&nbsp;&nbsp; Oval: 0.004<br>&nbsp;&nbsp;柄: 0.003<br>&nbsp;&nbsp; Burb: 0.003<br>",
           "<b>Entropy:</b> 0.122<br><b>Pred:</b> ティ<br><b>Top-5:</b><br>&nbsp;&nbsp;ティ: 0.006<br>&nbsp;&nbsp;atak: 0.005<br>&nbsp;&nbsp; polled: 0.005<br>&nbsp;&nbsp; pockets: 0.004<br>&nbsp;&nbsp; Pul: 0.003<br>",
           "<b>Entropy:</b> 0.070<br><b>Pred:</b>  consequential<br><b>Top-5:</b><br>&nbsp;&nbsp; consequential: 0.003<br>&nbsp;&nbsp; Incorporated: 0.002<br>&nbsp;&nbsp; Weekly: 0.002<br>&nbsp;&nbsp; salvage: 0.002<br>&nbsp;&nbsp;egen: 0.002<br>"
          ],
          [
           "<b>Entropy:</b> 0.119<br><b>Pred:</b>  Kv<br><b>Top-5:</b><br>&nbsp;&nbsp; Kv: 0.007<br>&nbsp;&nbsp;orth: 0.005<br>&nbsp;&nbsp; Orta: 0.004<br>&nbsp;&nbsp; Threshold: 0.004<br>&nbsp;&nbsp; Quarter: 0.002<br>",
           "<b>Entropy:</b> 0.223<br><b>Pred:</b> ụ<br><b>Top-5:</b><br>&nbsp;&nbsp;ụ: 0.024<br>&nbsp;&nbsp; Clay: 0.012<br>&nbsp;&nbsp;oose: 0.009<br>&nbsp;&nbsp;ηγ: 0.003<br>&nbsp;&nbsp; Deadline: 0.003<br>",
           "<b>Entropy:</b> 0.092<br><b>Pred:</b> 望<br><b>Top-5:</b><br>&nbsp;&nbsp;望: 0.005<br>&nbsp;&nbsp; خام: 0.004<br>&nbsp;&nbsp;柄: 0.004<br>&nbsp;&nbsp; ingest: 0.002<br>&nbsp;&nbsp; uploaded: 0.002<br>",
           "<b>Entropy:</b> 0.169<br><b>Pred:</b> umb<br><b>Top-5:</b><br>&nbsp;&nbsp;umb: 0.011<br>&nbsp;&nbsp; angels: 0.007<br>&nbsp;&nbsp;ή: 0.006<br>&nbsp;&nbsp; Kra: 0.006<br>&nbsp;&nbsp;UMB: 0.004<br>",
           "<b>Entropy:</b> 0.106<br><b>Pred:</b>  bulk<br><b>Top-5:</b><br>&nbsp;&nbsp; bulk: 0.007<br>&nbsp;&nbsp; age: 0.004<br>&nbsp;&nbsp;年代: 0.003<br>&nbsp;&nbsp;ech: 0.003<br>&nbsp;&nbsp;alent: 0.002<br>"
          ],
          [
           "<b>Entropy:</b> 0.124<br><b>Pred:</b> wand<br><b>Top-5:</b><br>&nbsp;&nbsp;wand: 0.009<br>&nbsp;&nbsp; I: 0.005<br>&nbsp;&nbsp;-cert: 0.004<br>&nbsp;&nbsp;elt: 0.003<br>&nbsp;&nbsp;opia: 0.003<br>",
           "<b>Entropy:</b> 0.263<br><b>Pred:</b>  unders<br><b>Top-5:</b><br>&nbsp;&nbsp; unders: 0.023<br>&nbsp;&nbsp; ro: 0.011<br>&nbsp;&nbsp; primary: 0.010<br>&nbsp;&nbsp;z: 0.010<br>&nbsp;&nbsp; peak: 0.007<br>",
           "<b>Entropy:</b> 0.123<br><b>Pred:</b> uard<br><b>Top-5:</b><br>&nbsp;&nbsp;uard: 0.007<br>&nbsp;&nbsp;Neutral: 0.005<br>&nbsp;&nbsp; Neutral: 0.005<br>&nbsp;&nbsp; understandable: 0.004<br>&nbsp;&nbsp;vang: 0.003<br>",
           "<b>Entropy:</b> 0.901<br><b>Pred:</b> -<br><b>Top-5:</b><br>&nbsp;&nbsp;-: 0.106<br>&nbsp;&nbsp;,: 0.083<br>&nbsp;&nbsp; : 0.066<br>&nbsp;&nbsp; C: 0.055<br>&nbsp;&nbsp; or: 0.035<br>",
           "<b>Entropy:</b> 0.154<br><b>Pred:</b> -rich<br><b>Top-5:</b><br>&nbsp;&nbsp;-rich: 0.009<br>&nbsp;&nbsp; : 0.007<br>&nbsp;&nbsp; mark: 0.005<br>&nbsp;&nbsp; influence: 0.005<br>&nbsp;&nbsp; partition: 0.004<br>"
          ],
          [
           "<b>Entropy:</b> 0.000<br><b>Pred:</b> <|begin_of_text|><br><b>Top-5:</b><br>&nbsp;&nbsp;<|begin_of_text|>: 1.000<br>&nbsp;&nbsp; SUBJECT: 0.000<br>&nbsp;&nbsp;linear: 0.000<br>&nbsp;&nbsp; redistributed: 0.000<br>&nbsp;&nbsp; GPLv: 0.000<br>",
           "<b>Entropy:</b> 0.000<br><b>Pred:</b> when<br><b>Top-5:</b><br>&nbsp;&nbsp;when: 1.000<br>&nbsp;&nbsp; when: 0.000<br>&nbsp;&nbsp; WHEN: 0.000<br>&nbsp;&nbsp;\twhen: 0.000<br>&nbsp;&nbsp;_when: 0.000<br>",
           "<b>Entropy:</b> 0.000<br><b>Pred:</b>  did<br><b>Top-5:</b><br>&nbsp;&nbsp; did: 1.000<br>&nbsp;&nbsp;did: 0.000<br>&nbsp;&nbsp; Did: 0.000<br>&nbsp;&nbsp;Did: 0.000<br>&nbsp;&nbsp; DID: 0.000<br>",
           "<b>Entropy:</b> 0.000<br><b>Pred:</b>  rich<br><b>Top-5:</b><br>&nbsp;&nbsp; rich: 1.000<br>&nbsp;&nbsp; Rich: 0.000<br>&nbsp;&nbsp;-rich: 0.000<br>&nbsp;&nbsp;rich: 0.000<br>&nbsp;&nbsp; richer: 0.000<br>",
           "<b>Entropy:</b> 0.000<br><b>Pred:</b> mond<br><b>Top-5:</b><br>&nbsp;&nbsp;mond: 1.000<br>&nbsp;&nbsp;monds: 0.000<br>&nbsp;&nbsp;MON: 0.000<br>&nbsp;&nbsp; mond: 0.000<br>&nbsp;&nbsp; Mond: 0.000<br>"
          ]
         ],
         "text": [
          [
           "umat",
           "osl",
           "afa",
           " Daw",
           "اید"
          ],
          [
           "umat",
           "ụ",
           " Agency",
           " Daw",
           "اید"
          ],
          [
           "umat",
           "ụ",
           " Agency",
           " Placeholder",
           "ouch"
          ],
          [
           "umat",
           " branches",
           " Agency",
           " polled",
           "asz"
          ],
          [
           "ker",
           "ụ",
           " Agency",
           " lou",
           "ouch"
          ],
          [
           "般",
           "ụ",
           "Uploaded",
           "ティ",
           " consequential"
          ],
          [
           " Kv",
           "ụ",
           "望",
           "umb",
           " bulk"
          ],
          [
           "wand",
           " unders",
           "uard",
           "-",
           "-rich"
          ],
          [
           "<|begin_of_text|>",
           "when",
           " did",
           " rich",
           "mond"
          ]
         ],
         "textfont": {
          "size": 30
         },
         "texttemplate": "%{text}",
         "type": "heatmap",
         "x": [
          0,
          1,
          2,
          3,
          4
         ],
         "y": [
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8
         ],
         "z": {
          "bdata": "mJXlPaYWVz4cEfc96NLnPYSjAD4lzzU+hQlePofI0z0D0hQ+Yk4DPlCNIz4Mrk8+1goGPscBDj7r+BA+grzXPfKpKD4mIgA+dOXOPXeQHT6UGAI+pfUzPnuPuz0UceE9t/MJPgOd+j3Xqik+wgAFPiPdCj48Gp89WaQHPjx0fT53T9I9A+I/Pm2e8D17cw0+jqmVPihdCz4AAIA/BngvPjdU/RgAX/4yxt2tNhM4XiwAAAAA",
          "dtype": "f4",
          "shape": "9, 5"
         },
         "zmax": 1,
         "zmin": 0
        },
        {
         "hoverinfo": "skip",
         "marker": {
          "opacity": 0
         },
         "mode": "markers",
         "showlegend": false,
         "type": "scatter",
         "x": [
          0,
          1,
          2,
          3,
          4
         ],
         "xaxis": "x2",
         "y": [
          null,
          null,
          null,
          null,
          null
         ]
        }
       ],
       "layout": {
        "font": {
         "family": "DejaVu Sans",
         "size": 14
        },
        "height": 600,
        "margin": {
         "b": 10,
         "l": 20,
         "r": 10,
         "t": 40
        },
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermap": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermap"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "width": 1200,
        "xaxis": {
         "anchor": "y",
         "domain": [
          0,
          1
         ],
         "side": "bottom",
         "tickmode": "array",
         "ticktext": [
          "<|begin_of_text|>",
          "when",
          " did",
          " rich",
          "mond"
         ],
         "tickvals": [
          0,
          1,
          2,
          3,
          4
         ],
         "title": {
          "text": "Input Token"
         }
        },
        "xaxis2": {
         "anchor": "free",
         "overlaying": "x",
         "position": 1,
         "showline": true,
         "side": "top",
         "tickmode": "array",
         "ticks": "outside",
         "ticktext": [
          "when",
          " did",
          " rich",
          "mond",
          " last"
         ],
         "tickvals": [
          0,
          1,
          2,
          3,
          4
         ]
        },
        "yaxis": {
         "autorange": "reversed",
         "tickmode": "array",
         "ticktext": [
          "layers.27",
          "layers.24",
          "layers.20",
          "layers.16",
          "layers.12",
          "layers.8",
          "layers.4",
          "layers.0",
          "embed_tokens"
         ],
         "tickvals": [
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8
         ],
         "title": {
          "text": "Layer"
         }
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "logit_lens.plot_topk_logit_lens(\n",
    "    model=dh3b_bitnet_fp32_ptsq,\n",
    "    tokenizer=dh3b_tokenizer,\n",
    "    inputs=nq_queries[0],\n",
    "    start_ix=0, end_ix=5,\n",
    "    topk=5,\n",
    "    topk_mean=True,\n",
    "    plot_topk_lens=True,\n",
    "    entropy=True,\n",
    "    block_step=2,\n",
    "    token_font_size=30,\n",
    "    #json_log_path=None,\n",
    "    #json_log_path='logs/nq_answers/dh.3b-bnb4bit.fp32', # 20 samples\n",
    "    #save_fig_path=None,\n",
    "    #save_fig_path='Outputs/LogitLens/DH3B/logits_3b_fp32_math.jpg',\n",
    "    model_precision=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logit_lens.plot_topk_logit_lens(\n",
    "    model=hfbit1_fp32,\n",
    "    tokenizer=hfbit1_tokenizer,\n",
    "    inputs=MiscPrompts.Q11.value,\n",
    "    start_ix=0, end_ix=15,\n",
    "    topk=5,\n",
    "    topk_mean=True,\n",
    "    plot_topk_lens=True,\n",
    "    entropy=True,\n",
    "    block_step=1,\n",
    "    token_font_size=18,\n",
    "    #json_log_path=None,\n",
    "    #json_log_path='logs/nq_answers/dh.3b-bnb4bit.fp32', # 20 samples\n",
    "    #save_fig_path=None,\n",
    "    #save_fig_path='Outputs/LogitLens/DH3B/logits_3b_fp32_math.jpg',\n",
    "    model_precision=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Debug] Layer names being passed: ['model.layers.0', 'model.layers.1', 'model.layers.2', 'model.layers.3', 'model.layers.4', 'model.layers.5', 'model.layers.6', 'model.layers.7', 'model.layers.8', 'model.layers.9', 'model.layers.10', 'model.layers.11', 'model.layers.12', 'model.layers.13', 'model.layers.14', 'model.layers.15', 'model.layers.16', 'model.layers.17', 'model.layers.18', 'model.layers.19', 'model.layers.20', 'model.layers.21', 'model.layers.22', 'model.layers.23', 'model.layers.24', 'model.layers.25', 'model.layers.26', 'model.layers.27', 'model.embed_tokens']\n",
      "[Debug] Trying to access layer: model.layers.0\n",
      "[Debug] Successfully found layer: model.layers.0\n",
      "[Debug] Trying to access layer: model.layers.1\n",
      "[Debug] Successfully found layer: model.layers.1\n",
      "[Debug] Trying to access layer: model.layers.2\n",
      "[Debug] Successfully found layer: model.layers.2\n",
      "[Debug] Trying to access layer: model.layers.3\n",
      "[Debug] Successfully found layer: model.layers.3\n",
      "[Debug] Trying to access layer: model.layers.4\n",
      "[Debug] Successfully found layer: model.layers.4\n",
      "[Debug] Trying to access layer: model.layers.5\n",
      "[Debug] Successfully found layer: model.layers.5\n",
      "[Debug] Trying to access layer: model.layers.6\n",
      "[Debug] Successfully found layer: model.layers.6\n",
      "[Debug] Trying to access layer: model.layers.7\n",
      "[Debug] Successfully found layer: model.layers.7\n",
      "[Debug] Trying to access layer: model.layers.8\n",
      "[Debug] Successfully found layer: model.layers.8\n",
      "[Debug] Trying to access layer: model.layers.9\n",
      "[Debug] Successfully found layer: model.layers.9\n",
      "[Debug] Trying to access layer: model.layers.10\n",
      "[Debug] Successfully found layer: model.layers.10\n",
      "[Debug] Trying to access layer: model.layers.11\n",
      "[Debug] Successfully found layer: model.layers.11\n",
      "[Debug] Trying to access layer: model.layers.12\n",
      "[Debug] Successfully found layer: model.layers.12\n",
      "[Debug] Trying to access layer: model.layers.13\n",
      "[Debug] Successfully found layer: model.layers.13\n",
      "[Debug] Trying to access layer: model.layers.14\n",
      "[Debug] Successfully found layer: model.layers.14\n",
      "[Debug] Trying to access layer: model.layers.15\n",
      "[Debug] Successfully found layer: model.layers.15\n",
      "[Debug] Trying to access layer: model.layers.16\n",
      "[Debug] Successfully found layer: model.layers.16\n",
      "[Debug] Trying to access layer: model.layers.17\n",
      "[Debug] Successfully found layer: model.layers.17\n",
      "[Debug] Trying to access layer: model.layers.18\n",
      "[Debug] Successfully found layer: model.layers.18\n",
      "[Debug] Trying to access layer: model.layers.19\n",
      "[Debug] Successfully found layer: model.layers.19\n",
      "[Debug] Trying to access layer: model.layers.20\n",
      "[Debug] Successfully found layer: model.layers.20\n",
      "[Debug] Trying to access layer: model.layers.21\n",
      "[Debug] Successfully found layer: model.layers.21\n",
      "[Debug] Trying to access layer: model.layers.22\n",
      "[Debug] Successfully found layer: model.layers.22\n",
      "[Debug] Trying to access layer: model.layers.23\n",
      "[Debug] Successfully found layer: model.layers.23\n",
      "[Debug] Trying to access layer: model.layers.24\n",
      "[Debug] Successfully found layer: model.layers.24\n",
      "[Debug] Trying to access layer: model.layers.25\n",
      "[Debug] Successfully found layer: model.layers.25\n",
      "[Debug] Trying to access layer: model.layers.26\n",
      "[Debug] Successfully found layer: model.layers.26\n",
      "[Debug] Trying to access layer: model.layers.27\n",
      "[Debug] Successfully found layer: model.layers.27\n",
      "[Debug] Trying to access layer: model.embed_tokens\n",
      "[Debug] Successfully found layer: model.embed_tokens\n",
      "[Hook] Layer Embedding(128256, 3072, padding_idx=128004) received input (tensor([[128000,  28276,  12669,  21424,  10349,  35348,   6137,    220,    679,\n",
      "             22,    449,    220,     20,   7833,  15160,     11,    264,  12627,\n",
      "            433,   1047,    539,  17427,   2533,    220,   2550,     20,     13,\n",
      "            362,   4101,    315,   3345,  18151,  13824,  43868,    279,  40816,\n",
      "           6957,    279,   6278,    315,    279,   3280,     11,   2737,    264,\n",
      "            220,     20,  16983,   4814,    311,    279,  11104,  76564,     11,\n",
      "            220,     17,  16983,   4814,    311,  68310,    519,    273,     11,\n",
      "            323,    264,    220,     18,  16983,   4814,    311,    279,  30835,\n",
      "             13,  35348,   9670,    279,   3280,  16917,    449,  40661,  46146,\n",
      "            927,  68310,    519,    273,    323,    800,    735,  56261,    304,\n",
      "            279,   1620,   1403,  20101,     11,  12231,   1113,    279,   6469,\n",
      "            311,    220,     18,   6634,    389,    279,  36865,     13,  35348,\n",
      "            596,   1176,   1620,    315,    279,   3280,   2403,    279,  51849,\n",
      "            520,    279,    386,   8974,  29123,    264,   3335,  37214,   1620,\n",
      "          13734,    315,    220,   2721,     11,  22000,     26,    279,  40816,\n",
      "           2834,    555,    220,   3971,   3585,     13,  20636,  11084,    311,\n",
      "            279,   1176,  33269,  41402,    369,    279,   1176,    892,   2533,\n",
      "            220,   1049,     16,     11,  35348,  24164,  33381,  11104,  21972,\n",
      "            555,    220,   1927,   3585,    304,   4156,    315,    264,  13734,\n",
      "            315,    220,   6281,     11,  15966,    311,   5208,    311,    279,\n",
      "          10517,  13321,   2403,  50301,     11,    872,   1176,  10517,  13321,\n",
      "          11341,   2533,    220,   3753,     17,     13,    578,  28116,    574,\n",
      "            220,   1041,     11,  11592,     11,    279,   7928,  13734,    311,\n",
      "            264,   6800,   1620,   2533,    220,   3753,     21,     13,    578,\n",
      "            356,   1849,   6197,    520,   8502,    892,    323,   6197,    555,\n",
      "            439,   1690,    439,    220,   1032,     11,    719,    279,  40816,\n",
      "           3952,    927,    279,   1847,    439,    433,  62916,    323,  16957,\n",
      "           8254,   7833,   9021,    520,    832,   1486,     13,   2435,   9778,\n",
      "           1053,   3243,    555,    220,   2166,   3585,   1389,    220,    845,\n",
      "             13,    717,    320,   6640,      8,    311,  50301,    596,    220,\n",
      "             23,     13,    717,    320,   1399,      8,   1389,    311,    842,\n",
      "            872,    220,   1806,   4771,   5292,  37846,   8032,   1313,     60,\n",
      "          79418,  11826,   1101,   6244,    279,   1176,   2851,    311,   3243,\n",
      "            264,  97988,  37712,     11,    279,  10690,  10516,  17867,    323,\n",
      "            279,  20935,   9259,  17867,    304,    279,   1890,   3280,     11,\n",
      "           1418,  90012,  11481,  21878,    574,   7086,  58018,   3623,  14576,\n",
      "          10229,  28275,    315,    279,   9941,     13,  35348,    596,   7940,\n",
      "            505,    220,   1032,    339,    311,   6954,   4918,   1101,  13160,\n",
      "            279,   8706,   7940,    505,    832,  58018,   3280,    311,    279,\n",
      "           1828,     13]]),) and output tensor([[[-0.0010, -0.0007, -0.0046,  ..., -0.0014, -0.0020,  0.0020],\n",
      "         [ 0.0027,  0.0219,  0.0253,  ..., -0.0189,  0.0037,  0.0266],\n",
      "         [ 0.0045, -0.0031,  0.0315,  ..., -0.0073, -0.0049, -0.0361],\n",
      "         ...,\n",
      "         [-0.0120,  0.0072, -0.0232,  ...,  0.0187,  0.0081, -0.0134],\n",
      "         [-0.0315, -0.0087,  0.0075,  ...,  0.0245,  0.0060, -0.0228],\n",
      "         [ 0.0093, -0.0031,  0.0278,  ...,  0.0117, -0.0084,  0.0096]]])\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[-0.0010, -0.0007, -0.0046,  ..., -0.0014, -0.0020,  0.0020],\n",
      "         [ 0.0027,  0.0219,  0.0253,  ..., -0.0189,  0.0037,  0.0266],\n",
      "         [ 0.0045, -0.0031,  0.0315,  ..., -0.0073, -0.0049, -0.0361],\n",
      "         ...,\n",
      "         [-0.0120,  0.0072, -0.0232,  ...,  0.0187,  0.0081, -0.0134],\n",
      "         [-0.0315, -0.0087,  0.0075,  ...,  0.0245,  0.0060, -0.0228],\n",
      "         [ 0.0093, -0.0031,  0.0278,  ...,  0.0117, -0.0084,  0.0096]]]),) and output (tensor([[[ 442.9990,  239.9993,  302.9954,  ..., -154.0014,  -15.0020,\n",
      "           185.0020],\n",
      "         [   2.0027,  -58.9781,   99.0253,  ..., -119.0189,  -70.9963,\n",
      "           113.0266],\n",
      "         [  -6.9955,  -41.0031,  108.0315,  ...,  -28.0073,  -27.0049,\n",
      "           -43.0361],\n",
      "         ...,\n",
      "         [   4.9880, -222.9928,  -61.0232,  ..., -421.9813,   34.0081,\n",
      "           -81.0134],\n",
      "         [ -23.0315, -166.0087,  226.0075,  ...,  238.0245, -363.9940,\n",
      "           198.9772],\n",
      "         [ 180.0093,  498.9969,  398.0278,  ..., -331.9883, -207.0084,\n",
      "          -404.9904]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[ 442.9990,  239.9993,  302.9954,  ..., -154.0014,  -15.0020,\n",
      "           185.0020],\n",
      "         [   2.0027,  -58.9781,   99.0253,  ..., -119.0189,  -70.9963,\n",
      "           113.0266],\n",
      "         [  -6.9955,  -41.0031,  108.0315,  ...,  -28.0073,  -27.0049,\n",
      "           -43.0361],\n",
      "         ...,\n",
      "         [   4.9880, -222.9928,  -61.0232,  ..., -421.9813,   34.0081,\n",
      "           -81.0134],\n",
      "         [ -23.0315, -166.0087,  226.0075,  ...,  238.0245, -363.9940,\n",
      "           198.9772],\n",
      "         [ 180.0093,  498.9969,  398.0278,  ..., -331.9883, -207.0084,\n",
      "          -404.9904]]]),) and output (tensor([[[  439.9990,  -144.0007,  -410.0046,  ...,  -765.0015,\n",
      "           1193.9979, -1027.9980],\n",
      "         [ 1170.0027,   989.0219,    63.0253,  ...,  -890.0189,\n",
      "            199.0037,   778.0266],\n",
      "         [ -624.9955, -1806.0032,   229.0315,  ..., -1674.0073,\n",
      "           -685.0049,   398.9639],\n",
      "         ...,\n",
      "         [ -442.0120, -1455.9928, -1011.0232,  ...,    95.0187,\n",
      "           -597.9919,   317.9866],\n",
      "         [ -366.0315,   144.9913,  -341.9925,  ...,   134.0245,\n",
      "           -996.9940,  1359.9772],\n",
      "         [  997.0093,    23.9969,   337.0278,  ...,  -246.9883,\n",
      "           -831.0084,  -710.9904]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[  439.9990,  -144.0007,  -410.0046,  ...,  -765.0015,\n",
      "           1193.9979, -1027.9980],\n",
      "         [ 1170.0027,   989.0219,    63.0253,  ...,  -890.0189,\n",
      "            199.0037,   778.0266],\n",
      "         [ -624.9955, -1806.0032,   229.0315,  ..., -1674.0073,\n",
      "           -685.0049,   398.9639],\n",
      "         ...,\n",
      "         [ -442.0120, -1455.9928, -1011.0232,  ...,    95.0187,\n",
      "           -597.9919,   317.9866],\n",
      "         [ -366.0315,   144.9913,  -341.9925,  ...,   134.0245,\n",
      "           -996.9940,  1359.9772],\n",
      "         [  997.0093,    23.9969,   337.0278,  ...,  -246.9883,\n",
      "           -831.0084,  -710.9904]]]),) and output (tensor([[[ 5.5200e+02,  9.9400e+02, -1.7900e+03,  ...,  4.4600e+02,\n",
      "          -1.3860e+03, -1.2460e+03],\n",
      "         [ 8.9300e+02,  2.2080e+03,  1.9590e+03,  ...,  6.9598e+02,\n",
      "           2.0700e+02,  1.7850e+03],\n",
      "         [-5.2730e+03, -1.9130e+03, -3.9685e+00,  ..., -1.6580e+03,\n",
      "          -1.7330e+03,  1.3490e+03],\n",
      "         ...,\n",
      "         [-2.6501e+02, -2.3990e+03, -3.8460e+03,  ...,  9.8202e+02,\n",
      "          -5.2060e+03, -2.6470e+03],\n",
      "         [-3.6790e+03,  2.0900e+03,  1.3670e+03,  ...,  2.1000e+03,\n",
      "           3.7101e+02,  2.1580e+03],\n",
      "         [ 1.4520e+03, -1.7710e+03, -3.0597e+02,  ..., -4.7340e+03,\n",
      "          -1.6520e+03, -1.7470e+03]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[ 5.5200e+02,  9.9400e+02, -1.7900e+03,  ...,  4.4600e+02,\n",
      "          -1.3860e+03, -1.2460e+03],\n",
      "         [ 8.9300e+02,  2.2080e+03,  1.9590e+03,  ...,  6.9598e+02,\n",
      "           2.0700e+02,  1.7850e+03],\n",
      "         [-5.2730e+03, -1.9130e+03, -3.9685e+00,  ..., -1.6580e+03,\n",
      "          -1.7330e+03,  1.3490e+03],\n",
      "         ...,\n",
      "         [-2.6501e+02, -2.3990e+03, -3.8460e+03,  ...,  9.8202e+02,\n",
      "          -5.2060e+03, -2.6470e+03],\n",
      "         [-3.6790e+03,  2.0900e+03,  1.3670e+03,  ...,  2.1000e+03,\n",
      "           3.7101e+02,  2.1580e+03],\n",
      "         [ 1.4520e+03, -1.7710e+03, -3.0597e+02,  ..., -4.7340e+03,\n",
      "          -1.6520e+03, -1.7470e+03]]]),) and output (tensor([[[ 2131.9990,  -737.0007, -1071.0046,  ..., -2781.0015,\n",
      "           -885.0021,    86.0020],\n",
      "         [-4278.9971,  5410.0220,  1085.0254,  ..., -1698.0189,\n",
      "            770.0037,  1578.0266],\n",
      "         [-9464.9961, -2454.0032, -2288.9685,  ...,  1340.9927,\n",
      "          -4303.0049, -1201.0361],\n",
      "         ...,\n",
      "         [-3046.0120,   -96.9927,  1305.9768,  ...,  -863.9813,\n",
      "          -5319.9922, -4086.0134],\n",
      "         [-4395.0312,  4856.9912,   552.0074,  ...,  3446.0244,\n",
      "          -1158.9940, -1486.0229],\n",
      "         [ 2062.0093, -5646.0029,  1920.0278,  ..., -1494.9883,\n",
      "           3283.9917, -9874.9902]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[ 2131.9990,  -737.0007, -1071.0046,  ..., -2781.0015,\n",
      "           -885.0021,    86.0020],\n",
      "         [-4278.9971,  5410.0220,  1085.0254,  ..., -1698.0189,\n",
      "            770.0037,  1578.0266],\n",
      "         [-9464.9961, -2454.0032, -2288.9685,  ...,  1340.9927,\n",
      "          -4303.0049, -1201.0361],\n",
      "         ...,\n",
      "         [-3046.0120,   -96.9927,  1305.9768,  ...,  -863.9813,\n",
      "          -5319.9922, -4086.0134],\n",
      "         [-4395.0312,  4856.9912,   552.0074,  ...,  3446.0244,\n",
      "          -1158.9940, -1486.0229],\n",
      "         [ 2062.0093, -5646.0029,  1920.0278,  ..., -1494.9883,\n",
      "           3283.9917, -9874.9902]]]),) and output (tensor([[[  3420.9990,    760.9993,  -5376.0049,  ...,  -4193.0015,\n",
      "            2860.9980,  -4080.9980],\n",
      "         [ -8336.9971,   7126.0220,  -2426.9746,  ...,  -2873.0190,\n",
      "            3355.0037,   4776.0264],\n",
      "         [-15277.9961,  -2892.0032,    795.0315,  ...,   7925.9927,\n",
      "           -2875.0049,  -2282.0361],\n",
      "         ...,\n",
      "         [ -5630.0117,  -2386.9927,   2387.9768,  ...,   -215.9814,\n",
      "           -5517.9922,  -2931.0137],\n",
      "         [  4912.9688,   4120.9912,    280.0074,  ...,   6413.0244,\n",
      "           -6087.9941,  -5129.0229],\n",
      "         [ -1077.9907,  -7572.0029,    897.0278,  ...,    224.0117,\n",
      "            4684.9917, -19176.9902]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[  3420.9990,    760.9993,  -5376.0049,  ...,  -4193.0015,\n",
      "            2860.9980,  -4080.9980],\n",
      "         [ -8336.9971,   7126.0220,  -2426.9746,  ...,  -2873.0190,\n",
      "            3355.0037,   4776.0264],\n",
      "         [-15277.9961,  -2892.0032,    795.0315,  ...,   7925.9927,\n",
      "           -2875.0049,  -2282.0361],\n",
      "         ...,\n",
      "         [ -5630.0117,  -2386.9927,   2387.9768,  ...,   -215.9814,\n",
      "           -5517.9922,  -2931.0137],\n",
      "         [  4912.9688,   4120.9912,    280.0074,  ...,   6413.0244,\n",
      "           -6087.9941,  -5129.0229],\n",
      "         [ -1077.9907,  -7572.0029,    897.0278,  ...,    224.0117,\n",
      "            4684.9917, -19176.9902]]]),) and output (tensor([[[  2742.9990,  -1512.0007, -10493.0049,  ...,  -9945.0020,\n",
      "           -2017.0020,  -1802.9980],\n",
      "         [ -3377.9971,   5812.0220,  -2703.9746,  ...,  -5016.0190,\n",
      "            7394.0039,  10950.0264],\n",
      "         [-22069.9961, -10567.0029,    662.0315,  ...,   4578.9927,\n",
      "           -2842.0049,  -2061.0361],\n",
      "         ...,\n",
      "         [-10193.0117,  -4004.9927,   4912.9766,  ...,   -498.9814,\n",
      "           -6986.9922,   -568.0137],\n",
      "         [ 15009.9688,    313.9912,  -2974.9927,  ...,  10922.0244,\n",
      "           -3109.9941,  -4171.0229],\n",
      "         [ -1673.9907, -13026.0029,   1394.0278,  ...,  -1415.9883,\n",
      "            1021.9917, -20259.9902]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[  2742.9990,  -1512.0007, -10493.0049,  ...,  -9945.0020,\n",
      "           -2017.0020,  -1802.9980],\n",
      "         [ -3377.9971,   5812.0220,  -2703.9746,  ...,  -5016.0190,\n",
      "            7394.0039,  10950.0264],\n",
      "         [-22069.9961, -10567.0029,    662.0315,  ...,   4578.9927,\n",
      "           -2842.0049,  -2061.0361],\n",
      "         ...,\n",
      "         [-10193.0117,  -4004.9927,   4912.9766,  ...,   -498.9814,\n",
      "           -6986.9922,   -568.0137],\n",
      "         [ 15009.9688,    313.9912,  -2974.9927,  ...,  10922.0244,\n",
      "           -3109.9941,  -4171.0229],\n",
      "         [ -1673.9907, -13026.0029,   1394.0278,  ...,  -1415.9883,\n",
      "            1021.9917, -20259.9902]]]),) and output (tensor([[[ 3.0330e+03, -3.4200e+03, -1.0291e+04,  ..., -6.5110e+03,\n",
      "          -1.9530e+03,  2.0010e+03],\n",
      "         [-5.5710e+03, -1.0650e+03, -5.5897e+02,  ..., -7.1430e+03,\n",
      "           1.0586e+04,  4.4450e+03],\n",
      "         [-2.0419e+04, -8.9120e+03, -4.5000e+03,  ...,  4.2700e+03,\n",
      "          -3.0049e+00, -4.0410e+03],\n",
      "         ...,\n",
      "         [-8.2650e+03, -4.6670e+03,  2.8360e+03,  ..., -1.5360e+03,\n",
      "           2.3220e+03, -5.2980e+03],\n",
      "         [ 1.3516e+04, -4.6860e+03, -7.6720e+03,  ...,  8.5970e+03,\n",
      "          -5.0960e+03, -1.0367e+04],\n",
      "         [-3.2150e+03, -8.0290e+03, -2.5600e+03,  ...,  3.8110e+03,\n",
      "          -5.7010e+03, -3.0194e+04]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[ 3.0330e+03, -3.4200e+03, -1.0291e+04,  ..., -6.5110e+03,\n",
      "          -1.9530e+03,  2.0010e+03],\n",
      "         [-5.5710e+03, -1.0650e+03, -5.5897e+02,  ..., -7.1430e+03,\n",
      "           1.0586e+04,  4.4450e+03],\n",
      "         [-2.0419e+04, -8.9120e+03, -4.5000e+03,  ...,  4.2700e+03,\n",
      "          -3.0049e+00, -4.0410e+03],\n",
      "         ...,\n",
      "         [-8.2650e+03, -4.6670e+03,  2.8360e+03,  ..., -1.5360e+03,\n",
      "           2.3220e+03, -5.2980e+03],\n",
      "         [ 1.3516e+04, -4.6860e+03, -7.6720e+03,  ...,  8.5970e+03,\n",
      "          -5.0960e+03, -1.0367e+04],\n",
      "         [-3.2150e+03, -8.0290e+03, -2.5600e+03,  ...,  3.8110e+03,\n",
      "          -5.7010e+03, -3.0194e+04]]]),) and output (tensor([[[  3132.9990,  -1970.0007, -11544.0049,  ...,  -5780.0020,\n",
      "           -2486.0020,  -1921.9980],\n",
      "         [ -6217.9971,    420.0220,  -5974.9746,  ..., -13089.0195,\n",
      "           13024.0039,   5166.0264],\n",
      "         [-23289.9961,  -8768.0029,  -6199.9688,  ...,  -2600.0073,\n",
      "             521.9951,  -3463.0361],\n",
      "         ...,\n",
      "         [ -2540.0117,  -8482.9922,   5846.9766,  ...,  -4767.9814,\n",
      "            6979.0078,  -7064.0137],\n",
      "         [ 17860.9688,  -7942.0088, -11502.9922,  ...,   1376.0244,\n",
      "           -7416.9941,  -7570.0234],\n",
      "         [ -6885.9907,  -9613.0029,  -3061.9722,  ...,   7337.0117,\n",
      "            2629.9917, -38352.9922]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[  3132.9990,  -1970.0007, -11544.0049,  ...,  -5780.0020,\n",
      "           -2486.0020,  -1921.9980],\n",
      "         [ -6217.9971,    420.0220,  -5974.9746,  ..., -13089.0195,\n",
      "           13024.0039,   5166.0264],\n",
      "         [-23289.9961,  -8768.0029,  -6199.9688,  ...,  -2600.0073,\n",
      "             521.9951,  -3463.0361],\n",
      "         ...,\n",
      "         [ -2540.0117,  -8482.9922,   5846.9766,  ...,  -4767.9814,\n",
      "            6979.0078,  -7064.0137],\n",
      "         [ 17860.9688,  -7942.0088, -11502.9922,  ...,   1376.0244,\n",
      "           -7416.9941,  -7570.0234],\n",
      "         [ -6885.9907,  -9613.0029,  -3061.9722,  ...,   7337.0117,\n",
      "            2629.9917, -38352.9922]]]),) and output (tensor([[[  1046.9990,   2403.9993, -14486.0049,  ..., -10922.0020,\n",
      "           -1419.0020,  -2983.9980],\n",
      "         [ -8089.9971,    480.0220,  -5452.9746,  ..., -15035.0195,\n",
      "           13110.0039,    669.0264],\n",
      "         [-25218.9961,  -3741.0029, -11455.9688,  ...,  -1186.0073,\n",
      "            5376.9951,    216.9639],\n",
      "         ...,\n",
      "         [ -3440.0117, -16022.9922,  10303.9766,  ...,  -2202.9814,\n",
      "           14064.0078, -13262.0137],\n",
      "         [ 20739.9688,  -2824.0088, -11735.9922,  ...,   3273.0244,\n",
      "           -7180.9941, -12287.0234],\n",
      "         [-11168.9902, -19700.0039,  -4409.9722,  ...,  11772.0117,\n",
      "            6333.9917, -44533.9922]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[  1046.9990,   2403.9993, -14486.0049,  ..., -10922.0020,\n",
      "           -1419.0020,  -2983.9980],\n",
      "         [ -8089.9971,    480.0220,  -5452.9746,  ..., -15035.0195,\n",
      "           13110.0039,    669.0264],\n",
      "         [-25218.9961,  -3741.0029, -11455.9688,  ...,  -1186.0073,\n",
      "            5376.9951,    216.9639],\n",
      "         ...,\n",
      "         [ -3440.0117, -16022.9922,  10303.9766,  ...,  -2202.9814,\n",
      "           14064.0078, -13262.0137],\n",
      "         [ 20739.9688,  -2824.0088, -11735.9922,  ...,   3273.0244,\n",
      "           -7180.9941, -12287.0234],\n",
      "         [-11168.9902, -19700.0039,  -4409.9722,  ...,  11772.0117,\n",
      "            6333.9917, -44533.9922]]]),) and output (tensor([[[ -3767.0010,   5307.9990, -14221.0049,  ..., -15035.0020,\n",
      "           -3630.0020,   4168.0020],\n",
      "         [ -8208.9971,  -2365.9780,  -8760.9746,  ..., -17383.0195,\n",
      "           13878.0039,  -1305.9736],\n",
      "         [-25066.9961,  -5346.0029, -12443.9688,  ...,  -4649.0073,\n",
      "            6960.9951,   1943.9639],\n",
      "         ...,\n",
      "         [ -1759.0117, -18793.9922,   8845.9766,  ...,  -3697.9814,\n",
      "           13984.0078, -17239.0137],\n",
      "         [ 18607.9688,  -3820.0088, -15306.9922,  ...,   -270.9756,\n",
      "          -10859.9941, -15028.0234],\n",
      "         [-14890.9902, -22141.0039,  -1787.9722,  ...,   6620.0117,\n",
      "           10329.9922, -50344.9922]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[ -3767.0010,   5307.9990, -14221.0049,  ..., -15035.0020,\n",
      "           -3630.0020,   4168.0020],\n",
      "         [ -8208.9971,  -2365.9780,  -8760.9746,  ..., -17383.0195,\n",
      "           13878.0039,  -1305.9736],\n",
      "         [-25066.9961,  -5346.0029, -12443.9688,  ...,  -4649.0073,\n",
      "            6960.9951,   1943.9639],\n",
      "         ...,\n",
      "         [ -1759.0117, -18793.9922,   8845.9766,  ...,  -3697.9814,\n",
      "           13984.0078, -17239.0137],\n",
      "         [ 18607.9688,  -3820.0088, -15306.9922,  ...,   -270.9756,\n",
      "          -10859.9941, -15028.0234],\n",
      "         [-14890.9902, -22141.0039,  -1787.9722,  ...,   6620.0117,\n",
      "           10329.9922, -50344.9922]]]),) and output (tensor([[[ -1326.0010,   1223.9990, -16643.0039,  ..., -13240.0020,\n",
      "           -5677.0020,   9639.0020],\n",
      "         [ -7803.9971,   1216.0220,  -5232.9746,  ..., -14208.0195,\n",
      "           10642.0039,   1395.0264],\n",
      "         [-26894.9961,  -7654.0029, -12029.9688,  ..., -11332.0078,\n",
      "           11017.9951,    532.9639],\n",
      "         ...,\n",
      "         [  -936.0117, -18856.9922,   9142.9766,  ...,   -370.9814,\n",
      "           17190.0078, -15014.0137],\n",
      "         [ 27578.9688,  -1202.0088, -17954.9922,  ...,  -2671.9756,\n",
      "           -8953.9941, -14790.0234],\n",
      "         [ -9803.9902, -29795.0039,  -1030.9722,  ...,   5621.0117,\n",
      "           12229.9922, -50322.9922]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[ -1326.0010,   1223.9990, -16643.0039,  ..., -13240.0020,\n",
      "           -5677.0020,   9639.0020],\n",
      "         [ -7803.9971,   1216.0220,  -5232.9746,  ..., -14208.0195,\n",
      "           10642.0039,   1395.0264],\n",
      "         [-26894.9961,  -7654.0029, -12029.9688,  ..., -11332.0078,\n",
      "           11017.9951,    532.9639],\n",
      "         ...,\n",
      "         [  -936.0117, -18856.9922,   9142.9766,  ...,   -370.9814,\n",
      "           17190.0078, -15014.0137],\n",
      "         [ 27578.9688,  -1202.0088, -17954.9922,  ...,  -2671.9756,\n",
      "           -8953.9941, -14790.0234],\n",
      "         [ -9803.9902, -29795.0039,  -1030.9722,  ...,   5621.0117,\n",
      "           12229.9922, -50322.9922]]]),) and output (tensor([[[ -5120.0010,   2246.9990, -15993.0039,  ..., -19818.0020,\n",
      "           -9385.0020,  11924.0020],\n",
      "         [-10120.9971,   3638.0220,  -5200.9746,  ..., -15966.0195,\n",
      "            8553.0039,    151.0264],\n",
      "         [-33294.9961,  -5255.0029, -11678.9688,  ...,  -8391.0078,\n",
      "            7088.9951,   4422.9639],\n",
      "         ...,\n",
      "         [  4634.9883, -14758.9922,   8721.9766,  ...,   -939.9814,\n",
      "           18625.0078, -16082.0137],\n",
      "         [ 27301.9688,    761.9912, -18307.9922,  ...,  -2600.9756,\n",
      "           -5666.9941, -13675.0234],\n",
      "         [-14555.9902, -36298.0039,  -3453.9722,  ...,  -2434.9883,\n",
      "           15092.9922, -58126.9922]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[ -5120.0010,   2246.9990, -15993.0039,  ..., -19818.0020,\n",
      "           -9385.0020,  11924.0020],\n",
      "         [-10120.9971,   3638.0220,  -5200.9746,  ..., -15966.0195,\n",
      "            8553.0039,    151.0264],\n",
      "         [-33294.9961,  -5255.0029, -11678.9688,  ...,  -8391.0078,\n",
      "            7088.9951,   4422.9639],\n",
      "         ...,\n",
      "         [  4634.9883, -14758.9922,   8721.9766,  ...,   -939.9814,\n",
      "           18625.0078, -16082.0137],\n",
      "         [ 27301.9688,    761.9912, -18307.9922,  ...,  -2600.9756,\n",
      "           -5666.9941, -13675.0234],\n",
      "         [-14555.9902, -36298.0039,  -3453.9722,  ...,  -2434.9883,\n",
      "           15092.9922, -58126.9922]]]),) and output (tensor([[[ -6685.0010,  -1278.0010, -15553.0039,  ..., -19846.0020,\n",
      "          -10418.0020,  13376.0020],\n",
      "         [ -9686.9971,   6741.0220,  -6426.9746,  ..., -13200.0195,\n",
      "            9685.0039,   -528.9736],\n",
      "         [-38856.9961,  -9022.0029,  -8689.9688,  ...,  -6366.0078,\n",
      "            2714.9951,   8106.9639],\n",
      "         ...,\n",
      "         [  5879.9883, -19275.9922,   8920.9766,  ...,   -586.9814,\n",
      "           15659.0078, -13827.0137],\n",
      "         [ 24993.9688,   6723.9912, -20406.9922,  ...,  -3952.9756,\n",
      "          -11764.9941, -13960.0234],\n",
      "         [-23318.9902, -45712.0039,  -3896.9722,  ...,  -1091.9883,\n",
      "           16156.9922, -70076.9922]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[ -6685.0010,  -1278.0010, -15553.0039,  ..., -19846.0020,\n",
      "          -10418.0020,  13376.0020],\n",
      "         [ -9686.9971,   6741.0220,  -6426.9746,  ..., -13200.0195,\n",
      "            9685.0039,   -528.9736],\n",
      "         [-38856.9961,  -9022.0029,  -8689.9688,  ...,  -6366.0078,\n",
      "            2714.9951,   8106.9639],\n",
      "         ...,\n",
      "         [  5879.9883, -19275.9922,   8920.9766,  ...,   -586.9814,\n",
      "           15659.0078, -13827.0137],\n",
      "         [ 24993.9688,   6723.9912, -20406.9922,  ...,  -3952.9756,\n",
      "          -11764.9941, -13960.0234],\n",
      "         [-23318.9902, -45712.0039,  -3896.9722,  ...,  -1091.9883,\n",
      "           16156.9922, -70076.9922]]]),) and output (tensor([[[ -8422.0010,    339.9990, -15289.0039,  ..., -20884.0020,\n",
      "           -6908.0020,  11014.0020],\n",
      "         [-10028.9971,  10058.0215,  -6050.9746,  ..., -16346.0195,\n",
      "            6391.0039,   -750.9736],\n",
      "         [-41565.9961,  -4753.0029,  -7255.9688,  ...,  -4054.0078,\n",
      "            4544.9951,   6986.9639],\n",
      "         ...,\n",
      "         [  5097.9883, -21319.9922,   7154.9766,  ...,   -762.9814,\n",
      "           13189.0078, -11935.0137],\n",
      "         [ 26564.9688,   2338.9912, -20825.9922,  ...,  -8185.9756,\n",
      "          -10781.9941, -12049.0234],\n",
      "         [-29396.9902, -48458.0039,  -3963.9722,  ...,   4790.0117,\n",
      "           15891.9922, -75185.9922]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[ -8422.0010,    339.9990, -15289.0039,  ..., -20884.0020,\n",
      "           -6908.0020,  11014.0020],\n",
      "         [-10028.9971,  10058.0215,  -6050.9746,  ..., -16346.0195,\n",
      "            6391.0039,   -750.9736],\n",
      "         [-41565.9961,  -4753.0029,  -7255.9688,  ...,  -4054.0078,\n",
      "            4544.9951,   6986.9639],\n",
      "         ...,\n",
      "         [  5097.9883, -21319.9922,   7154.9766,  ...,   -762.9814,\n",
      "           13189.0078, -11935.0137],\n",
      "         [ 26564.9688,   2338.9912, -20825.9922,  ...,  -8185.9756,\n",
      "          -10781.9941, -12049.0234],\n",
      "         [-29396.9902, -48458.0039,  -3963.9722,  ...,   4790.0117,\n",
      "           15891.9922, -75185.9922]]]),) and output (tensor([[[-10989.0010,   7034.9990, -13964.0039,  ..., -23968.0020,\n",
      "           -3749.0020,  11730.0020],\n",
      "         [ -8947.9971,  13440.0215,  -7910.9746,  ..., -12319.0195,\n",
      "            -451.9961,   -420.9736],\n",
      "         [-45539.9961, -10989.0029,  -7840.9688,  ...,  -4291.0078,\n",
      "            4663.9951,   3935.9639],\n",
      "         ...,\n",
      "         [  3650.9883, -17062.9922,   5620.9766,  ...,  -2581.9814,\n",
      "           16117.0078, -13813.0137],\n",
      "         [ 27285.9688,  -1970.0088, -22630.9922,  ...,  -4986.9756,\n",
      "           -9211.9941, -15839.0234],\n",
      "         [-31255.9902, -52371.0039,  -1529.9722,  ...,  11302.0117,\n",
      "           13904.9922, -86498.9922]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[-10989.0010,   7034.9990, -13964.0039,  ..., -23968.0020,\n",
      "           -3749.0020,  11730.0020],\n",
      "         [ -8947.9971,  13440.0215,  -7910.9746,  ..., -12319.0195,\n",
      "            -451.9961,   -420.9736],\n",
      "         [-45539.9961, -10989.0029,  -7840.9688,  ...,  -4291.0078,\n",
      "            4663.9951,   3935.9639],\n",
      "         ...,\n",
      "         [  3650.9883, -17062.9922,   5620.9766,  ...,  -2581.9814,\n",
      "           16117.0078, -13813.0137],\n",
      "         [ 27285.9688,  -1970.0088, -22630.9922,  ...,  -4986.9756,\n",
      "           -9211.9941, -15839.0234],\n",
      "         [-31255.9902, -52371.0039,  -1529.9722,  ...,  11302.0117,\n",
      "           13904.9922, -86498.9922]]]),) and output (tensor([[[ -7159.0010,   6621.9990, -10967.0039,  ..., -17477.0020,\n",
      "           -2510.0020,  11888.0020],\n",
      "         [ -9727.9971,   8315.0215,  -5807.9746,  ..., -11570.0195,\n",
      "           -2943.9961,   -612.9736],\n",
      "         [-47592.9961, -13648.0029,  -7354.9688,  ...,  -3422.0078,\n",
      "            4414.9951,   2978.9639],\n",
      "         ...,\n",
      "         [  2290.9883, -15030.9922,   9250.9766,  ...,   -437.9814,\n",
      "           18824.0078, -10513.0137],\n",
      "         [ 19534.9688,     98.9912, -29773.9922,  ...,  -5801.9756,\n",
      "           -8659.9941, -16067.0234],\n",
      "         [-30287.9922, -60620.0039,   2044.0278,  ...,   3687.0117,\n",
      "            4519.9922, -94248.9922]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[ -7159.0010,   6621.9990, -10967.0039,  ..., -17477.0020,\n",
      "           -2510.0020,  11888.0020],\n",
      "         [ -9727.9971,   8315.0215,  -5807.9746,  ..., -11570.0195,\n",
      "           -2943.9961,   -612.9736],\n",
      "         [-47592.9961, -13648.0029,  -7354.9688,  ...,  -3422.0078,\n",
      "            4414.9951,   2978.9639],\n",
      "         ...,\n",
      "         [  2290.9883, -15030.9922,   9250.9766,  ...,   -437.9814,\n",
      "           18824.0078, -10513.0137],\n",
      "         [ 19534.9688,     98.9912, -29773.9922,  ...,  -5801.9756,\n",
      "           -8659.9941, -16067.0234],\n",
      "         [-30287.9922, -60620.0039,   2044.0278,  ...,   3687.0117,\n",
      "            4519.9922, -94248.9922]]]),) and output (tensor([[[  -4822.0010,    3124.9990,   -3885.0039,  ...,  -19007.0020,\n",
      "             -551.0020,   16285.0020],\n",
      "         [  -5865.9971,   12382.0215,   -8430.9746,  ...,   -8043.0195,\n",
      "             1137.0039,    4043.0264],\n",
      "         [ -55048.9961,  -13938.0029,   -4274.9688,  ...,   -2079.0078,\n",
      "            10359.9951,    3981.9639],\n",
      "         ...,\n",
      "         [  -3348.0117,  -17697.9922,    8137.9766,  ...,    -846.9814,\n",
      "            11779.0078,   -8597.0137],\n",
      "         [  15216.9688,   -5344.0088,  -25962.9922,  ...,  -11986.9756,\n",
      "            -8117.9941,  -17546.0234],\n",
      "         [ -29406.9922,  -64577.0039,   -4743.9722,  ...,    5816.0117,\n",
      "            10822.9922, -100887.9922]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[  -4822.0010,    3124.9990,   -3885.0039,  ...,  -19007.0020,\n",
      "             -551.0020,   16285.0020],\n",
      "         [  -5865.9971,   12382.0215,   -8430.9746,  ...,   -8043.0195,\n",
      "             1137.0039,    4043.0264],\n",
      "         [ -55048.9961,  -13938.0029,   -4274.9688,  ...,   -2079.0078,\n",
      "            10359.9951,    3981.9639],\n",
      "         ...,\n",
      "         [  -3348.0117,  -17697.9922,    8137.9766,  ...,    -846.9814,\n",
      "            11779.0078,   -8597.0137],\n",
      "         [  15216.9688,   -5344.0088,  -25962.9922,  ...,  -11986.9756,\n",
      "            -8117.9941,  -17546.0234],\n",
      "         [ -29406.9922,  -64577.0039,   -4743.9722,  ...,    5816.0117,\n",
      "            10822.9922, -100887.9922]]]),) and output (tensor([[[  -7235.0010,    6583.9990,   -4212.0039,  ...,  -19104.0020,\n",
      "             3087.9980,   17084.0020],\n",
      "         [  -7212.9971,   13653.0215,  -11389.9746,  ...,   -7959.0195,\n",
      "            -1123.9961,    6681.0264],\n",
      "         [ -67865.0000,  -19275.0039,   -1357.9688,  ...,     538.9922,\n",
      "            11031.9951,    4727.9639],\n",
      "         ...,\n",
      "         [  -3555.0117,  -19242.9922,   11208.9766,  ...,    1520.0186,\n",
      "            10032.0078,    -628.0137],\n",
      "         [  19652.9688,   -7867.0088,  -28078.9922,  ...,  -10302.9756,\n",
      "           -14603.9941,  -15271.0234],\n",
      "         [ -31281.9922,  -68640.0000,   -4707.9722,  ...,    4520.0117,\n",
      "             4047.9922, -104628.9922]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[  -7235.0010,    6583.9990,   -4212.0039,  ...,  -19104.0020,\n",
      "             3087.9980,   17084.0020],\n",
      "         [  -7212.9971,   13653.0215,  -11389.9746,  ...,   -7959.0195,\n",
      "            -1123.9961,    6681.0264],\n",
      "         [ -67865.0000,  -19275.0039,   -1357.9688,  ...,     538.9922,\n",
      "            11031.9951,    4727.9639],\n",
      "         ...,\n",
      "         [  -3555.0117,  -19242.9922,   11208.9766,  ...,    1520.0186,\n",
      "            10032.0078,    -628.0137],\n",
      "         [  19652.9688,   -7867.0088,  -28078.9922,  ...,  -10302.9756,\n",
      "           -14603.9941,  -15271.0234],\n",
      "         [ -31281.9922,  -68640.0000,   -4707.9722,  ...,    4520.0117,\n",
      "             4047.9922, -104628.9922]]]),) and output (tensor([[[  -5586.0010,    7645.9990,    -505.0039,  ...,  -15346.0020,\n",
      "             -623.0020,   12986.0020],\n",
      "         [  -6417.9971,   11835.0215,  -13570.9746,  ...,  -10717.0195,\n",
      "             -486.9961,    6131.0264],\n",
      "         [ -67851.0000,  -24797.0039,    6736.0312,  ...,    5223.9922,\n",
      "             3461.9951,    6670.9639],\n",
      "         ...,\n",
      "         [   5170.9883,  -21764.9922,   12546.9766,  ...,    2827.0186,\n",
      "             3876.0078,   -3262.0137],\n",
      "         [  13880.9688,   -6315.0088,  -32626.9922,  ...,   -9232.9756,\n",
      "           -14095.9941,   -7348.0234],\n",
      "         [ -30046.9922,  -70566.0000,   -2556.9722,  ...,    8071.0117,\n",
      "             1862.9922, -109150.9922]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[  -5586.0010,    7645.9990,    -505.0039,  ...,  -15346.0020,\n",
      "             -623.0020,   12986.0020],\n",
      "         [  -6417.9971,   11835.0215,  -13570.9746,  ...,  -10717.0195,\n",
      "             -486.9961,    6131.0264],\n",
      "         [ -67851.0000,  -24797.0039,    6736.0312,  ...,    5223.9922,\n",
      "             3461.9951,    6670.9639],\n",
      "         ...,\n",
      "         [   5170.9883,  -21764.9922,   12546.9766,  ...,    2827.0186,\n",
      "             3876.0078,   -3262.0137],\n",
      "         [  13880.9688,   -6315.0088,  -32626.9922,  ...,   -9232.9756,\n",
      "           -14095.9941,   -7348.0234],\n",
      "         [ -30046.9922,  -70566.0000,   -2556.9722,  ...,    8071.0117,\n",
      "             1862.9922, -109150.9922]]]),) and output (tensor([[[  -8916.0010,    2362.9990,   -4833.0039,  ...,  -12075.0020,\n",
      "            -1124.0020,   12438.0020],\n",
      "         [  -5624.9971,    8488.0215,  -10748.9746,  ...,  -11398.0195,\n",
      "            -4566.9961,    5678.0264],\n",
      "         [ -78022.0000,  -28519.0039,    5043.0312,  ...,   12370.9922,\n",
      "            11104.9951,    7692.9639],\n",
      "         ...,\n",
      "         [   6155.9883,  -28175.9922,   18442.9766,  ...,    4536.0186,\n",
      "             6917.0078,   -6290.0137],\n",
      "         [  15290.9688,   -7041.0088,  -38375.9922,  ...,  -10519.9756,\n",
      "           -17399.9941,   -7256.0234],\n",
      "         [ -36689.9922,  -75931.0000,    -772.9722,  ...,    7266.0117,\n",
      "            -1451.0078, -115666.9922]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[  -8916.0010,    2362.9990,   -4833.0039,  ...,  -12075.0020,\n",
      "            -1124.0020,   12438.0020],\n",
      "         [  -5624.9971,    8488.0215,  -10748.9746,  ...,  -11398.0195,\n",
      "            -4566.9961,    5678.0264],\n",
      "         [ -78022.0000,  -28519.0039,    5043.0312,  ...,   12370.9922,\n",
      "            11104.9951,    7692.9639],\n",
      "         ...,\n",
      "         [   6155.9883,  -28175.9922,   18442.9766,  ...,    4536.0186,\n",
      "             6917.0078,   -6290.0137],\n",
      "         [  15290.9688,   -7041.0088,  -38375.9922,  ...,  -10519.9756,\n",
      "           -17399.9941,   -7256.0234],\n",
      "         [ -36689.9922,  -75931.0000,    -772.9722,  ...,    7266.0117,\n",
      "            -1451.0078, -115666.9922]]]),) and output (tensor([[[ -20230.0000,    7877.9990,     161.9961,  ...,  -10250.0020,\n",
      "              676.9980,    7095.0020],\n",
      "         [  -9626.9971,   12674.0215,  -10644.9746,  ...,  -15361.0195,\n",
      "             3657.0039,    8925.0264],\n",
      "         [ -82325.0000,  -31388.0039,    1134.0312,  ...,    9913.9922,\n",
      "             8776.9951,    7091.9639],\n",
      "         ...,\n",
      "         [   8406.9883,  -29303.9922,   17143.9766,  ...,     296.0186,\n",
      "             5534.0078,   -5715.0137],\n",
      "         [  12619.9688,  -12433.0088,  -32827.9922,  ...,   -8428.9756,\n",
      "           -19438.9941,  -11889.0234],\n",
      "         [ -30145.9922,  -72750.0000,   -5750.9722,  ...,    2790.0117,\n",
      "             5425.9922, -122769.9922]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[ -20230.0000,    7877.9990,     161.9961,  ...,  -10250.0020,\n",
      "              676.9980,    7095.0020],\n",
      "         [  -9626.9971,   12674.0215,  -10644.9746,  ...,  -15361.0195,\n",
      "             3657.0039,    8925.0264],\n",
      "         [ -82325.0000,  -31388.0039,    1134.0312,  ...,    9913.9922,\n",
      "             8776.9951,    7091.9639],\n",
      "         ...,\n",
      "         [   8406.9883,  -29303.9922,   17143.9766,  ...,     296.0186,\n",
      "             5534.0078,   -5715.0137],\n",
      "         [  12619.9688,  -12433.0088,  -32827.9922,  ...,   -8428.9756,\n",
      "           -19438.9941,  -11889.0234],\n",
      "         [ -30145.9922,  -72750.0000,   -5750.9722,  ...,    2790.0117,\n",
      "             5425.9922, -122769.9922]]]),) and output (tensor([[[ -25670.0000,   13293.9990,    9099.9961,  ...,   -6547.0020,\n",
      "            -5658.0020,    4192.0020],\n",
      "         [ -14003.9971,    8430.0215,   -6354.9746,  ...,   -9161.0195,\n",
      "             4558.0039,   14055.0264],\n",
      "         [ -80065.0000,  -30210.0039,   -3052.9688,  ...,    5574.9922,\n",
      "             7463.9951,    2487.9639],\n",
      "         ...,\n",
      "         [  14412.9883,  -24848.9922,   20584.9766,  ...,   -8468.9814,\n",
      "             5081.0078,   -9764.0137],\n",
      "         [  18246.9688,  -13049.0088,  -31355.9922,  ...,   -9588.9756,\n",
      "           -23991.9941,  -18310.0234],\n",
      "         [ -31288.9922,  -78355.0000,   -6480.9722,  ...,     149.0117,\n",
      "             5837.9922, -128823.9922]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[ -25670.0000,   13293.9990,    9099.9961,  ...,   -6547.0020,\n",
      "            -5658.0020,    4192.0020],\n",
      "         [ -14003.9971,    8430.0215,   -6354.9746,  ...,   -9161.0195,\n",
      "             4558.0039,   14055.0264],\n",
      "         [ -80065.0000,  -30210.0039,   -3052.9688,  ...,    5574.9922,\n",
      "             7463.9951,    2487.9639],\n",
      "         ...,\n",
      "         [  14412.9883,  -24848.9922,   20584.9766,  ...,   -8468.9814,\n",
      "             5081.0078,   -9764.0137],\n",
      "         [  18246.9688,  -13049.0088,  -31355.9922,  ...,   -9588.9756,\n",
      "           -23991.9941,  -18310.0234],\n",
      "         [ -31288.9922,  -78355.0000,   -6480.9722,  ...,     149.0117,\n",
      "             5837.9922, -128823.9922]]]),) and output (tensor([[[ -25652.0000,   13180.9990,   15728.9961,  ...,   -2831.0020,\n",
      "              158.9980,    2204.0020],\n",
      "         [ -28325.9961,   12040.0215,  -10609.9746,  ...,   -9138.0195,\n",
      "             1495.0039,    2016.0264],\n",
      "         [ -76544.0000,  -47533.0039,    1553.0312,  ...,   -5071.0078,\n",
      "             6560.9951,    3297.9639],\n",
      "         ...,\n",
      "         [  15301.9883,  -24397.9922,   19802.9766,  ...,  -14057.9814,\n",
      "            11903.0078,  -10127.0137],\n",
      "         [  13276.9688,  -17864.0078,  -32974.9922,  ...,  -14122.9756,\n",
      "           -21319.9941,  -29734.0234],\n",
      "         [ -26703.9922,  -74685.0000,   -3636.9722,  ...,   -5187.9883,\n",
      "             3197.9922, -127463.9922]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[ -25652.0000,   13180.9990,   15728.9961,  ...,   -2831.0020,\n",
      "              158.9980,    2204.0020],\n",
      "         [ -28325.9961,   12040.0215,  -10609.9746,  ...,   -9138.0195,\n",
      "             1495.0039,    2016.0264],\n",
      "         [ -76544.0000,  -47533.0039,    1553.0312,  ...,   -5071.0078,\n",
      "             6560.9951,    3297.9639],\n",
      "         ...,\n",
      "         [  15301.9883,  -24397.9922,   19802.9766,  ...,  -14057.9814,\n",
      "            11903.0078,  -10127.0137],\n",
      "         [  13276.9688,  -17864.0078,  -32974.9922,  ...,  -14122.9756,\n",
      "           -21319.9941,  -29734.0234],\n",
      "         [ -26703.9922,  -74685.0000,   -3636.9722,  ...,   -5187.9883,\n",
      "             3197.9922, -127463.9922]]]),) and output (tensor([[[ -14791.0000,   15299.9990,   16129.9961,  ...,    3187.9980,\n",
      "            -2552.0020,    5626.0020],\n",
      "         [ -25622.9961,   14966.0215,  -15686.9746,  ...,  -15832.0195,\n",
      "             2656.0039,    2818.0264],\n",
      "         [ -73902.0000,  -45382.0039,   10316.0312,  ...,   -9257.0078,\n",
      "             2050.9951,    4656.9639],\n",
      "         ...,\n",
      "         [   9509.9883,  -22347.9922,   21786.9766,  ...,  -21345.9805,\n",
      "             8017.0078,  -15088.0137],\n",
      "         [  26867.9688,  -23929.0078,  -28241.9922,  ...,  -23316.9766,\n",
      "           -17076.9941,  -23466.0234],\n",
      "         [ -29222.9922,  -65083.0000,   -3309.9722,  ...,   -4455.9883,\n",
      "            -5605.0078, -121391.9922]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[ -14791.0000,   15299.9990,   16129.9961,  ...,    3187.9980,\n",
      "            -2552.0020,    5626.0020],\n",
      "         [ -25622.9961,   14966.0215,  -15686.9746,  ...,  -15832.0195,\n",
      "             2656.0039,    2818.0264],\n",
      "         [ -73902.0000,  -45382.0039,   10316.0312,  ...,   -9257.0078,\n",
      "             2050.9951,    4656.9639],\n",
      "         ...,\n",
      "         [   9509.9883,  -22347.9922,   21786.9766,  ...,  -21345.9805,\n",
      "             8017.0078,  -15088.0137],\n",
      "         [  26867.9688,  -23929.0078,  -28241.9922,  ...,  -23316.9766,\n",
      "           -17076.9941,  -23466.0234],\n",
      "         [ -29222.9922,  -65083.0000,   -3309.9722,  ...,   -4455.9883,\n",
      "            -5605.0078, -121391.9922]]]),) and output (tensor([[[ -13284.0000,   26639.0000,    7467.9961,  ...,    4113.9980,\n",
      "            -6608.0020,    5336.0020],\n",
      "         [ -19005.9961,    2796.0215,  -13401.9746,  ...,  -12915.0195,\n",
      "             4987.0039,    6712.0264],\n",
      "         [ -61714.0000,  -42286.0039,   13653.0312,  ...,   -9831.0078,\n",
      "            -6737.0049,   10033.9639],\n",
      "         ...,\n",
      "         [   7945.9883,  -17957.9922,   19484.9766,  ...,  -19270.9805,\n",
      "             4519.0078,  -17031.0137],\n",
      "         [  31776.9688,  -26719.0078,  -23868.9922,  ...,  -28631.9766,\n",
      "           -12657.9941,  -16207.0234],\n",
      "         [ -40781.9922,  -64972.0000,    -864.9722,  ...,   -3043.9883,\n",
      "             1783.9922, -117175.9922]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[ -13284.0000,   26639.0000,    7467.9961,  ...,    4113.9980,\n",
      "            -6608.0020,    5336.0020],\n",
      "         [ -19005.9961,    2796.0215,  -13401.9746,  ...,  -12915.0195,\n",
      "             4987.0039,    6712.0264],\n",
      "         [ -61714.0000,  -42286.0039,   13653.0312,  ...,   -9831.0078,\n",
      "            -6737.0049,   10033.9639],\n",
      "         ...,\n",
      "         [   7945.9883,  -17957.9922,   19484.9766,  ...,  -19270.9805,\n",
      "             4519.0078,  -17031.0137],\n",
      "         [  31776.9688,  -26719.0078,  -23868.9922,  ...,  -28631.9766,\n",
      "           -12657.9941,  -16207.0234],\n",
      "         [ -40781.9922,  -64972.0000,    -864.9722,  ...,   -3043.9883,\n",
      "             1783.9922, -117175.9922]]]),) and output (tensor([[[ -21532.0000,   32796.0000,   16281.9961,  ...,   18208.9980,\n",
      "            -4282.0020,    5756.0020],\n",
      "         [ -19082.9961,    2569.0215,   -7715.9746,  ...,   -5270.0195,\n",
      "            -4090.9961,    6984.0264],\n",
      "         [ -60712.0000,  -47588.0039,   17567.0312,  ...,   -7852.0078,\n",
      "           -14142.0049,   12338.9639],\n",
      "         ...,\n",
      "         [   9064.9883,  -28488.9922,   21953.9766,  ...,  -10752.9805,\n",
      "             2030.0078,  -15491.0137],\n",
      "         [  24362.9688,  -26245.0078,  -32482.9922,  ...,  -21719.9766,\n",
      "           -15832.9941,  -27155.0234],\n",
      "         [ -34104.9922,  -64072.0000,   -1414.9722,  ...,   -7017.9883,\n",
      "            -6175.0078, -113561.9922]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[ -21532.0000,   32796.0000,   16281.9961,  ...,   18208.9980,\n",
      "            -4282.0020,    5756.0020],\n",
      "         [ -19082.9961,    2569.0215,   -7715.9746,  ...,   -5270.0195,\n",
      "            -4090.9961,    6984.0264],\n",
      "         [ -60712.0000,  -47588.0039,   17567.0312,  ...,   -7852.0078,\n",
      "           -14142.0049,   12338.9639],\n",
      "         ...,\n",
      "         [   9064.9883,  -28488.9922,   21953.9766,  ...,  -10752.9805,\n",
      "             2030.0078,  -15491.0137],\n",
      "         [  24362.9688,  -26245.0078,  -32482.9922,  ...,  -21719.9766,\n",
      "           -15832.9941,  -27155.0234],\n",
      "         [ -34104.9922,  -64072.0000,   -1414.9722,  ...,   -7017.9883,\n",
      "            -6175.0078, -113561.9922]]]),) and output (tensor([[[ -24082.0000,   33387.0000,   15374.9961,  ...,   18359.9980,\n",
      "            -7500.0020,   11519.0020],\n",
      "         [  -9756.9961,    6446.0215,  -12850.9746,  ...,   -7039.0195,\n",
      "            -4975.9961,    3030.0264],\n",
      "         [ -47772.0000,  -57297.0039,   15121.0312,  ...,  -10046.0078,\n",
      "           -26991.0039,    3528.9639],\n",
      "         ...,\n",
      "         [  12664.9883,  -18584.9922,   20044.9766,  ...,   -5967.9805,\n",
      "            -4228.9922,  -15095.0137],\n",
      "         [  23130.9688,  -29243.0078,  -25483.9922,  ...,  -17895.9766,\n",
      "           -17594.9941,  -26998.0234],\n",
      "         [ -28576.9922,  -51412.0000,   -1377.9722,  ...,  -13346.9883,\n",
      "            -7333.0078, -105745.9922]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[ -24082.0000,   33387.0000,   15374.9961,  ...,   18359.9980,\n",
      "            -7500.0020,   11519.0020],\n",
      "         [  -9756.9961,    6446.0215,  -12850.9746,  ...,   -7039.0195,\n",
      "            -4975.9961,    3030.0264],\n",
      "         [ -47772.0000,  -57297.0039,   15121.0312,  ...,  -10046.0078,\n",
      "           -26991.0039,    3528.9639],\n",
      "         ...,\n",
      "         [  12664.9883,  -18584.9922,   20044.9766,  ...,   -5967.9805,\n",
      "            -4228.9922,  -15095.0137],\n",
      "         [  23130.9688,  -29243.0078,  -25483.9922,  ...,  -17895.9766,\n",
      "           -17594.9941,  -26998.0234],\n",
      "         [ -28576.9922,  -51412.0000,   -1377.9722,  ...,  -13346.9883,\n",
      "            -7333.0078, -105745.9922]]]),) and output (tensor([[[-14869.0000,  32284.0000,   3742.9961,  ...,  12458.9980,\n",
      "           -5452.0020,   7590.0020],\n",
      "         [ -5565.9961,  -1322.9785, -11417.9746,  ...,  -4509.0195,\n",
      "            5738.0039,   9755.0264],\n",
      "         [-44036.0000, -59793.0039,  13878.0312,  ...,    525.9922,\n",
      "          -18281.0039,   7367.9639],\n",
      "         ...,\n",
      "         [ 23910.9883, -15157.9922,  19415.9766,  ...,  -7972.9805,\n",
      "           -6233.9922, -17539.0137],\n",
      "         [ 27303.9688, -29340.0078, -23558.9922,  ..., -21633.9766,\n",
      "          -10965.9941, -26597.0234],\n",
      "         [-24816.9922, -50915.0000,   6937.0278,  ...,  -3467.9883,\n",
      "          -13069.0078, -99096.9922]]]),)\n",
      "[Debug] Layer names being passed: ['model.layers.0', 'model.layers.1', 'model.layers.2', 'model.layers.3', 'model.layers.4', 'model.layers.5', 'model.layers.6', 'model.layers.7', 'model.layers.8', 'model.layers.9', 'model.layers.10', 'model.layers.11', 'model.layers.12', 'model.layers.13', 'model.layers.14', 'model.layers.15', 'model.layers.16', 'model.layers.17', 'model.layers.18', 'model.layers.19', 'model.layers.20', 'model.layers.21', 'model.layers.22', 'model.layers.23', 'model.layers.24', 'model.layers.25', 'model.layers.26', 'model.layers.27', 'model.embed_tokens']\n",
      "[Debug] Trying to access layer: model.layers.0\n",
      "[Debug] Successfully found layer: model.layers.0\n",
      "[Debug] Trying to access layer: model.layers.1\n",
      "[Debug] Successfully found layer: model.layers.1\n",
      "[Debug] Trying to access layer: model.layers.2\n",
      "[Debug] Successfully found layer: model.layers.2\n",
      "[Debug] Trying to access layer: model.layers.3\n",
      "[Debug] Successfully found layer: model.layers.3\n",
      "[Debug] Trying to access layer: model.layers.4\n",
      "[Debug] Successfully found layer: model.layers.4\n",
      "[Debug] Trying to access layer: model.layers.5\n",
      "[Debug] Successfully found layer: model.layers.5\n",
      "[Debug] Trying to access layer: model.layers.6\n",
      "[Debug] Successfully found layer: model.layers.6\n",
      "[Debug] Trying to access layer: model.layers.7\n",
      "[Debug] Successfully found layer: model.layers.7\n",
      "[Debug] Trying to access layer: model.layers.8\n",
      "[Debug] Successfully found layer: model.layers.8\n",
      "[Debug] Trying to access layer: model.layers.9\n",
      "[Debug] Successfully found layer: model.layers.9\n",
      "[Debug] Trying to access layer: model.layers.10\n",
      "[Debug] Successfully found layer: model.layers.10\n",
      "[Debug] Trying to access layer: model.layers.11\n",
      "[Debug] Successfully found layer: model.layers.11\n",
      "[Debug] Trying to access layer: model.layers.12\n",
      "[Debug] Successfully found layer: model.layers.12\n",
      "[Debug] Trying to access layer: model.layers.13\n",
      "[Debug] Successfully found layer: model.layers.13\n",
      "[Debug] Trying to access layer: model.layers.14\n",
      "[Debug] Successfully found layer: model.layers.14\n",
      "[Debug] Trying to access layer: model.layers.15\n",
      "[Debug] Successfully found layer: model.layers.15\n",
      "[Debug] Trying to access layer: model.layers.16\n",
      "[Debug] Successfully found layer: model.layers.16\n",
      "[Debug] Trying to access layer: model.layers.17\n",
      "[Debug] Successfully found layer: model.layers.17\n",
      "[Debug] Trying to access layer: model.layers.18\n",
      "[Debug] Successfully found layer: model.layers.18\n",
      "[Debug] Trying to access layer: model.layers.19\n",
      "[Debug] Successfully found layer: model.layers.19\n",
      "[Debug] Trying to access layer: model.layers.20\n",
      "[Debug] Successfully found layer: model.layers.20\n",
      "[Debug] Trying to access layer: model.layers.21\n",
      "[Debug] Successfully found layer: model.layers.21\n",
      "[Debug] Trying to access layer: model.layers.22\n",
      "[Debug] Successfully found layer: model.layers.22\n",
      "[Debug] Trying to access layer: model.layers.23\n",
      "[Debug] Successfully found layer: model.layers.23\n",
      "[Debug] Trying to access layer: model.layers.24\n",
      "[Debug] Successfully found layer: model.layers.24\n",
      "[Debug] Trying to access layer: model.layers.25\n",
      "[Debug] Successfully found layer: model.layers.25\n",
      "[Debug] Trying to access layer: model.layers.26\n",
      "[Debug] Successfully found layer: model.layers.26\n",
      "[Debug] Trying to access layer: model.layers.27\n",
      "[Debug] Successfully found layer: model.layers.27\n",
      "[Debug] Trying to access layer: model.embed_tokens\n",
      "[Debug] Successfully found layer: model.embed_tokens\n",
      "[Hook] Layer Embedding(128256, 3072, padding_idx=128004) received input (tensor([[128000,  33731,  10016,    320,     82,   5248,      8,   2468,    279,\n",
      "           7314,    315,    220,   5162,     15,     11,  10016,   1578,   5614,\n",
      "           3335,   9382,     11,    420,    892,    311,   7054,  19856,  22293,\n",
      "           8032,     16,     60,   1283,   1243,  12715,   3116,  67293,   8166,\n",
      "            220,   1041,  13280,   4194,   4235,    330,   3923,    304,    279,\n",
      "           4435,    596,  15936,   6193,   1472,      1,  30183,     20,    705,\n",
      "            330,     33,  54444,  77339,      1,  30183,     18,      8,    293,\n",
      "           6458,    330,  12174,  15013,   3861,      1,  30183,   1958,    705,\n",
      "            323,    330,   2181,   8442,  35800,   6901,  61133,      1,  30183,\n",
      "           1987,  94638,     16,     60,    330,   3923,    304,    279,   4435,\n",
      "            596,  15936,   6193,   1472,      1,    574,  10016,    596,   2132,\n",
      "           6761,   2624,  13946,   8032,     21,     60,  10016,   8738,    311,\n",
      "           3335,    323,   2804,   2391,    279,    220,   5162,     15,     82,\n",
      "            323,    220,   4468,     15,     82,   8032,     16,     60,   5414,\n",
      "           5609,    330,   2675,   2351,   4702,   2175,  57071,      6,  24327,\n",
      "              1,   8813,    279,   3224,  27223,    304,    220,   4468,     19,\n",
      "           8032,     16,     60,    763,   3297,    220,   4468,     22,     11,\n",
      "          10016,  12715,    264,  89694,   3882,    369,  18588,  13792,   4194,\n",
      "             16,   2624,    503,  18369,     11,   3842,  89694,     13]]),) and output tensor([[[-0.0010, -0.0007, -0.0046,  ..., -0.0014, -0.0020,  0.0020],\n",
      "         [-0.0056, -0.0038,  0.0167,  ...,  0.0139, -0.0273,  0.0048],\n",
      "         [ 0.0125,  0.0008, -0.0131,  ..., -0.0206,  0.0089,  0.0094],\n",
      "         ...,\n",
      "         [-0.0037,  0.0102, -0.0101,  ..., -0.0154,  0.0078, -0.0240],\n",
      "         [ 0.0203,  0.0030, -0.0006,  ..., -0.0618, -0.0209, -0.0016],\n",
      "         [ 0.0093, -0.0031,  0.0278,  ...,  0.0117, -0.0084,  0.0096]]])\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[-0.0010, -0.0007, -0.0046,  ..., -0.0014, -0.0020,  0.0020],\n",
      "         [-0.0056, -0.0038,  0.0167,  ...,  0.0139, -0.0273,  0.0048],\n",
      "         [ 0.0125,  0.0008, -0.0131,  ..., -0.0206,  0.0089,  0.0094],\n",
      "         ...,\n",
      "         [-0.0037,  0.0102, -0.0101,  ..., -0.0154,  0.0078, -0.0240],\n",
      "         [ 0.0203,  0.0030, -0.0006,  ..., -0.0618, -0.0209, -0.0016],\n",
      "         [ 0.0093, -0.0031,  0.0278,  ...,  0.0117, -0.0084,  0.0096]]]),) and output (tensor([[[  442.9990,   239.9993,   302.9954,  ...,  -154.0014,\n",
      "            -15.0020,   185.0020],\n",
      "         [  449.9944,  -109.0038,   971.0167,  ..., -1481.9861,\n",
      "            276.9727,   179.0048],\n",
      "         [  -49.9875,   464.0008,   124.9869,  ...,    38.9794,\n",
      "           -101.9911,   287.0094],\n",
      "         ...,\n",
      "         [   99.9963,    18.0102,    -4.0101,  ...,    39.9846,\n",
      "             55.0078,   -59.0240],\n",
      "         [ -135.9797,  -317.9969,  -159.0006,  ...,  -582.0618,\n",
      "            351.9791,   410.9984],\n",
      "         [  116.0093,   -53.0031,   627.0278,  ...,  -221.9883,\n",
      "             53.9916,    64.0096]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[  442.9990,   239.9993,   302.9954,  ...,  -154.0014,\n",
      "            -15.0020,   185.0020],\n",
      "         [  449.9944,  -109.0038,   971.0167,  ..., -1481.9861,\n",
      "            276.9727,   179.0048],\n",
      "         [  -49.9875,   464.0008,   124.9869,  ...,    38.9794,\n",
      "           -101.9911,   287.0094],\n",
      "         ...,\n",
      "         [   99.9963,    18.0102,    -4.0101,  ...,    39.9846,\n",
      "             55.0078,   -59.0240],\n",
      "         [ -135.9797,  -317.9969,  -159.0006,  ...,  -582.0618,\n",
      "            351.9791,   410.9984],\n",
      "         [  116.0093,   -53.0031,   627.0278,  ...,  -221.9883,\n",
      "             53.9916,    64.0096]]]),) and output (tensor([[[  439.9990,  -144.0007,  -410.0046,  ...,  -765.0015,\n",
      "           1193.9979, -1027.9980],\n",
      "         [  548.9944,   310.9962,   555.0167,  ..., -2931.9861,\n",
      "            541.9727,    29.0048],\n",
      "         [ -293.9875,   586.0008,   553.9869,  ...,  -831.0206,\n",
      "           1165.0089,   318.0094],\n",
      "         ...,\n",
      "         [ 1122.9963,   525.0102,    80.9899,  ...,  -679.0154,\n",
      "           1183.0078,   145.9760],\n",
      "         [ -570.9797,  -606.9969,   304.9994,  ...,  -753.0618,\n",
      "            369.9791,   214.9984],\n",
      "         [ -687.9907,  -180.0031,  1993.0278,  ..., -1053.9883,\n",
      "            234.9916,   278.0096]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[  439.9990,  -144.0007,  -410.0046,  ...,  -765.0015,\n",
      "           1193.9979, -1027.9980],\n",
      "         [  548.9944,   310.9962,   555.0167,  ..., -2931.9861,\n",
      "            541.9727,    29.0048],\n",
      "         [ -293.9875,   586.0008,   553.9869,  ...,  -831.0206,\n",
      "           1165.0089,   318.0094],\n",
      "         ...,\n",
      "         [ 1122.9963,   525.0102,    80.9899,  ...,  -679.0154,\n",
      "           1183.0078,   145.9760],\n",
      "         [ -570.9797,  -606.9969,   304.9994,  ...,  -753.0618,\n",
      "            369.9791,   214.9984],\n",
      "         [ -687.9907,  -180.0031,  1993.0278,  ..., -1053.9883,\n",
      "            234.9916,   278.0096]]]),) and output (tensor([[[  551.9990,   993.9993, -1790.0046,  ...,   445.9985,\n",
      "          -1386.0021, -1245.9980],\n",
      "         [ 2753.9944, -1672.0038,   428.0167,  ..., -1707.9861,\n",
      "            -54.0273,  -162.9952],\n",
      "         [  895.0125, -2302.9993,  -791.0131,  ...,  1251.9794,\n",
      "          -1120.9911,  2681.0093],\n",
      "         ...,\n",
      "         [ 1171.9963, -1294.9897,  -761.0100,  ...,  -886.0154,\n",
      "           -342.9922,  1258.9760],\n",
      "         [  466.0203, -1721.9971, -1633.0006,  ...,  -852.0618,\n",
      "           6558.9790,  2113.9983],\n",
      "         [-1253.9907,  2543.9971,  3059.0278,  ...,  2240.0117,\n",
      "          -2231.0083,  2067.0098]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[  551.9990,   993.9993, -1790.0046,  ...,   445.9985,\n",
      "          -1386.0021, -1245.9980],\n",
      "         [ 2753.9944, -1672.0038,   428.0167,  ..., -1707.9861,\n",
      "            -54.0273,  -162.9952],\n",
      "         [  895.0125, -2302.9993,  -791.0131,  ...,  1251.9794,\n",
      "          -1120.9911,  2681.0093],\n",
      "         ...,\n",
      "         [ 1171.9963, -1294.9897,  -761.0100,  ...,  -886.0154,\n",
      "           -342.9922,  1258.9760],\n",
      "         [  466.0203, -1721.9971, -1633.0006,  ...,  -852.0618,\n",
      "           6558.9790,  2113.9983],\n",
      "         [-1253.9907,  2543.9971,  3059.0278,  ...,  2240.0117,\n",
      "          -2231.0083,  2067.0098]]]),) and output (tensor([[[  2131.9990,   -737.0007,  -1071.0046,  ...,  -2781.0015,\n",
      "            -885.0021,     86.0020],\n",
      "         [  4087.9944,   1207.9961,   1181.0167,  ...,  -3062.9861,\n",
      "            3029.9727,   2967.0049],\n",
      "         [ -3084.9875,  -8420.9990,  -2695.0132,  ...,   -921.0205,\n",
      "            -410.9911,   2102.0093],\n",
      "         ...,\n",
      "         [ -1057.0037, -11159.9902,  -3213.0100,  ...,  -3019.0154,\n",
      "            1679.0078,   -994.0240],\n",
      "         [  9968.0205,   -595.9971,  -1858.0005,  ...,  -6862.0615,\n",
      "            8656.9785,    692.9983],\n",
      "         [  2035.0093,   3152.9971,   4048.0278,  ...,   1243.0117,\n",
      "            2390.9917,  -9646.9902]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[  2131.9990,   -737.0007,  -1071.0046,  ...,  -2781.0015,\n",
      "            -885.0021,     86.0020],\n",
      "         [  4087.9944,   1207.9961,   1181.0167,  ...,  -3062.9861,\n",
      "            3029.9727,   2967.0049],\n",
      "         [ -3084.9875,  -8420.9990,  -2695.0132,  ...,   -921.0205,\n",
      "            -410.9911,   2102.0093],\n",
      "         ...,\n",
      "         [ -1057.0037, -11159.9902,  -3213.0100,  ...,  -3019.0154,\n",
      "            1679.0078,   -994.0240],\n",
      "         [  9968.0205,   -595.9971,  -1858.0005,  ...,  -6862.0615,\n",
      "            8656.9785,    692.9983],\n",
      "         [  2035.0093,   3152.9971,   4048.0278,  ...,   1243.0117,\n",
      "            2390.9917,  -9646.9902]]]),) and output (tensor([[[  3420.9990,    760.9993,  -5376.0049,  ...,  -4193.0015,\n",
      "            2860.9980,  -4080.9980],\n",
      "         [  3820.9944,   1502.9961,   5584.0166,  ...,    486.0139,\n",
      "             434.9727,   6336.0049],\n",
      "         [ -2016.9875,  -8895.9990,  -5359.0132,  ...,  -6138.0205,\n",
      "            1227.0089,   7114.0093],\n",
      "         ...,\n",
      "         [ -2949.0037, -21938.9902,  -1603.0100,  ...,  -5972.0156,\n",
      "           -1859.9922,  -7673.0239],\n",
      "         [ 16785.0195,   -680.9971,   -365.0005,  ...,  -5794.0615,\n",
      "           12087.9785,  -1665.0017],\n",
      "         [   817.0093,    597.9971,   4671.0278,  ...,   6108.0117,\n",
      "            2101.9917, -14795.9902]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[  3420.9990,    760.9993,  -5376.0049,  ...,  -4193.0015,\n",
      "            2860.9980,  -4080.9980],\n",
      "         [  3820.9944,   1502.9961,   5584.0166,  ...,    486.0139,\n",
      "             434.9727,   6336.0049],\n",
      "         [ -2016.9875,  -8895.9990,  -5359.0132,  ...,  -6138.0205,\n",
      "            1227.0089,   7114.0093],\n",
      "         ...,\n",
      "         [ -2949.0037, -21938.9902,  -1603.0100,  ...,  -5972.0156,\n",
      "           -1859.9922,  -7673.0239],\n",
      "         [ 16785.0195,   -680.9971,   -365.0005,  ...,  -5794.0615,\n",
      "           12087.9785,  -1665.0017],\n",
      "         [   817.0093,    597.9971,   4671.0278,  ...,   6108.0117,\n",
      "            2101.9917, -14795.9902]]]),) and output (tensor([[[ 2.7430e+03, -1.5120e+03, -1.0493e+04,  ..., -9.9450e+03,\n",
      "          -2.0170e+03, -1.8030e+03],\n",
      "         [ 8.3400e+03, -2.0039e+00,  5.4630e+03,  ...,  6.7701e+02,\n",
      "           1.2820e+03,  1.2838e+04],\n",
      "         [ 4.1012e+01, -1.0634e+04, -4.5490e+03,  ..., -4.6470e+03,\n",
      "           9.3601e+02,  9.2710e+03],\n",
      "         ...,\n",
      "         [-3.2400e+02, -2.2651e+04, -4.2430e+03,  ..., -2.0660e+03,\n",
      "          -9.0690e+03, -4.0040e+03],\n",
      "         [ 2.5973e+04, -2.5980e+03,  2.9940e+03,  ..., -7.4971e+03,\n",
      "           9.5198e+02, -5.3002e+01],\n",
      "         [-2.6860e+03, -4.9900e+02,  4.8540e+03,  ...,  6.3170e+03,\n",
      "           2.1992e+01, -1.3528e+04]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[ 2.7430e+03, -1.5120e+03, -1.0493e+04,  ..., -9.9450e+03,\n",
      "          -2.0170e+03, -1.8030e+03],\n",
      "         [ 8.3400e+03, -2.0039e+00,  5.4630e+03,  ...,  6.7701e+02,\n",
      "           1.2820e+03,  1.2838e+04],\n",
      "         [ 4.1012e+01, -1.0634e+04, -4.5490e+03,  ..., -4.6470e+03,\n",
      "           9.3601e+02,  9.2710e+03],\n",
      "         ...,\n",
      "         [-3.2400e+02, -2.2651e+04, -4.2430e+03,  ..., -2.0660e+03,\n",
      "          -9.0690e+03, -4.0040e+03],\n",
      "         [ 2.5973e+04, -2.5980e+03,  2.9940e+03,  ..., -7.4971e+03,\n",
      "           9.5198e+02, -5.3002e+01],\n",
      "         [-2.6860e+03, -4.9900e+02,  4.8540e+03,  ...,  6.3170e+03,\n",
      "           2.1992e+01, -1.3528e+04]]]),) and output (tensor([[[  3032.9990,  -3420.0007, -10291.0049,  ...,  -6511.0020,\n",
      "           -1953.0020,   2001.0020],\n",
      "         [  8320.9941,   -983.0039,   3881.0166,  ...,    533.0139,\n",
      "            1518.9727,  10744.0049],\n",
      "         [ 11115.0127,  -8668.9990,  -3778.0132,  ...,  -3012.0205,\n",
      "           -1685.9911,  16557.0098],\n",
      "         ...,\n",
      "         [ -2714.0037, -27508.9902,   -597.0098,  ...,  -7516.0156,\n",
      "          -11198.9922,  -4373.0234],\n",
      "         [ 37189.0195,  -3433.9971,   3993.9995,  ...,  -7602.0615,\n",
      "           -4791.0215,    344.9983],\n",
      "         [   948.0093,   -826.0029,   6546.0278,  ...,   1942.0117,\n",
      "           -4582.0083,  -9004.9902]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[  3032.9990,  -3420.0007, -10291.0049,  ...,  -6511.0020,\n",
      "           -1953.0020,   2001.0020],\n",
      "         [  8320.9941,   -983.0039,   3881.0166,  ...,    533.0139,\n",
      "            1518.9727,  10744.0049],\n",
      "         [ 11115.0127,  -8668.9990,  -3778.0132,  ...,  -3012.0205,\n",
      "           -1685.9911,  16557.0098],\n",
      "         ...,\n",
      "         [ -2714.0037, -27508.9902,   -597.0098,  ...,  -7516.0156,\n",
      "          -11198.9922,  -4373.0234],\n",
      "         [ 37189.0195,  -3433.9971,   3993.9995,  ...,  -7602.0615,\n",
      "           -4791.0215,    344.9983],\n",
      "         [   948.0093,   -826.0029,   6546.0278,  ...,   1942.0117,\n",
      "           -4582.0083,  -9004.9902]]]),) and output (tensor([[[  3132.9990,  -1970.0007, -11544.0049,  ...,  -5780.0020,\n",
      "           -2486.0020,  -1921.9980],\n",
      "         [ 12409.9941,  -2963.0039,   1742.0166,  ...,   2759.0139,\n",
      "             903.9727,  11909.0049],\n",
      "         [ 12857.0127, -13363.9990,  -6215.0132,  ...,  -1624.0205,\n",
      "           -1248.9911,  20662.0098],\n",
      "         ...,\n",
      "         [ -3011.0037, -24986.9902,  -2422.0098,  ...,  -6806.0156,\n",
      "           -3809.9922,  -9084.0234],\n",
      "         [ 46399.0195,   -584.9971,   1925.9995,  ..., -13572.0615,\n",
      "            2112.9785,   5519.9980],\n",
      "         [ -6845.9907,  -6244.0029,   6377.0278,  ...,  -2406.9883,\n",
      "            1700.9917,  -2599.9902]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[  3132.9990,  -1970.0007, -11544.0049,  ...,  -5780.0020,\n",
      "           -2486.0020,  -1921.9980],\n",
      "         [ 12409.9941,  -2963.0039,   1742.0166,  ...,   2759.0139,\n",
      "             903.9727,  11909.0049],\n",
      "         [ 12857.0127, -13363.9990,  -6215.0132,  ...,  -1624.0205,\n",
      "           -1248.9911,  20662.0098],\n",
      "         ...,\n",
      "         [ -3011.0037, -24986.9902,  -2422.0098,  ...,  -6806.0156,\n",
      "           -3809.9922,  -9084.0234],\n",
      "         [ 46399.0195,   -584.9971,   1925.9995,  ..., -13572.0615,\n",
      "            2112.9785,   5519.9980],\n",
      "         [ -6845.9907,  -6244.0029,   6377.0278,  ...,  -2406.9883,\n",
      "            1700.9917,  -2599.9902]]]),) and output (tensor([[[  1046.9990,   2403.9993, -14486.0049,  ..., -10922.0020,\n",
      "           -1419.0020,  -2983.9980],\n",
      "         [  6948.9941,   1890.9961,  -1621.9834,  ...,   6775.0137,\n",
      "           -1108.0273,  14427.0049],\n",
      "         [ 13306.0127, -13709.9990,  -9147.0137,  ...,  -4546.0205,\n",
      "             979.0089,  34775.0078],\n",
      "         ...,\n",
      "         [   868.9963, -25568.9902,  -1535.0098,  ...,  -6528.0156,\n",
      "           -8258.9922,  -9264.0234],\n",
      "         [ 51710.0195,   2893.0029,  -2831.0005,  ..., -13216.0615,\n",
      "            1575.9785,   1570.9980],\n",
      "         [ -9101.9902,  -1127.0029,  10820.0273,  ...,  -1655.9883,\n",
      "            1486.9917,  -5489.9902]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[  1046.9990,   2403.9993, -14486.0049,  ..., -10922.0020,\n",
      "           -1419.0020,  -2983.9980],\n",
      "         [  6948.9941,   1890.9961,  -1621.9834,  ...,   6775.0137,\n",
      "           -1108.0273,  14427.0049],\n",
      "         [ 13306.0127, -13709.9990,  -9147.0137,  ...,  -4546.0205,\n",
      "             979.0089,  34775.0078],\n",
      "         ...,\n",
      "         [   868.9963, -25568.9902,  -1535.0098,  ...,  -6528.0156,\n",
      "           -8258.9922,  -9264.0234],\n",
      "         [ 51710.0195,   2893.0029,  -2831.0005,  ..., -13216.0615,\n",
      "            1575.9785,   1570.9980],\n",
      "         [ -9101.9902,  -1127.0029,  10820.0273,  ...,  -1655.9883,\n",
      "            1486.9917,  -5489.9902]]]),) and output (tensor([[[ -3767.0010,   5307.9990, -14221.0049,  ..., -15035.0020,\n",
      "           -3630.0020,   4168.0020],\n",
      "         [  1694.9941,  -6606.0039,  -3039.9834,  ...,   3912.0137,\n",
      "           -2889.0273,  10027.0049],\n",
      "         [ 11454.0127, -11541.9990,  -8608.0137,  ...,  -7147.0205,\n",
      "             374.0088,  44566.0078],\n",
      "         ...,\n",
      "         [  3454.9963, -30920.9902,  -2485.0098,  ...,  -6042.0156,\n",
      "          -10798.9922,  -8107.0234],\n",
      "         [ 58211.0195,   5341.0029,  -5061.0005,  ..., -14422.0615,\n",
      "           -6712.0215,    220.9980],\n",
      "         [ -8327.9902,  -3324.0029,  12726.0273,  ...,   1888.0117,\n",
      "            3008.9917,  -5584.9902]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[ -3767.0010,   5307.9990, -14221.0049,  ..., -15035.0020,\n",
      "           -3630.0020,   4168.0020],\n",
      "         [  1694.9941,  -6606.0039,  -3039.9834,  ...,   3912.0137,\n",
      "           -2889.0273,  10027.0049],\n",
      "         [ 11454.0127, -11541.9990,  -8608.0137,  ...,  -7147.0205,\n",
      "             374.0088,  44566.0078],\n",
      "         ...,\n",
      "         [  3454.9963, -30920.9902,  -2485.0098,  ...,  -6042.0156,\n",
      "          -10798.9922,  -8107.0234],\n",
      "         [ 58211.0195,   5341.0029,  -5061.0005,  ..., -14422.0615,\n",
      "           -6712.0215,    220.9980],\n",
      "         [ -8327.9902,  -3324.0029,  12726.0273,  ...,   1888.0117,\n",
      "            3008.9917,  -5584.9902]]]),) and output (tensor([[[ -1326.0010,   1223.9990, -16643.0039,  ..., -13240.0020,\n",
      "           -5677.0020,   9639.0020],\n",
      "         [  4129.9941,  -8818.0039,  -1451.9834,  ...,    949.0137,\n",
      "           -3353.0273,  15646.0049],\n",
      "         [  4209.0127, -12615.9990,  -8833.0137,  ...,  -6123.0205,\n",
      "             568.0088,  51873.0078],\n",
      "         ...,\n",
      "         [  7046.9961, -33679.9922,  -3521.0098,  ...,  -2306.0156,\n",
      "          -15402.9922,  -6651.0234],\n",
      "         [ 66124.0156,  10878.0029,  -7696.0005,  ..., -14367.0615,\n",
      "           -4132.0215,    357.9980],\n",
      "         [ -6977.9902,  -4191.0029,  11994.0273,  ...,   1198.0117,\n",
      "           -1740.0083,  -2886.9902]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[ -1326.0010,   1223.9990, -16643.0039,  ..., -13240.0020,\n",
      "           -5677.0020,   9639.0020],\n",
      "         [  4129.9941,  -8818.0039,  -1451.9834,  ...,    949.0137,\n",
      "           -3353.0273,  15646.0049],\n",
      "         [  4209.0127, -12615.9990,  -8833.0137,  ...,  -6123.0205,\n",
      "             568.0088,  51873.0078],\n",
      "         ...,\n",
      "         [  7046.9961, -33679.9922,  -3521.0098,  ...,  -2306.0156,\n",
      "          -15402.9922,  -6651.0234],\n",
      "         [ 66124.0156,  10878.0029,  -7696.0005,  ..., -14367.0615,\n",
      "           -4132.0215,    357.9980],\n",
      "         [ -6977.9902,  -4191.0029,  11994.0273,  ...,   1198.0117,\n",
      "           -1740.0083,  -2886.9902]]]),) and output (tensor([[[ -5120.0010,   2246.9990, -15993.0039,  ..., -19818.0020,\n",
      "           -9385.0020,  11924.0020],\n",
      "         [  3574.9941,  -9539.0039,   1250.0166,  ...,   1397.0137,\n",
      "           -6763.0273,  18077.0039],\n",
      "         [   989.0127, -13186.9990, -10368.0137,  ...,  -6486.0205,\n",
      "           -5316.9912,  52449.0078],\n",
      "         ...,\n",
      "         [  8497.9961, -35634.9922,  -5090.0098,  ...,  -4343.0156,\n",
      "          -10563.9922,  -7632.0234],\n",
      "         [ 71998.0156,  18218.0039,  -7971.0005,  ..., -11373.0615,\n",
      "           -7147.0215,   7752.9980],\n",
      "         [ -7500.9902,   2615.9971,  12446.0273,  ...,   5389.0117,\n",
      "           -7566.0083,  -2476.9902]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[ -5120.0010,   2246.9990, -15993.0039,  ..., -19818.0020,\n",
      "           -9385.0020,  11924.0020],\n",
      "         [  3574.9941,  -9539.0039,   1250.0166,  ...,   1397.0137,\n",
      "           -6763.0273,  18077.0039],\n",
      "         [   989.0127, -13186.9990, -10368.0137,  ...,  -6486.0205,\n",
      "           -5316.9912,  52449.0078],\n",
      "         ...,\n",
      "         [  8497.9961, -35634.9922,  -5090.0098,  ...,  -4343.0156,\n",
      "          -10563.9922,  -7632.0234],\n",
      "         [ 71998.0156,  18218.0039,  -7971.0005,  ..., -11373.0615,\n",
      "           -7147.0215,   7752.9980],\n",
      "         [ -7500.9902,   2615.9971,  12446.0273,  ...,   5389.0117,\n",
      "           -7566.0083,  -2476.9902]]]),) and output (tensor([[[ -6685.0010,  -1278.0010, -15553.0039,  ..., -19846.0020,\n",
      "          -10418.0020,  13376.0020],\n",
      "         [  1071.9941, -11492.0039,   4457.0166,  ...,    315.0137,\n",
      "          -13150.0273,  16900.0039],\n",
      "         [  3303.0127,  -5355.9990,  -9954.0137,  ...,  -1847.0205,\n",
      "          -11392.9912,  54924.0078],\n",
      "         ...,\n",
      "         [  3476.9961, -36758.9922,  -3040.0098,  ...,  -3221.0156,\n",
      "          -11533.9922,  -6497.0234],\n",
      "         [ 72766.0156,  19393.0039,  -8015.0005,  ...,  -3078.0615,\n",
      "           -9128.0215,  10591.9980],\n",
      "         [ -9204.9902,  -3361.0029,  12779.0273,  ...,   4296.0117,\n",
      "           -2059.0083,  -2449.9902]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[ -6685.0010,  -1278.0010, -15553.0039,  ..., -19846.0020,\n",
      "          -10418.0020,  13376.0020],\n",
      "         [  1071.9941, -11492.0039,   4457.0166,  ...,    315.0137,\n",
      "          -13150.0273,  16900.0039],\n",
      "         [  3303.0127,  -5355.9990,  -9954.0137,  ...,  -1847.0205,\n",
      "          -11392.9912,  54924.0078],\n",
      "         ...,\n",
      "         [  3476.9961, -36758.9922,  -3040.0098,  ...,  -3221.0156,\n",
      "          -11533.9922,  -6497.0234],\n",
      "         [ 72766.0156,  19393.0039,  -8015.0005,  ...,  -3078.0615,\n",
      "           -9128.0215,  10591.9980],\n",
      "         [ -9204.9902,  -3361.0029,  12779.0273,  ...,   4296.0117,\n",
      "           -2059.0083,  -2449.9902]]]),) and output (tensor([[[-8.4220e+03,  3.4000e+02, -1.5289e+04,  ..., -2.0884e+04,\n",
      "          -6.9080e+03,  1.1014e+04],\n",
      "         [-1.9330e+03, -9.8010e+03,  2.9017e+01,  ..., -2.0300e+03,\n",
      "          -1.4126e+04,  1.7281e+04],\n",
      "         [ 4.2690e+03, -3.1280e+03, -1.5102e+04,  ..., -1.0202e+02,\n",
      "          -1.2405e+04,  6.0679e+04],\n",
      "         ...,\n",
      "         [ 3.1770e+03, -3.8356e+04, -8.4630e+03,  ..., -9.2810e+03,\n",
      "          -6.8120e+03, -7.5240e+03],\n",
      "         [ 7.6880e+04,  1.7138e+04, -4.1430e+03,  ...,  4.2869e+03,\n",
      "          -5.5290e+03,  7.3760e+03],\n",
      "         [-9.4820e+03,  3.2180e+03,  9.3980e+03,  ...,  2.4680e+03,\n",
      "           5.8340e+03, -4.3160e+03]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[-8.4220e+03,  3.4000e+02, -1.5289e+04,  ..., -2.0884e+04,\n",
      "          -6.9080e+03,  1.1014e+04],\n",
      "         [-1.9330e+03, -9.8010e+03,  2.9017e+01,  ..., -2.0300e+03,\n",
      "          -1.4126e+04,  1.7281e+04],\n",
      "         [ 4.2690e+03, -3.1280e+03, -1.5102e+04,  ..., -1.0202e+02,\n",
      "          -1.2405e+04,  6.0679e+04],\n",
      "         ...,\n",
      "         [ 3.1770e+03, -3.8356e+04, -8.4630e+03,  ..., -9.2810e+03,\n",
      "          -6.8120e+03, -7.5240e+03],\n",
      "         [ 7.6880e+04,  1.7138e+04, -4.1430e+03,  ...,  4.2869e+03,\n",
      "          -5.5290e+03,  7.3760e+03],\n",
      "         [-9.4820e+03,  3.2180e+03,  9.3980e+03,  ...,  2.4680e+03,\n",
      "           5.8340e+03, -4.3160e+03]]]),) and output (tensor([[[-10989.0010,   7034.9990, -13964.0039,  ..., -23968.0020,\n",
      "           -3749.0020,  11730.0020],\n",
      "         [ -5585.0059,  -8325.0039,  -3813.9834,  ...,  -1694.9863,\n",
      "          -15263.0273,  17500.0039],\n",
      "         [ 12492.0127,    725.0010, -16668.0137,  ...,  -5252.0205,\n",
      "           -2055.9912,  69154.0078],\n",
      "         ...,\n",
      "         [  1036.9961, -46215.9922,  -8269.0098,  ..., -12251.0156,\n",
      "           -1376.9922, -14400.0234],\n",
      "         [ 81766.0156,  15162.0039,  -8484.0000,  ...,   3246.9385,\n",
      "           -2887.0215,  12811.9980],\n",
      "         [ -9380.9902,  -1053.0029,  10420.0273,  ...,   2024.0117,\n",
      "            6204.9917,  -3190.9902]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[-10989.0010,   7034.9990, -13964.0039,  ..., -23968.0020,\n",
      "           -3749.0020,  11730.0020],\n",
      "         [ -5585.0059,  -8325.0039,  -3813.9834,  ...,  -1694.9863,\n",
      "          -15263.0273,  17500.0039],\n",
      "         [ 12492.0127,    725.0010, -16668.0137,  ...,  -5252.0205,\n",
      "           -2055.9912,  69154.0078],\n",
      "         ...,\n",
      "         [  1036.9961, -46215.9922,  -8269.0098,  ..., -12251.0156,\n",
      "           -1376.9922, -14400.0234],\n",
      "         [ 81766.0156,  15162.0039,  -8484.0000,  ...,   3246.9385,\n",
      "           -2887.0215,  12811.9980],\n",
      "         [ -9380.9902,  -1053.0029,  10420.0273,  ...,   2024.0117,\n",
      "            6204.9917,  -3190.9902]]]),) and output (tensor([[[-7.1590e+03,  6.6220e+03, -1.0967e+04,  ..., -1.7477e+04,\n",
      "          -2.5100e+03,  1.1888e+04],\n",
      "         [-7.5730e+03, -1.0783e+04, -8.9320e+03,  ..., -5.4199e+02,\n",
      "          -1.3406e+04,  1.1849e+04],\n",
      "         [ 1.5584e+04,  5.1750e+03, -1.5055e+04,  ..., -6.7630e+03,\n",
      "           3.2740e+03,  7.5994e+04],\n",
      "         ...,\n",
      "         [ 4.5470e+03, -5.3718e+04, -8.6740e+03,  ..., -1.2017e+04,\n",
      "          -2.7310e+03, -9.7370e+03],\n",
      "         [ 8.8030e+04,  1.3799e+04, -1.1761e+04,  ..., -3.1062e+01,\n",
      "          -6.4490e+03,  1.5216e+04],\n",
      "         [-1.0575e+04, -1.6960e+03,  9.1130e+03,  ..., -2.9770e+03,\n",
      "           7.3690e+03, -5.3670e+03]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[-7.1590e+03,  6.6220e+03, -1.0967e+04,  ..., -1.7477e+04,\n",
      "          -2.5100e+03,  1.1888e+04],\n",
      "         [-7.5730e+03, -1.0783e+04, -8.9320e+03,  ..., -5.4199e+02,\n",
      "          -1.3406e+04,  1.1849e+04],\n",
      "         [ 1.5584e+04,  5.1750e+03, -1.5055e+04,  ..., -6.7630e+03,\n",
      "           3.2740e+03,  7.5994e+04],\n",
      "         ...,\n",
      "         [ 4.5470e+03, -5.3718e+04, -8.6740e+03,  ..., -1.2017e+04,\n",
      "          -2.7310e+03, -9.7370e+03],\n",
      "         [ 8.8030e+04,  1.3799e+04, -1.1761e+04,  ..., -3.1062e+01,\n",
      "          -6.4490e+03,  1.5216e+04],\n",
      "         [-1.0575e+04, -1.6960e+03,  9.1130e+03,  ..., -2.9770e+03,\n",
      "           7.3690e+03, -5.3670e+03]]]),) and output (tensor([[[-4.8220e+03,  3.1250e+03, -3.8850e+03,  ..., -1.9007e+04,\n",
      "          -5.5100e+02,  1.6285e+04],\n",
      "         [-6.7150e+03, -1.5579e+04, -1.1941e+04,  ..., -5.5986e+01,\n",
      "          -1.0753e+04,  1.5825e+04],\n",
      "         [ 9.0040e+03,  1.7100e+02, -1.6090e+04,  ..., -8.5750e+03,\n",
      "          -1.9500e+03,  8.0495e+04],\n",
      "         ...,\n",
      "         [ 8.1580e+03, -6.6269e+04, -7.5360e+03,  ..., -7.9240e+03,\n",
      "           1.4360e+03, -9.7560e+03],\n",
      "         [ 9.4459e+04,  5.1790e+03, -1.2324e+04,  ..., -2.9961e+03,\n",
      "          -7.3830e+03,  1.5507e+04],\n",
      "         [-1.3301e+04,  5.4200e+02,  3.9560e+03,  ..., -9.2170e+03,\n",
      "           7.8880e+03, -4.6680e+03]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[-4.8220e+03,  3.1250e+03, -3.8850e+03,  ..., -1.9007e+04,\n",
      "          -5.5100e+02,  1.6285e+04],\n",
      "         [-6.7150e+03, -1.5579e+04, -1.1941e+04,  ..., -5.5986e+01,\n",
      "          -1.0753e+04,  1.5825e+04],\n",
      "         [ 9.0040e+03,  1.7100e+02, -1.6090e+04,  ..., -8.5750e+03,\n",
      "          -1.9500e+03,  8.0495e+04],\n",
      "         ...,\n",
      "         [ 8.1580e+03, -6.6269e+04, -7.5360e+03,  ..., -7.9240e+03,\n",
      "           1.4360e+03, -9.7560e+03],\n",
      "         [ 9.4459e+04,  5.1790e+03, -1.2324e+04,  ..., -2.9961e+03,\n",
      "          -7.3830e+03,  1.5507e+04],\n",
      "         [-1.3301e+04,  5.4200e+02,  3.9560e+03,  ..., -9.2170e+03,\n",
      "           7.8880e+03, -4.6680e+03]]]),) and output (tensor([[[ -7235.0010,   6583.9990,  -4212.0039,  ..., -19104.0020,\n",
      "            3087.9980,  17084.0020],\n",
      "         [ -7348.0059, -27522.0039,  -4979.9834,  ...,   1119.0137,\n",
      "          -12682.0273,   8694.0039],\n",
      "         [ 15048.0127,   8491.0010, -20881.0137,  ...,  -3775.0205,\n",
      "            2641.0088,  81140.0078],\n",
      "         ...,\n",
      "         [ 12691.9961, -70358.9922,  -8535.0098,  ...,  -4608.0156,\n",
      "            9486.0078,  -4893.0234],\n",
      "         [ 97591.0156,  10960.0039, -13406.0000,  ...,    399.9385,\n",
      "           -5444.0215,  11448.9980],\n",
      "         [-16007.9902,  -3252.0029,  -3975.9727,  ...,  -3839.9883,\n",
      "           11060.9922, -12152.9902]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[ -7235.0010,   6583.9990,  -4212.0039,  ..., -19104.0020,\n",
      "            3087.9980,  17084.0020],\n",
      "         [ -7348.0059, -27522.0039,  -4979.9834,  ...,   1119.0137,\n",
      "          -12682.0273,   8694.0039],\n",
      "         [ 15048.0127,   8491.0010, -20881.0137,  ...,  -3775.0205,\n",
      "            2641.0088,  81140.0078],\n",
      "         ...,\n",
      "         [ 12691.9961, -70358.9922,  -8535.0098,  ...,  -4608.0156,\n",
      "            9486.0078,  -4893.0234],\n",
      "         [ 97591.0156,  10960.0039, -13406.0000,  ...,    399.9385,\n",
      "           -5444.0215,  11448.9980],\n",
      "         [-16007.9902,  -3252.0029,  -3975.9727,  ...,  -3839.9883,\n",
      "           11060.9922, -12152.9902]]]),) and output (tensor([[[-5.5860e+03,  7.6460e+03, -5.0500e+02,  ..., -1.5346e+04,\n",
      "          -6.2300e+02,  1.2986e+04],\n",
      "         [-5.8170e+03, -2.7785e+04, -7.0320e+03,  ...,  5.0290e+03,\n",
      "          -1.3385e+04,  6.3620e+03],\n",
      "         [ 1.7751e+04,  1.2694e+04, -2.0994e+04,  ..., -4.0002e+02,\n",
      "           4.3030e+03,  8.6150e+04],\n",
      "         ...,\n",
      "         [ 1.3401e+04, -6.8221e+04, -1.2099e+04,  ..., -7.0016e+01,\n",
      "           1.0068e+04, -7.5230e+03],\n",
      "         [ 9.4443e+04,  9.2480e+03, -1.2223e+04,  ..., -4.2251e+03,\n",
      "          -8.3230e+03,  1.0084e+04],\n",
      "         [-2.1394e+04, -5.9280e+03, -6.3220e+03,  ..., -3.3610e+03,\n",
      "           1.4121e+04, -1.1618e+04]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[-5.5860e+03,  7.6460e+03, -5.0500e+02,  ..., -1.5346e+04,\n",
      "          -6.2300e+02,  1.2986e+04],\n",
      "         [-5.8170e+03, -2.7785e+04, -7.0320e+03,  ...,  5.0290e+03,\n",
      "          -1.3385e+04,  6.3620e+03],\n",
      "         [ 1.7751e+04,  1.2694e+04, -2.0994e+04,  ..., -4.0002e+02,\n",
      "           4.3030e+03,  8.6150e+04],\n",
      "         ...,\n",
      "         [ 1.3401e+04, -6.8221e+04, -1.2099e+04,  ..., -7.0016e+01,\n",
      "           1.0068e+04, -7.5230e+03],\n",
      "         [ 9.4443e+04,  9.2480e+03, -1.2223e+04,  ..., -4.2251e+03,\n",
      "          -8.3230e+03,  1.0084e+04],\n",
      "         [-2.1394e+04, -5.9280e+03, -6.3220e+03,  ..., -3.3610e+03,\n",
      "           1.4121e+04, -1.1618e+04]]]),) and output (tensor([[[ -8916.0010,   2362.9990,  -4833.0039,  ..., -12075.0020,\n",
      "           -1124.0020,  12438.0020],\n",
      "         [ -1092.0059, -24725.0039, -12086.9834,  ...,   4584.0137,\n",
      "          -15454.0273,   3992.0039],\n",
      "         [ 17977.0117,  12728.0010, -26220.0137,  ...,  -4327.0205,\n",
      "            6307.0088,  82549.0078],\n",
      "         ...,\n",
      "         [ 12173.9961, -68143.9922, -14768.0098,  ...,  -7464.0156,\n",
      "           16258.0078, -12397.0234],\n",
      "         [ 90648.0156,   5966.0039,  -4878.0000,  ...,  -4361.0615,\n",
      "           -9804.0215,   6496.9980],\n",
      "         [-29390.9902,  -6552.0029, -13366.9727,  ...,   5464.0117,\n",
      "           11763.9922, -12887.9902]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[ -8916.0010,   2362.9990,  -4833.0039,  ..., -12075.0020,\n",
      "           -1124.0020,  12438.0020],\n",
      "         [ -1092.0059, -24725.0039, -12086.9834,  ...,   4584.0137,\n",
      "          -15454.0273,   3992.0039],\n",
      "         [ 17977.0117,  12728.0010, -26220.0137,  ...,  -4327.0205,\n",
      "            6307.0088,  82549.0078],\n",
      "         ...,\n",
      "         [ 12173.9961, -68143.9922, -14768.0098,  ...,  -7464.0156,\n",
      "           16258.0078, -12397.0234],\n",
      "         [ 90648.0156,   5966.0039,  -4878.0000,  ...,  -4361.0615,\n",
      "           -9804.0215,   6496.9980],\n",
      "         [-29390.9902,  -6552.0029, -13366.9727,  ...,   5464.0117,\n",
      "           11763.9922, -12887.9902]]]),) and output (tensor([[[-20230.0000,   7877.9990,    161.9961,  ..., -10250.0020,\n",
      "             676.9980,   7095.0020],\n",
      "         [-11867.0059, -25134.0039, -12004.9834,  ...,   8960.0137,\n",
      "          -18222.0273,  -2076.9961],\n",
      "         [ 25361.0117,  11065.0010, -29612.0137,  ...,  -9796.0205,\n",
      "            8765.0088,  82348.0078],\n",
      "         ...,\n",
      "         [ 11075.9961, -77381.9922, -15742.0098,  ...,  -5145.0156,\n",
      "           17441.0078, -13162.0234],\n",
      "         [ 91971.0156,   8041.0039,  -2328.0000,  ...,  -7612.0615,\n",
      "          -11827.0215,   1314.9980],\n",
      "         [-32867.9922,  -2746.0029,  -9850.9727,  ...,  11504.0117,\n",
      "            9001.9922, -24387.9902]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[-20230.0000,   7877.9990,    161.9961,  ..., -10250.0020,\n",
      "             676.9980,   7095.0020],\n",
      "         [-11867.0059, -25134.0039, -12004.9834,  ...,   8960.0137,\n",
      "          -18222.0273,  -2076.9961],\n",
      "         [ 25361.0117,  11065.0010, -29612.0137,  ...,  -9796.0205,\n",
      "            8765.0088,  82348.0078],\n",
      "         ...,\n",
      "         [ 11075.9961, -77381.9922, -15742.0098,  ...,  -5145.0156,\n",
      "           17441.0078, -13162.0234],\n",
      "         [ 91971.0156,   8041.0039,  -2328.0000,  ...,  -7612.0615,\n",
      "          -11827.0215,   1314.9980],\n",
      "         [-32867.9922,  -2746.0029,  -9850.9727,  ...,  11504.0117,\n",
      "            9001.9922, -24387.9902]]]),) and output (tensor([[[-25670.0000,  13293.9990,   9099.9961,  ...,  -6547.0020,\n",
      "           -5658.0020,   4192.0020],\n",
      "         [ -2464.0059, -22378.0039,  -5376.9834,  ...,   8032.0137,\n",
      "          -26512.0273,   -495.9961],\n",
      "         [ 26076.0117,  16035.0010, -34239.0156,  ..., -22470.0195,\n",
      "            6454.0088,  87776.0078],\n",
      "         ...,\n",
      "         [  8948.9961, -71500.9922, -19254.0098,  ...,  -9888.0156,\n",
      "           16720.0078, -15581.0234],\n",
      "         [ 97281.0156,  -4934.9961,  -2605.0000,  ...,  -2393.0615,\n",
      "          -18662.0215,  -5454.0020],\n",
      "         [-36859.9922,  -7883.0029, -11673.9727,  ...,   1030.0117,\n",
      "            3300.9922, -20324.9902]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[-25670.0000,  13293.9990,   9099.9961,  ...,  -6547.0020,\n",
      "           -5658.0020,   4192.0020],\n",
      "         [ -2464.0059, -22378.0039,  -5376.9834,  ...,   8032.0137,\n",
      "          -26512.0273,   -495.9961],\n",
      "         [ 26076.0117,  16035.0010, -34239.0156,  ..., -22470.0195,\n",
      "            6454.0088,  87776.0078],\n",
      "         ...,\n",
      "         [  8948.9961, -71500.9922, -19254.0098,  ...,  -9888.0156,\n",
      "           16720.0078, -15581.0234],\n",
      "         [ 97281.0156,  -4934.9961,  -2605.0000,  ...,  -2393.0615,\n",
      "          -18662.0215,  -5454.0020],\n",
      "         [-36859.9922,  -7883.0029, -11673.9727,  ...,   1030.0117,\n",
      "            3300.9922, -20324.9902]]]),) and output (tensor([[[-25652.0000,  13180.9990,  15728.9961,  ...,  -2831.0020,\n",
      "             158.9980,   2204.0020],\n",
      "         [  3633.9941, -29672.0039,  -2758.9834,  ...,   2321.0137,\n",
      "          -30669.0273,  -1555.9961],\n",
      "         [ 24201.0117,  12788.0010, -30698.0156,  ..., -26784.0195,\n",
      "           13224.0088,  94482.0078],\n",
      "         ...,\n",
      "         [  -659.0039, -75445.9922, -22200.0098,  ..., -10219.0156,\n",
      "           21670.0078, -14103.0234],\n",
      "         [110381.0156,  -6784.9961,   -292.0000,  ...,   1424.9385,\n",
      "          -34134.0234,  -5548.0020],\n",
      "         [-37901.9922,  -9575.0029, -19076.9727,  ...,  -3279.9883,\n",
      "             908.9922, -25536.9902]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[-25652.0000,  13180.9990,  15728.9961,  ...,  -2831.0020,\n",
      "             158.9980,   2204.0020],\n",
      "         [  3633.9941, -29672.0039,  -2758.9834,  ...,   2321.0137,\n",
      "          -30669.0273,  -1555.9961],\n",
      "         [ 24201.0117,  12788.0010, -30698.0156,  ..., -26784.0195,\n",
      "           13224.0088,  94482.0078],\n",
      "         ...,\n",
      "         [  -659.0039, -75445.9922, -22200.0098,  ..., -10219.0156,\n",
      "           21670.0078, -14103.0234],\n",
      "         [110381.0156,  -6784.9961,   -292.0000,  ...,   1424.9385,\n",
      "          -34134.0234,  -5548.0020],\n",
      "         [-37901.9922,  -9575.0029, -19076.9727,  ...,  -3279.9883,\n",
      "             908.9922, -25536.9902]]]),) and output (tensor([[[-14791.0000,  15299.9990,  16129.9961,  ...,   3187.9980,\n",
      "           -2552.0020,   5626.0020],\n",
      "         [  3507.9941, -32274.0039,   2193.0166,  ...,  -1059.9863,\n",
      "          -33461.0273,  -4446.9961],\n",
      "         [ 18165.0117,  19874.0000, -28686.0156,  ..., -18340.0195,\n",
      "           18971.0078,  90273.0078],\n",
      "         ...,\n",
      "         [-10800.0039, -79713.9922, -15622.0098,  ..., -12641.0156,\n",
      "           22248.0078, -21834.0234],\n",
      "         [112018.0156,  -8066.9961,    629.0000,  ...,  11635.9385,\n",
      "          -37243.0234,  -2222.0020],\n",
      "         [-39865.9922, -13483.0029, -12247.9727,  ..., -11325.9883,\n",
      "            -362.0078, -32753.9902]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[-14791.0000,  15299.9990,  16129.9961,  ...,   3187.9980,\n",
      "           -2552.0020,   5626.0020],\n",
      "         [  3507.9941, -32274.0039,   2193.0166,  ...,  -1059.9863,\n",
      "          -33461.0273,  -4446.9961],\n",
      "         [ 18165.0117,  19874.0000, -28686.0156,  ..., -18340.0195,\n",
      "           18971.0078,  90273.0078],\n",
      "         ...,\n",
      "         [-10800.0039, -79713.9922, -15622.0098,  ..., -12641.0156,\n",
      "           22248.0078, -21834.0234],\n",
      "         [112018.0156,  -8066.9961,    629.0000,  ...,  11635.9385,\n",
      "          -37243.0234,  -2222.0020],\n",
      "         [-39865.9922, -13483.0029, -12247.9727,  ..., -11325.9883,\n",
      "            -362.0078, -32753.9902]]]),) and output (tensor([[[-13284.0000,  26639.0000,   7467.9961,  ...,   4113.9980,\n",
      "           -6608.0020,   5336.0020],\n",
      "         [  3338.9941, -26444.0039,  -2472.9834,  ...,   5251.0137,\n",
      "          -32285.0273,  -3049.9961],\n",
      "         [ 26441.0117,  21825.0000, -28631.0156,  ..., -15470.0195,\n",
      "           17713.0078,  86924.0078],\n",
      "         ...,\n",
      "         [-17101.0039, -80410.9922, -23185.0098,  ...,  -4252.0156,\n",
      "           19039.0078, -22885.0234],\n",
      "         [113827.0156,  -8956.9961,  11099.0000,  ...,  14235.9385,\n",
      "          -39235.0234,  -4841.0020],\n",
      "         [-49662.9922,  -9771.0029, -11308.9727,  ...,  -9699.9883,\n",
      "            1865.9922, -26070.9902]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[-13284.0000,  26639.0000,   7467.9961,  ...,   4113.9980,\n",
      "           -6608.0020,   5336.0020],\n",
      "         [  3338.9941, -26444.0039,  -2472.9834,  ...,   5251.0137,\n",
      "          -32285.0273,  -3049.9961],\n",
      "         [ 26441.0117,  21825.0000, -28631.0156,  ..., -15470.0195,\n",
      "           17713.0078,  86924.0078],\n",
      "         ...,\n",
      "         [-17101.0039, -80410.9922, -23185.0098,  ...,  -4252.0156,\n",
      "           19039.0078, -22885.0234],\n",
      "         [113827.0156,  -8956.9961,  11099.0000,  ...,  14235.9385,\n",
      "          -39235.0234,  -4841.0020],\n",
      "         [-49662.9922,  -9771.0029, -11308.9727,  ...,  -9699.9883,\n",
      "            1865.9922, -26070.9902]]]),) and output (tensor([[[-21532.0000,  32796.0000,  16281.9961,  ...,  18208.9980,\n",
      "           -4282.0020,   5756.0020],\n",
      "         [  9326.9941, -29230.0039,  -8927.9834,  ...,   4327.0137,\n",
      "          -25868.0273,  10131.0039],\n",
      "         [ 22169.0117,  14368.0000, -23980.0156,  ..., -22452.0195,\n",
      "           19539.0078,  77440.0078],\n",
      "         ...,\n",
      "         [-12190.0039, -89476.9922, -19024.0098,  ...,  -2827.0156,\n",
      "           15601.0078, -17118.0234],\n",
      "         [120614.0156,  -9775.9961,  19321.0000,  ...,  12582.9385,\n",
      "          -41999.0234,  -6167.0020],\n",
      "         [-48375.9922,    607.9971, -14404.9727,  ..., -10840.9883,\n",
      "            6562.9922, -22133.9902]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[-21532.0000,  32796.0000,  16281.9961,  ...,  18208.9980,\n",
      "           -4282.0020,   5756.0020],\n",
      "         [  9326.9941, -29230.0039,  -8927.9834,  ...,   4327.0137,\n",
      "          -25868.0273,  10131.0039],\n",
      "         [ 22169.0117,  14368.0000, -23980.0156,  ..., -22452.0195,\n",
      "           19539.0078,  77440.0078],\n",
      "         ...,\n",
      "         [-12190.0039, -89476.9922, -19024.0098,  ...,  -2827.0156,\n",
      "           15601.0078, -17118.0234],\n",
      "         [120614.0156,  -9775.9961,  19321.0000,  ...,  12582.9385,\n",
      "          -41999.0234,  -6167.0020],\n",
      "         [-48375.9922,    607.9971, -14404.9727,  ..., -10840.9883,\n",
      "            6562.9922, -22133.9902]]]),) and output (tensor([[[-2.4082e+04,  3.3387e+04,  1.5375e+04,  ...,  1.8360e+04,\n",
      "          -7.5000e+03,  1.1519e+04],\n",
      "         [ 1.4126e+04, -2.8382e+04, -8.5200e+03,  ...,  3.7650e+03,\n",
      "          -2.9188e+04,  5.1590e+03],\n",
      "         [ 1.5634e+04,  1.8004e+04, -2.4196e+04,  ..., -2.1478e+04,\n",
      "           2.1998e+04,  6.4972e+04],\n",
      "         ...,\n",
      "         [ 1.1500e+02, -7.8970e+04, -1.6016e+04,  ...,  3.7020e+03,\n",
      "           6.4030e+03, -2.0412e+04],\n",
      "         [ 1.2518e+05, -1.7591e+04,  2.4544e+04,  ...,  8.2459e+03,\n",
      "          -4.5528e+04, -1.7212e+04],\n",
      "         [-4.8036e+04,  1.6473e+04, -1.2928e+04,  ..., -1.9810e+04,\n",
      "           5.5299e+02, -2.0872e+04]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[-2.4082e+04,  3.3387e+04,  1.5375e+04,  ...,  1.8360e+04,\n",
      "          -7.5000e+03,  1.1519e+04],\n",
      "         [ 1.4126e+04, -2.8382e+04, -8.5200e+03,  ...,  3.7650e+03,\n",
      "          -2.9188e+04,  5.1590e+03],\n",
      "         [ 1.5634e+04,  1.8004e+04, -2.4196e+04,  ..., -2.1478e+04,\n",
      "           2.1998e+04,  6.4972e+04],\n",
      "         ...,\n",
      "         [ 1.1500e+02, -7.8970e+04, -1.6016e+04,  ...,  3.7020e+03,\n",
      "           6.4030e+03, -2.0412e+04],\n",
      "         [ 1.2518e+05, -1.7591e+04,  2.4544e+04,  ...,  8.2459e+03,\n",
      "          -4.5528e+04, -1.7212e+04],\n",
      "         [-4.8036e+04,  1.6473e+04, -1.2928e+04,  ..., -1.9810e+04,\n",
      "           5.5299e+02, -2.0872e+04]]]),) and output (tensor([[[-14869.0000,  32284.0000,   3742.9961,  ...,  12458.9980,\n",
      "           -5452.0020,   7590.0020],\n",
      "         [ 12419.9941, -17346.0039,   1989.0166,  ...,   6397.0137,\n",
      "          -28220.0273,   4817.0039],\n",
      "         [ 19640.0117,  11113.0000, -14348.0156,  ..., -21413.0195,\n",
      "           23288.0078,  68637.0078],\n",
      "         ...,\n",
      "         [ -8959.0039, -78974.9922, -12763.0098,  ...,   7412.9844,\n",
      "            -789.9922, -18285.0234],\n",
      "         [116601.0156, -17967.9961,  27805.0000,  ...,  13408.9385,\n",
      "          -31717.0234, -20045.0020],\n",
      "         [-42690.9922,  11990.9961,  -9563.9727,  ..., -22798.9883,\n",
      "           -3885.0078, -24773.9902]]]),)\n",
      "[Debug] Layer names being passed: ['model.layers.0', 'model.layers.1', 'model.layers.2', 'model.layers.3', 'model.layers.4', 'model.layers.5', 'model.layers.6', 'model.layers.7', 'model.layers.8', 'model.layers.9', 'model.layers.10', 'model.layers.11', 'model.layers.12', 'model.layers.13', 'model.layers.14', 'model.layers.15', 'model.layers.16', 'model.layers.17', 'model.layers.18', 'model.layers.19', 'model.layers.20', 'model.layers.21', 'model.layers.22', 'model.layers.23', 'model.layers.24', 'model.layers.25', 'model.layers.26', 'model.layers.27', 'model.embed_tokens']\n",
      "[Debug] Trying to access layer: model.layers.0\n",
      "[Debug] Successfully found layer: model.layers.0\n",
      "[Debug] Trying to access layer: model.layers.1\n",
      "[Debug] Successfully found layer: model.layers.1\n",
      "[Debug] Trying to access layer: model.layers.2\n",
      "[Debug] Successfully found layer: model.layers.2\n",
      "[Debug] Trying to access layer: model.layers.3\n",
      "[Debug] Successfully found layer: model.layers.3\n",
      "[Debug] Trying to access layer: model.layers.4\n",
      "[Debug] Successfully found layer: model.layers.4\n",
      "[Debug] Trying to access layer: model.layers.5\n",
      "[Debug] Successfully found layer: model.layers.5\n",
      "[Debug] Trying to access layer: model.layers.6\n",
      "[Debug] Successfully found layer: model.layers.6\n",
      "[Debug] Trying to access layer: model.layers.7\n",
      "[Debug] Successfully found layer: model.layers.7\n",
      "[Debug] Trying to access layer: model.layers.8\n",
      "[Debug] Successfully found layer: model.layers.8\n",
      "[Debug] Trying to access layer: model.layers.9\n",
      "[Debug] Successfully found layer: model.layers.9\n",
      "[Debug] Trying to access layer: model.layers.10\n",
      "[Debug] Successfully found layer: model.layers.10\n",
      "[Debug] Trying to access layer: model.layers.11\n",
      "[Debug] Successfully found layer: model.layers.11\n",
      "[Debug] Trying to access layer: model.layers.12\n",
      "[Debug] Successfully found layer: model.layers.12\n",
      "[Debug] Trying to access layer: model.layers.13\n",
      "[Debug] Successfully found layer: model.layers.13\n",
      "[Debug] Trying to access layer: model.layers.14\n",
      "[Debug] Successfully found layer: model.layers.14\n",
      "[Debug] Trying to access layer: model.layers.15\n",
      "[Debug] Successfully found layer: model.layers.15\n",
      "[Debug] Trying to access layer: model.layers.16\n",
      "[Debug] Successfully found layer: model.layers.16\n",
      "[Debug] Trying to access layer: model.layers.17\n",
      "[Debug] Successfully found layer: model.layers.17\n",
      "[Debug] Trying to access layer: model.layers.18\n",
      "[Debug] Successfully found layer: model.layers.18\n",
      "[Debug] Trying to access layer: model.layers.19\n",
      "[Debug] Successfully found layer: model.layers.19\n",
      "[Debug] Trying to access layer: model.layers.20\n",
      "[Debug] Successfully found layer: model.layers.20\n",
      "[Debug] Trying to access layer: model.layers.21\n",
      "[Debug] Successfully found layer: model.layers.21\n",
      "[Debug] Trying to access layer: model.layers.22\n",
      "[Debug] Successfully found layer: model.layers.22\n",
      "[Debug] Trying to access layer: model.layers.23\n",
      "[Debug] Successfully found layer: model.layers.23\n",
      "[Debug] Trying to access layer: model.layers.24\n",
      "[Debug] Successfully found layer: model.layers.24\n",
      "[Debug] Trying to access layer: model.layers.25\n",
      "[Debug] Successfully found layer: model.layers.25\n",
      "[Debug] Trying to access layer: model.layers.26\n",
      "[Debug] Successfully found layer: model.layers.26\n",
      "[Debug] Trying to access layer: model.layers.27\n",
      "[Debug] Successfully found layer: model.layers.27\n",
      "[Debug] Trying to access layer: model.embed_tokens\n",
      "[Debug] Successfully found layer: model.embed_tokens\n",
      "[Hook] Layer Embedding(128256, 3072, padding_idx=128004) received input (tensor([[128000,     54,   1786,   8121,  39640,   5788,    374,    922,    220,\n",
      "             17,   3610,  52021,    824,   1060,     11,    315,    902,    220,\n",
      "           1399,      4,   5900,   1139,  55425,     13,  47400,  41095,   2211,\n",
      "            220,     18,      4,    315,    279,   3728,  66638,   3157,     11,\n",
      "            719,   1202,    907,    374,   5190,  56612,    311,  23069,    323,\n",
      "           1023,  29882,    315,    279,   3769,   8032,     16,     60,   8494,\n",
      "            374,    264,   6522,  17276,    315,  39640,    902,    374,  10213,\n",
      "            505,   8930,   3394,  33012,    719,    706,   1027,  93534,    291,\n",
      "            555,   5734,    304,   3878,    315,   2860,   4785,   8032,    966,\n",
      "             60,   1561,  17340,    320,    679,     21,      8,    374,    279,\n",
      "           4948,  68067,  17276,    315,  39640,     11,    323,    279,   7928,\n",
      "          17276,    315,   5425,  91842,  39640,     13,  11681,   6910,   1778,\n",
      "            439,  25379,     11,  26386,     11,  31941,  84782,     11,    323,\n",
      "          69518,  35283,   8356,   1080,  10642,  49774,     11,    323,  39640,\n",
      "            505,   1521,  33012,    374,   6118,   1511,    369,   3339,  89341,\n",
      "             13]]),) and output tensor([[[-0.0010, -0.0007, -0.0046,  ..., -0.0014, -0.0020,  0.0020],\n",
      "         [ 0.0067,  0.0116, -0.0062,  ..., -0.0054,  0.0166, -0.0060],\n",
      "         [-0.0408, -0.0092, -0.0089,  ...,  0.0300,  0.0062, -0.0466],\n",
      "         ...,\n",
      "         [-0.0322,  0.0135, -0.0056,  ...,  0.0084,  0.0181, -0.0050],\n",
      "         [ 0.0081,  0.0208,  0.0437,  ..., -0.0011,  0.0086,  0.0311],\n",
      "         [ 0.0093, -0.0031,  0.0278,  ...,  0.0117, -0.0084,  0.0096]]])\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[-0.0010, -0.0007, -0.0046,  ..., -0.0014, -0.0020,  0.0020],\n",
      "         [ 0.0067,  0.0116, -0.0062,  ..., -0.0054,  0.0166, -0.0060],\n",
      "         [-0.0408, -0.0092, -0.0089,  ...,  0.0300,  0.0062, -0.0466],\n",
      "         ...,\n",
      "         [-0.0322,  0.0135, -0.0056,  ...,  0.0084,  0.0181, -0.0050],\n",
      "         [ 0.0081,  0.0208,  0.0437,  ..., -0.0011,  0.0086,  0.0311],\n",
      "         [ 0.0093, -0.0031,  0.0278,  ...,  0.0117, -0.0084,  0.0096]]]),) and output (tensor([[[ 4.4300e+02,  2.4000e+02,  3.0300e+02,  ..., -1.5400e+02,\n",
      "          -1.5002e+01,  1.8500e+02],\n",
      "         [ 8.5007e+01,  8.6012e+01, -1.3001e+02,  ...,  1.9946e+00,\n",
      "          -3.6983e+01,  8.9940e+00],\n",
      "         [-1.4104e+02, -1.5801e+02, -8.3009e+01,  ..., -2.2970e+01,\n",
      "           6.1951e-03,  8.0953e+01],\n",
      "         ...,\n",
      "         [ 1.9968e+01, -3.7986e+01, -1.2201e+02,  ..., -4.2992e+01,\n",
      "          -1.0998e+02,  2.6995e+01],\n",
      "         [ 5.5008e+01, -1.0979e+01,  4.5044e+01,  ..., -1.8000e+02,\n",
      "          -7.1991e+01, -8.8969e+01],\n",
      "         [-8.8991e+01, -2.9003e+01,  6.6028e+01,  ..., -1.1988e+01,\n",
      "          -7.0008e+01, -1.4990e+01]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[ 4.4300e+02,  2.4000e+02,  3.0300e+02,  ..., -1.5400e+02,\n",
      "          -1.5002e+01,  1.8500e+02],\n",
      "         [ 8.5007e+01,  8.6012e+01, -1.3001e+02,  ...,  1.9946e+00,\n",
      "          -3.6983e+01,  8.9940e+00],\n",
      "         [-1.4104e+02, -1.5801e+02, -8.3009e+01,  ..., -2.2970e+01,\n",
      "           6.1951e-03,  8.0953e+01],\n",
      "         ...,\n",
      "         [ 1.9968e+01, -3.7986e+01, -1.2201e+02,  ..., -4.2992e+01,\n",
      "          -1.0998e+02,  2.6995e+01],\n",
      "         [ 5.5008e+01, -1.0979e+01,  4.5044e+01,  ..., -1.8000e+02,\n",
      "          -7.1991e+01, -8.8969e+01],\n",
      "         [-8.8991e+01, -2.9003e+01,  6.6028e+01,  ..., -1.1988e+01,\n",
      "          -7.0008e+01, -1.4990e+01]]]),) and output (tensor([[[  439.9990,  -144.0007,  -410.0046,  ...,  -765.0015,\n",
      "           1193.9979, -1027.9980],\n",
      "         [  851.0067,  -140.9884,  -552.0062,  ...,   -15.0054,\n",
      "           1236.0166,  -592.0060],\n",
      "         [ -498.0408,  -266.0092,   241.9911,  ...,  -418.9700,\n",
      "            141.0062,   359.9534],\n",
      "         ...,\n",
      "         [ -208.0322,  -118.9865, -1455.0056,  ...,  -306.9916,\n",
      "             12.0181,   442.9950],\n",
      "         [ -214.9919,   660.0208,  -338.9563,  ...,   123.9989,\n",
      "            764.0086,   316.0311],\n",
      "         [ -147.9907,  -231.0031,  -620.9722,  ...,  -444.9883,\n",
      "           -645.0084,  -106.9904]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[  439.9990,  -144.0007,  -410.0046,  ...,  -765.0015,\n",
      "           1193.9979, -1027.9980],\n",
      "         [  851.0067,  -140.9884,  -552.0062,  ...,   -15.0054,\n",
      "           1236.0166,  -592.0060],\n",
      "         [ -498.0408,  -266.0092,   241.9911,  ...,  -418.9700,\n",
      "            141.0062,   359.9534],\n",
      "         ...,\n",
      "         [ -208.0322,  -118.9865, -1455.0056,  ...,  -306.9916,\n",
      "             12.0181,   442.9950],\n",
      "         [ -214.9919,   660.0208,  -338.9563,  ...,   123.9989,\n",
      "            764.0086,   316.0311],\n",
      "         [ -147.9907,  -231.0031,  -620.9722,  ...,  -444.9883,\n",
      "           -645.0084,  -106.9904]]]),) and output (tensor([[[  551.9990,   993.9993, -1790.0046,  ...,   445.9985,\n",
      "          -1386.0021, -1245.9980],\n",
      "         [ 3987.0066,  1325.0116,  -842.0062,  ...,  3824.9946,\n",
      "           1476.0166, -3806.0059],\n",
      "         [ 2965.9592,  -492.0093,  1569.9911,  ...,   901.0300,\n",
      "            887.0062,  -895.0466],\n",
      "         ...,\n",
      "         [-1425.0322, -2350.9863, -1044.0056,  ...,  -931.9916,\n",
      "           -925.9819,    49.9950],\n",
      "         [ 1543.0081,  3210.0208, -1905.9563,  ...,  3722.9990,\n",
      "           1567.0085,   278.0311],\n",
      "         [-3615.9907,   405.9969, -4484.9722,  ...,  -896.9883,\n",
      "          -1157.0083,  2053.0098]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[  551.9990,   993.9993, -1790.0046,  ...,   445.9985,\n",
      "          -1386.0021, -1245.9980],\n",
      "         [ 3987.0066,  1325.0116,  -842.0062,  ...,  3824.9946,\n",
      "           1476.0166, -3806.0059],\n",
      "         [ 2965.9592,  -492.0093,  1569.9911,  ...,   901.0300,\n",
      "            887.0062,  -895.0466],\n",
      "         ...,\n",
      "         [-1425.0322, -2350.9863, -1044.0056,  ...,  -931.9916,\n",
      "           -925.9819,    49.9950],\n",
      "         [ 1543.0081,  3210.0208, -1905.9563,  ...,  3722.9990,\n",
      "           1567.0085,   278.0311],\n",
      "         [-3615.9907,   405.9969, -4484.9722,  ...,  -896.9883,\n",
      "          -1157.0083,  2053.0098]]]),) and output (tensor([[[ 2131.9990,  -737.0007, -1071.0046,  ..., -2781.0015,\n",
      "           -885.0021,    86.0020],\n",
      "         [ 2439.0066,  1918.0116,   247.9939,  ...,  4567.9946,\n",
      "           4352.0166, -1635.0059],\n",
      "         [ 8542.9590, -2710.0093,  4725.9912,  ...,   570.0300,\n",
      "           3245.0063,    54.9534],\n",
      "         ...,\n",
      "         [ 1451.9678, -2187.9863,   545.9944,  ...,   922.0084,\n",
      "          -6653.9819, -2376.0051],\n",
      "         [-1588.9919, -4123.9795, -4134.9561,  ...,   704.9990,\n",
      "           2230.0085,  -829.9689],\n",
      "         [ 1634.0093, -5240.0029, -4508.9722,  ...,  -536.9883,\n",
      "          -4635.0083,  4822.0098]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[ 2131.9990,  -737.0007, -1071.0046,  ..., -2781.0015,\n",
      "           -885.0021,    86.0020],\n",
      "         [ 2439.0066,  1918.0116,   247.9939,  ...,  4567.9946,\n",
      "           4352.0166, -1635.0059],\n",
      "         [ 8542.9590, -2710.0093,  4725.9912,  ...,   570.0300,\n",
      "           3245.0063,    54.9534],\n",
      "         ...,\n",
      "         [ 1451.9678, -2187.9863,   545.9944,  ...,   922.0084,\n",
      "          -6653.9819, -2376.0051],\n",
      "         [-1588.9919, -4123.9795, -4134.9561,  ...,   704.9990,\n",
      "           2230.0085,  -829.9689],\n",
      "         [ 1634.0093, -5240.0029, -4508.9722,  ...,  -536.9883,\n",
      "          -4635.0083,  4822.0098]]]),) and output (tensor([[[ 3420.9990,   760.9993, -5376.0049,  ..., -4193.0015,\n",
      "           2860.9980, -4080.9980],\n",
      "         [ 7058.0068,   501.0117,  1371.9939,  ...,  4617.9946,\n",
      "           5867.0166, -3899.0059],\n",
      "         [12810.9590, -5151.0093,  4110.9912,  ...,   -81.9700,\n",
      "           1374.0063, -7072.0469],\n",
      "         ...,\n",
      "         [ 2583.9678,  4530.0137, -2966.0056,  ..., -1303.9916,\n",
      "          -4756.9819, -1496.0051],\n",
      "         [-4905.9922,   893.0205, -3145.9561,  ..., -1055.0010,\n",
      "          -2584.9915, -3669.9688],\n",
      "         [-2210.9907, -7824.0029,  -663.9722,  ...,  1980.0117,\n",
      "          -4586.0083,   562.0098]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[ 3420.9990,   760.9993, -5376.0049,  ..., -4193.0015,\n",
      "           2860.9980, -4080.9980],\n",
      "         [ 7058.0068,   501.0117,  1371.9939,  ...,  4617.9946,\n",
      "           5867.0166, -3899.0059],\n",
      "         [12810.9590, -5151.0093,  4110.9912,  ...,   -81.9700,\n",
      "           1374.0063, -7072.0469],\n",
      "         ...,\n",
      "         [ 2583.9678,  4530.0137, -2966.0056,  ..., -1303.9916,\n",
      "          -4756.9819, -1496.0051],\n",
      "         [-4905.9922,   893.0205, -3145.9561,  ..., -1055.0010,\n",
      "          -2584.9915, -3669.9688],\n",
      "         [-2210.9907, -7824.0029,  -663.9722,  ...,  1980.0117,\n",
      "          -4586.0083,   562.0098]]]),) and output (tensor([[[  2742.9990,  -1512.0007, -10493.0049,  ...,  -9945.0020,\n",
      "           -2017.0020,  -1802.9980],\n",
      "         [  6229.0068,   1672.0117,  -3233.0061,  ...,   3400.9946,\n",
      "            6949.0166,  -2866.0059],\n",
      "         [ 13612.9590, -12275.0098,    260.9912,  ...,  -3171.9700,\n",
      "            2073.0063,  -1758.0469],\n",
      "         ...,\n",
      "         [  5902.9678,   8024.0137,  -2402.0056,  ...,  -1162.9916,\n",
      "           -4397.9819,  -6162.0049],\n",
      "         [ -4485.9922,  -1456.9795,  -4898.9561,  ...,  -7106.0010,\n",
      "             -52.9915,  -4653.9688],\n",
      "         [ -1571.9907,  -5418.0029,  -5620.9722,  ...,   6035.0117,\n",
      "            2787.9917,  -1731.9902]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[  2742.9990,  -1512.0007, -10493.0049,  ...,  -9945.0020,\n",
      "           -2017.0020,  -1802.9980],\n",
      "         [  6229.0068,   1672.0117,  -3233.0061,  ...,   3400.9946,\n",
      "            6949.0166,  -2866.0059],\n",
      "         [ 13612.9590, -12275.0098,    260.9912,  ...,  -3171.9700,\n",
      "            2073.0063,  -1758.0469],\n",
      "         ...,\n",
      "         [  5902.9678,   8024.0137,  -2402.0056,  ...,  -1162.9916,\n",
      "           -4397.9819,  -6162.0049],\n",
      "         [ -4485.9922,  -1456.9795,  -4898.9561,  ...,  -7106.0010,\n",
      "             -52.9915,  -4653.9688],\n",
      "         [ -1571.9907,  -5418.0029,  -5620.9722,  ...,   6035.0117,\n",
      "            2787.9917,  -1731.9902]]]),) and output (tensor([[[  3032.9990,  -3420.0007, -10291.0049,  ...,  -6511.0020,\n",
      "           -1953.0020,   2001.0020],\n",
      "         [  9961.0068,   4387.0117,  -5390.0059,  ...,   2388.9946,\n",
      "            9664.0166,  -6948.0059],\n",
      "         [ 12684.9590, -12501.0098,  -2600.0088,  ...,  -2443.9700,\n",
      "           -3219.9937,  -8736.0469],\n",
      "         ...,\n",
      "         [  8651.9678,    735.0137,  -5971.0059,  ...,  -3185.9917,\n",
      "           -9438.9824,  -8646.0049],\n",
      "         [   396.0078,   2414.0205,  -8847.9561,  ...,  -4460.0010,\n",
      "           -2806.9915, -12436.9688],\n",
      "         [  2807.0093,  -2988.0029,  -5066.9722,  ...,   9360.0117,\n",
      "            6634.9917,  -2282.9902]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[  3032.9990,  -3420.0007, -10291.0049,  ...,  -6511.0020,\n",
      "           -1953.0020,   2001.0020],\n",
      "         [  9961.0068,   4387.0117,  -5390.0059,  ...,   2388.9946,\n",
      "            9664.0166,  -6948.0059],\n",
      "         [ 12684.9590, -12501.0098,  -2600.0088,  ...,  -2443.9700,\n",
      "           -3219.9937,  -8736.0469],\n",
      "         ...,\n",
      "         [  8651.9678,    735.0137,  -5971.0059,  ...,  -3185.9917,\n",
      "           -9438.9824,  -8646.0049],\n",
      "         [   396.0078,   2414.0205,  -8847.9561,  ...,  -4460.0010,\n",
      "           -2806.9915, -12436.9688],\n",
      "         [  2807.0093,  -2988.0029,  -5066.9722,  ...,   9360.0117,\n",
      "            6634.9917,  -2282.9902]]]),) and output (tensor([[[  3132.9990,  -1970.0007, -11544.0049,  ...,  -5780.0020,\n",
      "           -2486.0020,  -1921.9980],\n",
      "         [ 12994.0068,   6095.0117,  -7043.0059,  ...,   2176.9946,\n",
      "           10653.0166,  -5791.0059],\n",
      "         [ 10773.9590, -10555.0098,  -5099.0088,  ...,   2988.0300,\n",
      "           -3858.9937,  -4319.0469],\n",
      "         ...,\n",
      "         [  3600.9678,    488.0137,  -3354.0059,  ...,   -922.9917,\n",
      "           -7205.9824,  -5817.0049],\n",
      "         [  5363.0078,  -3613.9795, -11560.9561,  ...,  -6043.0010,\n",
      "           -1068.9915,  -8922.9688],\n",
      "         [    75.0093,  -1166.0029,  -4044.9722,  ...,  13564.0117,\n",
      "            3407.9917, -11494.9902]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[  3132.9990,  -1970.0007, -11544.0049,  ...,  -5780.0020,\n",
      "           -2486.0020,  -1921.9980],\n",
      "         [ 12994.0068,   6095.0117,  -7043.0059,  ...,   2176.9946,\n",
      "           10653.0166,  -5791.0059],\n",
      "         [ 10773.9590, -10555.0098,  -5099.0088,  ...,   2988.0300,\n",
      "           -3858.9937,  -4319.0469],\n",
      "         ...,\n",
      "         [  3600.9678,    488.0137,  -3354.0059,  ...,   -922.9917,\n",
      "           -7205.9824,  -5817.0049],\n",
      "         [  5363.0078,  -3613.9795, -11560.9561,  ...,  -6043.0010,\n",
      "           -1068.9915,  -8922.9688],\n",
      "         [    75.0093,  -1166.0029,  -4044.9722,  ...,  13564.0117,\n",
      "            3407.9917, -11494.9902]]]),) and output (tensor([[[  1046.9990,   2403.9993, -14486.0049,  ..., -10922.0020,\n",
      "           -1419.0020,  -2983.9980],\n",
      "         [ 10905.0068,   1422.0117, -11190.0059,  ...,     33.9946,\n",
      "           13172.0166,  -1883.0059],\n",
      "         [  9288.9590,  -8382.0098,  -8303.0088,  ...,   4142.0303,\n",
      "          -11510.9941,  -3267.0469],\n",
      "         ...,\n",
      "         [   -59.0322,   3718.0137,  -9047.0059,  ...,   5261.0083,\n",
      "           -5226.9824,  -6021.0049],\n",
      "         [  4210.0078,    661.0205,  -9601.9561,  ...,  -5093.0010,\n",
      "           -5479.9912, -13507.9688],\n",
      "         [  2104.0093,   3700.9971,  -8481.9727,  ...,  15280.0117,\n",
      "            6369.9917, -12196.9902]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[  1046.9990,   2403.9993, -14486.0049,  ..., -10922.0020,\n",
      "           -1419.0020,  -2983.9980],\n",
      "         [ 10905.0068,   1422.0117, -11190.0059,  ...,     33.9946,\n",
      "           13172.0166,  -1883.0059],\n",
      "         [  9288.9590,  -8382.0098,  -8303.0088,  ...,   4142.0303,\n",
      "          -11510.9941,  -3267.0469],\n",
      "         ...,\n",
      "         [   -59.0322,   3718.0137,  -9047.0059,  ...,   5261.0083,\n",
      "           -5226.9824,  -6021.0049],\n",
      "         [  4210.0078,    661.0205,  -9601.9561,  ...,  -5093.0010,\n",
      "           -5479.9912, -13507.9688],\n",
      "         [  2104.0093,   3700.9971,  -8481.9727,  ...,  15280.0117,\n",
      "            6369.9917, -12196.9902]]]),) and output (tensor([[[ -3767.0010,   5307.9990, -14221.0049,  ..., -15035.0020,\n",
      "           -3630.0020,   4168.0020],\n",
      "         [  7727.0068,   3185.0117, -12330.0059,  ...,    532.9946,\n",
      "           12257.0166,  -2084.0059],\n",
      "         [  5910.9590,  -8251.0098,  -6995.0088,  ...,    -73.9697,\n",
      "          -14962.9941,  -3650.0469],\n",
      "         ...,\n",
      "         [ -7862.0322,  -1659.9863,  -5701.0059,  ...,    871.0083,\n",
      "           -7344.9824,  -4915.0049],\n",
      "         [  1687.0078,  -2749.9795, -11142.9561,  ...,  -5091.0010,\n",
      "           -7494.9912, -12416.9688],\n",
      "         [  -138.9907,   4181.9971,  -7840.9727,  ...,  12148.0117,\n",
      "            1972.9917,  -8301.9902]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[ -3767.0010,   5307.9990, -14221.0049,  ..., -15035.0020,\n",
      "           -3630.0020,   4168.0020],\n",
      "         [  7727.0068,   3185.0117, -12330.0059,  ...,    532.9946,\n",
      "           12257.0166,  -2084.0059],\n",
      "         [  5910.9590,  -8251.0098,  -6995.0088,  ...,    -73.9697,\n",
      "          -14962.9941,  -3650.0469],\n",
      "         ...,\n",
      "         [ -7862.0322,  -1659.9863,  -5701.0059,  ...,    871.0083,\n",
      "           -7344.9824,  -4915.0049],\n",
      "         [  1687.0078,  -2749.9795, -11142.9561,  ...,  -5091.0010,\n",
      "           -7494.9912, -12416.9688],\n",
      "         [  -138.9907,   4181.9971,  -7840.9727,  ...,  12148.0117,\n",
      "            1972.9917,  -8301.9902]]]),) and output (tensor([[[-1.3260e+03,  1.2240e+03, -1.6643e+04,  ..., -1.3240e+04,\n",
      "          -5.6770e+03,  9.6390e+03],\n",
      "         [ 1.3382e+04,  4.2810e+03, -1.1032e+04,  ..., -2.2790e+03,\n",
      "           1.1086e+04, -3.0670e+03],\n",
      "         [ 1.0064e+04, -1.1470e+04, -6.7050e+03,  ..., -1.7380e+03,\n",
      "          -2.0855e+04,  3.0750e+03],\n",
      "         ...,\n",
      "         [-5.5803e+02,  2.1270e+03, -1.0180e+04,  ..., -1.7010e+03,\n",
      "          -1.1340e+03, -3.6190e+03],\n",
      "         [-2.2600e+03,  3.5210e+03, -8.4310e+03,  ..., -4.1710e+03,\n",
      "          -5.5300e+03, -1.2548e+04],\n",
      "         [ 3.0093e+00,  1.3210e+03, -8.4650e+03,  ...,  1.4141e+04,\n",
      "          -8.1801e+02, -8.5060e+03]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[-1.3260e+03,  1.2240e+03, -1.6643e+04,  ..., -1.3240e+04,\n",
      "          -5.6770e+03,  9.6390e+03],\n",
      "         [ 1.3382e+04,  4.2810e+03, -1.1032e+04,  ..., -2.2790e+03,\n",
      "           1.1086e+04, -3.0670e+03],\n",
      "         [ 1.0064e+04, -1.1470e+04, -6.7050e+03,  ..., -1.7380e+03,\n",
      "          -2.0855e+04,  3.0750e+03],\n",
      "         ...,\n",
      "         [-5.5803e+02,  2.1270e+03, -1.0180e+04,  ..., -1.7010e+03,\n",
      "          -1.1340e+03, -3.6190e+03],\n",
      "         [-2.2600e+03,  3.5210e+03, -8.4310e+03,  ..., -4.1710e+03,\n",
      "          -5.5300e+03, -1.2548e+04],\n",
      "         [ 3.0093e+00,  1.3210e+03, -8.4650e+03,  ...,  1.4141e+04,\n",
      "          -8.1801e+02, -8.5060e+03]]]),) and output (tensor([[[ -5120.0010,   2246.9990, -15993.0039,  ..., -19818.0020,\n",
      "           -9385.0020,  11924.0020],\n",
      "         [ 11644.0068,  12528.0117,  -9634.0059,  ...,   -318.0054,\n",
      "           14923.0166,  -6532.0059],\n",
      "         [  6741.9590, -12659.0098,  -3446.0088,  ...,  -2798.9697,\n",
      "          -21559.9941,  -1553.0469],\n",
      "         ...,\n",
      "         [ -4649.0322,   -818.9863,  -6236.0059,  ...,  -1875.9917,\n",
      "            4320.0176,  -1425.0049],\n",
      "         [ -5352.9922,   -588.9795,  -9732.9561,  ...,  -2311.0010,\n",
      "           -6941.9912, -15767.9688],\n",
      "         [   722.0093,   3691.9971, -10999.9727,  ...,  16202.0117,\n",
      "            4410.9917,  -3253.9902]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[ -5120.0010,   2246.9990, -15993.0039,  ..., -19818.0020,\n",
      "           -9385.0020,  11924.0020],\n",
      "         [ 11644.0068,  12528.0117,  -9634.0059,  ...,   -318.0054,\n",
      "           14923.0166,  -6532.0059],\n",
      "         [  6741.9590, -12659.0098,  -3446.0088,  ...,  -2798.9697,\n",
      "          -21559.9941,  -1553.0469],\n",
      "         ...,\n",
      "         [ -4649.0322,   -818.9863,  -6236.0059,  ...,  -1875.9917,\n",
      "            4320.0176,  -1425.0049],\n",
      "         [ -5352.9922,   -588.9795,  -9732.9561,  ...,  -2311.0010,\n",
      "           -6941.9912, -15767.9688],\n",
      "         [   722.0093,   3691.9971, -10999.9727,  ...,  16202.0117,\n",
      "            4410.9917,  -3253.9902]]]),) and output (tensor([[[ -6685.0010,  -1278.0010, -15553.0039,  ..., -19846.0020,\n",
      "          -10418.0020,  13376.0020],\n",
      "         [ 14258.0068,  15620.0117,  -8455.0059,  ...,   1875.9946,\n",
      "           11801.0166,  -7169.0059],\n",
      "         [ 11292.9590, -14855.0098,  -7953.0088,  ...,    141.0303,\n",
      "          -25752.9941,    384.9531],\n",
      "         ...,\n",
      "         [   359.9678,   -684.9863,  -7620.0059,  ...,    542.0083,\n",
      "            4505.0176,  -3866.0049],\n",
      "         [ -6072.9922,   1108.0205, -13280.9561,  ...,   4881.9990,\n",
      "           -2194.9912, -15533.9688],\n",
      "         [  1040.0093,   6883.9971, -11365.9727,  ...,  18005.0117,\n",
      "            6463.9917,  -3057.9902]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[ -6685.0010,  -1278.0010, -15553.0039,  ..., -19846.0020,\n",
      "          -10418.0020,  13376.0020],\n",
      "         [ 14258.0068,  15620.0117,  -8455.0059,  ...,   1875.9946,\n",
      "           11801.0166,  -7169.0059],\n",
      "         [ 11292.9590, -14855.0098,  -7953.0088,  ...,    141.0303,\n",
      "          -25752.9941,    384.9531],\n",
      "         ...,\n",
      "         [   359.9678,   -684.9863,  -7620.0059,  ...,    542.0083,\n",
      "            4505.0176,  -3866.0049],\n",
      "         [ -6072.9922,   1108.0205, -13280.9561,  ...,   4881.9990,\n",
      "           -2194.9912, -15533.9688],\n",
      "         [  1040.0093,   6883.9971, -11365.9727,  ...,  18005.0117,\n",
      "            6463.9917,  -3057.9902]]]),) and output (tensor([[[ -8422.0010,    339.9990, -15289.0039,  ..., -20884.0020,\n",
      "           -6908.0020,  11014.0020],\n",
      "         [ 17455.0078,  11432.0117, -13927.0059,  ...,   2490.9946,\n",
      "           11263.0166, -13322.0059],\n",
      "         [  8362.9590, -12948.0098,  -9764.0088,  ...,  -4212.9697,\n",
      "          -21537.9941,   2659.9531],\n",
      "         ...,\n",
      "         [  3277.9678,  -7287.9863,  -8820.0059,  ...,  -1335.9917,\n",
      "           13144.0176,   -973.0049],\n",
      "         [ -6008.9922,   4286.0205, -10452.9561,  ...,   6015.9990,\n",
      "           -5626.9912, -16716.9688],\n",
      "         [ -2047.9907,   3678.9971, -13611.9727,  ...,  24792.0117,\n",
      "            3079.9917,  -1578.9902]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[ -8422.0010,    339.9990, -15289.0039,  ..., -20884.0020,\n",
      "           -6908.0020,  11014.0020],\n",
      "         [ 17455.0078,  11432.0117, -13927.0059,  ...,   2490.9946,\n",
      "           11263.0166, -13322.0059],\n",
      "         [  8362.9590, -12948.0098,  -9764.0088,  ...,  -4212.9697,\n",
      "          -21537.9941,   2659.9531],\n",
      "         ...,\n",
      "         [  3277.9678,  -7287.9863,  -8820.0059,  ...,  -1335.9917,\n",
      "           13144.0176,   -973.0049],\n",
      "         [ -6008.9922,   4286.0205, -10452.9561,  ...,   6015.9990,\n",
      "           -5626.9912, -16716.9688],\n",
      "         [ -2047.9907,   3678.9971, -13611.9727,  ...,  24792.0117,\n",
      "            3079.9917,  -1578.9902]]]),) and output (tensor([[[-10989.0010,   7034.9990, -13964.0039,  ..., -23968.0020,\n",
      "           -3749.0020,  11730.0020],\n",
      "         [ 19349.0078,  21714.0117, -16611.0059,  ...,   8494.9941,\n",
      "           12984.0166, -10663.0059],\n",
      "         [ 15368.9590, -17636.0098, -14014.0088,  ...,    393.0303,\n",
      "          -19370.9941,   2141.9531],\n",
      "         ...,\n",
      "         [  2600.9678,  -1271.9863, -15004.0059,  ...,  -2825.9917,\n",
      "           11948.0176,  -6861.0049],\n",
      "         [ -1776.9922,  -4294.9795, -11270.9561,  ...,  11392.9990,\n",
      "           -1508.9912, -23654.9688],\n",
      "         [ -1931.9907,   3258.9971,  -4452.9727,  ...,  29313.0117,\n",
      "            2664.9917,  -4524.9902]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[-10989.0010,   7034.9990, -13964.0039,  ..., -23968.0020,\n",
      "           -3749.0020,  11730.0020],\n",
      "         [ 19349.0078,  21714.0117, -16611.0059,  ...,   8494.9941,\n",
      "           12984.0166, -10663.0059],\n",
      "         [ 15368.9590, -17636.0098, -14014.0088,  ...,    393.0303,\n",
      "          -19370.9941,   2141.9531],\n",
      "         ...,\n",
      "         [  2600.9678,  -1271.9863, -15004.0059,  ...,  -2825.9917,\n",
      "           11948.0176,  -6861.0049],\n",
      "         [ -1776.9922,  -4294.9795, -11270.9561,  ...,  11392.9990,\n",
      "           -1508.9912, -23654.9688],\n",
      "         [ -1931.9907,   3258.9971,  -4452.9727,  ...,  29313.0117,\n",
      "            2664.9917,  -4524.9902]]]),) and output (tensor([[[ -7159.0010,   6621.9990, -10967.0039,  ..., -17477.0020,\n",
      "           -2510.0020,  11888.0020],\n",
      "         [ 22520.0078,  14193.0117, -11568.0059,  ...,  14140.9941,\n",
      "           14987.0166,  -7020.0059],\n",
      "         [ 10296.9590, -20108.0098, -16125.0088,  ...,   1223.0303,\n",
      "          -19186.9941,  -5655.0469],\n",
      "         ...,\n",
      "         [  6860.9678,   5305.0137, -19269.0059,  ...,  -2073.9917,\n",
      "            1355.0176,  -4837.0049],\n",
      "         [  8372.0078,  -8153.9795, -16626.9570,  ...,   9737.9990,\n",
      "           -4316.9912, -24733.9688],\n",
      "         [  1364.0093,   6773.9971,  -3837.9727,  ...,  37020.0117,\n",
      "            2847.9917,  -3012.9902]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[ -7159.0010,   6621.9990, -10967.0039,  ..., -17477.0020,\n",
      "           -2510.0020,  11888.0020],\n",
      "         [ 22520.0078,  14193.0117, -11568.0059,  ...,  14140.9941,\n",
      "           14987.0166,  -7020.0059],\n",
      "         [ 10296.9590, -20108.0098, -16125.0088,  ...,   1223.0303,\n",
      "          -19186.9941,  -5655.0469],\n",
      "         ...,\n",
      "         [  6860.9678,   5305.0137, -19269.0059,  ...,  -2073.9917,\n",
      "            1355.0176,  -4837.0049],\n",
      "         [  8372.0078,  -8153.9795, -16626.9570,  ...,   9737.9990,\n",
      "           -4316.9912, -24733.9688],\n",
      "         [  1364.0093,   6773.9971,  -3837.9727,  ...,  37020.0117,\n",
      "            2847.9917,  -3012.9902]]]),) and output (tensor([[[ -4822.0010,   3124.9990,  -3885.0039,  ..., -19007.0020,\n",
      "            -551.0020,  16285.0020],\n",
      "         [ 22314.0078,  14711.0117,  -7888.0059,  ...,  16427.9941,\n",
      "           12656.0166,  -1586.0059],\n",
      "         [  2163.9590, -14643.0098, -15238.0088,  ...,    331.0303,\n",
      "          -22345.9941,  -3167.0469],\n",
      "         ...,\n",
      "         [ -3373.0322,   3408.0137, -20415.0059,  ...,  -2942.9917,\n",
      "            2426.0176,   -427.0049],\n",
      "         [ 11014.0078,  -5693.9795, -18495.9570,  ...,  15664.9990,\n",
      "           -6214.9912, -21668.9688],\n",
      "         [  3075.0093,   8653.9971,  -3212.9727,  ...,  44233.0117,\n",
      "             875.9917,  -3841.9902]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[ -4822.0010,   3124.9990,  -3885.0039,  ..., -19007.0020,\n",
      "            -551.0020,  16285.0020],\n",
      "         [ 22314.0078,  14711.0117,  -7888.0059,  ...,  16427.9941,\n",
      "           12656.0166,  -1586.0059],\n",
      "         [  2163.9590, -14643.0098, -15238.0088,  ...,    331.0303,\n",
      "          -22345.9941,  -3167.0469],\n",
      "         ...,\n",
      "         [ -3373.0322,   3408.0137, -20415.0059,  ...,  -2942.9917,\n",
      "            2426.0176,   -427.0049],\n",
      "         [ 11014.0078,  -5693.9795, -18495.9570,  ...,  15664.9990,\n",
      "           -6214.9912, -21668.9688],\n",
      "         [  3075.0093,   8653.9971,  -3212.9727,  ...,  44233.0117,\n",
      "             875.9917,  -3841.9902]]]),) and output (tensor([[[ -7235.0010,   6583.9990,  -4212.0039,  ..., -19104.0020,\n",
      "            3087.9980,  17084.0020],\n",
      "         [ 24519.0078,  15482.0117, -12325.0059,  ...,  11566.9941,\n",
      "           12082.0166,   3850.9941],\n",
      "         [  9823.9590, -15099.0098, -11700.0088,  ...,   3025.0303,\n",
      "          -26784.9941,   -661.0469],\n",
      "         ...,\n",
      "         [-12459.0322,  -1452.9863, -20714.0059,  ...,  -7032.9917,\n",
      "            4212.0176,   6846.9951],\n",
      "         [  9063.0078,  -9719.9795, -20504.9570,  ...,  20381.0000,\n",
      "           -9716.9912, -27311.9688],\n",
      "         [   899.0093,  11993.9971,   -731.9727,  ...,  41198.0117,\n",
      "           11334.9922,  -9366.9902]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[ -7235.0010,   6583.9990,  -4212.0039,  ..., -19104.0020,\n",
      "            3087.9980,  17084.0020],\n",
      "         [ 24519.0078,  15482.0117, -12325.0059,  ...,  11566.9941,\n",
      "           12082.0166,   3850.9941],\n",
      "         [  9823.9590, -15099.0098, -11700.0088,  ...,   3025.0303,\n",
      "          -26784.9941,   -661.0469],\n",
      "         ...,\n",
      "         [-12459.0322,  -1452.9863, -20714.0059,  ...,  -7032.9917,\n",
      "            4212.0176,   6846.9951],\n",
      "         [  9063.0078,  -9719.9795, -20504.9570,  ...,  20381.0000,\n",
      "           -9716.9912, -27311.9688],\n",
      "         [   899.0093,  11993.9971,   -731.9727,  ...,  41198.0117,\n",
      "           11334.9922,  -9366.9902]]]),) and output (tensor([[[ -5586.0010,   7645.9990,   -505.0039,  ..., -15346.0020,\n",
      "            -623.0020,  12986.0020],\n",
      "         [ 29496.0078,  14297.0117,  -8632.0059,  ...,  11193.9941,\n",
      "           10331.0166,   1825.9941],\n",
      "         [  7284.9590, -12821.0098,  -8777.0088,  ...,  -2410.9697,\n",
      "          -30101.9941,   3387.9531],\n",
      "         ...,\n",
      "         [ -6868.0322,  -3229.9863, -14594.0059,  ...,  -3331.9917,\n",
      "             406.0176,   5543.9951],\n",
      "         [  8672.0078, -14036.9795, -14705.9570,  ...,  14500.0000,\n",
      "           -4060.9912, -25866.9688],\n",
      "         [ -1488.9907,  16133.9971,    569.0273,  ...,  45757.0117,\n",
      "           16311.9922,  -9416.9902]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[ -5586.0010,   7645.9990,   -505.0039,  ..., -15346.0020,\n",
      "            -623.0020,  12986.0020],\n",
      "         [ 29496.0078,  14297.0117,  -8632.0059,  ...,  11193.9941,\n",
      "           10331.0166,   1825.9941],\n",
      "         [  7284.9590, -12821.0098,  -8777.0088,  ...,  -2410.9697,\n",
      "          -30101.9941,   3387.9531],\n",
      "         ...,\n",
      "         [ -6868.0322,  -3229.9863, -14594.0059,  ...,  -3331.9917,\n",
      "             406.0176,   5543.9951],\n",
      "         [  8672.0078, -14036.9795, -14705.9570,  ...,  14500.0000,\n",
      "           -4060.9912, -25866.9688],\n",
      "         [ -1488.9907,  16133.9971,    569.0273,  ...,  45757.0117,\n",
      "           16311.9922,  -9416.9902]]]),) and output (tensor([[[ -8916.0010,   2362.9990,  -4833.0039,  ..., -12075.0020,\n",
      "           -1124.0020,  12438.0020],\n",
      "         [ 24830.0078,  16559.0117,  -5021.0059,  ...,   9763.9941,\n",
      "           13881.0166,   1800.9941],\n",
      "         [  6259.9590, -15523.0098,   2293.9912,  ...,  -8551.9697,\n",
      "          -27424.9941,   4776.9531],\n",
      "         ...,\n",
      "         [-12481.0322,  -1773.9863, -12678.0059,  ...,  -2867.9917,\n",
      "             558.0176,  13632.9951],\n",
      "         [ 10887.0078, -24146.9805, -15188.9570,  ...,  12041.0000,\n",
      "           -4941.9912, -21983.9688],\n",
      "         [  2127.0093,  26970.9961,   1596.0273,  ...,  47568.0117,\n",
      "           22151.9922,  -6688.9902]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[ -8916.0010,   2362.9990,  -4833.0039,  ..., -12075.0020,\n",
      "           -1124.0020,  12438.0020],\n",
      "         [ 24830.0078,  16559.0117,  -5021.0059,  ...,   9763.9941,\n",
      "           13881.0166,   1800.9941],\n",
      "         [  6259.9590, -15523.0098,   2293.9912,  ...,  -8551.9697,\n",
      "          -27424.9941,   4776.9531],\n",
      "         ...,\n",
      "         [-12481.0322,  -1773.9863, -12678.0059,  ...,  -2867.9917,\n",
      "             558.0176,  13632.9951],\n",
      "         [ 10887.0078, -24146.9805, -15188.9570,  ...,  12041.0000,\n",
      "           -4941.9912, -21983.9688],\n",
      "         [  2127.0093,  26970.9961,   1596.0273,  ...,  47568.0117,\n",
      "           22151.9922,  -6688.9902]]]),) and output (tensor([[[-20230.0000,   7877.9990,    161.9961,  ..., -10250.0020,\n",
      "             676.9980,   7095.0020],\n",
      "         [ 32679.0078,  17546.0117,  -4016.0059,  ...,  10164.9941,\n",
      "           20364.0156,  -7120.0059],\n",
      "         [  8840.9590, -18569.0098,   4152.9912,  ...,  -9286.9697,\n",
      "          -25801.9941,  -2248.0469],\n",
      "         ...,\n",
      "         [-13859.0322,  -3342.9863,  -9733.0059,  ...,  -9749.9922,\n",
      "            4230.0176,   4803.9951],\n",
      "         [  5880.0078, -24862.9805, -12262.9570,  ...,   5887.0000,\n",
      "           -3622.9912, -23256.9688],\n",
      "         [  4288.0093,  29783.9961,  -1040.9727,  ...,  53062.0117,\n",
      "           20728.9922,   -685.9902]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[-20230.0000,   7877.9990,    161.9961,  ..., -10250.0020,\n",
      "             676.9980,   7095.0020],\n",
      "         [ 32679.0078,  17546.0117,  -4016.0059,  ...,  10164.9941,\n",
      "           20364.0156,  -7120.0059],\n",
      "         [  8840.9590, -18569.0098,   4152.9912,  ...,  -9286.9697,\n",
      "          -25801.9941,  -2248.0469],\n",
      "         ...,\n",
      "         [-13859.0322,  -3342.9863,  -9733.0059,  ...,  -9749.9922,\n",
      "            4230.0176,   4803.9951],\n",
      "         [  5880.0078, -24862.9805, -12262.9570,  ...,   5887.0000,\n",
      "           -3622.9912, -23256.9688],\n",
      "         [  4288.0093,  29783.9961,  -1040.9727,  ...,  53062.0117,\n",
      "           20728.9922,   -685.9902]]]),) and output (tensor([[[-25670.0000,  13293.9990,   9099.9961,  ...,  -6547.0020,\n",
      "           -5658.0020,   4192.0020],\n",
      "         [ 45867.0078,  18942.0117,   2430.9941,  ...,  11278.9941,\n",
      "           25359.0156,  -3142.0059],\n",
      "         [ 13434.9590, -28313.0098,   6348.9912,  ...,  -9387.9697,\n",
      "          -24346.9941,  -7749.0469],\n",
      "         ...,\n",
      "         [-17793.0312,  -1669.9863,  -7895.0059,  ...,  -5683.9922,\n",
      "           -2066.9824,  -1628.0049],\n",
      "         [ 13556.0078,  -8671.9805,  -8958.9570,  ...,   8636.0000,\n",
      "           -2881.9912, -18384.9688],\n",
      "         [  5411.0093,  27274.9961,  -1653.9727,  ...,  55090.0117,\n",
      "           18995.9922,    175.0098]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[-25670.0000,  13293.9990,   9099.9961,  ...,  -6547.0020,\n",
      "           -5658.0020,   4192.0020],\n",
      "         [ 45867.0078,  18942.0117,   2430.9941,  ...,  11278.9941,\n",
      "           25359.0156,  -3142.0059],\n",
      "         [ 13434.9590, -28313.0098,   6348.9912,  ...,  -9387.9697,\n",
      "          -24346.9941,  -7749.0469],\n",
      "         ...,\n",
      "         [-17793.0312,  -1669.9863,  -7895.0059,  ...,  -5683.9922,\n",
      "           -2066.9824,  -1628.0049],\n",
      "         [ 13556.0078,  -8671.9805,  -8958.9570,  ...,   8636.0000,\n",
      "           -2881.9912, -18384.9688],\n",
      "         [  5411.0093,  27274.9961,  -1653.9727,  ...,  55090.0117,\n",
      "           18995.9922,    175.0098]]]),) and output (tensor([[[-25652.0000,  13180.9990,  15728.9961,  ...,  -2831.0020,\n",
      "             158.9980,   2204.0020],\n",
      "         [ 48003.0078,  18658.0117,   1229.9941,  ...,   7221.9941,\n",
      "           25396.0156, -15206.0059],\n",
      "         [ 11398.9590, -27727.0098,   5300.9912,  ...,  -7599.9697,\n",
      "          -18318.9941, -12543.0469],\n",
      "         ...,\n",
      "         [-14113.0312,   -522.9863, -11979.0059,  ...,  -4649.9922,\n",
      "            -665.9824,   1195.9951],\n",
      "         [ 15676.0078,  -5336.9805,  -9797.9570,  ...,  12860.0000,\n",
      "           -6456.9912, -20139.9688],\n",
      "         [  6166.0093,  27252.9961,  -2554.9727,  ...,  53081.0117,\n",
      "           13901.9922,    373.0098]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[-25652.0000,  13180.9990,  15728.9961,  ...,  -2831.0020,\n",
      "             158.9980,   2204.0020],\n",
      "         [ 48003.0078,  18658.0117,   1229.9941,  ...,   7221.9941,\n",
      "           25396.0156, -15206.0059],\n",
      "         [ 11398.9590, -27727.0098,   5300.9912,  ...,  -7599.9697,\n",
      "          -18318.9941, -12543.0469],\n",
      "         ...,\n",
      "         [-14113.0312,   -522.9863, -11979.0059,  ...,  -4649.9922,\n",
      "            -665.9824,   1195.9951],\n",
      "         [ 15676.0078,  -5336.9805,  -9797.9570,  ...,  12860.0000,\n",
      "           -6456.9912, -20139.9688],\n",
      "         [  6166.0093,  27252.9961,  -2554.9727,  ...,  53081.0117,\n",
      "           13901.9922,    373.0098]]]),) and output (tensor([[[-14791.0000,  15299.9990,  16129.9961,  ...,   3187.9980,\n",
      "           -2552.0020,   5626.0020],\n",
      "         [ 51705.0078,  12612.0117,   3078.9941,  ...,   6974.9941,\n",
      "           23667.0156, -20513.0059],\n",
      "         [ 14359.9590, -41397.0078,   7175.9912,  ...,  -5993.9697,\n",
      "          -25295.9941, -11201.0469],\n",
      "         ...,\n",
      "         [-13334.0312,  -3860.9863,  -6007.0059,  ..., -11221.9922,\n",
      "           -4640.9824,   2983.9951],\n",
      "         [ 20029.0078,  -1756.9805, -13954.9570,  ...,  13349.0000,\n",
      "           -5635.9912, -22010.9688],\n",
      "         [  4508.0093,  33776.9961,  -4678.9727,  ...,  56429.0117,\n",
      "           14325.9922,  -2851.9902]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[-14791.0000,  15299.9990,  16129.9961,  ...,   3187.9980,\n",
      "           -2552.0020,   5626.0020],\n",
      "         [ 51705.0078,  12612.0117,   3078.9941,  ...,   6974.9941,\n",
      "           23667.0156, -20513.0059],\n",
      "         [ 14359.9590, -41397.0078,   7175.9912,  ...,  -5993.9697,\n",
      "          -25295.9941, -11201.0469],\n",
      "         ...,\n",
      "         [-13334.0312,  -3860.9863,  -6007.0059,  ..., -11221.9922,\n",
      "           -4640.9824,   2983.9951],\n",
      "         [ 20029.0078,  -1756.9805, -13954.9570,  ...,  13349.0000,\n",
      "           -5635.9912, -22010.9688],\n",
      "         [  4508.0093,  33776.9961,  -4678.9727,  ...,  56429.0117,\n",
      "           14325.9922,  -2851.9902]]]),) and output (tensor([[[-13284.0000,  26639.0000,   7467.9961,  ...,   4113.9980,\n",
      "           -6608.0020,   5336.0020],\n",
      "         [ 52699.0078,  17712.0117,    945.9941,  ...,   7592.9941,\n",
      "           17998.0156, -14724.0059],\n",
      "         [ 11121.9590, -54231.0078,   9081.9912,  ...,  -5384.9697,\n",
      "          -21312.9941,  -5383.0469],\n",
      "         ...,\n",
      "         [ -9111.0312,  -3286.9863,    474.9941,  ...,  -4115.9922,\n",
      "           -3098.9824,  -1761.0049],\n",
      "         [ 13199.0078, -13299.9805, -19257.9570,  ...,  10627.0000,\n",
      "           -9306.9912, -16349.9688],\n",
      "         [ -2632.9907,  25633.9961,  -6584.9727,  ...,  56725.0117,\n",
      "            8673.9922,   -568.9902]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[-13284.0000,  26639.0000,   7467.9961,  ...,   4113.9980,\n",
      "           -6608.0020,   5336.0020],\n",
      "         [ 52699.0078,  17712.0117,    945.9941,  ...,   7592.9941,\n",
      "           17998.0156, -14724.0059],\n",
      "         [ 11121.9590, -54231.0078,   9081.9912,  ...,  -5384.9697,\n",
      "          -21312.9941,  -5383.0469],\n",
      "         ...,\n",
      "         [ -9111.0312,  -3286.9863,    474.9941,  ...,  -4115.9922,\n",
      "           -3098.9824,  -1761.0049],\n",
      "         [ 13199.0078, -13299.9805, -19257.9570,  ...,  10627.0000,\n",
      "           -9306.9912, -16349.9688],\n",
      "         [ -2632.9907,  25633.9961,  -6584.9727,  ...,  56725.0117,\n",
      "            8673.9922,   -568.9902]]]),) and output (tensor([[[-21532.0000,  32796.0000,  16281.9961,  ...,  18208.9980,\n",
      "           -4282.0020,   5756.0020],\n",
      "         [ 54415.0078,  16682.0117,   5945.9941,  ...,  12273.9941,\n",
      "           23325.0156, -27854.0059],\n",
      "         [ 14106.9590, -49750.0078,   4831.9912,  ..., -12310.9697,\n",
      "          -18390.9941,  -4092.0469],\n",
      "         ...,\n",
      "         [-16480.0312,  -6877.9863,   3279.9941,  ..., -14724.9922,\n",
      "           -8746.9824,  -4428.0049],\n",
      "         [ 10574.0078,  -9801.9805, -25262.9570,  ...,  10259.0000,\n",
      "          -14790.9912, -16216.9688],\n",
      "         [  4482.0093,  16038.9961, -10475.9727,  ...,  57131.0117,\n",
      "            6967.9922,    143.0098]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[-21532.0000,  32796.0000,  16281.9961,  ...,  18208.9980,\n",
      "           -4282.0020,   5756.0020],\n",
      "         [ 54415.0078,  16682.0117,   5945.9941,  ...,  12273.9941,\n",
      "           23325.0156, -27854.0059],\n",
      "         [ 14106.9590, -49750.0078,   4831.9912,  ..., -12310.9697,\n",
      "          -18390.9941,  -4092.0469],\n",
      "         ...,\n",
      "         [-16480.0312,  -6877.9863,   3279.9941,  ..., -14724.9922,\n",
      "           -8746.9824,  -4428.0049],\n",
      "         [ 10574.0078,  -9801.9805, -25262.9570,  ...,  10259.0000,\n",
      "          -14790.9912, -16216.9688],\n",
      "         [  4482.0093,  16038.9961, -10475.9727,  ...,  57131.0117,\n",
      "            6967.9922,    143.0098]]]),) and output (tensor([[[-24082.0000,  33387.0000,  15374.9961,  ...,  18359.9980,\n",
      "           -7500.0020,  11519.0020],\n",
      "         [ 42750.0078,  22227.0117,  10474.9941,  ...,  16794.9941,\n",
      "           19994.0156, -20828.0059],\n",
      "         [ 13777.9590, -44504.0078,  12208.9912,  ..., -12604.9697,\n",
      "          -17504.9941,   1031.9531],\n",
      "         ...,\n",
      "         [-23090.0312,  -7157.9863,  10737.9941,  ..., -18975.9922,\n",
      "          -14651.9824,    216.9951],\n",
      "         [ 16185.0078,   -458.9805, -20521.9570,  ...,   3332.0000,\n",
      "          -12652.9912, -15224.9688],\n",
      "         [ 16153.0098,  18587.9961,  -9266.9727,  ...,  53882.0117,\n",
      "            2298.9922,   1580.0098]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[-24082.0000,  33387.0000,  15374.9961,  ...,  18359.9980,\n",
      "           -7500.0020,  11519.0020],\n",
      "         [ 42750.0078,  22227.0117,  10474.9941,  ...,  16794.9941,\n",
      "           19994.0156, -20828.0059],\n",
      "         [ 13777.9590, -44504.0078,  12208.9912,  ..., -12604.9697,\n",
      "          -17504.9941,   1031.9531],\n",
      "         ...,\n",
      "         [-23090.0312,  -7157.9863,  10737.9941,  ..., -18975.9922,\n",
      "          -14651.9824,    216.9951],\n",
      "         [ 16185.0078,   -458.9805, -20521.9570,  ...,   3332.0000,\n",
      "          -12652.9912, -15224.9688],\n",
      "         [ 16153.0098,  18587.9961,  -9266.9727,  ...,  53882.0117,\n",
      "            2298.9922,   1580.0098]]]),) and output (tensor([[[-14869.0000,  32284.0000,   3742.9961,  ...,  12458.9980,\n",
      "           -5452.0020,   7590.0020],\n",
      "         [ 39066.0078,  20349.0117,  25497.9941,  ...,  27121.9941,\n",
      "           29170.0156, -21003.0059],\n",
      "         [ 24641.9590, -43587.0078,  20684.9922,  ..., -15913.9697,\n",
      "          -18727.9941,  -3258.0469],\n",
      "         ...,\n",
      "         [-29714.0312,  -4921.9863,   2937.9941,  ..., -23030.9922,\n",
      "          -20434.9824,   2843.9951],\n",
      "         [ 15439.0078,    539.0195, -24142.9570,  ...,   1926.0000,\n",
      "          -19139.9922, -10181.9688],\n",
      "         [ 14936.0098,  16624.9961,  -5234.9727,  ...,  53637.0117,\n",
      "           -2996.0078,  -1496.9902]]]),)\n",
      "[Debug] Layer names being passed: ['model.layers.0', 'model.layers.1', 'model.layers.2', 'model.layers.3', 'model.layers.4', 'model.layers.5', 'model.layers.6', 'model.layers.7', 'model.layers.8', 'model.layers.9', 'model.layers.10', 'model.layers.11', 'model.layers.12', 'model.layers.13', 'model.layers.14', 'model.layers.15', 'model.layers.16', 'model.layers.17', 'model.layers.18', 'model.layers.19', 'model.layers.20', 'model.layers.21', 'model.layers.22', 'model.layers.23', 'model.layers.24', 'model.layers.25', 'model.layers.26', 'model.layers.27', 'model.embed_tokens']\n",
      "[Debug] Trying to access layer: model.layers.0\n",
      "[Debug] Successfully found layer: model.layers.0\n",
      "[Debug] Trying to access layer: model.layers.1\n",
      "[Debug] Successfully found layer: model.layers.1\n",
      "[Debug] Trying to access layer: model.layers.2\n",
      "[Debug] Successfully found layer: model.layers.2\n",
      "[Debug] Trying to access layer: model.layers.3\n",
      "[Debug] Successfully found layer: model.layers.3\n",
      "[Debug] Trying to access layer: model.layers.4\n",
      "[Debug] Successfully found layer: model.layers.4\n",
      "[Debug] Trying to access layer: model.layers.5\n",
      "[Debug] Successfully found layer: model.layers.5\n",
      "[Debug] Trying to access layer: model.layers.6\n",
      "[Debug] Successfully found layer: model.layers.6\n",
      "[Debug] Trying to access layer: model.layers.7\n",
      "[Debug] Successfully found layer: model.layers.7\n",
      "[Debug] Trying to access layer: model.layers.8\n",
      "[Debug] Successfully found layer: model.layers.8\n",
      "[Debug] Trying to access layer: model.layers.9\n",
      "[Debug] Successfully found layer: model.layers.9\n",
      "[Debug] Trying to access layer: model.layers.10\n",
      "[Debug] Successfully found layer: model.layers.10\n",
      "[Debug] Trying to access layer: model.layers.11\n",
      "[Debug] Successfully found layer: model.layers.11\n",
      "[Debug] Trying to access layer: model.layers.12\n",
      "[Debug] Successfully found layer: model.layers.12\n",
      "[Debug] Trying to access layer: model.layers.13\n",
      "[Debug] Successfully found layer: model.layers.13\n",
      "[Debug] Trying to access layer: model.layers.14\n",
      "[Debug] Successfully found layer: model.layers.14\n",
      "[Debug] Trying to access layer: model.layers.15\n",
      "[Debug] Successfully found layer: model.layers.15\n",
      "[Debug] Trying to access layer: model.layers.16\n",
      "[Debug] Successfully found layer: model.layers.16\n",
      "[Debug] Trying to access layer: model.layers.17\n",
      "[Debug] Successfully found layer: model.layers.17\n",
      "[Debug] Trying to access layer: model.layers.18\n",
      "[Debug] Successfully found layer: model.layers.18\n",
      "[Debug] Trying to access layer: model.layers.19\n",
      "[Debug] Successfully found layer: model.layers.19\n",
      "[Debug] Trying to access layer: model.layers.20\n",
      "[Debug] Successfully found layer: model.layers.20\n",
      "[Debug] Trying to access layer: model.layers.21\n",
      "[Debug] Successfully found layer: model.layers.21\n",
      "[Debug] Trying to access layer: model.layers.22\n",
      "[Debug] Successfully found layer: model.layers.22\n",
      "[Debug] Trying to access layer: model.layers.23\n",
      "[Debug] Successfully found layer: model.layers.23\n",
      "[Debug] Trying to access layer: model.layers.24\n",
      "[Debug] Successfully found layer: model.layers.24\n",
      "[Debug] Trying to access layer: model.layers.25\n",
      "[Debug] Successfully found layer: model.layers.25\n",
      "[Debug] Trying to access layer: model.layers.26\n",
      "[Debug] Successfully found layer: model.layers.26\n",
      "[Debug] Trying to access layer: model.layers.27\n",
      "[Debug] Successfully found layer: model.layers.27\n",
      "[Debug] Trying to access layer: model.embed_tokens\n",
      "[Debug] Successfully found layer: model.embed_tokens\n",
      "[Hook] Layer Embedding(128256, 3072, padding_idx=128004) received input (tensor([[128000,   2149,  17309,     25,    578,   8155,  58418,  28366,     25,\n",
      "            578,   8155,  58418,    374,    459,   3778,   8903,  14994,  12707,\n",
      "           4101,    389,    279,  39193,  13740,     11,   5131,    304,   1202,\n",
      "            220,     22,    339,   3280,    315,  13195,     13,    578,   1501,\n",
      "           9477,    279,  11838,  38988,   9211,   3070,     11,  49446,    315,\n",
      "          30791,  20550,    323,   1708,  79451,  83407,     11,    816,   1130,\n",
      "            323,  42893,  38988,   9211,     11,    520,    872,   5105,  73437,\n",
      "            220,    806,   8931,   4994,    315,  66805,   8032,     16,     60,\n",
      "           3296,   5496,   2085,  44288,    477,   6617,  24494,     11,    279,\n",
      "          39562,  41011,    311,   5258,    380,    555,  33489,     11,  23330,\n",
      "            323,  20646,    369,    279,   1317,  86082,   8032,     17,     60,\n",
      "            578,  38988,   9211,   3070,    527,  29658,    315,    279,  23597,\n",
      "          84304,  17706,     16,   1483,     18,     60,    889,    706,   9922,\n",
      "            389,    279,   1501,   8032,     19,     60]]),) and output tensor([[[-0.0010, -0.0007, -0.0046,  ..., -0.0014, -0.0020,  0.0020],\n",
      "         [ 0.0023,  0.0082, -0.0148,  ..., -0.0215,  0.0015,  0.0129],\n",
      "         [-0.0005, -0.0254,  0.0135,  ..., -0.0023, -0.0073, -0.0013],\n",
      "         ...,\n",
      "         [ 0.0145, -0.0280, -0.0145,  ...,  0.0136, -0.0054, -0.0027],\n",
      "         [ 0.0190, -0.0011,  0.0210,  ..., -0.0170, -0.0048, -0.0023],\n",
      "         [-0.0045, -0.0086,  0.0126,  ...,  0.0082,  0.0127,  0.0007]]])\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[-0.0010, -0.0007, -0.0046,  ..., -0.0014, -0.0020,  0.0020],\n",
      "         [ 0.0023,  0.0082, -0.0148,  ..., -0.0215,  0.0015,  0.0129],\n",
      "         [-0.0005, -0.0254,  0.0135,  ..., -0.0023, -0.0073, -0.0013],\n",
      "         ...,\n",
      "         [ 0.0145, -0.0280, -0.0145,  ...,  0.0136, -0.0054, -0.0027],\n",
      "         [ 0.0190, -0.0011,  0.0210,  ..., -0.0170, -0.0048, -0.0023],\n",
      "         [-0.0045, -0.0086,  0.0126,  ...,  0.0082,  0.0127,  0.0007]]]),) and output (tensor([[[  442.9990,   239.9993,   302.9954,  ...,  -154.0014,\n",
      "            -15.0020,   185.0020],\n",
      "         [  -58.9977,   -52.9918,   -64.0148,  ...,    75.9785,\n",
      "            184.0015,  -133.9871],\n",
      "         [  117.9995,   -95.0254,    84.0135,  ...,   -42.0023,\n",
      "             83.9927,    71.9987],\n",
      "         ...,\n",
      "         [ -173.9855,    78.9720,  -577.0145,  ...,   -79.9864,\n",
      "           -260.0054,   518.9973],\n",
      "         [ -170.9810,    43.9989,   212.0210,  ...,   162.9830,\n",
      "           -133.0048,   -82.0023],\n",
      "         [ -394.0045,  -345.0086,   573.0126,  ..., -1006.9918,\n",
      "            -97.9873,  -227.9993]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[  442.9990,   239.9993,   302.9954,  ...,  -154.0014,\n",
      "            -15.0020,   185.0020],\n",
      "         [  -58.9977,   -52.9918,   -64.0148,  ...,    75.9785,\n",
      "            184.0015,  -133.9871],\n",
      "         [  117.9995,   -95.0254,    84.0135,  ...,   -42.0023,\n",
      "             83.9927,    71.9987],\n",
      "         ...,\n",
      "         [ -173.9855,    78.9720,  -577.0145,  ...,   -79.9864,\n",
      "           -260.0054,   518.9973],\n",
      "         [ -170.9810,    43.9989,   212.0210,  ...,   162.9830,\n",
      "           -133.0048,   -82.0023],\n",
      "         [ -394.0045,  -345.0086,   573.0126,  ..., -1006.9918,\n",
      "            -97.9873,  -227.9993]]]),) and output (tensor([[[  439.9990,  -144.0007,  -410.0046,  ...,  -765.0015,\n",
      "           1193.9979, -1027.9980],\n",
      "         [   89.0023,  -658.9918,  -173.0148,  ...,   -83.0215,\n",
      "            480.0015,  -682.9871],\n",
      "         [-1616.0005,   283.9746,  1210.0135,  ...,  1133.9977,\n",
      "            718.9927,   385.9987],\n",
      "         ...,\n",
      "         [ -591.9855,  1046.9720,  -666.0145,  ...,  -158.9864,\n",
      "           -899.0054,   274.9973],\n",
      "         [-1541.9810,     4.9989,  -690.9790,  ...,   105.9830,\n",
      "           1446.9952,  -730.0023],\n",
      "         [ -401.0045,   351.9914,   528.0126,  ..., -1003.9918,\n",
      "             88.0127,  -641.9993]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[  439.9990,  -144.0007,  -410.0046,  ...,  -765.0015,\n",
      "           1193.9979, -1027.9980],\n",
      "         [   89.0023,  -658.9918,  -173.0148,  ...,   -83.0215,\n",
      "            480.0015,  -682.9871],\n",
      "         [-1616.0005,   283.9746,  1210.0135,  ...,  1133.9977,\n",
      "            718.9927,   385.9987],\n",
      "         ...,\n",
      "         [ -591.9855,  1046.9720,  -666.0145,  ...,  -158.9864,\n",
      "           -899.0054,   274.9973],\n",
      "         [-1541.9810,     4.9989,  -690.9790,  ...,   105.9830,\n",
      "           1446.9952,  -730.0023],\n",
      "         [ -401.0045,   351.9914,   528.0126,  ..., -1003.9918,\n",
      "             88.0127,  -641.9993]]]),) and output (tensor([[[  551.9990,   993.9993, -1790.0046,  ...,   445.9985,\n",
      "          -1386.0021, -1245.9980],\n",
      "         [-3476.9978,   581.0082, -2354.0149,  ..., -2543.0215,\n",
      "           2942.0015,  -331.9871],\n",
      "         [  529.9995,  1454.9746,  4415.0137,  ...,  3340.9976,\n",
      "          -5432.0073,  1630.9988],\n",
      "         ...,\n",
      "         [-1178.9855,  3528.9722,   998.9855,  ...,  -234.9864,\n",
      "          -1201.0054, -1302.0027],\n",
      "         [  986.0190, -4420.0010, -4271.9790,  ...,  -174.0170,\n",
      "           1204.9952,  1280.9977],\n",
      "         [-2125.0044,   986.9915, -1352.9874,  ...,  1090.0083,\n",
      "           -282.9873, -2050.9993]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[  551.9990,   993.9993, -1790.0046,  ...,   445.9985,\n",
      "          -1386.0021, -1245.9980],\n",
      "         [-3476.9978,   581.0082, -2354.0149,  ..., -2543.0215,\n",
      "           2942.0015,  -331.9871],\n",
      "         [  529.9995,  1454.9746,  4415.0137,  ...,  3340.9976,\n",
      "          -5432.0073,  1630.9988],\n",
      "         ...,\n",
      "         [-1178.9855,  3528.9722,   998.9855,  ...,  -234.9864,\n",
      "          -1201.0054, -1302.0027],\n",
      "         [  986.0190, -4420.0010, -4271.9790,  ...,  -174.0170,\n",
      "           1204.9952,  1280.9977],\n",
      "         [-2125.0044,   986.9915, -1352.9874,  ...,  1090.0083,\n",
      "           -282.9873, -2050.9993]]]),) and output (tensor([[[  2131.9990,   -737.0007,  -1071.0046,  ...,  -2781.0015,\n",
      "            -885.0021,     86.0020],\n",
      "         [ -5527.9980,  -2082.9917,  -5062.0146,  ...,  -5899.0215,\n",
      "            3967.0015,  -3927.9871],\n",
      "         [   688.9995,   1341.9746,   4530.0137,  ...,   3479.9976,\n",
      "          -11807.0078,    602.9988],\n",
      "         ...,\n",
      "         [  -122.9855,   5642.9722,   2128.9854,  ...,   1301.0137,\n",
      "            2046.9946,  -3533.0027],\n",
      "         [  -469.9810,  -6667.0010,  -5853.9790,  ...,   1905.9830,\n",
      "            3862.9951,   2562.9976],\n",
      "         [ -1194.0044,   3861.9915,    204.0126,  ...,  -1499.9917,\n",
      "           -6878.9873,  -1749.9993]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[  2131.9990,   -737.0007,  -1071.0046,  ...,  -2781.0015,\n",
      "            -885.0021,     86.0020],\n",
      "         [ -5527.9980,  -2082.9917,  -5062.0146,  ...,  -5899.0215,\n",
      "            3967.0015,  -3927.9871],\n",
      "         [   688.9995,   1341.9746,   4530.0137,  ...,   3479.9976,\n",
      "          -11807.0078,    602.9988],\n",
      "         ...,\n",
      "         [  -122.9855,   5642.9722,   2128.9854,  ...,   1301.0137,\n",
      "            2046.9946,  -3533.0027],\n",
      "         [  -469.9810,  -6667.0010,  -5853.9790,  ...,   1905.9830,\n",
      "            3862.9951,   2562.9976],\n",
      "         [ -1194.0044,   3861.9915,    204.0126,  ...,  -1499.9917,\n",
      "           -6878.9873,  -1749.9993]]]),) and output (tensor([[[  3420.9990,    760.9993,  -5376.0049,  ...,  -4193.0015,\n",
      "            2860.9980,  -4080.9980],\n",
      "         [ -4269.9980,  -2908.9917,  -3756.0146,  ...,  -8833.0215,\n",
      "            5037.0015,  -3876.9871],\n",
      "         [  1969.9995,    412.9746,   2579.0137,  ...,   6684.9976,\n",
      "          -17387.0078,  -1947.0012],\n",
      "         ...,\n",
      "         [  2007.0145,   1898.9722,   -854.0146,  ...,   2396.0137,\n",
      "            2100.9946,  -5171.0029],\n",
      "         [  1640.0190,  -4964.0010,  -6624.9790,  ...,   -380.0171,\n",
      "            7015.9951,   6865.9976],\n",
      "         [  2586.9956,   7567.9912,   1744.0126,  ...,  -7528.9917,\n",
      "           -9605.9873,  -8157.9990]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[  3420.9990,    760.9993,  -5376.0049,  ...,  -4193.0015,\n",
      "            2860.9980,  -4080.9980],\n",
      "         [ -4269.9980,  -2908.9917,  -3756.0146,  ...,  -8833.0215,\n",
      "            5037.0015,  -3876.9871],\n",
      "         [  1969.9995,    412.9746,   2579.0137,  ...,   6684.9976,\n",
      "          -17387.0078,  -1947.0012],\n",
      "         ...,\n",
      "         [  2007.0145,   1898.9722,   -854.0146,  ...,   2396.0137,\n",
      "            2100.9946,  -5171.0029],\n",
      "         [  1640.0190,  -4964.0010,  -6624.9790,  ...,   -380.0171,\n",
      "            7015.9951,   6865.9976],\n",
      "         [  2586.9956,   7567.9912,   1744.0126,  ...,  -7528.9917,\n",
      "           -9605.9873,  -8157.9990]]]),) and output (tensor([[[  2742.9990,  -1512.0007, -10493.0049,  ...,  -9945.0020,\n",
      "           -2017.0020,  -1802.9980],\n",
      "         [    30.0020,  -2031.9917,  -3030.0146,  ..., -11488.0215,\n",
      "            3497.0015,  -1074.9871],\n",
      "         [   838.9995,   1266.9746,   3655.0137,  ...,   7968.9976,\n",
      "          -15599.0078,    299.9988],\n",
      "         ...,\n",
      "         [  4204.0146,   4244.9722,  -1915.0146,  ...,   3404.0137,\n",
      "            -432.0054,  -2627.0029],\n",
      "         [ -2264.9810,  -5670.0010,  -5687.9790,  ...,   2654.9829,\n",
      "            7081.9951,   5564.9976],\n",
      "         [  4800.9956,   3204.9912,   -256.9874,  ...,  -9921.9922,\n",
      "          -14946.9873,   -666.9990]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[  2742.9990,  -1512.0007, -10493.0049,  ...,  -9945.0020,\n",
      "           -2017.0020,  -1802.9980],\n",
      "         [    30.0020,  -2031.9917,  -3030.0146,  ..., -11488.0215,\n",
      "            3497.0015,  -1074.9871],\n",
      "         [   838.9995,   1266.9746,   3655.0137,  ...,   7968.9976,\n",
      "          -15599.0078,    299.9988],\n",
      "         ...,\n",
      "         [  4204.0146,   4244.9722,  -1915.0146,  ...,   3404.0137,\n",
      "            -432.0054,  -2627.0029],\n",
      "         [ -2264.9810,  -5670.0010,  -5687.9790,  ...,   2654.9829,\n",
      "            7081.9951,   5564.9976],\n",
      "         [  4800.9956,   3204.9912,   -256.9874,  ...,  -9921.9922,\n",
      "          -14946.9873,   -666.9990]]]),) and output (tensor([[[  3032.9990,  -3420.0007, -10291.0049,  ...,  -6511.0020,\n",
      "           -1953.0020,   2001.0020],\n",
      "         [  7914.0020,  -4441.9917,  -3597.0146,  ..., -11666.0215,\n",
      "            -417.9985,  -1480.9871],\n",
      "         [  7996.9995,  -3224.0254,   6721.0137,  ...,  12347.9980,\n",
      "          -17201.0078,   -381.0012],\n",
      "         ...,\n",
      "         [  -771.9854,   7927.9722,   1251.9854,  ...,   2897.0137,\n",
      "            2299.9946,   -829.0029],\n",
      "         [  -277.9810,  -6126.0010,  -8478.9785,  ...,   7454.9829,\n",
      "            8780.9951,   4891.9976],\n",
      "         [  4325.9956,   7737.9912,   3057.0127,  ...,  -4529.9922,\n",
      "          -20211.9883,   -482.9990]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[  3032.9990,  -3420.0007, -10291.0049,  ...,  -6511.0020,\n",
      "           -1953.0020,   2001.0020],\n",
      "         [  7914.0020,  -4441.9917,  -3597.0146,  ..., -11666.0215,\n",
      "            -417.9985,  -1480.9871],\n",
      "         [  7996.9995,  -3224.0254,   6721.0137,  ...,  12347.9980,\n",
      "          -17201.0078,   -381.0012],\n",
      "         ...,\n",
      "         [  -771.9854,   7927.9722,   1251.9854,  ...,   2897.0137,\n",
      "            2299.9946,   -829.0029],\n",
      "         [  -277.9810,  -6126.0010,  -8478.9785,  ...,   7454.9829,\n",
      "            8780.9951,   4891.9976],\n",
      "         [  4325.9956,   7737.9912,   3057.0127,  ...,  -4529.9922,\n",
      "          -20211.9883,   -482.9990]]]),) and output (tensor([[[  3132.9990,  -1970.0007, -11544.0049,  ...,  -5780.0020,\n",
      "           -2486.0020,  -1921.9980],\n",
      "         [ 10397.0020,  -5658.9917,  -2370.0146,  ..., -14709.0215,\n",
      "            2036.0015,  -4237.9873],\n",
      "         [  2361.9995,   -563.0254,   3969.0137,  ...,   7706.9980,\n",
      "          -23437.0078,  -3624.0012],\n",
      "         ...,\n",
      "         [   131.0146,   7096.9727,  -1560.0146,  ...,   2175.0137,\n",
      "            4075.9946,   7650.9971],\n",
      "         [  -348.9810,  -5203.0010,  -6259.9785,  ...,  12569.9824,\n",
      "            9539.9951,   1363.9976],\n",
      "         [ -1798.0044,   7420.9912,     85.0127,  ...,  -6427.9922,\n",
      "          -22482.9883,   1986.0010]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[  3132.9990,  -1970.0007, -11544.0049,  ...,  -5780.0020,\n",
      "           -2486.0020,  -1921.9980],\n",
      "         [ 10397.0020,  -5658.9917,  -2370.0146,  ..., -14709.0215,\n",
      "            2036.0015,  -4237.9873],\n",
      "         [  2361.9995,   -563.0254,   3969.0137,  ...,   7706.9980,\n",
      "          -23437.0078,  -3624.0012],\n",
      "         ...,\n",
      "         [   131.0146,   7096.9727,  -1560.0146,  ...,   2175.0137,\n",
      "            4075.9946,   7650.9971],\n",
      "         [  -348.9810,  -5203.0010,  -6259.9785,  ...,  12569.9824,\n",
      "            9539.9951,   1363.9976],\n",
      "         [ -1798.0044,   7420.9912,     85.0127,  ...,  -6427.9922,\n",
      "          -22482.9883,   1986.0010]]]),) and output (tensor([[[  1046.9990,   2403.9993, -14486.0049,  ..., -10922.0020,\n",
      "           -1419.0020,  -2983.9980],\n",
      "         [  9893.0020,  -5091.9917,  -1776.0146,  ..., -15101.0215,\n",
      "            5190.0015,  -2657.9873],\n",
      "         [ -2394.0005,  -8110.0254,    413.0137,  ...,   2592.9980,\n",
      "          -29862.0078, -14315.0010],\n",
      "         ...,\n",
      "         [ -1120.9854,  10805.9727,  -2267.0146,  ...,   8500.0137,\n",
      "           -4215.0054,   5085.9971],\n",
      "         [ -9184.9805,   -898.0010, -10611.9785,  ...,  16989.9824,\n",
      "           10170.9951,  -4567.0024],\n",
      "         [ -6047.0044,   9760.9912,  -5274.9873,  ...,  -4154.9922,\n",
      "          -25092.9883,   -189.9990]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[  1046.9990,   2403.9993, -14486.0049,  ..., -10922.0020,\n",
      "           -1419.0020,  -2983.9980],\n",
      "         [  9893.0020,  -5091.9917,  -1776.0146,  ..., -15101.0215,\n",
      "            5190.0015,  -2657.9873],\n",
      "         [ -2394.0005,  -8110.0254,    413.0137,  ...,   2592.9980,\n",
      "          -29862.0078, -14315.0010],\n",
      "         ...,\n",
      "         [ -1120.9854,  10805.9727,  -2267.0146,  ...,   8500.0137,\n",
      "           -4215.0054,   5085.9971],\n",
      "         [ -9184.9805,   -898.0010, -10611.9785,  ...,  16989.9824,\n",
      "           10170.9951,  -4567.0024],\n",
      "         [ -6047.0044,   9760.9912,  -5274.9873,  ...,  -4154.9922,\n",
      "          -25092.9883,   -189.9990]]]),) and output (tensor([[[ -3767.0010,   5307.9990, -14221.0049,  ..., -15035.0020,\n",
      "           -3630.0020,   4168.0020],\n",
      "         [  8655.0020,  -7538.9917,  -1822.0146,  ..., -10937.0215,\n",
      "            3425.0015,  -2800.9873],\n",
      "         [ -3264.0005,  -8297.0254,   1318.0137,  ...,   2486.9980,\n",
      "          -31871.0078, -15671.0010],\n",
      "         ...,\n",
      "         [ -2320.9854,  12653.9727,  -2257.0146,  ...,   8439.0137,\n",
      "           -2285.0054,   8425.9971],\n",
      "         [-14931.9805,  -1338.0010, -10910.9785,  ...,  18341.9824,\n",
      "            1910.9951,  -2178.0024],\n",
      "         [ -3747.0044,   7736.9912,  -6792.9873,  ...,  -6679.9922,\n",
      "          -23727.9883,   3322.0010]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[ -3767.0010,   5307.9990, -14221.0049,  ..., -15035.0020,\n",
      "           -3630.0020,   4168.0020],\n",
      "         [  8655.0020,  -7538.9917,  -1822.0146,  ..., -10937.0215,\n",
      "            3425.0015,  -2800.9873],\n",
      "         [ -3264.0005,  -8297.0254,   1318.0137,  ...,   2486.9980,\n",
      "          -31871.0078, -15671.0010],\n",
      "         ...,\n",
      "         [ -2320.9854,  12653.9727,  -2257.0146,  ...,   8439.0137,\n",
      "           -2285.0054,   8425.9971],\n",
      "         [-14931.9805,  -1338.0010, -10910.9785,  ...,  18341.9824,\n",
      "            1910.9951,  -2178.0024],\n",
      "         [ -3747.0044,   7736.9912,  -6792.9873,  ...,  -6679.9922,\n",
      "          -23727.9883,   3322.0010]]]),) and output (tensor([[[ -1326.0010,   1223.9990, -16643.0039,  ..., -13240.0020,\n",
      "           -5677.0020,   9639.0020],\n",
      "         [  7715.0020, -11907.9922,  -2666.0146,  ...,  -9725.0215,\n",
      "           -2013.9985,   1571.0127],\n",
      "         [ -1287.0005,  -7655.0254,   4906.0137,  ...,  -4822.0020,\n",
      "          -36948.0078, -18184.0000],\n",
      "         ...,\n",
      "         [ -4697.9854,  13882.9727,    981.9854,  ...,  12097.0137,\n",
      "           -5144.0054,   4624.9971],\n",
      "         [-12255.9805,  -2772.0010, -13230.9785,  ...,  19041.9824,\n",
      "            3638.9951,   3619.9976],\n",
      "         [  -529.0044,   1593.9912,  -8448.9873,  ...,  -6946.9922,\n",
      "          -25330.9883,   -685.9990]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[ -1326.0010,   1223.9990, -16643.0039,  ..., -13240.0020,\n",
      "           -5677.0020,   9639.0020],\n",
      "         [  7715.0020, -11907.9922,  -2666.0146,  ...,  -9725.0215,\n",
      "           -2013.9985,   1571.0127],\n",
      "         [ -1287.0005,  -7655.0254,   4906.0137,  ...,  -4822.0020,\n",
      "          -36948.0078, -18184.0000],\n",
      "         ...,\n",
      "         [ -4697.9854,  13882.9727,    981.9854,  ...,  12097.0137,\n",
      "           -5144.0054,   4624.9971],\n",
      "         [-12255.9805,  -2772.0010, -13230.9785,  ...,  19041.9824,\n",
      "            3638.9951,   3619.9976],\n",
      "         [  -529.0044,   1593.9912,  -8448.9873,  ...,  -6946.9922,\n",
      "          -25330.9883,   -685.9990]]]),) and output (tensor([[[ -5120.0010,   2246.9990, -15993.0039,  ..., -19818.0020,\n",
      "           -9385.0020,  11924.0020],\n",
      "         [  8587.0020, -13793.9922,  -5307.0146,  ...,  -9188.0215,\n",
      "           -3875.9985,    438.0127],\n",
      "         [ -4211.0005,  -5870.0254,   3189.0137,  ...,  -1210.0020,\n",
      "          -34622.0078, -21524.0000],\n",
      "         ...,\n",
      "         [ -7654.9854,  14562.9727,  -2368.0146,  ...,   2823.0137,\n",
      "           -3766.0054,   5360.9971],\n",
      "         [ -8752.9805,  -2066.0010, -14249.9785,  ...,  13770.9824,\n",
      "            4041.9951,    392.9976],\n",
      "         [ -2673.0044,    557.9912,  -8903.9873,  ...,  -8789.9922,\n",
      "          -35497.9883,   2325.0010]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[ -5120.0010,   2246.9990, -15993.0039,  ..., -19818.0020,\n",
      "           -9385.0020,  11924.0020],\n",
      "         [  8587.0020, -13793.9922,  -5307.0146,  ...,  -9188.0215,\n",
      "           -3875.9985,    438.0127],\n",
      "         [ -4211.0005,  -5870.0254,   3189.0137,  ...,  -1210.0020,\n",
      "          -34622.0078, -21524.0000],\n",
      "         ...,\n",
      "         [ -7654.9854,  14562.9727,  -2368.0146,  ...,   2823.0137,\n",
      "           -3766.0054,   5360.9971],\n",
      "         [ -8752.9805,  -2066.0010, -14249.9785,  ...,  13770.9824,\n",
      "            4041.9951,    392.9976],\n",
      "         [ -2673.0044,    557.9912,  -8903.9873,  ...,  -8789.9922,\n",
      "          -35497.9883,   2325.0010]]]),) and output (tensor([[[ -6685.0010,  -1278.0010, -15553.0039,  ..., -19846.0020,\n",
      "          -10418.0020,  13376.0020],\n",
      "         [ 13268.0020, -19219.9922,  -4582.0146,  ...,  -8949.0215,\n",
      "           -8382.9980,  -1501.9873],\n",
      "         [ -6819.0005,  -3228.0254,   2610.0137,  ...,   -680.0020,\n",
      "          -37837.0078, -19824.0000],\n",
      "         ...,\n",
      "         [ -9381.9854,  13521.9727,  -8541.0146,  ...,   4410.0137,\n",
      "            1195.9946,   8195.9971],\n",
      "         [-12625.9805,  10665.9990, -16081.9785,  ...,  11910.9824,\n",
      "            2171.9951,   4977.9976],\n",
      "         [   -96.0044,  -1781.0088, -14062.9873,  ...,  -9841.9922,\n",
      "          -40759.9883,   3730.0010]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[ -6685.0010,  -1278.0010, -15553.0039,  ..., -19846.0020,\n",
      "          -10418.0020,  13376.0020],\n",
      "         [ 13268.0020, -19219.9922,  -4582.0146,  ...,  -8949.0215,\n",
      "           -8382.9980,  -1501.9873],\n",
      "         [ -6819.0005,  -3228.0254,   2610.0137,  ...,   -680.0020,\n",
      "          -37837.0078, -19824.0000],\n",
      "         ...,\n",
      "         [ -9381.9854,  13521.9727,  -8541.0146,  ...,   4410.0137,\n",
      "            1195.9946,   8195.9971],\n",
      "         [-12625.9805,  10665.9990, -16081.9785,  ...,  11910.9824,\n",
      "            2171.9951,   4977.9976],\n",
      "         [   -96.0044,  -1781.0088, -14062.9873,  ...,  -9841.9922,\n",
      "          -40759.9883,   3730.0010]]]),) and output (tensor([[[ -8422.0010,    339.9990, -15289.0039,  ..., -20884.0020,\n",
      "           -6908.0020,  11014.0020],\n",
      "         [ 12017.0020, -14929.9922,  -3492.0146,  ...,  -6892.0215,\n",
      "           -7859.9980,    195.0127],\n",
      "         [  1114.9995,   -468.0254,   1870.0137,  ...,   1495.9980,\n",
      "          -40190.0078, -19381.0000],\n",
      "         ...,\n",
      "         [-16904.9844,  18883.9727, -10391.0146,  ...,   6339.0137,\n",
      "             -72.0054,  14083.9971],\n",
      "         [-14345.9805,  10258.9990, -19316.9785,  ...,  12469.9824,\n",
      "            5037.9951,  -1837.0024],\n",
      "         [   993.9956,  -1217.0088, -21377.9883,  ..., -11483.9922,\n",
      "          -45696.9883,   7941.0010]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[ -8422.0010,    339.9990, -15289.0039,  ..., -20884.0020,\n",
      "           -6908.0020,  11014.0020],\n",
      "         [ 12017.0020, -14929.9922,  -3492.0146,  ...,  -6892.0215,\n",
      "           -7859.9980,    195.0127],\n",
      "         [  1114.9995,   -468.0254,   1870.0137,  ...,   1495.9980,\n",
      "          -40190.0078, -19381.0000],\n",
      "         ...,\n",
      "         [-16904.9844,  18883.9727, -10391.0146,  ...,   6339.0137,\n",
      "             -72.0054,  14083.9971],\n",
      "         [-14345.9805,  10258.9990, -19316.9785,  ...,  12469.9824,\n",
      "            5037.9951,  -1837.0024],\n",
      "         [   993.9956,  -1217.0088, -21377.9883,  ..., -11483.9922,\n",
      "          -45696.9883,   7941.0010]]]),) and output (tensor([[[-10989.0010,   7034.9990, -13964.0039,  ..., -23968.0020,\n",
      "           -3749.0020,  11730.0020],\n",
      "         [ 14191.0020, -21276.9922,  -9793.0146,  ...,  -4601.0215,\n",
      "           -9456.9980,   -728.9873],\n",
      "         [ -1062.0005,  -3798.0254,   5971.0137,  ...,   6875.9980,\n",
      "          -40776.0078, -15951.0000],\n",
      "         ...,\n",
      "         [-23841.9844,  18517.9727,  -6471.0146,  ...,  12348.0137,\n",
      "           -3449.0054,  12459.9971],\n",
      "         [ -7009.9805,   6081.9990, -20406.9785,  ...,  17103.9824,\n",
      "            4224.9951,  -3931.0024],\n",
      "         [  5898.9956,   2591.9912, -28882.9883,  ..., -13890.9922,\n",
      "          -46327.9883,   1985.0010]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[-10989.0010,   7034.9990, -13964.0039,  ..., -23968.0020,\n",
      "           -3749.0020,  11730.0020],\n",
      "         [ 14191.0020, -21276.9922,  -9793.0146,  ...,  -4601.0215,\n",
      "           -9456.9980,   -728.9873],\n",
      "         [ -1062.0005,  -3798.0254,   5971.0137,  ...,   6875.9980,\n",
      "          -40776.0078, -15951.0000],\n",
      "         ...,\n",
      "         [-23841.9844,  18517.9727,  -6471.0146,  ...,  12348.0137,\n",
      "           -3449.0054,  12459.9971],\n",
      "         [ -7009.9805,   6081.9990, -20406.9785,  ...,  17103.9824,\n",
      "            4224.9951,  -3931.0024],\n",
      "         [  5898.9956,   2591.9912, -28882.9883,  ..., -13890.9922,\n",
      "          -46327.9883,   1985.0010]]]),) and output (tensor([[[ -7159.0010,   6621.9990, -10967.0039,  ..., -17477.0020,\n",
      "           -2510.0020,  11888.0020],\n",
      "         [ 11733.0020, -24720.9922,  -5820.0146,  ...,    -89.0215,\n",
      "          -11260.9980,    547.0127],\n",
      "         [  -168.0005,  -7332.0254,  10108.0137,  ...,   4594.9980,\n",
      "          -46169.0078, -19664.0000],\n",
      "         ...,\n",
      "         [-20298.9844,  15828.9727,  -8673.0146,  ...,  14279.0137,\n",
      "           -2961.0054,  16927.9961],\n",
      "         [ -5672.9805,   9471.9990, -19641.9785,  ...,  19322.9824,\n",
      "            4617.9951,  -6497.0024],\n",
      "         [  4926.9956,   5285.9912, -32530.9883,  ..., -14579.9922,\n",
      "          -47276.9883,  -4243.9990]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[ -7159.0010,   6621.9990, -10967.0039,  ..., -17477.0020,\n",
      "           -2510.0020,  11888.0020],\n",
      "         [ 11733.0020, -24720.9922,  -5820.0146,  ...,    -89.0215,\n",
      "          -11260.9980,    547.0127],\n",
      "         [  -168.0005,  -7332.0254,  10108.0137,  ...,   4594.9980,\n",
      "          -46169.0078, -19664.0000],\n",
      "         ...,\n",
      "         [-20298.9844,  15828.9727,  -8673.0146,  ...,  14279.0137,\n",
      "           -2961.0054,  16927.9961],\n",
      "         [ -5672.9805,   9471.9990, -19641.9785,  ...,  19322.9824,\n",
      "            4617.9951,  -6497.0024],\n",
      "         [  4926.9956,   5285.9912, -32530.9883,  ..., -14579.9922,\n",
      "          -47276.9883,  -4243.9990]]]),) and output (tensor([[[-4.8220e+03,  3.1250e+03, -3.8850e+03,  ..., -1.9007e+04,\n",
      "          -5.5100e+02,  1.6285e+04],\n",
      "         [ 9.1830e+03, -2.7662e+04, -7.2740e+03,  ..., -1.0040e+03,\n",
      "          -9.9170e+03,  2.7910e+03],\n",
      "         [ 3.3210e+03, -3.9170e+03,  8.0290e+03,  ...,  4.1130e+03,\n",
      "          -4.9987e+04, -2.6502e+04],\n",
      "         ...,\n",
      "         [-1.8112e+04,  1.2234e+04, -1.9940e+04,  ...,  1.3093e+04,\n",
      "          -1.8730e+03,  9.8130e+03],\n",
      "         [-8.5240e+03,  9.5300e+03, -1.4863e+04,  ...,  1.8166e+04,\n",
      "           2.8910e+03, -5.6670e+03],\n",
      "         [-2.0380e+03,  5.6340e+03, -3.5834e+04,  ..., -1.1713e+04,\n",
      "          -5.6965e+04, -2.9999e+01]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[-4.8220e+03,  3.1250e+03, -3.8850e+03,  ..., -1.9007e+04,\n",
      "          -5.5100e+02,  1.6285e+04],\n",
      "         [ 9.1830e+03, -2.7662e+04, -7.2740e+03,  ..., -1.0040e+03,\n",
      "          -9.9170e+03,  2.7910e+03],\n",
      "         [ 3.3210e+03, -3.9170e+03,  8.0290e+03,  ...,  4.1130e+03,\n",
      "          -4.9987e+04, -2.6502e+04],\n",
      "         ...,\n",
      "         [-1.8112e+04,  1.2234e+04, -1.9940e+04,  ...,  1.3093e+04,\n",
      "          -1.8730e+03,  9.8130e+03],\n",
      "         [-8.5240e+03,  9.5300e+03, -1.4863e+04,  ...,  1.8166e+04,\n",
      "           2.8910e+03, -5.6670e+03],\n",
      "         [-2.0380e+03,  5.6340e+03, -3.5834e+04,  ..., -1.1713e+04,\n",
      "          -5.6965e+04, -2.9999e+01]]]),) and output (tensor([[[ -7235.0010,   6583.9990,  -4212.0039,  ..., -19104.0020,\n",
      "            3087.9980,  17084.0020],\n",
      "         [  6121.0020, -39461.9922,  -9547.0146,  ...,   8192.9785,\n",
      "           -4136.9980,  -2031.9873],\n",
      "         [  8857.0000,  -1871.0254,   9600.0137,  ...,   -255.0020,\n",
      "          -53995.0078, -26808.0000],\n",
      "         ...,\n",
      "         [-14898.9844,   6399.9727, -18917.0156,  ...,  20111.0137,\n",
      "           -8279.0059,  10213.9961],\n",
      "         [-15330.9805,  12481.9990, -11753.9785,  ...,  13829.9824,\n",
      "            2866.9951,  -2633.0024],\n",
      "         [ -6068.0044,   2384.9912, -36736.9883,  ...,  -6890.9922,\n",
      "          -61156.9883,    807.0010]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[ -7235.0010,   6583.9990,  -4212.0039,  ..., -19104.0020,\n",
      "            3087.9980,  17084.0020],\n",
      "         [  6121.0020, -39461.9922,  -9547.0146,  ...,   8192.9785,\n",
      "           -4136.9980,  -2031.9873],\n",
      "         [  8857.0000,  -1871.0254,   9600.0137,  ...,   -255.0020,\n",
      "          -53995.0078, -26808.0000],\n",
      "         ...,\n",
      "         [-14898.9844,   6399.9727, -18917.0156,  ...,  20111.0137,\n",
      "           -8279.0059,  10213.9961],\n",
      "         [-15330.9805,  12481.9990, -11753.9785,  ...,  13829.9824,\n",
      "            2866.9951,  -2633.0024],\n",
      "         [ -6068.0044,   2384.9912, -36736.9883,  ...,  -6890.9922,\n",
      "          -61156.9883,    807.0010]]]),) and output (tensor([[[ -5586.0010,   7645.9990,   -505.0039,  ..., -15346.0020,\n",
      "            -623.0020,  12986.0020],\n",
      "         [  8172.0020, -44698.9922,  -9584.0146,  ...,  12642.9785,\n",
      "          -12800.9980,  13777.0127],\n",
      "         [  7972.0000,  -2260.0254,  13912.0137,  ...,  -1627.0020,\n",
      "          -58444.0078, -29501.0000],\n",
      "         ...,\n",
      "         [-13639.9844,   6662.9727, -15456.0156,  ...,  23130.0137,\n",
      "          -14065.0059,   6592.9961],\n",
      "         [-11917.9805,   8628.9990,  -4656.9785,  ...,  16997.9824,\n",
      "            4026.9951,  -6149.0024],\n",
      "         [-10652.0039,   5845.9912, -39713.9883,  ..., -11207.9922,\n",
      "          -59380.9883,   -293.9990]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[ -5586.0010,   7645.9990,   -505.0039,  ..., -15346.0020,\n",
      "            -623.0020,  12986.0020],\n",
      "         [  8172.0020, -44698.9922,  -9584.0146,  ...,  12642.9785,\n",
      "          -12800.9980,  13777.0127],\n",
      "         [  7972.0000,  -2260.0254,  13912.0137,  ...,  -1627.0020,\n",
      "          -58444.0078, -29501.0000],\n",
      "         ...,\n",
      "         [-13639.9844,   6662.9727, -15456.0156,  ...,  23130.0137,\n",
      "          -14065.0059,   6592.9961],\n",
      "         [-11917.9805,   8628.9990,  -4656.9785,  ...,  16997.9824,\n",
      "            4026.9951,  -6149.0024],\n",
      "         [-10652.0039,   5845.9912, -39713.9883,  ..., -11207.9922,\n",
      "          -59380.9883,   -293.9990]]]),) and output (tensor([[[ -8916.0010,   2362.9990,  -4833.0039,  ..., -12075.0020,\n",
      "           -1124.0020,  12438.0020],\n",
      "         [ 12693.0020, -46750.9922,  -5603.0146,  ...,  17495.9785,\n",
      "          -21775.9980,  21326.0117],\n",
      "         [ 10268.0000,  -8558.0254,  12568.0137,  ...,  -7362.0020,\n",
      "          -53248.0078, -34133.0000],\n",
      "         ...,\n",
      "         [-11641.9844,   8790.9727, -21063.0156,  ...,  28570.0137,\n",
      "          -14041.0059,   -322.0039],\n",
      "         [-11823.9805,  11033.9990,  -3553.9785,  ...,  24406.9824,\n",
      "            4113.9951,  -2725.0024],\n",
      "         [ -9079.0039,   6347.9912, -45636.9883,  ..., -10351.9922,\n",
      "          -67077.9844,  -5676.9990]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[ -8916.0010,   2362.9990,  -4833.0039,  ..., -12075.0020,\n",
      "           -1124.0020,  12438.0020],\n",
      "         [ 12693.0020, -46750.9922,  -5603.0146,  ...,  17495.9785,\n",
      "          -21775.9980,  21326.0117],\n",
      "         [ 10268.0000,  -8558.0254,  12568.0137,  ...,  -7362.0020,\n",
      "          -53248.0078, -34133.0000],\n",
      "         ...,\n",
      "         [-11641.9844,   8790.9727, -21063.0156,  ...,  28570.0137,\n",
      "          -14041.0059,   -322.0039],\n",
      "         [-11823.9805,  11033.9990,  -3553.9785,  ...,  24406.9824,\n",
      "            4113.9951,  -2725.0024],\n",
      "         [ -9079.0039,   6347.9912, -45636.9883,  ..., -10351.9922,\n",
      "          -67077.9844,  -5676.9990]]]),) and output (tensor([[[-20230.0000,   7877.9990,    161.9961,  ..., -10250.0020,\n",
      "             676.9980,   7095.0020],\n",
      "         [ 10282.0020, -46177.9922,    514.9854,  ...,  12438.9785,\n",
      "          -20846.9980,  24003.0117],\n",
      "         [  7695.0000,  -8135.0254,  20462.0137,  ...,  -8468.0020,\n",
      "          -54023.0078, -35098.0000],\n",
      "         ...,\n",
      "         [-14097.9844,  17571.9727, -19513.0156,  ...,  27595.0137,\n",
      "           -1112.0059,  -2532.0039],\n",
      "         [-12734.9805,  13284.9990,    437.0215,  ...,  20324.9824,\n",
      "            9274.9951,   2983.9976],\n",
      "         [ -9599.0039,   6842.9912, -37707.9883,  ...,  -2657.9922,\n",
      "          -71994.9844, -10156.9990]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[-20230.0000,   7877.9990,    161.9961,  ..., -10250.0020,\n",
      "             676.9980,   7095.0020],\n",
      "         [ 10282.0020, -46177.9922,    514.9854,  ...,  12438.9785,\n",
      "          -20846.9980,  24003.0117],\n",
      "         [  7695.0000,  -8135.0254,  20462.0137,  ...,  -8468.0020,\n",
      "          -54023.0078, -35098.0000],\n",
      "         ...,\n",
      "         [-14097.9844,  17571.9727, -19513.0156,  ...,  27595.0137,\n",
      "           -1112.0059,  -2532.0039],\n",
      "         [-12734.9805,  13284.9990,    437.0215,  ...,  20324.9824,\n",
      "            9274.9951,   2983.9976],\n",
      "         [ -9599.0039,   6842.9912, -37707.9883,  ...,  -2657.9922,\n",
      "          -71994.9844, -10156.9990]]]),) and output (tensor([[[-25670.0000,  13293.9990,   9099.9961,  ...,  -6547.0020,\n",
      "           -5658.0020,   4192.0020],\n",
      "         [ 12458.0020, -43757.9922,   4374.9854,  ...,  13347.9785,\n",
      "          -32417.9980,  23991.0117],\n",
      "         [  4028.0000,  -2948.0254,  21984.0137,  ...,  -6501.0020,\n",
      "          -56247.0078, -28814.0000],\n",
      "         ...,\n",
      "         [-12237.9844,  14396.9727, -20959.0156,  ...,  22516.0137,\n",
      "            1968.9941,  -3066.0039],\n",
      "         [-14342.9805,  12324.9990,   7684.0215,  ...,  28248.9824,\n",
      "            3286.9951,   6370.9976],\n",
      "         [-16460.0039,  17387.9922, -38747.9883,  ...,   1163.0078,\n",
      "          -73023.9844, -18043.0000]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[-25670.0000,  13293.9990,   9099.9961,  ...,  -6547.0020,\n",
      "           -5658.0020,   4192.0020],\n",
      "         [ 12458.0020, -43757.9922,   4374.9854,  ...,  13347.9785,\n",
      "          -32417.9980,  23991.0117],\n",
      "         [  4028.0000,  -2948.0254,  21984.0137,  ...,  -6501.0020,\n",
      "          -56247.0078, -28814.0000],\n",
      "         ...,\n",
      "         [-12237.9844,  14396.9727, -20959.0156,  ...,  22516.0137,\n",
      "            1968.9941,  -3066.0039],\n",
      "         [-14342.9805,  12324.9990,   7684.0215,  ...,  28248.9824,\n",
      "            3286.9951,   6370.9976],\n",
      "         [-16460.0039,  17387.9922, -38747.9883,  ...,   1163.0078,\n",
      "          -73023.9844, -18043.0000]]]),) and output (tensor([[[-25652.0000,  13180.9990,  15728.9961,  ...,  -2831.0020,\n",
      "             158.9980,   2204.0020],\n",
      "         [ 19377.0020, -40001.9922,    128.9854,  ...,  10672.9785,\n",
      "          -25628.9980,  22200.0117],\n",
      "         [  3731.0000,   -836.0254,  27158.0137,  ...,  -3981.0020,\n",
      "          -47751.0078, -28649.0000],\n",
      "         ...,\n",
      "         [ -8984.9844,   9401.9727, -21233.0156,  ...,  27637.0137,\n",
      "           -3357.0059,   1343.9961],\n",
      "         [-19797.9805,  16376.9990,  10031.0215,  ...,  28066.9824,\n",
      "           -1023.0049,   4015.9976],\n",
      "         [-12838.0039,  20187.9922, -40546.9883,  ...,   3256.0078,\n",
      "          -75306.9844, -23385.0000]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[-25652.0000,  13180.9990,  15728.9961,  ...,  -2831.0020,\n",
      "             158.9980,   2204.0020],\n",
      "         [ 19377.0020, -40001.9922,    128.9854,  ...,  10672.9785,\n",
      "          -25628.9980,  22200.0117],\n",
      "         [  3731.0000,   -836.0254,  27158.0137,  ...,  -3981.0020,\n",
      "          -47751.0078, -28649.0000],\n",
      "         ...,\n",
      "         [ -8984.9844,   9401.9727, -21233.0156,  ...,  27637.0137,\n",
      "           -3357.0059,   1343.9961],\n",
      "         [-19797.9805,  16376.9990,  10031.0215,  ...,  28066.9824,\n",
      "           -1023.0049,   4015.9976],\n",
      "         [-12838.0039,  20187.9922, -40546.9883,  ...,   3256.0078,\n",
      "          -75306.9844, -23385.0000]]]),) and output (tensor([[[-14791.0000,  15299.9990,  16129.9961,  ...,   3187.9980,\n",
      "           -2552.0020,   5626.0020],\n",
      "         [ 20246.0020, -29878.9922,  -1164.0146,  ...,  18366.9785,\n",
      "          -41291.0000,  29674.0117],\n",
      "         [ 11070.0000, -10347.0254,  29521.0137,  ...,  -4646.0020,\n",
      "          -50859.0078, -31774.0000],\n",
      "         ...,\n",
      "         [  2179.0156,  15233.9727, -23376.0156,  ...,  27280.0137,\n",
      "          -11002.0059,  -1176.0039],\n",
      "         [ -9733.9805,  13759.9990,  11040.0215,  ...,  22192.9824,\n",
      "           -4647.0049,   1230.9976],\n",
      "         [ -4315.0039,   8386.9922, -47661.9883,  ...,   4474.0078,\n",
      "          -78429.9844, -21920.0000]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[-14791.0000,  15299.9990,  16129.9961,  ...,   3187.9980,\n",
      "           -2552.0020,   5626.0020],\n",
      "         [ 20246.0020, -29878.9922,  -1164.0146,  ...,  18366.9785,\n",
      "          -41291.0000,  29674.0117],\n",
      "         [ 11070.0000, -10347.0254,  29521.0137,  ...,  -4646.0020,\n",
      "          -50859.0078, -31774.0000],\n",
      "         ...,\n",
      "         [  2179.0156,  15233.9727, -23376.0156,  ...,  27280.0137,\n",
      "          -11002.0059,  -1176.0039],\n",
      "         [ -9733.9805,  13759.9990,  11040.0215,  ...,  22192.9824,\n",
      "           -4647.0049,   1230.9976],\n",
      "         [ -4315.0039,   8386.9922, -47661.9883,  ...,   4474.0078,\n",
      "          -78429.9844, -21920.0000]]]),) and output (tensor([[[-1.3284e+04,  2.6639e+04,  7.4680e+03,  ...,  4.1140e+03,\n",
      "          -6.6080e+03,  5.3360e+03],\n",
      "         [ 1.9486e+04, -2.7669e+04, -3.1301e+02,  ...,  2.5119e+04,\n",
      "          -4.3711e+04,  2.1585e+04],\n",
      "         [ 5.9140e+03, -7.6050e+03,  2.8839e+04,  ..., -1.0444e+04,\n",
      "          -5.0569e+04, -3.3664e+04],\n",
      "         ...,\n",
      "         [-2.3000e+03,  1.5113e+04, -2.8455e+04,  ...,  2.8449e+04,\n",
      "          -2.4134e+04,  6.0996e+01],\n",
      "         [-5.4550e+03,  1.3645e+04,  1.0241e+04,  ...,  2.4389e+04,\n",
      "          -1.6869e+04,  1.6110e+03],\n",
      "         [-7.9370e+03, -3.1901e+02, -4.8027e+04,  ...,  4.8360e+03,\n",
      "          -7.6970e+04, -2.2706e+04]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[-1.3284e+04,  2.6639e+04,  7.4680e+03,  ...,  4.1140e+03,\n",
      "          -6.6080e+03,  5.3360e+03],\n",
      "         [ 1.9486e+04, -2.7669e+04, -3.1301e+02,  ...,  2.5119e+04,\n",
      "          -4.3711e+04,  2.1585e+04],\n",
      "         [ 5.9140e+03, -7.6050e+03,  2.8839e+04,  ..., -1.0444e+04,\n",
      "          -5.0569e+04, -3.3664e+04],\n",
      "         ...,\n",
      "         [-2.3000e+03,  1.5113e+04, -2.8455e+04,  ...,  2.8449e+04,\n",
      "          -2.4134e+04,  6.0996e+01],\n",
      "         [-5.4550e+03,  1.3645e+04,  1.0241e+04,  ...,  2.4389e+04,\n",
      "          -1.6869e+04,  1.6110e+03],\n",
      "         [-7.9370e+03, -3.1901e+02, -4.8027e+04,  ...,  4.8360e+03,\n",
      "          -7.6970e+04, -2.2706e+04]]]),) and output (tensor([[[-21532.0000,  32796.0000,  16281.9961,  ...,  18208.9980,\n",
      "           -4282.0020,   5756.0020],\n",
      "         [ 15028.0020, -36125.9922,   6979.9854,  ...,  21830.9785,\n",
      "          -40380.0000,  17393.0117],\n",
      "         [  6998.0000, -10815.0254,  37178.0156,  ..., -12888.0020,\n",
      "          -33794.0078, -21882.0000],\n",
      "         ...,\n",
      "         [  4108.0156,  15269.9727, -35020.0156,  ...,  20310.0137,\n",
      "          -26144.0059,   3448.9961],\n",
      "         [ -7475.9805,   7631.9990,  21802.0215,  ...,  28537.9824,\n",
      "          -11516.0039,  -3952.0024],\n",
      "         [-15491.0039,   1962.9922, -49203.9883,  ...,    869.0078,\n",
      "          -80459.9844, -22019.0000]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[-21532.0000,  32796.0000,  16281.9961,  ...,  18208.9980,\n",
      "           -4282.0020,   5756.0020],\n",
      "         [ 15028.0020, -36125.9922,   6979.9854,  ...,  21830.9785,\n",
      "          -40380.0000,  17393.0117],\n",
      "         [  6998.0000, -10815.0254,  37178.0156,  ..., -12888.0020,\n",
      "          -33794.0078, -21882.0000],\n",
      "         ...,\n",
      "         [  4108.0156,  15269.9727, -35020.0156,  ...,  20310.0137,\n",
      "          -26144.0059,   3448.9961],\n",
      "         [ -7475.9805,   7631.9990,  21802.0215,  ...,  28537.9824,\n",
      "          -11516.0039,  -3952.0024],\n",
      "         [-15491.0039,   1962.9922, -49203.9883,  ...,    869.0078,\n",
      "          -80459.9844, -22019.0000]]]),) and output (tensor([[[-24082.0000,  33387.0000,  15374.9961,  ...,  18359.9980,\n",
      "           -7500.0020,  11519.0020],\n",
      "         [ 11843.0020, -38116.9922,   7034.9854,  ...,  16349.9785,\n",
      "          -36845.0000,  18862.0117],\n",
      "         [  3018.0000, -17399.0254,  35109.0156,  ..., -12020.0020,\n",
      "          -34788.0078, -15221.0000],\n",
      "         ...,\n",
      "         [  4341.0156,  11991.9727, -39821.0156,  ...,  20735.0137,\n",
      "          -30102.0059,  -2179.0039],\n",
      "         [   582.0195,  14179.9990,  27209.0215,  ...,  31171.9824,\n",
      "           -7166.0039,   8750.9980],\n",
      "         [ -9314.0039,   2298.9922, -42373.9883,  ...,  -6624.9922,\n",
      "          -80607.9844, -28147.0000]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[-24082.0000,  33387.0000,  15374.9961,  ...,  18359.9980,\n",
      "           -7500.0020,  11519.0020],\n",
      "         [ 11843.0020, -38116.9922,   7034.9854,  ...,  16349.9785,\n",
      "          -36845.0000,  18862.0117],\n",
      "         [  3018.0000, -17399.0254,  35109.0156,  ..., -12020.0020,\n",
      "          -34788.0078, -15221.0000],\n",
      "         ...,\n",
      "         [  4341.0156,  11991.9727, -39821.0156,  ...,  20735.0137,\n",
      "          -30102.0059,  -2179.0039],\n",
      "         [   582.0195,  14179.9990,  27209.0215,  ...,  31171.9824,\n",
      "           -7166.0039,   8750.9980],\n",
      "         [ -9314.0039,   2298.9922, -42373.9883,  ...,  -6624.9922,\n",
      "          -80607.9844, -28147.0000]]]),) and output (tensor([[[-1.4869e+04,  3.2284e+04,  3.7430e+03,  ...,  1.2459e+04,\n",
      "          -5.4520e+03,  7.5900e+03],\n",
      "         [ 2.7800e+03, -3.5029e+04,  3.0450e+03,  ...,  1.2198e+04,\n",
      "          -4.6565e+04,  1.5448e+04],\n",
      "         [ 4.1410e+03, -1.9966e+04,  3.6735e+04,  ..., -1.6256e+04,\n",
      "          -2.6197e+04, -9.5600e+03],\n",
      "         ...,\n",
      "         [ 1.1760e+03,  1.3116e+04, -3.3478e+04,  ...,  2.0370e+04,\n",
      "          -2.8630e+04,  2.8996e+01],\n",
      "         [-6.6200e+03,  1.2725e+04,  2.5670e+04,  ...,  3.1466e+04,\n",
      "          -5.2680e+03,  9.4500e+03],\n",
      "         [-6.6640e+03,  3.2810e+03, -4.5668e+04,  ..., -8.5380e+03,\n",
      "          -8.0504e+04, -2.1830e+04]]]),)\n",
      "[Debug] Layer names being passed: ['model.layers.0', 'model.layers.1', 'model.layers.2', 'model.layers.3', 'model.layers.4', 'model.layers.5', 'model.layers.6', 'model.layers.7', 'model.layers.8', 'model.layers.9', 'model.layers.10', 'model.layers.11', 'model.layers.12', 'model.layers.13', 'model.layers.14', 'model.layers.15', 'model.layers.16', 'model.layers.17', 'model.layers.18', 'model.layers.19', 'model.layers.20', 'model.layers.21', 'model.layers.22', 'model.layers.23', 'model.layers.24', 'model.layers.25', 'model.layers.26', 'model.layers.27', 'model.embed_tokens']\n",
      "[Debug] Trying to access layer: model.layers.0\n",
      "[Debug] Successfully found layer: model.layers.0\n",
      "[Debug] Trying to access layer: model.layers.1\n",
      "[Debug] Successfully found layer: model.layers.1\n",
      "[Debug] Trying to access layer: model.layers.2\n",
      "[Debug] Successfully found layer: model.layers.2\n",
      "[Debug] Trying to access layer: model.layers.3\n",
      "[Debug] Successfully found layer: model.layers.3\n",
      "[Debug] Trying to access layer: model.layers.4\n",
      "[Debug] Successfully found layer: model.layers.4\n",
      "[Debug] Trying to access layer: model.layers.5\n",
      "[Debug] Successfully found layer: model.layers.5\n",
      "[Debug] Trying to access layer: model.layers.6\n",
      "[Debug] Successfully found layer: model.layers.6\n",
      "[Debug] Trying to access layer: model.layers.7\n",
      "[Debug] Successfully found layer: model.layers.7\n",
      "[Debug] Trying to access layer: model.layers.8\n",
      "[Debug] Successfully found layer: model.layers.8\n",
      "[Debug] Trying to access layer: model.layers.9\n",
      "[Debug] Successfully found layer: model.layers.9\n",
      "[Debug] Trying to access layer: model.layers.10\n",
      "[Debug] Successfully found layer: model.layers.10\n",
      "[Debug] Trying to access layer: model.layers.11\n",
      "[Debug] Successfully found layer: model.layers.11\n",
      "[Debug] Trying to access layer: model.layers.12\n",
      "[Debug] Successfully found layer: model.layers.12\n",
      "[Debug] Trying to access layer: model.layers.13\n",
      "[Debug] Successfully found layer: model.layers.13\n",
      "[Debug] Trying to access layer: model.layers.14\n",
      "[Debug] Successfully found layer: model.layers.14\n",
      "[Debug] Trying to access layer: model.layers.15\n",
      "[Debug] Successfully found layer: model.layers.15\n",
      "[Debug] Trying to access layer: model.layers.16\n",
      "[Debug] Successfully found layer: model.layers.16\n",
      "[Debug] Trying to access layer: model.layers.17\n",
      "[Debug] Successfully found layer: model.layers.17\n",
      "[Debug] Trying to access layer: model.layers.18\n",
      "[Debug] Successfully found layer: model.layers.18\n",
      "[Debug] Trying to access layer: model.layers.19\n",
      "[Debug] Successfully found layer: model.layers.19\n",
      "[Debug] Trying to access layer: model.layers.20\n",
      "[Debug] Successfully found layer: model.layers.20\n",
      "[Debug] Trying to access layer: model.layers.21\n",
      "[Debug] Successfully found layer: model.layers.21\n",
      "[Debug] Trying to access layer: model.layers.22\n",
      "[Debug] Successfully found layer: model.layers.22\n",
      "[Debug] Trying to access layer: model.layers.23\n",
      "[Debug] Successfully found layer: model.layers.23\n",
      "[Debug] Trying to access layer: model.layers.24\n",
      "[Debug] Successfully found layer: model.layers.24\n",
      "[Debug] Trying to access layer: model.layers.25\n",
      "[Debug] Successfully found layer: model.layers.25\n",
      "[Debug] Trying to access layer: model.layers.26\n",
      "[Debug] Successfully found layer: model.layers.26\n",
      "[Debug] Trying to access layer: model.layers.27\n",
      "[Debug] Successfully found layer: model.layers.27\n",
      "[Debug] Trying to access layer: model.embed_tokens\n",
      "[Debug] Successfully found layer: model.embed_tokens\n",
      "[Hook] Layer Embedding(128256, 3072, padding_idx=128004) received input (tensor([[128000,   2460,    358,  24133,    320,     32,   6187,    311,  20474,\n",
      "           5609,      8,    578,   4731,   2835,    369,    279,   5609,     11,\n",
      "            902,    574,  42508,    304,   6664,    220,    679,     15,  17706,\n",
      "             19,     60,    574,   6004,    389,   6186,    220,     21,     11,\n",
      "            220,    679,     16,   8032,     20,     60,   1102,   4519,   3782,\n",
      "            437,    315,  12387,   5526,  21562,    323,  32629,     13,    578,\n",
      "           3782,    437,    527,     25,   8529,   9973,   3520,    320,     32,\n",
      "           6187,    311,  20474,    596,   4846,  62740,    705,  37373,  54859,\n",
      "             89,     11,  48208,   4584,   7368,    315,  68253,  16542,     11,\n",
      "            578,  40301,    468,   7596,   2394,   2649,     11,  40224,   2206,\n",
      "            279,  56551,     11,   8388,  25581,    315,  78113,     11,   9538,\n",
      "          32402,  14093,    315,   1666,    358,  45024,    423,   7169,     11,\n",
      "          15347,  12711,     11,  13929,  68450,     11,   6287,  51016,   3816,\n",
      "             11,  75725,   7834,     11,  13678,   1283,   2642,     88,    315,\n",
      "           1183,    344,   2411,     11,  44847,    435,  69837,    315,  50930,\n",
      "            279,  23404,    321,     11,  11519,  97108,    315,    386,     87,\n",
      "          62077,     11,    323,   2638,   4718,  55293,   8032,     20,     60,\n",
      "           9305,  14936,   2663,    279,   2835,    330,  84270,   9250,   3343,\n",
      "             58,     20,     60]]),) and output tensor([[[-9.9945e-04, -6.5613e-04, -4.6387e-03,  ..., -1.4267e-03,\n",
      "          -2.0447e-03,  1.9684e-03],\n",
      "         [ 2.8442e-02, -1.3053e-05,  1.4526e-02,  ...,  1.0803e-02,\n",
      "          -2.4536e-02,  1.3367e-02],\n",
      "         [-9.4604e-03,  1.5137e-02, -1.3794e-02,  ..., -2.2827e-02,\n",
      "          -8.1177e-03,  3.1891e-03],\n",
      "         ...,\n",
      "         [ 2.4780e-02, -1.1414e-02,  1.5991e-02,  ...,  1.6113e-02,\n",
      "           1.0315e-02, -1.8188e-02],\n",
      "         [ 2.4048e-02,  8.3008e-03,  1.9165e-02,  ..., -1.2512e-02,\n",
      "          -2.0508e-02,  3.1433e-03],\n",
      "         [-4.4861e-03, -8.6060e-03,  1.2573e-02,  ...,  8.2397e-03,\n",
      "           1.2695e-02,  7.4387e-04]]])\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[-9.9945e-04, -6.5613e-04, -4.6387e-03,  ..., -1.4267e-03,\n",
      "          -2.0447e-03,  1.9684e-03],\n",
      "         [ 2.8442e-02, -1.3053e-05,  1.4526e-02,  ...,  1.0803e-02,\n",
      "          -2.4536e-02,  1.3367e-02],\n",
      "         [-9.4604e-03,  1.5137e-02, -1.3794e-02,  ..., -2.2827e-02,\n",
      "          -8.1177e-03,  3.1891e-03],\n",
      "         ...,\n",
      "         [ 2.4780e-02, -1.1414e-02,  1.5991e-02,  ...,  1.6113e-02,\n",
      "           1.0315e-02, -1.8188e-02],\n",
      "         [ 2.4048e-02,  8.3008e-03,  1.9165e-02,  ..., -1.2512e-02,\n",
      "          -2.0508e-02,  3.1433e-03],\n",
      "         [-4.4861e-03, -8.6060e-03,  1.2573e-02,  ...,  8.2397e-03,\n",
      "           1.2695e-02,  7.4387e-04]]]),) and output (tensor([[[ 4.4300e+02,  2.4000e+02,  3.0300e+02,  ..., -1.5400e+02,\n",
      "          -1.5002e+01,  1.8500e+02],\n",
      "         [ 3.9028e+01, -1.4000e+01,  1.7015e+01,  ...,  6.4011e+01,\n",
      "           1.5975e+01,  1.2901e+02],\n",
      "         [ 6.4991e+01, -1.4098e+02, -1.3794e-02,  ..., -3.8023e+01,\n",
      "          -6.2008e+01, -1.5997e+01],\n",
      "         ...,\n",
      "         [-9.8975e+01,  1.7699e+02,  4.6202e+02,  ..., -4.3984e+01,\n",
      "           1.8101e+02, -6.3402e+02],\n",
      "         [ 6.5024e+01, -3.8992e+01, -9.8083e-01,  ...,  2.9875e+00,\n",
      "          -8.3021e+01, -1.6400e+02],\n",
      "         [ 1.0500e+02,  2.4599e+02, -1.2099e+02,  ..., -3.2399e+02,\n",
      "           3.9013e+01, -3.5999e+01]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[ 4.4300e+02,  2.4000e+02,  3.0300e+02,  ..., -1.5400e+02,\n",
      "          -1.5002e+01,  1.8500e+02],\n",
      "         [ 3.9028e+01, -1.4000e+01,  1.7015e+01,  ...,  6.4011e+01,\n",
      "           1.5975e+01,  1.2901e+02],\n",
      "         [ 6.4991e+01, -1.4098e+02, -1.3794e-02,  ..., -3.8023e+01,\n",
      "          -6.2008e+01, -1.5997e+01],\n",
      "         ...,\n",
      "         [-9.8975e+01,  1.7699e+02,  4.6202e+02,  ..., -4.3984e+01,\n",
      "           1.8101e+02, -6.3402e+02],\n",
      "         [ 6.5024e+01, -3.8992e+01, -9.8083e-01,  ...,  2.9875e+00,\n",
      "          -8.3021e+01, -1.6400e+02],\n",
      "         [ 1.0500e+02,  2.4599e+02, -1.2099e+02,  ..., -3.2399e+02,\n",
      "           3.9013e+01, -3.5999e+01]]]),) and output (tensor([[[  439.9990,  -144.0007,  -410.0046,  ...,  -765.0015,\n",
      "           1193.9979, -1027.9980],\n",
      "         [   44.0284,    38.0000,  -684.9855,  ...,  -629.9892,\n",
      "           -379.0245,   661.0134],\n",
      "         [-1089.0095, -1836.9849,  -345.0138,  ...,  -196.0228,\n",
      "            363.9919,   -37.9968],\n",
      "         ...,\n",
      "         [ -528.9752,  1251.9885,   400.0160,  ...,   -94.9839,\n",
      "           -370.9897,  -515.0182],\n",
      "         [   43.0240,   -13.9917,   917.0192,  ...,  1025.9875,\n",
      "          -1021.0205,   -70.9969],\n",
      "         [ -230.0045,   537.9914,   244.0126,  ...,  -141.9918,\n",
      "            593.0127,   -31.9993]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[  439.9990,  -144.0007,  -410.0046,  ...,  -765.0015,\n",
      "           1193.9979, -1027.9980],\n",
      "         [   44.0284,    38.0000,  -684.9855,  ...,  -629.9892,\n",
      "           -379.0245,   661.0134],\n",
      "         [-1089.0095, -1836.9849,  -345.0138,  ...,  -196.0228,\n",
      "            363.9919,   -37.9968],\n",
      "         ...,\n",
      "         [ -528.9752,  1251.9885,   400.0160,  ...,   -94.9839,\n",
      "           -370.9897,  -515.0182],\n",
      "         [   43.0240,   -13.9917,   917.0192,  ...,  1025.9875,\n",
      "          -1021.0205,   -70.9969],\n",
      "         [ -230.0045,   537.9914,   244.0126,  ...,  -141.9918,\n",
      "            593.0127,   -31.9993]]]),) and output (tensor([[[  551.9990,   993.9993, -1790.0046,  ...,   445.9985,\n",
      "          -1386.0021, -1245.9980],\n",
      "         [-3095.9717,  2887.0000,  -457.9855,  ...,  -422.9892,\n",
      "          -2920.0244,  4741.0132],\n",
      "         [-5613.0098, -3285.9849, -1004.0138,  ...,  1935.9772,\n",
      "           2450.9919, -3155.9968],\n",
      "         ...,\n",
      "         [ -229.9752,  1917.9885,  1083.0160,  ...,  1602.0161,\n",
      "           -349.9897, -3239.0181],\n",
      "         [  954.0240,  1760.0083,   763.0192,  ...,  -312.0125,\n",
      "           1044.9795,  -356.9968],\n",
      "         [-1133.0045,  1883.9915,  1423.0126,  ..., -2532.9917,\n",
      "           -416.9873, -1585.9993]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[  551.9990,   993.9993, -1790.0046,  ...,   445.9985,\n",
      "          -1386.0021, -1245.9980],\n",
      "         [-3095.9717,  2887.0000,  -457.9855,  ...,  -422.9892,\n",
      "          -2920.0244,  4741.0132],\n",
      "         [-5613.0098, -3285.9849, -1004.0138,  ...,  1935.9772,\n",
      "           2450.9919, -3155.9968],\n",
      "         ...,\n",
      "         [ -229.9752,  1917.9885,  1083.0160,  ...,  1602.0161,\n",
      "           -349.9897, -3239.0181],\n",
      "         [  954.0240,  1760.0083,   763.0192,  ...,  -312.0125,\n",
      "           1044.9795,  -356.9968],\n",
      "         [-1133.0045,  1883.9915,  1423.0126,  ..., -2532.9917,\n",
      "           -416.9873, -1585.9993]]]),) and output (tensor([[[  2131.9990,   -737.0007,  -1071.0046,  ...,  -2781.0015,\n",
      "            -885.0021,     86.0020],\n",
      "         [  -238.9717,   1188.0000,  -1270.9855,  ...,   1145.0107,\n",
      "           -2430.0244,  10285.0137],\n",
      "         [-11374.0098,  -2105.9849,   5188.9863,  ...,  -3315.0229,\n",
      "            7821.9922,  -4426.9971],\n",
      "         ...,\n",
      "         [   632.0248,   3906.9885,   2825.0161,  ...,  -1772.9839,\n",
      "            3864.0103,  -1339.0181],\n",
      "         [   586.0240,  -2361.9917,   1230.0190,  ...,  -1652.0125,\n",
      "            1564.9795,   -138.9968],\n",
      "         [  1062.9955,    836.9915,    -39.9874,  ...,  -1222.9917,\n",
      "           -2646.9873,   3112.0007]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[  2131.9990,   -737.0007,  -1071.0046,  ...,  -2781.0015,\n",
      "            -885.0021,     86.0020],\n",
      "         [  -238.9717,   1188.0000,  -1270.9855,  ...,   1145.0107,\n",
      "           -2430.0244,  10285.0137],\n",
      "         [-11374.0098,  -2105.9849,   5188.9863,  ...,  -3315.0229,\n",
      "            7821.9922,  -4426.9971],\n",
      "         ...,\n",
      "         [   632.0248,   3906.9885,   2825.0161,  ...,  -1772.9839,\n",
      "            3864.0103,  -1339.0181],\n",
      "         [   586.0240,  -2361.9917,   1230.0190,  ...,  -1652.0125,\n",
      "            1564.9795,   -138.9968],\n",
      "         [  1062.9955,    836.9915,    -39.9874,  ...,  -1222.9917,\n",
      "           -2646.9873,   3112.0007]]]),) and output (tensor([[[  3420.9990,    760.9993,  -5376.0049,  ...,  -4193.0015,\n",
      "            2860.9980,  -4080.9980],\n",
      "         [ -1459.9717,   9408.0000,  -3036.9854,  ...,   5214.0107,\n",
      "           -7646.0244,  15958.0137],\n",
      "         [-13259.0098,  -1955.9849,   5087.9863,  ...,  -3870.0229,\n",
      "            4206.9922,   1031.0029],\n",
      "         ...,\n",
      "         [  -785.9752,   9625.9883,   2167.0161,  ...,   5199.0161,\n",
      "            6804.0103,   -434.0181],\n",
      "         [   136.0240,  -2152.9917,    959.0190,  ...,  -1938.0125,\n",
      "            7281.9795,  -1910.9968],\n",
      "         [   372.9955,   -446.0085,  -2249.9873,  ...,   5363.0083,\n",
      "           -2517.9873,   5438.0010]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[  3420.9990,    760.9993,  -5376.0049,  ...,  -4193.0015,\n",
      "            2860.9980,  -4080.9980],\n",
      "         [ -1459.9717,   9408.0000,  -3036.9854,  ...,   5214.0107,\n",
      "           -7646.0244,  15958.0137],\n",
      "         [-13259.0098,  -1955.9849,   5087.9863,  ...,  -3870.0229,\n",
      "            4206.9922,   1031.0029],\n",
      "         ...,\n",
      "         [  -785.9752,   9625.9883,   2167.0161,  ...,   5199.0161,\n",
      "            6804.0103,   -434.0181],\n",
      "         [   136.0240,  -2152.9917,    959.0190,  ...,  -1938.0125,\n",
      "            7281.9795,  -1910.9968],\n",
      "         [   372.9955,   -446.0085,  -2249.9873,  ...,   5363.0083,\n",
      "           -2517.9873,   5438.0010]]]),) and output (tensor([[[  2742.9990,  -1512.0007, -10493.0049,  ...,  -9945.0020,\n",
      "           -2017.0020,  -1802.9980],\n",
      "         [ -1021.9717,   2975.0000,   -281.9854,  ...,   8084.0107,\n",
      "           -6612.0244,  20914.0137],\n",
      "         [-15992.0098,   1326.0151,   4050.9863,  ...,  -2796.0229,\n",
      "             584.9922,   2142.0029],\n",
      "         ...,\n",
      "         [  -628.9752,   6642.9883,   2416.0161,  ...,   5742.0161,\n",
      "           11405.0098,   1234.9819],\n",
      "         [  1681.0240,  -7466.9917,   8615.0195,  ...,  -2378.0125,\n",
      "            7290.9795,   2299.0032],\n",
      "         [ -2426.0044,  -3694.0085,  -2596.9873,  ...,   3266.0083,\n",
      "           -1218.9873,   4056.0010]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[  2742.9990,  -1512.0007, -10493.0049,  ...,  -9945.0020,\n",
      "           -2017.0020,  -1802.9980],\n",
      "         [ -1021.9717,   2975.0000,   -281.9854,  ...,   8084.0107,\n",
      "           -6612.0244,  20914.0137],\n",
      "         [-15992.0098,   1326.0151,   4050.9863,  ...,  -2796.0229,\n",
      "             584.9922,   2142.0029],\n",
      "         ...,\n",
      "         [  -628.9752,   6642.9883,   2416.0161,  ...,   5742.0161,\n",
      "           11405.0098,   1234.9819],\n",
      "         [  1681.0240,  -7466.9917,   8615.0195,  ...,  -2378.0125,\n",
      "            7290.9795,   2299.0032],\n",
      "         [ -2426.0044,  -3694.0085,  -2596.9873,  ...,   3266.0083,\n",
      "           -1218.9873,   4056.0010]]]),) and output (tensor([[[  3032.9990,  -3420.0007, -10291.0049,  ...,  -6511.0020,\n",
      "           -1953.0020,   2001.0020],\n",
      "         [  3124.0283,   3528.0000,   3000.0146,  ...,   6298.0107,\n",
      "          -12435.0244,  24495.0137],\n",
      "         [-17837.0098,   2690.0151,   5532.9863,  ...,  -5099.0229,\n",
      "           -3805.0078,   2059.0029],\n",
      "         ...,\n",
      "         [  2220.0249,   1735.9883,   1532.0161,  ...,   8075.0161,\n",
      "            3083.0098,  -1589.0181],\n",
      "         [  7492.0239,  -4510.9917,   6090.0195,  ...,   1663.9875,\n",
      "            5914.9795,  -1155.9968],\n",
      "         [    32.9956,  -5507.0088,  -2244.9873,  ...,   -313.9917,\n",
      "            -316.9873,   4177.0010]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[  3032.9990,  -3420.0007, -10291.0049,  ...,  -6511.0020,\n",
      "           -1953.0020,   2001.0020],\n",
      "         [  3124.0283,   3528.0000,   3000.0146,  ...,   6298.0107,\n",
      "          -12435.0244,  24495.0137],\n",
      "         [-17837.0098,   2690.0151,   5532.9863,  ...,  -5099.0229,\n",
      "           -3805.0078,   2059.0029],\n",
      "         ...,\n",
      "         [  2220.0249,   1735.9883,   1532.0161,  ...,   8075.0161,\n",
      "            3083.0098,  -1589.0181],\n",
      "         [  7492.0239,  -4510.9917,   6090.0195,  ...,   1663.9875,\n",
      "            5914.9795,  -1155.9968],\n",
      "         [    32.9956,  -5507.0088,  -2244.9873,  ...,   -313.9917,\n",
      "            -316.9873,   4177.0010]]]),) and output (tensor([[[  3132.9990,  -1970.0007, -11544.0049,  ...,  -5780.0020,\n",
      "           -2486.0020,  -1921.9980],\n",
      "         [  7782.0283,  10857.0000,   4035.0146,  ...,   5536.0107,\n",
      "          -11552.0244,  32121.0137],\n",
      "         [-22080.0098,   4299.0151,   5186.9863,  ...,  -6599.0229,\n",
      "             509.9922,  -1792.9971],\n",
      "         ...,\n",
      "         [  5065.0249,    956.9883,    205.0161,  ...,   9818.0156,\n",
      "             674.0098,  -1688.0181],\n",
      "         [  4310.0239,     58.0083,   4504.0195,  ...,   5595.9873,\n",
      "            8311.9795,   -738.9968],\n",
      "         [ -3091.0044,  -3938.0088,  -2900.9873,  ...,  -1548.9917,\n",
      "            -946.9873,   8744.0010]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[  3132.9990,  -1970.0007, -11544.0049,  ...,  -5780.0020,\n",
      "           -2486.0020,  -1921.9980],\n",
      "         [  7782.0283,  10857.0000,   4035.0146,  ...,   5536.0107,\n",
      "          -11552.0244,  32121.0137],\n",
      "         [-22080.0098,   4299.0151,   5186.9863,  ...,  -6599.0229,\n",
      "             509.9922,  -1792.9971],\n",
      "         ...,\n",
      "         [  5065.0249,    956.9883,    205.0161,  ...,   9818.0156,\n",
      "             674.0098,  -1688.0181],\n",
      "         [  4310.0239,     58.0083,   4504.0195,  ...,   5595.9873,\n",
      "            8311.9795,   -738.9968],\n",
      "         [ -3091.0044,  -3938.0088,  -2900.9873,  ...,  -1548.9917,\n",
      "            -946.9873,   8744.0010]]]),) and output (tensor([[[  1046.9990,   2403.9993, -14486.0049,  ..., -10922.0020,\n",
      "           -1419.0020,  -2983.9980],\n",
      "         [  5904.0283,   8951.0000,  -1159.9854,  ...,   7734.0107,\n",
      "          -14087.0244,  39264.0156],\n",
      "         [-21199.0098,   6289.0151,   6410.9863,  ...,  -3452.0229,\n",
      "           -4005.0078,   -298.9971],\n",
      "         ...,\n",
      "         [  6011.0249,   1728.9883,   -957.9839,  ...,   9979.0156,\n",
      "            3474.0098,   -768.0181],\n",
      "         [  3285.0239,   3079.0083,   5077.0195,  ...,   6132.9873,\n",
      "            5868.9795,   2938.0032],\n",
      "         [ -5338.0044,   -102.0088,    229.0127,  ...,   -258.9917,\n",
      "           -6526.9873,  12309.0010]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[  1046.9990,   2403.9993, -14486.0049,  ..., -10922.0020,\n",
      "           -1419.0020,  -2983.9980],\n",
      "         [  5904.0283,   8951.0000,  -1159.9854,  ...,   7734.0107,\n",
      "          -14087.0244,  39264.0156],\n",
      "         [-21199.0098,   6289.0151,   6410.9863,  ...,  -3452.0229,\n",
      "           -4005.0078,   -298.9971],\n",
      "         ...,\n",
      "         [  6011.0249,   1728.9883,   -957.9839,  ...,   9979.0156,\n",
      "            3474.0098,   -768.0181],\n",
      "         [  3285.0239,   3079.0083,   5077.0195,  ...,   6132.9873,\n",
      "            5868.9795,   2938.0032],\n",
      "         [ -5338.0044,   -102.0088,    229.0127,  ...,   -258.9917,\n",
      "           -6526.9873,  12309.0010]]]),) and output (tensor([[[ -3767.0010,   5307.9990, -14221.0049,  ..., -15035.0020,\n",
      "           -3630.0020,   4168.0020],\n",
      "         [  8528.0283,   4846.0000,  -1725.9854,  ...,   5807.0107,\n",
      "           -9394.0244,  41582.0156],\n",
      "         [-17864.0098,   4901.0151,   8687.9863,  ..., -11158.0234,\n",
      "           -6433.0078,  -3838.9971],\n",
      "         ...,\n",
      "         [  5062.0249,   2974.9883,  -3720.9839,  ...,   2165.0156,\n",
      "            5116.0098,  -2999.0181],\n",
      "         [  3994.0239,   1552.0083,   5951.0195,  ...,   6499.9873,\n",
      "            2732.9795,   -869.9968],\n",
      "         [ -6509.0044,  -2777.0088,  -1266.9873,  ...,    401.0083,\n",
      "           -7906.9873,  13723.0010]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[ -3767.0010,   5307.9990, -14221.0049,  ..., -15035.0020,\n",
      "           -3630.0020,   4168.0020],\n",
      "         [  8528.0283,   4846.0000,  -1725.9854,  ...,   5807.0107,\n",
      "           -9394.0244,  41582.0156],\n",
      "         [-17864.0098,   4901.0151,   8687.9863,  ..., -11158.0234,\n",
      "           -6433.0078,  -3838.9971],\n",
      "         ...,\n",
      "         [  5062.0249,   2974.9883,  -3720.9839,  ...,   2165.0156,\n",
      "            5116.0098,  -2999.0181],\n",
      "         [  3994.0239,   1552.0083,   5951.0195,  ...,   6499.9873,\n",
      "            2732.9795,   -869.9968],\n",
      "         [ -6509.0044,  -2777.0088,  -1266.9873,  ...,    401.0083,\n",
      "           -7906.9873,  13723.0010]]]),) and output (tensor([[[-1.3260e+03,  1.2240e+03, -1.6643e+04,  ..., -1.3240e+04,\n",
      "          -5.6770e+03,  9.6390e+03],\n",
      "         [ 6.4230e+03,  3.3030e+03, -3.2985e+01,  ...,  1.6160e+03,\n",
      "          -9.7110e+03,  4.7420e+04],\n",
      "         [-2.0713e+04,  9.9030e+03,  9.3880e+03,  ..., -2.1517e+04,\n",
      "          -7.3910e+03, -1.2830e+03],\n",
      "         ...,\n",
      "         [ 9.8480e+03,  6.2590e+03,  8.5002e+02,  ...,  2.1120e+03,\n",
      "          -4.0380e+03, -2.0840e+03],\n",
      "         [ 9.8290e+03,  3.5550e+03,  7.1160e+03,  ...,  5.1490e+03,\n",
      "           5.4390e+03, -6.1320e+03],\n",
      "         [-5.5530e+03, -4.8930e+03, -2.5499e+02,  ..., -8.6299e+02,\n",
      "          -1.3201e+04,  1.2484e+04]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[-1.3260e+03,  1.2240e+03, -1.6643e+04,  ..., -1.3240e+04,\n",
      "          -5.6770e+03,  9.6390e+03],\n",
      "         [ 6.4230e+03,  3.3030e+03, -3.2985e+01,  ...,  1.6160e+03,\n",
      "          -9.7110e+03,  4.7420e+04],\n",
      "         [-2.0713e+04,  9.9030e+03,  9.3880e+03,  ..., -2.1517e+04,\n",
      "          -7.3910e+03, -1.2830e+03],\n",
      "         ...,\n",
      "         [ 9.8480e+03,  6.2590e+03,  8.5002e+02,  ...,  2.1120e+03,\n",
      "          -4.0380e+03, -2.0840e+03],\n",
      "         [ 9.8290e+03,  3.5550e+03,  7.1160e+03,  ...,  5.1490e+03,\n",
      "           5.4390e+03, -6.1320e+03],\n",
      "         [-5.5530e+03, -4.8930e+03, -2.5499e+02,  ..., -8.6299e+02,\n",
      "          -1.3201e+04,  1.2484e+04]]]),) and output (tensor([[[ -5120.0010,   2246.9990, -15993.0039,  ..., -19818.0020,\n",
      "           -9385.0020,  11924.0020],\n",
      "         [ 10985.0283,   2543.0000,   -196.9854,  ...,  -5191.9893,\n",
      "           -6867.0244,  44958.0156],\n",
      "         [-15740.0098,   6284.0156,  11269.9863,  ..., -15581.0234,\n",
      "           -8577.0078,   1677.0029],\n",
      "         ...,\n",
      "         [ 11500.0254,    628.9883,   2463.0161,  ...,   2358.0156,\n",
      "           -4623.9902,  -6737.0181],\n",
      "         [ 11249.0234,  12927.0078,   6033.0195,  ...,   1718.9873,\n",
      "            2947.9795,  -4137.9971],\n",
      "         [   457.9956,  -5911.0088,   -540.9873,  ...,  -8956.9922,\n",
      "          -15494.9873,  10537.0010]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[ -5120.0010,   2246.9990, -15993.0039,  ..., -19818.0020,\n",
      "           -9385.0020,  11924.0020],\n",
      "         [ 10985.0283,   2543.0000,   -196.9854,  ...,  -5191.9893,\n",
      "           -6867.0244,  44958.0156],\n",
      "         [-15740.0098,   6284.0156,  11269.9863,  ..., -15581.0234,\n",
      "           -8577.0078,   1677.0029],\n",
      "         ...,\n",
      "         [ 11500.0254,    628.9883,   2463.0161,  ...,   2358.0156,\n",
      "           -4623.9902,  -6737.0181],\n",
      "         [ 11249.0234,  12927.0078,   6033.0195,  ...,   1718.9873,\n",
      "            2947.9795,  -4137.9971],\n",
      "         [   457.9956,  -5911.0088,   -540.9873,  ...,  -8956.9922,\n",
      "          -15494.9873,  10537.0010]]]),) and output (tensor([[[ -6685.0010,  -1278.0010, -15553.0039,  ..., -19846.0020,\n",
      "          -10418.0020,  13376.0020],\n",
      "         [  5823.0283,    746.0000,   1439.0146,  ...,  -4143.9893,\n",
      "           -9708.0244,  50482.0156],\n",
      "         [-13532.0098,   5270.0156,  14251.9863,  ...,  -9272.0234,\n",
      "          -11358.0078,  10486.0029],\n",
      "         ...,\n",
      "         [ 15584.0254,   -488.0117,   7925.0161,  ...,    503.0156,\n",
      "           -8206.9902,  -9967.0176],\n",
      "         [  7975.0234,  19453.0078,   9270.0195,  ...,   5572.9873,\n",
      "             -88.0205,   -750.9971],\n",
      "         [  2478.9956,  -4338.0088,  -1908.9873,  ...,  -7419.9922,\n",
      "          -11521.9873,  19091.0000]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[ -6685.0010,  -1278.0010, -15553.0039,  ..., -19846.0020,\n",
      "          -10418.0020,  13376.0020],\n",
      "         [  5823.0283,    746.0000,   1439.0146,  ...,  -4143.9893,\n",
      "           -9708.0244,  50482.0156],\n",
      "         [-13532.0098,   5270.0156,  14251.9863,  ...,  -9272.0234,\n",
      "          -11358.0078,  10486.0029],\n",
      "         ...,\n",
      "         [ 15584.0254,   -488.0117,   7925.0161,  ...,    503.0156,\n",
      "           -8206.9902,  -9967.0176],\n",
      "         [  7975.0234,  19453.0078,   9270.0195,  ...,   5572.9873,\n",
      "             -88.0205,   -750.9971],\n",
      "         [  2478.9956,  -4338.0088,  -1908.9873,  ...,  -7419.9922,\n",
      "          -11521.9873,  19091.0000]]]),) and output (tensor([[[ -8422.0010,    339.9990, -15289.0039,  ..., -20884.0020,\n",
      "           -6908.0020,  11014.0020],\n",
      "         [  2634.0283,   8498.0000,   -158.9854,  ...,  -2534.9893,\n",
      "          -11523.0244,  50547.0156],\n",
      "         [ -9446.0098,   4575.0156,  15623.9863,  ...,  -3942.0234,\n",
      "          -10730.0078,  10225.0029],\n",
      "         ...,\n",
      "         [ 17550.0254,   2617.9883,   7687.0156,  ...,  -2100.9844,\n",
      "           -9699.9902,  -6546.0176],\n",
      "         [  6988.0234,  15082.0078,   6844.0195,  ...,   7919.9873,\n",
      "           -5685.0205,  -6268.9971],\n",
      "         [ -6148.0044,  -4164.0088,  -3076.9873,  ...,  -8387.9922,\n",
      "          -14634.9873,  16929.0000]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[ -8422.0010,    339.9990, -15289.0039,  ..., -20884.0020,\n",
      "           -6908.0020,  11014.0020],\n",
      "         [  2634.0283,   8498.0000,   -158.9854,  ...,  -2534.9893,\n",
      "          -11523.0244,  50547.0156],\n",
      "         [ -9446.0098,   4575.0156,  15623.9863,  ...,  -3942.0234,\n",
      "          -10730.0078,  10225.0029],\n",
      "         ...,\n",
      "         [ 17550.0254,   2617.9883,   7687.0156,  ...,  -2100.9844,\n",
      "           -9699.9902,  -6546.0176],\n",
      "         [  6988.0234,  15082.0078,   6844.0195,  ...,   7919.9873,\n",
      "           -5685.0205,  -6268.9971],\n",
      "         [ -6148.0044,  -4164.0088,  -3076.9873,  ...,  -8387.9922,\n",
      "          -14634.9873,  16929.0000]]]),) and output (tensor([[[-10989.0010,   7034.9990, -13964.0039,  ..., -23968.0020,\n",
      "           -3749.0020,  11730.0020],\n",
      "         [  4167.0283,  12086.0000,  -3829.9854,  ...,  -2730.9893,\n",
      "          -12103.0244,  57941.0156],\n",
      "         [-10618.0098,   4930.0156,  11262.9863,  ...,  -1156.0234,\n",
      "           -9117.0078,  12159.0029],\n",
      "         ...,\n",
      "         [  8536.0254,   7824.9883,   9457.0156,  ...,   -252.9844,\n",
      "          -15761.9902,  -8133.0176],\n",
      "         [ 14849.0234,  21779.0078,   6414.0195,  ...,   9702.9873,\n",
      "          -10886.0205,  -5718.9971],\n",
      "         [ -5354.0044,  -5084.0088,   1497.0127,  ...,  -3875.9922,\n",
      "          -14148.9873,  23359.0000]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[-10989.0010,   7034.9990, -13964.0039,  ..., -23968.0020,\n",
      "           -3749.0020,  11730.0020],\n",
      "         [  4167.0283,  12086.0000,  -3829.9854,  ...,  -2730.9893,\n",
      "          -12103.0244,  57941.0156],\n",
      "         [-10618.0098,   4930.0156,  11262.9863,  ...,  -1156.0234,\n",
      "           -9117.0078,  12159.0029],\n",
      "         ...,\n",
      "         [  8536.0254,   7824.9883,   9457.0156,  ...,   -252.9844,\n",
      "          -15761.9902,  -8133.0176],\n",
      "         [ 14849.0234,  21779.0078,   6414.0195,  ...,   9702.9873,\n",
      "          -10886.0205,  -5718.9971],\n",
      "         [ -5354.0044,  -5084.0088,   1497.0127,  ...,  -3875.9922,\n",
      "          -14148.9873,  23359.0000]]]),) and output (tensor([[[ -7159.0010,   6621.9990, -10967.0039,  ..., -17477.0020,\n",
      "           -2510.0020,  11888.0020],\n",
      "         [  5007.0283,   9936.0000,   3029.0146,  ...,  -3173.9893,\n",
      "          -10633.0244,  62468.0156],\n",
      "         [ -5310.0098,   6848.0156,   9231.9863,  ...,   2280.9766,\n",
      "           -9234.0078,  16945.0039],\n",
      "         ...,\n",
      "         [ 14811.0254,  17099.9883,   9712.0156,  ...,   3905.0156,\n",
      "          -22531.9902,  -5960.0176],\n",
      "         [ 14261.0234,  20532.0078,   6529.0195,  ...,   3628.9873,\n",
      "           -7344.0205,  -3013.9971],\n",
      "         [ -1513.0044,  -7286.0088,  -1266.9873,  ..., -11148.9922,\n",
      "          -17089.9883,  21821.0000]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[ -7159.0010,   6621.9990, -10967.0039,  ..., -17477.0020,\n",
      "           -2510.0020,  11888.0020],\n",
      "         [  5007.0283,   9936.0000,   3029.0146,  ...,  -3173.9893,\n",
      "          -10633.0244,  62468.0156],\n",
      "         [ -5310.0098,   6848.0156,   9231.9863,  ...,   2280.9766,\n",
      "           -9234.0078,  16945.0039],\n",
      "         ...,\n",
      "         [ 14811.0254,  17099.9883,   9712.0156,  ...,   3905.0156,\n",
      "          -22531.9902,  -5960.0176],\n",
      "         [ 14261.0234,  20532.0078,   6529.0195,  ...,   3628.9873,\n",
      "           -7344.0205,  -3013.9971],\n",
      "         [ -1513.0044,  -7286.0088,  -1266.9873,  ..., -11148.9922,\n",
      "          -17089.9883,  21821.0000]]]),) and output (tensor([[[ -4822.0010,   3124.9990,  -3885.0039,  ..., -19007.0020,\n",
      "            -551.0020,  16285.0020],\n",
      "         [  3422.0283,   5191.0000,   7724.0146,  ...,   -770.9893,\n",
      "          -13006.0244,  62612.0156],\n",
      "         [  2132.9902,   7336.0156,   7148.9863,  ...,   8220.9766,\n",
      "          -13711.0078,  20192.0039],\n",
      "         ...,\n",
      "         [ 10812.0254,  13515.9883,   6292.0156,  ...,   6949.0156,\n",
      "           -9826.9902,  -6289.0176],\n",
      "         [  8139.0234,  25255.0078,   3007.0195,  ...,   1970.9873,\n",
      "          -12385.0205,  -3994.9971],\n",
      "         [   108.9956,  -4208.0088,  -1697.9873,  ...,  -5646.9922,\n",
      "          -19053.9883,  24347.0000]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[ -4822.0010,   3124.9990,  -3885.0039,  ..., -19007.0020,\n",
      "            -551.0020,  16285.0020],\n",
      "         [  3422.0283,   5191.0000,   7724.0146,  ...,   -770.9893,\n",
      "          -13006.0244,  62612.0156],\n",
      "         [  2132.9902,   7336.0156,   7148.9863,  ...,   8220.9766,\n",
      "          -13711.0078,  20192.0039],\n",
      "         ...,\n",
      "         [ 10812.0254,  13515.9883,   6292.0156,  ...,   6949.0156,\n",
      "           -9826.9902,  -6289.0176],\n",
      "         [  8139.0234,  25255.0078,   3007.0195,  ...,   1970.9873,\n",
      "          -12385.0205,  -3994.9971],\n",
      "         [   108.9956,  -4208.0088,  -1697.9873,  ...,  -5646.9922,\n",
      "          -19053.9883,  24347.0000]]]),) and output (tensor([[[ -7235.0010,   6583.9990,  -4212.0039,  ..., -19104.0020,\n",
      "            3087.9980,  17084.0020],\n",
      "         [  6990.0283,   1224.0000,   3727.0146,  ...,   2134.0107,\n",
      "          -12385.0244,  69534.0156],\n",
      "         [  6425.9902,  -1161.9844,    370.9863,  ...,  11613.9766,\n",
      "           -9915.0078,  21236.0039],\n",
      "         ...,\n",
      "         [ 11482.0254,  10541.9883,   9629.0156,  ...,   8432.0156,\n",
      "          -13609.9902,  -7107.0176],\n",
      "         [ 11081.0234,  27096.0078,   4487.0195,  ...,  -2985.0127,\n",
      "          -15056.0205,  -3473.9971],\n",
      "         [  1472.9956,    844.9912,   1461.0127,  ...,  -1963.9922,\n",
      "          -12603.9883,  25019.0000]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[ -7235.0010,   6583.9990,  -4212.0039,  ..., -19104.0020,\n",
      "            3087.9980,  17084.0020],\n",
      "         [  6990.0283,   1224.0000,   3727.0146,  ...,   2134.0107,\n",
      "          -12385.0244,  69534.0156],\n",
      "         [  6425.9902,  -1161.9844,    370.9863,  ...,  11613.9766,\n",
      "           -9915.0078,  21236.0039],\n",
      "         ...,\n",
      "         [ 11482.0254,  10541.9883,   9629.0156,  ...,   8432.0156,\n",
      "          -13609.9902,  -7107.0176],\n",
      "         [ 11081.0234,  27096.0078,   4487.0195,  ...,  -2985.0127,\n",
      "          -15056.0205,  -3473.9971],\n",
      "         [  1472.9956,    844.9912,   1461.0127,  ...,  -1963.9922,\n",
      "          -12603.9883,  25019.0000]]]),) and output (tensor([[[ -5586.0010,   7645.9990,   -505.0039,  ..., -15346.0020,\n",
      "            -623.0020,  12986.0020],\n",
      "         [ 10326.0283,   -789.0000,   8838.0146,  ...,    385.0107,\n",
      "          -14243.0244,  70896.0156],\n",
      "         [ 11373.9902,   1729.0156,   1004.9863,  ...,   8765.9766,\n",
      "           -8758.0078,  20132.0039],\n",
      "         ...,\n",
      "         [  9716.0254,    613.9883,  17384.0156,  ...,   1052.0156,\n",
      "           -4673.9902,  -6956.0176],\n",
      "         [ 10113.0234,  20473.0078,   4356.0195,  ...,  -7489.0127,\n",
      "          -12530.0195, -11629.9971],\n",
      "         [  6626.9956,   3204.9912,   8373.0127,  ...,   7752.0078,\n",
      "          -10505.9883,  22268.0000]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[ -5586.0010,   7645.9990,   -505.0039,  ..., -15346.0020,\n",
      "            -623.0020,  12986.0020],\n",
      "         [ 10326.0283,   -789.0000,   8838.0146,  ...,    385.0107,\n",
      "          -14243.0244,  70896.0156],\n",
      "         [ 11373.9902,   1729.0156,   1004.9863,  ...,   8765.9766,\n",
      "           -8758.0078,  20132.0039],\n",
      "         ...,\n",
      "         [  9716.0254,    613.9883,  17384.0156,  ...,   1052.0156,\n",
      "           -4673.9902,  -6956.0176],\n",
      "         [ 10113.0234,  20473.0078,   4356.0195,  ...,  -7489.0127,\n",
      "          -12530.0195, -11629.9971],\n",
      "         [  6626.9956,   3204.9912,   8373.0127,  ...,   7752.0078,\n",
      "          -10505.9883,  22268.0000]]]),) and output (tensor([[[ -8916.0010,   2362.9990,  -4833.0039,  ..., -12075.0020,\n",
      "           -1124.0020,  12438.0020],\n",
      "         [  7534.0283,   1710.0000,  14517.0146,  ...,   -430.9893,\n",
      "          -17369.0234,  78569.0156],\n",
      "         [ 18354.9902,  -1535.9844,   2255.9863,  ...,  16174.9766,\n",
      "          -10834.0078,  32520.0039],\n",
      "         ...,\n",
      "         [ 15919.0254,   7244.9883,  10852.0156,  ...,   2549.0156,\n",
      "           -6977.9902,  -4105.0176],\n",
      "         [ 11091.0234,  22652.0078,   6398.0195,  ...,  -2809.0127,\n",
      "           -7937.0195, -14477.9971],\n",
      "         [  7732.9956,   7216.9912,   9453.0127,  ...,   6925.0078,\n",
      "          -12802.9883,  21463.0000]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[ -8916.0010,   2362.9990,  -4833.0039,  ..., -12075.0020,\n",
      "           -1124.0020,  12438.0020],\n",
      "         [  7534.0283,   1710.0000,  14517.0146,  ...,   -430.9893,\n",
      "          -17369.0234,  78569.0156],\n",
      "         [ 18354.9902,  -1535.9844,   2255.9863,  ...,  16174.9766,\n",
      "          -10834.0078,  32520.0039],\n",
      "         ...,\n",
      "         [ 15919.0254,   7244.9883,  10852.0156,  ...,   2549.0156,\n",
      "           -6977.9902,  -4105.0176],\n",
      "         [ 11091.0234,  22652.0078,   6398.0195,  ...,  -2809.0127,\n",
      "           -7937.0195, -14477.9971],\n",
      "         [  7732.9956,   7216.9912,   9453.0127,  ...,   6925.0078,\n",
      "          -12802.9883,  21463.0000]]]),) and output (tensor([[[-20230.0000,   7877.9990,    161.9961,  ..., -10250.0020,\n",
      "             676.9980,   7095.0020],\n",
      "         [ 11502.0283,   4839.0000,  15598.0146,  ...,   2033.0107,\n",
      "          -16327.0234,  86655.0156],\n",
      "         [ 13251.9902,   1220.0156,   2598.9863,  ...,  11586.9766,\n",
      "          -13378.0078,  26472.0039],\n",
      "         ...,\n",
      "         [ 17282.0254,  11608.9883,   3931.0156,  ...,   4232.0156,\n",
      "             773.0098,   7093.9824],\n",
      "         [ 11271.0234,  33076.0078,  11311.0195,  ...,  -5560.0127,\n",
      "          -11999.0195, -21172.9961],\n",
      "         [ 10124.9961,  11552.9912,   8899.0127,  ...,   7666.0078,\n",
      "           -6714.9883,  14815.0000]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[-20230.0000,   7877.9990,    161.9961,  ..., -10250.0020,\n",
      "             676.9980,   7095.0020],\n",
      "         [ 11502.0283,   4839.0000,  15598.0146,  ...,   2033.0107,\n",
      "          -16327.0234,  86655.0156],\n",
      "         [ 13251.9902,   1220.0156,   2598.9863,  ...,  11586.9766,\n",
      "          -13378.0078,  26472.0039],\n",
      "         ...,\n",
      "         [ 17282.0254,  11608.9883,   3931.0156,  ...,   4232.0156,\n",
      "             773.0098,   7093.9824],\n",
      "         [ 11271.0234,  33076.0078,  11311.0195,  ...,  -5560.0127,\n",
      "          -11999.0195, -21172.9961],\n",
      "         [ 10124.9961,  11552.9912,   8899.0127,  ...,   7666.0078,\n",
      "           -6714.9883,  14815.0000]]]),) and output (tensor([[[-25670.0000,  13293.9990,   9099.9961,  ...,  -6547.0020,\n",
      "           -5658.0020,   4192.0020],\n",
      "         [ 13330.0283,   7273.0000,  19000.0156,  ...,   7472.0107,\n",
      "          -26545.0234,  87021.0156],\n",
      "         [ 15337.9902,   1170.0156,   2278.9863,  ...,  11762.9766,\n",
      "          -12439.0078,  26458.0039],\n",
      "         ...,\n",
      "         [ 23205.0254,  17388.9883,   9677.0156,  ...,  10379.0156,\n",
      "           -4109.9902,  17102.9824],\n",
      "         [  7470.0234,  29307.0078,   8527.0195,  ...,  -8246.0127,\n",
      "          -15736.0195, -15423.9961],\n",
      "         [ 15417.9961,   8305.9912,  10554.0127,  ...,  10255.0078,\n",
      "            1704.0117,   8171.0000]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[-25670.0000,  13293.9990,   9099.9961,  ...,  -6547.0020,\n",
      "           -5658.0020,   4192.0020],\n",
      "         [ 13330.0283,   7273.0000,  19000.0156,  ...,   7472.0107,\n",
      "          -26545.0234,  87021.0156],\n",
      "         [ 15337.9902,   1170.0156,   2278.9863,  ...,  11762.9766,\n",
      "          -12439.0078,  26458.0039],\n",
      "         ...,\n",
      "         [ 23205.0254,  17388.9883,   9677.0156,  ...,  10379.0156,\n",
      "           -4109.9902,  17102.9824],\n",
      "         [  7470.0234,  29307.0078,   8527.0195,  ...,  -8246.0127,\n",
      "          -15736.0195, -15423.9961],\n",
      "         [ 15417.9961,   8305.9912,  10554.0127,  ...,  10255.0078,\n",
      "            1704.0117,   8171.0000]]]),) and output (tensor([[[-25652.0000,  13180.9990,  15728.9961,  ...,  -2831.0020,\n",
      "             158.9980,   2204.0020],\n",
      "         [  7744.0283,   8163.0000,  19137.0156,  ...,   8572.0107,\n",
      "          -26513.0234,  82111.0156],\n",
      "         [ 13788.9902,  -3985.9844,  -3310.0137,  ...,  17727.9766,\n",
      "           -6538.0078,  21035.0039],\n",
      "         ...,\n",
      "         [ 28362.0254,  21233.9883,   7237.0156,  ...,    127.0156,\n",
      "           -8371.9902,  20429.9824],\n",
      "         [ 10552.0234,  33394.0078,   6871.0195,  ...,  -3458.0127,\n",
      "          -16624.0195, -22480.9961],\n",
      "         [ 19725.9961,   2762.9912,  11499.0127,  ...,  13115.0078,\n",
      "            2001.0117,  11405.0000]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[-25652.0000,  13180.9990,  15728.9961,  ...,  -2831.0020,\n",
      "             158.9980,   2204.0020],\n",
      "         [  7744.0283,   8163.0000,  19137.0156,  ...,   8572.0107,\n",
      "          -26513.0234,  82111.0156],\n",
      "         [ 13788.9902,  -3985.9844,  -3310.0137,  ...,  17727.9766,\n",
      "           -6538.0078,  21035.0039],\n",
      "         ...,\n",
      "         [ 28362.0254,  21233.9883,   7237.0156,  ...,    127.0156,\n",
      "           -8371.9902,  20429.9824],\n",
      "         [ 10552.0234,  33394.0078,   6871.0195,  ...,  -3458.0127,\n",
      "          -16624.0195, -22480.9961],\n",
      "         [ 19725.9961,   2762.9912,  11499.0127,  ...,  13115.0078,\n",
      "            2001.0117,  11405.0000]]]),) and output (tensor([[[-14791.0000,  15299.9990,  16129.9961,  ...,   3187.9980,\n",
      "           -2552.0020,   5626.0020],\n",
      "         [ 12363.0283,   9108.0000,  20156.0156,  ...,   7862.0107,\n",
      "          -26990.0234,  83810.0156],\n",
      "         [ 12998.9902,  -7923.9844,  -3409.0137,  ...,  21453.9766,\n",
      "          -14488.0078,  23888.0039],\n",
      "         ...,\n",
      "         [ 38963.0234,  17713.9883,   9168.0156,  ...,    274.0156,\n",
      "           -2835.9902,  26031.9824],\n",
      "         [ 14013.0234,  24277.0078,  10332.0195,  ...,   2796.9873,\n",
      "          -16428.0195, -25892.9961],\n",
      "         [ 16588.9961,   3964.9912,  16656.0117,  ...,   5949.0078,\n",
      "            1895.0117,  24374.0000]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[-14791.0000,  15299.9990,  16129.9961,  ...,   3187.9980,\n",
      "           -2552.0020,   5626.0020],\n",
      "         [ 12363.0283,   9108.0000,  20156.0156,  ...,   7862.0107,\n",
      "          -26990.0234,  83810.0156],\n",
      "         [ 12998.9902,  -7923.9844,  -3409.0137,  ...,  21453.9766,\n",
      "          -14488.0078,  23888.0039],\n",
      "         ...,\n",
      "         [ 38963.0234,  17713.9883,   9168.0156,  ...,    274.0156,\n",
      "           -2835.9902,  26031.9824],\n",
      "         [ 14013.0234,  24277.0078,  10332.0195,  ...,   2796.9873,\n",
      "          -16428.0195, -25892.9961],\n",
      "         [ 16588.9961,   3964.9912,  16656.0117,  ...,   5949.0078,\n",
      "            1895.0117,  24374.0000]]]),) and output (tensor([[[-13284.0000,  26639.0000,   7467.9961,  ...,   4113.9980,\n",
      "           -6608.0020,   5336.0020],\n",
      "         [ 16997.0273,   9376.0000,  23293.0156,  ...,   6506.0107,\n",
      "          -29249.0234,  82692.0156],\n",
      "         [ 13344.9902,  -7460.9844,  -4351.0137,  ...,  21443.9766,\n",
      "          -13179.0078,  17833.0039],\n",
      "         ...,\n",
      "         [ 42990.0234,  14887.9883,   5538.0156,  ...,  -8316.9844,\n",
      "           -6862.9902,  31543.9824],\n",
      "         [ 17105.0234,  19335.0078,  13026.0195,  ...,   1091.9873,\n",
      "          -18222.0195, -21884.9961],\n",
      "         [ 11739.9961,   2247.9912,  12482.0117,  ...,   7132.0078,\n",
      "            5578.0117,  33186.0000]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[-13284.0000,  26639.0000,   7467.9961,  ...,   4113.9980,\n",
      "           -6608.0020,   5336.0020],\n",
      "         [ 16997.0273,   9376.0000,  23293.0156,  ...,   6506.0107,\n",
      "          -29249.0234,  82692.0156],\n",
      "         [ 13344.9902,  -7460.9844,  -4351.0137,  ...,  21443.9766,\n",
      "          -13179.0078,  17833.0039],\n",
      "         ...,\n",
      "         [ 42990.0234,  14887.9883,   5538.0156,  ...,  -8316.9844,\n",
      "           -6862.9902,  31543.9824],\n",
      "         [ 17105.0234,  19335.0078,  13026.0195,  ...,   1091.9873,\n",
      "          -18222.0195, -21884.9961],\n",
      "         [ 11739.9961,   2247.9912,  12482.0117,  ...,   7132.0078,\n",
      "            5578.0117,  33186.0000]]]),) and output (tensor([[[-21532.0000,  32796.0000,  16281.9961,  ...,  18208.9980,\n",
      "           -4282.0020,   5756.0020],\n",
      "         [  6787.0273,   1034.0000,  32976.0156,  ...,  10083.0107,\n",
      "          -31494.0234,  80644.0156],\n",
      "         [ 18021.9902, -10935.9844,  -5918.0137,  ...,  16040.9766,\n",
      "           -9268.0078,  20157.0039],\n",
      "         ...,\n",
      "         [ 41327.0234,   9233.9883,   3616.0156,  ...,  -5727.9844,\n",
      "             188.0098,  31838.9824],\n",
      "         [ 11999.0234,  20412.0078,  15982.0195,  ...,    377.9873,\n",
      "           -9070.0195, -22215.9961],\n",
      "         [  1761.9961,  -3679.0088,  15680.0117,  ...,    929.0078,\n",
      "            2123.0117,  33001.0000]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[-21532.0000,  32796.0000,  16281.9961,  ...,  18208.9980,\n",
      "           -4282.0020,   5756.0020],\n",
      "         [  6787.0273,   1034.0000,  32976.0156,  ...,  10083.0107,\n",
      "          -31494.0234,  80644.0156],\n",
      "         [ 18021.9902, -10935.9844,  -5918.0137,  ...,  16040.9766,\n",
      "           -9268.0078,  20157.0039],\n",
      "         ...,\n",
      "         [ 41327.0234,   9233.9883,   3616.0156,  ...,  -5727.9844,\n",
      "             188.0098,  31838.9824],\n",
      "         [ 11999.0234,  20412.0078,  15982.0195,  ...,    377.9873,\n",
      "           -9070.0195, -22215.9961],\n",
      "         [  1761.9961,  -3679.0088,  15680.0117,  ...,    929.0078,\n",
      "            2123.0117,  33001.0000]]]),) and output (tensor([[[-24082.0000,  33387.0000,  15374.9961,  ...,  18359.9980,\n",
      "           -7500.0020,  11519.0020],\n",
      "         [  4832.0273,  -1008.0000,  28604.0156,  ...,  10278.0107,\n",
      "          -36666.0234,  82033.0156],\n",
      "         [  4836.9902,  -7185.9844,  -4302.0137,  ...,  14669.9766,\n",
      "             305.9922,  32473.0039],\n",
      "         ...,\n",
      "         [ 36704.0234,   5470.9883,   7187.0156,  ...,   2137.0156,\n",
      "           -2777.9902,  29756.9824],\n",
      "         [ 13846.0234,  22011.0078,  20247.0195,  ...,   7135.9873,\n",
      "           -6250.0195, -30669.9961],\n",
      "         [   848.9961,  -1182.0088,   3520.0117,  ...,   1210.0078,\n",
      "             856.0117,  42314.0000]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[-24082.0000,  33387.0000,  15374.9961,  ...,  18359.9980,\n",
      "           -7500.0020,  11519.0020],\n",
      "         [  4832.0273,  -1008.0000,  28604.0156,  ...,  10278.0107,\n",
      "          -36666.0234,  82033.0156],\n",
      "         [  4836.9902,  -7185.9844,  -4302.0137,  ...,  14669.9766,\n",
      "             305.9922,  32473.0039],\n",
      "         ...,\n",
      "         [ 36704.0234,   5470.9883,   7187.0156,  ...,   2137.0156,\n",
      "           -2777.9902,  29756.9824],\n",
      "         [ 13846.0234,  22011.0078,  20247.0195,  ...,   7135.9873,\n",
      "           -6250.0195, -30669.9961],\n",
      "         [   848.9961,  -1182.0088,   3520.0117,  ...,   1210.0078,\n",
      "             856.0117,  42314.0000]]]),) and output (tensor([[[-14869.0000,  32284.0000,   3742.9961,  ...,  12458.9980,\n",
      "           -5452.0020,   7590.0020],\n",
      "         [ -2517.9727,   2522.0000,  35237.0156,  ...,   9105.0107,\n",
      "          -33808.0234,  75270.0156],\n",
      "         [  4264.9902,   -956.9844,  -1327.0137,  ...,  17288.9766,\n",
      "           -3197.0078,  28204.0039],\n",
      "         ...,\n",
      "         [ 37127.0234,  19897.9883,  17355.0156,  ...,  11135.0156,\n",
      "          -10988.9902,  20585.9824],\n",
      "         [  7012.0234,  26780.0078,  31894.0195,  ...,  13986.9873,\n",
      "           -9360.0195, -31184.9961],\n",
      "         [  4240.9961,   2434.9912,  10210.0117,  ...,   6075.0078,\n",
      "           -2617.9883,  49304.0000]]]),)\n",
      "[Debug] Layer names being passed: ['model.layers.0', 'model.layers.1', 'model.layers.2', 'model.layers.3', 'model.layers.4', 'model.layers.5', 'model.layers.6', 'model.layers.7', 'model.layers.8', 'model.layers.9', 'model.layers.10', 'model.layers.11', 'model.layers.12', 'model.layers.13', 'model.layers.14', 'model.layers.15', 'model.layers.16', 'model.layers.17', 'model.layers.18', 'model.layers.19', 'model.layers.20', 'model.layers.21', 'model.layers.22', 'model.layers.23', 'model.layers.24', 'model.layers.25', 'model.layers.26', 'model.layers.27', 'model.embed_tokens']\n",
      "[Debug] Trying to access layer: model.layers.0\n",
      "[Debug] Successfully found layer: model.layers.0\n",
      "[Debug] Trying to access layer: model.layers.1\n",
      "[Debug] Successfully found layer: model.layers.1\n",
      "[Debug] Trying to access layer: model.layers.2\n",
      "[Debug] Successfully found layer: model.layers.2\n",
      "[Debug] Trying to access layer: model.layers.3\n",
      "[Debug] Successfully found layer: model.layers.3\n",
      "[Debug] Trying to access layer: model.layers.4\n",
      "[Debug] Successfully found layer: model.layers.4\n",
      "[Debug] Trying to access layer: model.layers.5\n",
      "[Debug] Successfully found layer: model.layers.5\n",
      "[Debug] Trying to access layer: model.layers.6\n",
      "[Debug] Successfully found layer: model.layers.6\n",
      "[Debug] Trying to access layer: model.layers.7\n",
      "[Debug] Successfully found layer: model.layers.7\n",
      "[Debug] Trying to access layer: model.layers.8\n",
      "[Debug] Successfully found layer: model.layers.8\n",
      "[Debug] Trying to access layer: model.layers.9\n",
      "[Debug] Successfully found layer: model.layers.9\n",
      "[Debug] Trying to access layer: model.layers.10\n",
      "[Debug] Successfully found layer: model.layers.10\n",
      "[Debug] Trying to access layer: model.layers.11\n",
      "[Debug] Successfully found layer: model.layers.11\n",
      "[Debug] Trying to access layer: model.layers.12\n",
      "[Debug] Successfully found layer: model.layers.12\n",
      "[Debug] Trying to access layer: model.layers.13\n",
      "[Debug] Successfully found layer: model.layers.13\n",
      "[Debug] Trying to access layer: model.layers.14\n",
      "[Debug] Successfully found layer: model.layers.14\n",
      "[Debug] Trying to access layer: model.layers.15\n",
      "[Debug] Successfully found layer: model.layers.15\n",
      "[Debug] Trying to access layer: model.layers.16\n",
      "[Debug] Successfully found layer: model.layers.16\n",
      "[Debug] Trying to access layer: model.layers.17\n",
      "[Debug] Successfully found layer: model.layers.17\n",
      "[Debug] Trying to access layer: model.layers.18\n",
      "[Debug] Successfully found layer: model.layers.18\n",
      "[Debug] Trying to access layer: model.layers.19\n",
      "[Debug] Successfully found layer: model.layers.19\n",
      "[Debug] Trying to access layer: model.layers.20\n",
      "[Debug] Successfully found layer: model.layers.20\n",
      "[Debug] Trying to access layer: model.layers.21\n",
      "[Debug] Successfully found layer: model.layers.21\n",
      "[Debug] Trying to access layer: model.layers.22\n",
      "[Debug] Successfully found layer: model.layers.22\n",
      "[Debug] Trying to access layer: model.layers.23\n",
      "[Debug] Successfully found layer: model.layers.23\n",
      "[Debug] Trying to access layer: model.layers.24\n",
      "[Debug] Successfully found layer: model.layers.24\n",
      "[Debug] Trying to access layer: model.layers.25\n",
      "[Debug] Successfully found layer: model.layers.25\n",
      "[Debug] Trying to access layer: model.layers.26\n",
      "[Debug] Successfully found layer: model.layers.26\n",
      "[Debug] Trying to access layer: model.layers.27\n",
      "[Debug] Successfully found layer: model.layers.27\n",
      "[Debug] Trying to access layer: model.embed_tokens\n",
      "[Debug] Successfully found layer: model.embed_tokens\n",
      "[Hook] Layer Embedding(128256, 3072, padding_idx=128004) received input (tensor([[128000,  12409,    315,    279,   3723,   4273,    578,   5292,    315,\n",
      "            279,   3723,   4273,    315,   5270,     11,   3629,  14183,    311,\n",
      "            439,    279,   3778,   5292,     11,    374,    279,   5426,   5292,\n",
      "            315,    279,   3723,   4273,     13,   1102,  17610,    315,  61759,\n",
      "           6273,  16600,  55788,    315,   2579,    320,   3565,    323,   5740,\n",
      "              8,  73462,    449,   4251,     11,    449,    264,   6437,  23596,\n",
      "            304,    279,  16869,    263,    320,    265,   5671,    311,  11951,\n",
      "            439,    279,    330,  16588,    909,  18534,  33517,   2678,     11,\n",
      "           4251,     11,   4330,  16983,    291,   9958,  28902,    304,  11888,\n",
      "           4445,  16600,   7123,     11,   1405,   7123,    315,   4848,   9958,\n",
      "            320,   3565,    323,   5740,      8,  25631,    449,   7123,    315,\n",
      "           4330,   9958,     13,    578,    220,   1135,   9958,    389,    279,\n",
      "           5292,   4097,    279,    220,   1135,   5415,    315,    279,   3723,\n",
      "           4273,    315,   5270,     11,    323,    279,    220,   1032,  55788,\n",
      "           4097,    279,  61759,   8013,  49028,    430,  14610,  24589,    505,\n",
      "            279,  15422,    315,   8681,  13527,     11,    323,   6244,    279,\n",
      "           1176,   5415,    304,    279,    549,    815,   8032,     16,     60,\n",
      "          15341,  11654,    369,    279,   5292,   2997,    578,  25676,    323,\n",
      "           4610,   9100,  17706,     17,     60,  10846,  59261,  17706,     18,\n",
      "             60,    323,    578,   7834,   6354,     79,  40040,  40714,     13]]),) and output tensor([[[-0.0010, -0.0007, -0.0046,  ..., -0.0014, -0.0020,  0.0020],\n",
      "         [ 0.0192,  0.0189,  0.0193,  ...,  0.0017,  0.0012,  0.0229],\n",
      "         [ 0.0045, -0.0109,  0.0005,  ...,  0.0231,  0.0219,  0.0065],\n",
      "         ...,\n",
      "         [-0.0216, -0.0023,  0.0135,  ...,  0.0281, -0.0104, -0.0115],\n",
      "         [ 0.0427, -0.0031,  0.0356,  ..., -0.0023, -0.0066, -0.0105],\n",
      "         [ 0.0093, -0.0031,  0.0278,  ...,  0.0117, -0.0084,  0.0096]]])\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[-0.0010, -0.0007, -0.0046,  ..., -0.0014, -0.0020,  0.0020],\n",
      "         [ 0.0192,  0.0189,  0.0193,  ...,  0.0017,  0.0012,  0.0229],\n",
      "         [ 0.0045, -0.0109,  0.0005,  ...,  0.0231,  0.0219,  0.0065],\n",
      "         ...,\n",
      "         [-0.0216, -0.0023,  0.0135,  ...,  0.0281, -0.0104, -0.0115],\n",
      "         [ 0.0427, -0.0031,  0.0356,  ..., -0.0023, -0.0066, -0.0105],\n",
      "         [ 0.0093, -0.0031,  0.0278,  ...,  0.0117, -0.0084,  0.0096]]]),) and output (tensor([[[ 442.9990,  239.9993,  302.9954,  ..., -154.0014,  -15.0020,\n",
      "           185.0020],\n",
      "         [  52.0192, -167.9811,   49.0193,  ..., -253.9983,  105.0012,\n",
      "            -5.9771],\n",
      "         [ 118.0045, -175.0109, -345.9995,  ..., -101.9769,  289.0219,\n",
      "           203.0065],\n",
      "         ...,\n",
      "         [ -24.0216,  139.9977,  -27.9865,  ...,    6.0281,  155.9896,\n",
      "           -92.0115],\n",
      "         [ 110.0427,  -35.0031,  306.0356,  ...,  -67.0023,   68.9934,\n",
      "           101.9895],\n",
      "         [  85.0093,  114.9969, -259.9722,  ..., -211.9883,  113.9916,\n",
      "            10.0096]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[ 442.9990,  239.9993,  302.9954,  ..., -154.0014,  -15.0020,\n",
      "           185.0020],\n",
      "         [  52.0192, -167.9811,   49.0193,  ..., -253.9983,  105.0012,\n",
      "            -5.9771],\n",
      "         [ 118.0045, -175.0109, -345.9995,  ..., -101.9769,  289.0219,\n",
      "           203.0065],\n",
      "         ...,\n",
      "         [ -24.0216,  139.9977,  -27.9865,  ...,    6.0281,  155.9896,\n",
      "           -92.0115],\n",
      "         [ 110.0427,  -35.0031,  306.0356,  ...,  -67.0023,   68.9934,\n",
      "           101.9895],\n",
      "         [  85.0093,  114.9969, -259.9722,  ..., -211.9883,  113.9916,\n",
      "            10.0096]]]),) and output (tensor([[[  439.9990,  -144.0007,  -410.0046,  ...,  -765.0015,\n",
      "           1193.9979, -1027.9980],\n",
      "         [ -316.9808,   134.0189,  -209.9807,  ...,  -630.9983,\n",
      "            642.0012,   142.0229],\n",
      "         [-1134.9956,  -508.0109,  -327.9995,  ...,  -117.9769,\n",
      "            224.0219,   392.0065],\n",
      "         ...,\n",
      "         [  820.9784,  1115.9977,  -647.9865,  ...,  -120.9719,\n",
      "            516.9896,   -34.0115],\n",
      "         [ -110.9573,   -45.0031, -1130.9644,  ...,  -140.0023,\n",
      "            -81.0066,    98.9895],\n",
      "         [ -568.9907,  2008.9969,  -218.9722,  ...,  -723.9883,\n",
      "           -158.0084,   199.0096]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[  439.9990,  -144.0007,  -410.0046,  ...,  -765.0015,\n",
      "           1193.9979, -1027.9980],\n",
      "         [ -316.9808,   134.0189,  -209.9807,  ...,  -630.9983,\n",
      "            642.0012,   142.0229],\n",
      "         [-1134.9956,  -508.0109,  -327.9995,  ...,  -117.9769,\n",
      "            224.0219,   392.0065],\n",
      "         ...,\n",
      "         [  820.9784,  1115.9977,  -647.9865,  ...,  -120.9719,\n",
      "            516.9896,   -34.0115],\n",
      "         [ -110.9573,   -45.0031, -1130.9644,  ...,  -140.0023,\n",
      "            -81.0066,    98.9895],\n",
      "         [ -568.9907,  2008.9969,  -218.9722,  ...,  -723.9883,\n",
      "           -158.0084,   199.0096]]]),) and output (tensor([[[  551.9990,   993.9993, -1790.0046,  ...,   445.9985,\n",
      "          -1386.0021, -1245.9980],\n",
      "         [-1010.9808, -2334.9810,  2886.0193,  ...,  1502.0017,\n",
      "           2252.0012,   -79.9771],\n",
      "         [-4847.9956,   710.9891,  2373.0005,  ..., -4870.9771,\n",
      "           -519.9781, -2016.9935],\n",
      "         ...,\n",
      "         [ 1107.9784, -1020.0023, -3626.9863,  ...,  2593.0281,\n",
      "          -2691.0103, -1382.0115],\n",
      "         [ 1615.0427, -4113.0029, -2200.9644,  ...,  -505.0023,\n",
      "          -1210.0066, -1281.0105],\n",
      "         [ -258.9907,  5942.9971,   -54.9722,  ..., -3444.9883,\n",
      "           -177.0084,  6558.0098]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[  551.9990,   993.9993, -1790.0046,  ...,   445.9985,\n",
      "          -1386.0021, -1245.9980],\n",
      "         [-1010.9808, -2334.9810,  2886.0193,  ...,  1502.0017,\n",
      "           2252.0012,   -79.9771],\n",
      "         [-4847.9956,   710.9891,  2373.0005,  ..., -4870.9771,\n",
      "           -519.9781, -2016.9935],\n",
      "         ...,\n",
      "         [ 1107.9784, -1020.0023, -3626.9863,  ...,  2593.0281,\n",
      "          -2691.0103, -1382.0115],\n",
      "         [ 1615.0427, -4113.0029, -2200.9644,  ...,  -505.0023,\n",
      "          -1210.0066, -1281.0105],\n",
      "         [ -258.9907,  5942.9971,   -54.9722,  ..., -3444.9883,\n",
      "           -177.0084,  6558.0098]]]),) and output (tensor([[[ 2131.9990,  -737.0007, -1071.0046,  ..., -2781.0015,\n",
      "           -885.0021,    86.0020],\n",
      "         [ 3079.0190, -1179.9810,  1889.0193,  ...,   678.0017,\n",
      "           5284.0010, -3513.9771],\n",
      "         [-6517.9956, -1283.0110,  2883.0005,  ..., -7431.9771,\n",
      "           2594.0220,  -397.9935],\n",
      "         ...,\n",
      "         [  685.9784, -1827.0023, -7427.9863,  ...,  2682.0281,\n",
      "           1718.9897, -4495.0117],\n",
      "         [  539.0427, -5939.0029, -2385.9644,  ..., -7009.0024,\n",
      "          -1911.0066, -2118.0105],\n",
      "         [-6058.9907,  8321.9971,  3188.0278,  ..., -7492.9883,\n",
      "           7840.9917,  9426.0098]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[ 2131.9990,  -737.0007, -1071.0046,  ..., -2781.0015,\n",
      "           -885.0021,    86.0020],\n",
      "         [ 3079.0190, -1179.9810,  1889.0193,  ...,   678.0017,\n",
      "           5284.0010, -3513.9771],\n",
      "         [-6517.9956, -1283.0110,  2883.0005,  ..., -7431.9771,\n",
      "           2594.0220,  -397.9935],\n",
      "         ...,\n",
      "         [  685.9784, -1827.0023, -7427.9863,  ...,  2682.0281,\n",
      "           1718.9897, -4495.0117],\n",
      "         [  539.0427, -5939.0029, -2385.9644,  ..., -7009.0024,\n",
      "          -1911.0066, -2118.0105],\n",
      "         [-6058.9907,  8321.9971,  3188.0278,  ..., -7492.9883,\n",
      "           7840.9917,  9426.0098]]]),) and output (tensor([[[ 3420.9990,   760.9993, -5376.0049,  ..., -4193.0015,\n",
      "           2860.9980, -4080.9980],\n",
      "         [ 5148.0190,  2085.0190,  -542.9807,  ...,   228.0017,\n",
      "            941.0010,   164.0229],\n",
      "         [-5321.9956, -2112.0110,  4346.0005,  ..., -5048.9771,\n",
      "          -5976.9780,  2471.0063],\n",
      "         ...,\n",
      "         [-1965.0215,  3232.9976,  -266.9863,  ...,  2207.0281,\n",
      "           -441.0103, -3686.0117],\n",
      "         [-1155.9573, -5523.0029, -3010.9644,  ..., -2455.0024,\n",
      "          -1537.0066, -5566.0107],\n",
      "         [-5153.9907,  4865.9971,  4506.0278,  ..., -9660.9883,\n",
      "           6339.9917, 16627.0098]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[ 3420.9990,   760.9993, -5376.0049,  ..., -4193.0015,\n",
      "           2860.9980, -4080.9980],\n",
      "         [ 5148.0190,  2085.0190,  -542.9807,  ...,   228.0017,\n",
      "            941.0010,   164.0229],\n",
      "         [-5321.9956, -2112.0110,  4346.0005,  ..., -5048.9771,\n",
      "          -5976.9780,  2471.0063],\n",
      "         ...,\n",
      "         [-1965.0215,  3232.9976,  -266.9863,  ...,  2207.0281,\n",
      "           -441.0103, -3686.0117],\n",
      "         [-1155.9573, -5523.0029, -3010.9644,  ..., -2455.0024,\n",
      "          -1537.0066, -5566.0107],\n",
      "         [-5153.9907,  4865.9971,  4506.0278,  ..., -9660.9883,\n",
      "           6339.9917, 16627.0098]]]),) and output (tensor([[[  2742.9990,  -1512.0007, -10493.0049,  ...,  -9945.0020,\n",
      "           -2017.0020,  -1802.9980],\n",
      "         [  2321.0190,  -1013.9810,   2910.0193,  ...,   1354.0017,\n",
      "            2723.0010,   1329.0229],\n",
      "         [  -638.9956,  -7471.0107,   5946.0005,  ...,  -6780.9771,\n",
      "           -4603.9780,   7845.0063],\n",
      "         ...,\n",
      "         [  1739.9785,   7158.9976,  -2550.9863,  ...,  -2054.9719,\n",
      "            3777.9897,  -2704.0117],\n",
      "         [  1303.0427, -11961.0029,  -2302.9644,  ...,  -5003.0024,\n",
      "           -3294.0066,   2301.9893],\n",
      "         [ -3834.9907,   7557.9971,    483.0278,  ...,  -9256.9883,\n",
      "            6832.9917,  31776.0098]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[  2742.9990,  -1512.0007, -10493.0049,  ...,  -9945.0020,\n",
      "           -2017.0020,  -1802.9980],\n",
      "         [  2321.0190,  -1013.9810,   2910.0193,  ...,   1354.0017,\n",
      "            2723.0010,   1329.0229],\n",
      "         [  -638.9956,  -7471.0107,   5946.0005,  ...,  -6780.9771,\n",
      "           -4603.9780,   7845.0063],\n",
      "         ...,\n",
      "         [  1739.9785,   7158.9976,  -2550.9863,  ...,  -2054.9719,\n",
      "            3777.9897,  -2704.0117],\n",
      "         [  1303.0427, -11961.0029,  -2302.9644,  ...,  -5003.0024,\n",
      "           -3294.0066,   2301.9893],\n",
      "         [ -3834.9907,   7557.9971,    483.0278,  ...,  -9256.9883,\n",
      "            6832.9917,  31776.0098]]]),) and output (tensor([[[  3032.9990,  -3420.0007, -10291.0049,  ...,  -6511.0020,\n",
      "           -1953.0020,   2001.0020],\n",
      "         [  2723.0190,   4277.0190,   3487.0193,  ...,   4940.0020,\n",
      "            -963.9990,   3053.0229],\n",
      "         [  -456.9956,  -8677.0107,   6238.0005,  ...,  -5743.9771,\n",
      "           -6495.9780,  13051.0059],\n",
      "         ...,\n",
      "         [  2166.9785,   9217.9980,    139.0137,  ...,   -368.9719,\n",
      "            2431.9897,    117.9883],\n",
      "         [  4443.0430, -13334.0029,  -6030.9644,  ...,  -2355.0024,\n",
      "           -3687.0066,   8057.9893],\n",
      "         [  5212.0093,   6530.9971,    962.0278,  ..., -10286.9883,\n",
      "           10023.9922,  36971.0078]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[  3032.9990,  -3420.0007, -10291.0049,  ...,  -6511.0020,\n",
      "           -1953.0020,   2001.0020],\n",
      "         [  2723.0190,   4277.0190,   3487.0193,  ...,   4940.0020,\n",
      "            -963.9990,   3053.0229],\n",
      "         [  -456.9956,  -8677.0107,   6238.0005,  ...,  -5743.9771,\n",
      "           -6495.9780,  13051.0059],\n",
      "         ...,\n",
      "         [  2166.9785,   9217.9980,    139.0137,  ...,   -368.9719,\n",
      "            2431.9897,    117.9883],\n",
      "         [  4443.0430, -13334.0029,  -6030.9644,  ...,  -2355.0024,\n",
      "           -3687.0066,   8057.9893],\n",
      "         [  5212.0093,   6530.9971,    962.0278,  ..., -10286.9883,\n",
      "           10023.9922,  36971.0078]]]),) and output (tensor([[[  3132.9990,  -1970.0007, -11544.0049,  ...,  -5780.0020,\n",
      "           -2486.0020,  -1921.9980],\n",
      "         [  2308.0190,   4286.0190,   1041.0193,  ...,   5858.0020,\n",
      "            2069.0010,   4186.0229],\n",
      "         [  2139.0044, -11502.0107,   7279.0005,  ...,  -8280.9766,\n",
      "           -2844.9780,   8067.0059],\n",
      "         ...,\n",
      "         [ -1890.0215,  13179.9980,  -4384.9863,  ...,  -2720.9719,\n",
      "            3676.9897,  -6274.0117],\n",
      "         [  3753.0430, -16946.0039,  -6867.9644,  ..., -10922.0020,\n",
      "           -5780.0068,   2588.9893],\n",
      "         [  2927.0093,    851.9971,   4509.0278,  ..., -12691.9883,\n",
      "            8398.9922,  46854.0078]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[  3132.9990,  -1970.0007, -11544.0049,  ...,  -5780.0020,\n",
      "           -2486.0020,  -1921.9980],\n",
      "         [  2308.0190,   4286.0190,   1041.0193,  ...,   5858.0020,\n",
      "            2069.0010,   4186.0229],\n",
      "         [  2139.0044, -11502.0107,   7279.0005,  ...,  -8280.9766,\n",
      "           -2844.9780,   8067.0059],\n",
      "         ...,\n",
      "         [ -1890.0215,  13179.9980,  -4384.9863,  ...,  -2720.9719,\n",
      "            3676.9897,  -6274.0117],\n",
      "         [  3753.0430, -16946.0039,  -6867.9644,  ..., -10922.0020,\n",
      "           -5780.0068,   2588.9893],\n",
      "         [  2927.0093,    851.9971,   4509.0278,  ..., -12691.9883,\n",
      "            8398.9922,  46854.0078]]]),) and output (tensor([[[ 1.0470e+03,  2.4040e+03, -1.4486e+04,  ..., -1.0922e+04,\n",
      "          -1.4190e+03, -2.9840e+03],\n",
      "         [-1.8680e+03,  7.7100e+03,  3.3760e+03,  ...,  5.1390e+03,\n",
      "           4.8820e+03,  1.0883e+04],\n",
      "         [ 2.4560e+03, -1.7037e+04,  7.5240e+03,  ..., -5.0730e+03,\n",
      "          -1.9610e+03,  1.2371e+04],\n",
      "         ...,\n",
      "         [ 2.9920e+03,  1.7383e+04, -2.8260e+03,  ..., -2.6080e+03,\n",
      "           6.4660e+03, -7.4480e+03],\n",
      "         [ 3.0390e+03, -1.2378e+04, -7.6820e+03,  ..., -7.2680e+03,\n",
      "          -3.8290e+03, -3.1301e+02],\n",
      "         [-2.4820e+03, -4.4003e+01,  4.7890e+03,  ..., -1.3861e+04,\n",
      "           1.1364e+04,  5.1201e+04]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[ 1.0470e+03,  2.4040e+03, -1.4486e+04,  ..., -1.0922e+04,\n",
      "          -1.4190e+03, -2.9840e+03],\n",
      "         [-1.8680e+03,  7.7100e+03,  3.3760e+03,  ...,  5.1390e+03,\n",
      "           4.8820e+03,  1.0883e+04],\n",
      "         [ 2.4560e+03, -1.7037e+04,  7.5240e+03,  ..., -5.0730e+03,\n",
      "          -1.9610e+03,  1.2371e+04],\n",
      "         ...,\n",
      "         [ 2.9920e+03,  1.7383e+04, -2.8260e+03,  ..., -2.6080e+03,\n",
      "           6.4660e+03, -7.4480e+03],\n",
      "         [ 3.0390e+03, -1.2378e+04, -7.6820e+03,  ..., -7.2680e+03,\n",
      "          -3.8290e+03, -3.1301e+02],\n",
      "         [-2.4820e+03, -4.4003e+01,  4.7890e+03,  ..., -1.3861e+04,\n",
      "           1.1364e+04,  5.1201e+04]]]),) and output (tensor([[[ -3767.0010,   5307.9990, -14221.0049,  ..., -15035.0020,\n",
      "           -3630.0020,   4168.0020],\n",
      "         [  -800.9810,  10385.0195,  -4595.9805,  ...,  -1389.9980,\n",
      "            6853.0010,  10584.0234],\n",
      "         [  1040.0044, -18723.0117,   9192.0000,  ...,  -3359.9766,\n",
      "           -5365.9780,  11630.0059],\n",
      "         ...,\n",
      "         [   139.9785,  10903.9980,   -551.9863,  ...,   3538.0281,\n",
      "             643.9897, -11157.0117],\n",
      "         [   654.0430, -13468.0039,  -9980.9648,  ..., -11688.0020,\n",
      "          -12775.0068,    186.9893],\n",
      "         [ -5073.9907,  -1285.0029,   5911.0278,  ..., -16326.9883,\n",
      "           17793.9922,  51996.0078]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[ -3767.0010,   5307.9990, -14221.0049,  ..., -15035.0020,\n",
      "           -3630.0020,   4168.0020],\n",
      "         [  -800.9810,  10385.0195,  -4595.9805,  ...,  -1389.9980,\n",
      "            6853.0010,  10584.0234],\n",
      "         [  1040.0044, -18723.0117,   9192.0000,  ...,  -3359.9766,\n",
      "           -5365.9780,  11630.0059],\n",
      "         ...,\n",
      "         [   139.9785,  10903.9980,   -551.9863,  ...,   3538.0281,\n",
      "             643.9897, -11157.0117],\n",
      "         [   654.0430, -13468.0039,  -9980.9648,  ..., -11688.0020,\n",
      "          -12775.0068,    186.9893],\n",
      "         [ -5073.9907,  -1285.0029,   5911.0278,  ..., -16326.9883,\n",
      "           17793.9922,  51996.0078]]]),) and output (tensor([[[ -1326.0010,   1223.9990, -16643.0039,  ..., -13240.0020,\n",
      "           -5677.0020,   9639.0020],\n",
      "         [ -1690.9810,  11980.0195,  -5726.9805,  ...,  -2417.9980,\n",
      "            4692.0010,  12155.0234],\n",
      "         [  4537.0044, -15833.0117,   8714.0000,  ...,  -5685.9766,\n",
      "          -12370.9785,  13961.0059],\n",
      "         ...,\n",
      "         [ -1942.0215,  14524.9980,   2437.0137,  ...,   6820.0283,\n",
      "           -2418.0103,  -5555.0117],\n",
      "         [ -2987.9570,  -8675.0039,  -4476.9648,  ..., -13188.0020,\n",
      "          -10311.0068,   3384.9893],\n",
      "         [ -5613.9907,   1688.9971,   2985.0278,  ..., -15578.9883,\n",
      "           12542.9922,  54979.0078]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[ -1326.0010,   1223.9990, -16643.0039,  ..., -13240.0020,\n",
      "           -5677.0020,   9639.0020],\n",
      "         [ -1690.9810,  11980.0195,  -5726.9805,  ...,  -2417.9980,\n",
      "            4692.0010,  12155.0234],\n",
      "         [  4537.0044, -15833.0117,   8714.0000,  ...,  -5685.9766,\n",
      "          -12370.9785,  13961.0059],\n",
      "         ...,\n",
      "         [ -1942.0215,  14524.9980,   2437.0137,  ...,   6820.0283,\n",
      "           -2418.0103,  -5555.0117],\n",
      "         [ -2987.9570,  -8675.0039,  -4476.9648,  ..., -13188.0020,\n",
      "          -10311.0068,   3384.9893],\n",
      "         [ -5613.9907,   1688.9971,   2985.0278,  ..., -15578.9883,\n",
      "           12542.9922,  54979.0078]]]),) and output (tensor([[[ -5120.0010,   2246.9990, -15993.0039,  ..., -19818.0020,\n",
      "           -9385.0020,  11924.0020],\n",
      "         [ -7217.9810,  15960.0195,  -7496.9805,  ...,  -4213.9980,\n",
      "            4416.0010,   4190.0234],\n",
      "         [  3319.0044, -20391.0117,   4110.0000,  ...,   -780.9766,\n",
      "          -12795.9785,  15985.0059],\n",
      "         ...,\n",
      "         [ -3553.0215,  11372.9980,   5047.0137,  ...,   3782.0283,\n",
      "            1552.9897, -13935.0117],\n",
      "         [ -5888.9570,  -5103.0039,  -4847.9648,  ..., -16024.0020,\n",
      "          -11251.0068,   1694.9893],\n",
      "         [ -4070.9907,    998.9971,   3526.0278,  ..., -18712.9883,\n",
      "           11603.9922,  63856.0078]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[ -5120.0010,   2246.9990, -15993.0039,  ..., -19818.0020,\n",
      "           -9385.0020,  11924.0020],\n",
      "         [ -7217.9810,  15960.0195,  -7496.9805,  ...,  -4213.9980,\n",
      "            4416.0010,   4190.0234],\n",
      "         [  3319.0044, -20391.0117,   4110.0000,  ...,   -780.9766,\n",
      "          -12795.9785,  15985.0059],\n",
      "         ...,\n",
      "         [ -3553.0215,  11372.9980,   5047.0137,  ...,   3782.0283,\n",
      "            1552.9897, -13935.0117],\n",
      "         [ -5888.9570,  -5103.0039,  -4847.9648,  ..., -16024.0020,\n",
      "          -11251.0068,   1694.9893],\n",
      "         [ -4070.9907,    998.9971,   3526.0278,  ..., -18712.9883,\n",
      "           11603.9922,  63856.0078]]]),) and output (tensor([[[-6.6850e+03, -1.2780e+03, -1.5553e+04,  ..., -1.9846e+04,\n",
      "          -1.0418e+04,  1.3376e+04],\n",
      "         [-7.8460e+03,  1.5015e+04, -5.0330e+03,  ..., -6.6010e+03,\n",
      "          -9.1700e+02,  2.5750e+03],\n",
      "         [ 1.4210e+03, -1.9741e+04,  8.1840e+03,  ...,  5.4023e+01,\n",
      "          -1.6122e+04,  1.1274e+04],\n",
      "         ...,\n",
      "         [-2.8100e+03,  1.1693e+04,  6.8560e+03,  ...,  2.3300e+03,\n",
      "          -2.5450e+03, -1.0651e+04],\n",
      "         [-7.9830e+03, -1.8540e+03, -5.1430e+03,  ..., -1.9694e+04,\n",
      "          -1.0664e+04,  4.3000e+03],\n",
      "         [-1.2180e+03,  4.8990e+03,  2.9050e+03,  ..., -2.2266e+04,\n",
      "           8.1900e+03,  6.3740e+04]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[-6.6850e+03, -1.2780e+03, -1.5553e+04,  ..., -1.9846e+04,\n",
      "          -1.0418e+04,  1.3376e+04],\n",
      "         [-7.8460e+03,  1.5015e+04, -5.0330e+03,  ..., -6.6010e+03,\n",
      "          -9.1700e+02,  2.5750e+03],\n",
      "         [ 1.4210e+03, -1.9741e+04,  8.1840e+03,  ...,  5.4023e+01,\n",
      "          -1.6122e+04,  1.1274e+04],\n",
      "         ...,\n",
      "         [-2.8100e+03,  1.1693e+04,  6.8560e+03,  ...,  2.3300e+03,\n",
      "          -2.5450e+03, -1.0651e+04],\n",
      "         [-7.9830e+03, -1.8540e+03, -5.1430e+03,  ..., -1.9694e+04,\n",
      "          -1.0664e+04,  4.3000e+03],\n",
      "         [-1.2180e+03,  4.8990e+03,  2.9050e+03,  ..., -2.2266e+04,\n",
      "           8.1900e+03,  6.3740e+04]]]),) and output (tensor([[[ -8422.0010,    339.9990, -15289.0039,  ..., -20884.0020,\n",
      "           -6908.0020,  11014.0020],\n",
      "         [-11084.9805,  19586.0195,  -6809.9805,  ...,  -7942.9980,\n",
      "            1265.0010,  -3330.9766],\n",
      "         [  1639.0044, -20336.0117,   7780.0000,  ...,   2387.0234,\n",
      "          -18734.9785,   3797.0059],\n",
      "         ...,\n",
      "         [ -5520.0215,  11979.9980,   3278.0137,  ...,  -2293.9717,\n",
      "             575.9897,  -8376.0117],\n",
      "         [ -4662.9570,   7643.9961,   3865.0352,  ..., -19775.0020,\n",
      "          -14357.0068,    223.9893],\n",
      "         [  3657.0093,   4259.9971,   2838.0278,  ..., -16192.9883,\n",
      "            2192.9922,  68779.0078]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[ -8422.0010,    339.9990, -15289.0039,  ..., -20884.0020,\n",
      "           -6908.0020,  11014.0020],\n",
      "         [-11084.9805,  19586.0195,  -6809.9805,  ...,  -7942.9980,\n",
      "            1265.0010,  -3330.9766],\n",
      "         [  1639.0044, -20336.0117,   7780.0000,  ...,   2387.0234,\n",
      "          -18734.9785,   3797.0059],\n",
      "         ...,\n",
      "         [ -5520.0215,  11979.9980,   3278.0137,  ...,  -2293.9717,\n",
      "             575.9897,  -8376.0117],\n",
      "         [ -4662.9570,   7643.9961,   3865.0352,  ..., -19775.0020,\n",
      "          -14357.0068,    223.9893],\n",
      "         [  3657.0093,   4259.9971,   2838.0278,  ..., -16192.9883,\n",
      "            2192.9922,  68779.0078]]]),) and output (tensor([[[-10989.0010,   7034.9990, -13964.0039,  ..., -23968.0020,\n",
      "           -3749.0020,  11730.0020],\n",
      "         [ -7592.9805,  18943.0195,  -5671.9805,  ...,  -5268.9980,\n",
      "           -2049.9990,  -4341.9766],\n",
      "         [ -2452.9956, -16468.0117,   2751.0000,  ...,   4086.0234,\n",
      "          -14887.9785,   8547.0059],\n",
      "         ...,\n",
      "         [ -5115.0215,  18457.9980,   4339.0137,  ...,   1936.0283,\n",
      "           -1742.0103,  -8516.0117],\n",
      "         [ -4794.9570,   6845.9961,   4404.0352,  ..., -21070.0020,\n",
      "          -11692.0068,   3588.9893],\n",
      "         [  1390.0093,   6428.9971,   3825.0278,  ..., -10927.9883,\n",
      "           -4142.0078,  68771.0078]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[-10989.0010,   7034.9990, -13964.0039,  ..., -23968.0020,\n",
      "           -3749.0020,  11730.0020],\n",
      "         [ -7592.9805,  18943.0195,  -5671.9805,  ...,  -5268.9980,\n",
      "           -2049.9990,  -4341.9766],\n",
      "         [ -2452.9956, -16468.0117,   2751.0000,  ...,   4086.0234,\n",
      "          -14887.9785,   8547.0059],\n",
      "         ...,\n",
      "         [ -5115.0215,  18457.9980,   4339.0137,  ...,   1936.0283,\n",
      "           -1742.0103,  -8516.0117],\n",
      "         [ -4794.9570,   6845.9961,   4404.0352,  ..., -21070.0020,\n",
      "          -11692.0068,   3588.9893],\n",
      "         [  1390.0093,   6428.9971,   3825.0278,  ..., -10927.9883,\n",
      "           -4142.0078,  68771.0078]]]),) and output (tensor([[[ -7159.0010,   6621.9990, -10967.0039,  ..., -17477.0020,\n",
      "           -2510.0020,  11888.0020],\n",
      "         [ -8217.9805,  19212.0195,  -5978.9805,  ..., -10389.9980,\n",
      "            1810.0010,  -5532.9766],\n",
      "         [ -1740.9956, -15075.0117,   6472.0000,  ...,   6088.0234,\n",
      "          -13331.9785,   8275.0059],\n",
      "         ...,\n",
      "         [ -6148.0215,  18480.9980,   4400.0137,  ...,   -126.9717,\n",
      "           -3430.0103,  -6987.0117],\n",
      "         [  2073.0430,   7557.9961,   3093.0352,  ..., -23904.0020,\n",
      "           -9181.0068,   8414.9893],\n",
      "         [  1939.0093,   4505.9971,   3949.0278,  ..., -18333.9883,\n",
      "           -4066.0078,  73007.0078]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[ -7159.0010,   6621.9990, -10967.0039,  ..., -17477.0020,\n",
      "           -2510.0020,  11888.0020],\n",
      "         [ -8217.9805,  19212.0195,  -5978.9805,  ..., -10389.9980,\n",
      "            1810.0010,  -5532.9766],\n",
      "         [ -1740.9956, -15075.0117,   6472.0000,  ...,   6088.0234,\n",
      "          -13331.9785,   8275.0059],\n",
      "         ...,\n",
      "         [ -6148.0215,  18480.9980,   4400.0137,  ...,   -126.9717,\n",
      "           -3430.0103,  -6987.0117],\n",
      "         [  2073.0430,   7557.9961,   3093.0352,  ..., -23904.0020,\n",
      "           -9181.0068,   8414.9893],\n",
      "         [  1939.0093,   4505.9971,   3949.0278,  ..., -18333.9883,\n",
      "           -4066.0078,  73007.0078]]]),) and output (tensor([[[-4.8220e+03,  3.1250e+03, -3.8850e+03,  ..., -1.9007e+04,\n",
      "          -5.5100e+02,  1.6285e+04],\n",
      "         [-2.1870e+03,  2.5539e+04, -6.5970e+03,  ..., -5.7990e+03,\n",
      "           6.1150e+03, -6.5620e+03],\n",
      "         [-3.6150e+03, -1.4000e+04,  1.9060e+03,  ...,  1.1993e+04,\n",
      "          -1.2796e+04,  4.4710e+03],\n",
      "         ...,\n",
      "         [-4.8090e+03,  2.3917e+04,  8.3790e+03,  ...,  1.5700e+03,\n",
      "           3.5560e+03, -6.7490e+03],\n",
      "         [ 5.5900e+03,  6.7890e+03,  2.7260e+03,  ..., -2.3343e+04,\n",
      "          -7.7680e+03,  7.5550e+03],\n",
      "         [ 1.2750e+03, -5.4003e+01,  5.9030e+03,  ..., -2.0764e+04,\n",
      "          -4.2400e+03,  7.9125e+04]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[-4.8220e+03,  3.1250e+03, -3.8850e+03,  ..., -1.9007e+04,\n",
      "          -5.5100e+02,  1.6285e+04],\n",
      "         [-2.1870e+03,  2.5539e+04, -6.5970e+03,  ..., -5.7990e+03,\n",
      "           6.1150e+03, -6.5620e+03],\n",
      "         [-3.6150e+03, -1.4000e+04,  1.9060e+03,  ...,  1.1993e+04,\n",
      "          -1.2796e+04,  4.4710e+03],\n",
      "         ...,\n",
      "         [-4.8090e+03,  2.3917e+04,  8.3790e+03,  ...,  1.5700e+03,\n",
      "           3.5560e+03, -6.7490e+03],\n",
      "         [ 5.5900e+03,  6.7890e+03,  2.7260e+03,  ..., -2.3343e+04,\n",
      "          -7.7680e+03,  7.5550e+03],\n",
      "         [ 1.2750e+03, -5.4003e+01,  5.9030e+03,  ..., -2.0764e+04,\n",
      "          -4.2400e+03,  7.9125e+04]]]),) and output (tensor([[[ -7235.0010,   6583.9990,  -4212.0039,  ..., -19104.0020,\n",
      "            3087.9980,  17084.0020],\n",
      "         [ -7512.9805,  19764.0195, -13682.9805,  ...,  -1888.9980,\n",
      "            4401.0010,  -1978.9766],\n",
      "         [ -8046.9956, -14209.0117,   5571.0000,  ...,   6166.0234,\n",
      "          -12097.9785,   4210.0059],\n",
      "         ...,\n",
      "         [-16887.0215,  20662.9980,  11163.0137,  ...,  -1469.9717,\n",
      "            2935.9897,  -2484.0117],\n",
      "         [  5231.0430,   -341.0039,   4720.0352,  ..., -29676.0020,\n",
      "           -6284.0068,   2805.9893],\n",
      "         [  8068.0093,  -2816.0029,   8441.0273,  ..., -17342.9883,\n",
      "           -7190.0078,  81562.0078]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[ -7235.0010,   6583.9990,  -4212.0039,  ..., -19104.0020,\n",
      "            3087.9980,  17084.0020],\n",
      "         [ -7512.9805,  19764.0195, -13682.9805,  ...,  -1888.9980,\n",
      "            4401.0010,  -1978.9766],\n",
      "         [ -8046.9956, -14209.0117,   5571.0000,  ...,   6166.0234,\n",
      "          -12097.9785,   4210.0059],\n",
      "         ...,\n",
      "         [-16887.0215,  20662.9980,  11163.0137,  ...,  -1469.9717,\n",
      "            2935.9897,  -2484.0117],\n",
      "         [  5231.0430,   -341.0039,   4720.0352,  ..., -29676.0020,\n",
      "           -6284.0068,   2805.9893],\n",
      "         [  8068.0093,  -2816.0029,   8441.0273,  ..., -17342.9883,\n",
      "           -7190.0078,  81562.0078]]]),) and output (tensor([[[ -5586.0010,   7645.9990,   -505.0039,  ..., -15346.0020,\n",
      "            -623.0020,  12986.0020],\n",
      "         [ -6196.9805,  17809.0195, -10748.9805,  ...,   1960.0020,\n",
      "            5861.0010,  -1944.9766],\n",
      "         [-11876.9961, -15673.0117,   -420.0000,  ...,  13620.0234,\n",
      "          -14034.9785,  10241.0059],\n",
      "         ...,\n",
      "         [ -9750.0215,   9457.9980,  14333.0137,  ...,  -3488.9717,\n",
      "            5501.9897,  -2827.0117],\n",
      "         [  8565.0430,    770.9961,   2927.0352,  ..., -21923.0020,\n",
      "          -13200.0068,  -6055.0107],\n",
      "         [  5058.0098,  -6655.0029,   4964.0273,  ..., -22138.9883,\n",
      "          -15360.0078,  81838.0078]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[ -5586.0010,   7645.9990,   -505.0039,  ..., -15346.0020,\n",
      "            -623.0020,  12986.0020],\n",
      "         [ -6196.9805,  17809.0195, -10748.9805,  ...,   1960.0020,\n",
      "            5861.0010,  -1944.9766],\n",
      "         [-11876.9961, -15673.0117,   -420.0000,  ...,  13620.0234,\n",
      "          -14034.9785,  10241.0059],\n",
      "         ...,\n",
      "         [ -9750.0215,   9457.9980,  14333.0137,  ...,  -3488.9717,\n",
      "            5501.9897,  -2827.0117],\n",
      "         [  8565.0430,    770.9961,   2927.0352,  ..., -21923.0020,\n",
      "          -13200.0068,  -6055.0107],\n",
      "         [  5058.0098,  -6655.0029,   4964.0273,  ..., -22138.9883,\n",
      "          -15360.0078,  81838.0078]]]),) and output (tensor([[[-8.9160e+03,  2.3630e+03, -4.8330e+03,  ..., -1.2075e+04,\n",
      "          -1.1240e+03,  1.2438e+04],\n",
      "         [ 7.7690e+03,  1.5387e+04, -9.0330e+03,  ...,  6.0050e+03,\n",
      "           5.1390e+03,  4.1160e+03],\n",
      "         [-9.7680e+03, -2.1177e+04,  1.6360e+03,  ...,  1.9686e+04,\n",
      "          -1.9122e+04,  1.5815e+04],\n",
      "         ...,\n",
      "         [-6.0200e+03,  1.3997e+04,  1.8916e+04,  ..., -1.1220e+04,\n",
      "          -8.0010e+01,  1.8090e+03],\n",
      "         [ 1.2719e+04,  7.1630e+03,  5.0560e+03,  ..., -2.5109e+04,\n",
      "          -9.9530e+03, -1.1474e+04],\n",
      "         [-2.1680e+03, -6.6820e+03,  3.9710e+03,  ..., -2.3684e+04,\n",
      "          -1.0214e+04,  8.9406e+04]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[-8.9160e+03,  2.3630e+03, -4.8330e+03,  ..., -1.2075e+04,\n",
      "          -1.1240e+03,  1.2438e+04],\n",
      "         [ 7.7690e+03,  1.5387e+04, -9.0330e+03,  ...,  6.0050e+03,\n",
      "           5.1390e+03,  4.1160e+03],\n",
      "         [-9.7680e+03, -2.1177e+04,  1.6360e+03,  ...,  1.9686e+04,\n",
      "          -1.9122e+04,  1.5815e+04],\n",
      "         ...,\n",
      "         [-6.0200e+03,  1.3997e+04,  1.8916e+04,  ..., -1.1220e+04,\n",
      "          -8.0010e+01,  1.8090e+03],\n",
      "         [ 1.2719e+04,  7.1630e+03,  5.0560e+03,  ..., -2.5109e+04,\n",
      "          -9.9530e+03, -1.1474e+04],\n",
      "         [-2.1680e+03, -6.6820e+03,  3.9710e+03,  ..., -2.3684e+04,\n",
      "          -1.0214e+04,  8.9406e+04]]]),) and output (tensor([[[-20230.0000,   7877.9990,    161.9961,  ..., -10250.0020,\n",
      "             676.9980,   7095.0020],\n",
      "         [ -1133.9805,  13732.0195,  -3508.9805,  ...,  11335.0020,\n",
      "            5701.0010,   8330.0234],\n",
      "         [-25968.9961, -21499.0117,   4788.0000,  ...,  18540.0234,\n",
      "          -18038.9785,  15866.0059],\n",
      "         ...,\n",
      "         [ -3363.0215,  13521.9980,  19622.0137,  ..., -15203.9717,\n",
      "            -379.0103,  -4161.0117],\n",
      "         [  8814.0430,   3792.9961,   3183.0352,  ..., -22326.0020,\n",
      "          -15558.0068, -21401.0117],\n",
      "         [ -4932.9902,  -5366.0029,   9563.0273,  ..., -25501.9883,\n",
      "          -14308.0078,  90378.0078]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[-20230.0000,   7877.9990,    161.9961,  ..., -10250.0020,\n",
      "             676.9980,   7095.0020],\n",
      "         [ -1133.9805,  13732.0195,  -3508.9805,  ...,  11335.0020,\n",
      "            5701.0010,   8330.0234],\n",
      "         [-25968.9961, -21499.0117,   4788.0000,  ...,  18540.0234,\n",
      "          -18038.9785,  15866.0059],\n",
      "         ...,\n",
      "         [ -3363.0215,  13521.9980,  19622.0137,  ..., -15203.9717,\n",
      "            -379.0103,  -4161.0117],\n",
      "         [  8814.0430,   3792.9961,   3183.0352,  ..., -22326.0020,\n",
      "          -15558.0068, -21401.0117],\n",
      "         [ -4932.9902,  -5366.0029,   9563.0273,  ..., -25501.9883,\n",
      "          -14308.0078,  90378.0078]]]),) and output (tensor([[[-25670.0000,  13293.9990,   9099.9961,  ...,  -6547.0020,\n",
      "           -5658.0020,   4192.0020],\n",
      "         [  6650.0195,  14835.0195,   -140.9805,  ...,   4386.0020,\n",
      "            6487.0010,  16933.0234],\n",
      "         [-26305.9961, -21230.0117,   2881.0000,  ...,  15338.0234,\n",
      "          -17320.9785,  23460.0059],\n",
      "         ...,\n",
      "         [-11771.0215,  15126.9980,  21961.0137,  ..., -15201.9717,\n",
      "           -4233.0103,  -8926.0117],\n",
      "         [  7549.0430,   3275.9961,   1993.0352,  ..., -20377.0020,\n",
      "          -16281.0068, -25403.0117],\n",
      "         [ -5805.9902,  -4356.0029,   8576.0273,  ..., -23159.9883,\n",
      "          -14363.0078,  84078.0078]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[-25670.0000,  13293.9990,   9099.9961,  ...,  -6547.0020,\n",
      "           -5658.0020,   4192.0020],\n",
      "         [  6650.0195,  14835.0195,   -140.9805,  ...,   4386.0020,\n",
      "            6487.0010,  16933.0234],\n",
      "         [-26305.9961, -21230.0117,   2881.0000,  ...,  15338.0234,\n",
      "          -17320.9785,  23460.0059],\n",
      "         ...,\n",
      "         [-11771.0215,  15126.9980,  21961.0137,  ..., -15201.9717,\n",
      "           -4233.0103,  -8926.0117],\n",
      "         [  7549.0430,   3275.9961,   1993.0352,  ..., -20377.0020,\n",
      "          -16281.0068, -25403.0117],\n",
      "         [ -5805.9902,  -4356.0029,   8576.0273,  ..., -23159.9883,\n",
      "          -14363.0078,  84078.0078]]]),) and output (tensor([[[-25652.0000,  13180.9990,  15728.9961,  ...,  -2831.0020,\n",
      "             158.9980,   2204.0020],\n",
      "         [  1847.0195,  18970.0195,  10180.0195,  ...,  -1366.9980,\n",
      "           11297.0010,  20088.0234],\n",
      "         [-24453.9961, -19542.0117,   3915.0000,  ...,   8872.0234,\n",
      "          -12881.9785,  24042.0059],\n",
      "         ...,\n",
      "         [-13278.0215,   6264.9980,  20720.0137,  ..., -21004.9727,\n",
      "           -4864.0103,  -2212.0117],\n",
      "         [  5004.0430,  12113.9961,   5885.0352,  ..., -21977.0020,\n",
      "          -22615.0078, -19304.0117],\n",
      "         [ -4113.9902,   2401.9971,  13793.0273,  ..., -26423.9883,\n",
      "          -17598.0078,  88526.0078]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[-25652.0000,  13180.9990,  15728.9961,  ...,  -2831.0020,\n",
      "             158.9980,   2204.0020],\n",
      "         [  1847.0195,  18970.0195,  10180.0195,  ...,  -1366.9980,\n",
      "           11297.0010,  20088.0234],\n",
      "         [-24453.9961, -19542.0117,   3915.0000,  ...,   8872.0234,\n",
      "          -12881.9785,  24042.0059],\n",
      "         ...,\n",
      "         [-13278.0215,   6264.9980,  20720.0137,  ..., -21004.9727,\n",
      "           -4864.0103,  -2212.0117],\n",
      "         [  5004.0430,  12113.9961,   5885.0352,  ..., -21977.0020,\n",
      "          -22615.0078, -19304.0117],\n",
      "         [ -4113.9902,   2401.9971,  13793.0273,  ..., -26423.9883,\n",
      "          -17598.0078,  88526.0078]]]),) and output (tensor([[[-14791.0000,  15299.9990,  16129.9961,  ...,   3187.9980,\n",
      "           -2552.0020,   5626.0020],\n",
      "         [  2389.0195,  13641.0195,  11321.0195,  ...,   1326.0020,\n",
      "            8237.0010,  14016.0234],\n",
      "         [-24745.9961, -17885.0117,  13794.0000,  ...,   6206.0234,\n",
      "          -21082.9785,  25945.0059],\n",
      "         ...,\n",
      "         [-13133.0215,   2172.9980,  24090.0137,  ..., -13129.9727,\n",
      "           -8190.0103,  -2462.0117],\n",
      "         [   754.0430,  10268.9961,  10340.0352,  ..., -21660.0020,\n",
      "          -31453.0078, -23633.0117],\n",
      "         [ -6213.9902,   7178.9971,  18621.0273,  ..., -13914.9883,\n",
      "          -14208.0078,  84897.0078]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[-14791.0000,  15299.9990,  16129.9961,  ...,   3187.9980,\n",
      "           -2552.0020,   5626.0020],\n",
      "         [  2389.0195,  13641.0195,  11321.0195,  ...,   1326.0020,\n",
      "            8237.0010,  14016.0234],\n",
      "         [-24745.9961, -17885.0117,  13794.0000,  ...,   6206.0234,\n",
      "          -21082.9785,  25945.0059],\n",
      "         ...,\n",
      "         [-13133.0215,   2172.9980,  24090.0137,  ..., -13129.9727,\n",
      "           -8190.0103,  -2462.0117],\n",
      "         [   754.0430,  10268.9961,  10340.0352,  ..., -21660.0020,\n",
      "          -31453.0078, -23633.0117],\n",
      "         [ -6213.9902,   7178.9971,  18621.0273,  ..., -13914.9883,\n",
      "          -14208.0078,  84897.0078]]]),) and output (tensor([[[-13284.0000,  26639.0000,   7467.9961,  ...,   4113.9980,\n",
      "           -6608.0020,   5336.0020],\n",
      "         [ -4474.9805,  10234.0195,  15269.0195,  ...,   1754.0020,\n",
      "            7532.0010,  11647.0234],\n",
      "         [-21697.9961, -12641.0117,  10234.0000,  ...,  12513.0234,\n",
      "          -27933.9785,  30207.0059],\n",
      "         ...,\n",
      "         [-16417.0215,   4672.9980,  28839.0137,  ...,  -8969.9727,\n",
      "          -12444.0098,  -5502.0117],\n",
      "         [  -777.9570,  16272.9961,  11052.0352,  ..., -23002.0020,\n",
      "          -33401.0078, -27045.0117],\n",
      "         [ -3954.9902,   8150.9971,  25748.0273,  ...,  -9406.9883,\n",
      "          -10034.0078,  88448.0078]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[-13284.0000,  26639.0000,   7467.9961,  ...,   4113.9980,\n",
      "           -6608.0020,   5336.0020],\n",
      "         [ -4474.9805,  10234.0195,  15269.0195,  ...,   1754.0020,\n",
      "            7532.0010,  11647.0234],\n",
      "         [-21697.9961, -12641.0117,  10234.0000,  ...,  12513.0234,\n",
      "          -27933.9785,  30207.0059],\n",
      "         ...,\n",
      "         [-16417.0215,   4672.9980,  28839.0137,  ...,  -8969.9727,\n",
      "          -12444.0098,  -5502.0117],\n",
      "         [  -777.9570,  16272.9961,  11052.0352,  ..., -23002.0020,\n",
      "          -33401.0078, -27045.0117],\n",
      "         [ -3954.9902,   8150.9971,  25748.0273,  ...,  -9406.9883,\n",
      "          -10034.0078,  88448.0078]]]),) and output (tensor([[[-21532.0000,  32796.0000,  16281.9961,  ...,  18208.9980,\n",
      "           -4282.0020,   5756.0020],\n",
      "         [-15232.9805,   5789.0195,  17679.0195,  ...,  13636.0020,\n",
      "           -1302.9990,   6077.0234],\n",
      "         [-29548.9961, -25574.0117,  15394.0000,  ...,  15914.0234,\n",
      "          -30202.9785,  28687.0059],\n",
      "         ...,\n",
      "         [-23349.0215,   4418.9980,  34263.0156,  ..., -10641.9727,\n",
      "          -19971.0098,  -4870.0117],\n",
      "         [ -2319.9570,  13965.9961,  17233.0352,  ..., -37618.0000,\n",
      "          -32827.0078, -29207.0117],\n",
      "         [  -589.9902,   5665.9971,  23403.0273,  ...,   -392.9883,\n",
      "          -14607.0078,  89568.0078]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[-21532.0000,  32796.0000,  16281.9961,  ...,  18208.9980,\n",
      "           -4282.0020,   5756.0020],\n",
      "         [-15232.9805,   5789.0195,  17679.0195,  ...,  13636.0020,\n",
      "           -1302.9990,   6077.0234],\n",
      "         [-29548.9961, -25574.0117,  15394.0000,  ...,  15914.0234,\n",
      "          -30202.9785,  28687.0059],\n",
      "         ...,\n",
      "         [-23349.0215,   4418.9980,  34263.0156,  ..., -10641.9727,\n",
      "          -19971.0098,  -4870.0117],\n",
      "         [ -2319.9570,  13965.9961,  17233.0352,  ..., -37618.0000,\n",
      "          -32827.0078, -29207.0117],\n",
      "         [  -589.9902,   5665.9971,  23403.0273,  ...,   -392.9883,\n",
      "          -14607.0078,  89568.0078]]]),) and output (tensor([[[-24082.0000,  33387.0000,  15374.9961,  ...,  18359.9980,\n",
      "           -7500.0020,  11519.0020],\n",
      "         [-17896.9805,   9044.0195,  25165.0195,  ...,  18824.0020,\n",
      "            -544.9990,   7425.0234],\n",
      "         [-25523.9961, -31766.0117,  17234.0000,  ...,   7842.0234,\n",
      "          -32894.9766,  27856.0059],\n",
      "         ...,\n",
      "         [-26363.0215,  -1414.0020,  32219.0156,  ..., -14504.9727,\n",
      "          -27397.0098,  -4144.0117],\n",
      "         [   581.0430,  18212.9961,  14299.0352,  ..., -48577.0000,\n",
      "          -38656.0078, -36157.0117],\n",
      "         [  -605.9902,  -3406.0029,  23932.0273,  ...,  -1279.9883,\n",
      "          -10201.0078,  80232.0078]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[-24082.0000,  33387.0000,  15374.9961,  ...,  18359.9980,\n",
      "           -7500.0020,  11519.0020],\n",
      "         [-17896.9805,   9044.0195,  25165.0195,  ...,  18824.0020,\n",
      "            -544.9990,   7425.0234],\n",
      "         [-25523.9961, -31766.0117,  17234.0000,  ...,   7842.0234,\n",
      "          -32894.9766,  27856.0059],\n",
      "         ...,\n",
      "         [-26363.0215,  -1414.0020,  32219.0156,  ..., -14504.9727,\n",
      "          -27397.0098,  -4144.0117],\n",
      "         [   581.0430,  18212.9961,  14299.0352,  ..., -48577.0000,\n",
      "          -38656.0078, -36157.0117],\n",
      "         [  -605.9902,  -3406.0029,  23932.0273,  ...,  -1279.9883,\n",
      "          -10201.0078,  80232.0078]]]),) and output (tensor([[[-14869.0000,  32284.0000,   3742.9961,  ...,  12458.9980,\n",
      "           -5452.0020,   7590.0020],\n",
      "         [-23611.9805,   5194.0195,  25579.0195,  ...,  20899.0020,\n",
      "           -3629.9990,   5695.0234],\n",
      "         [-21950.9961, -27561.0117,  20284.0000,  ...,   7215.0234,\n",
      "          -33778.9766,  27887.0059],\n",
      "         ...,\n",
      "         [-23074.0215,   6789.9980,  36925.0156,  ...,  -4640.9727,\n",
      "          -31117.0098,    617.9883],\n",
      "         [   987.0430,  15336.9961,   6303.0352,  ..., -41972.0000,\n",
      "          -34099.0078, -41871.0117],\n",
      "         [ -2242.9902, -14538.0029,  26963.0273,  ...,  -5267.9883,\n",
      "            2335.9922,  82656.0078]]]),)\n",
      "[Debug] Layer names being passed: ['model.layers.0', 'model.layers.1', 'model.layers.2', 'model.layers.3', 'model.layers.4', 'model.layers.5', 'model.layers.6', 'model.layers.7', 'model.layers.8', 'model.layers.9', 'model.layers.10', 'model.layers.11', 'model.layers.12', 'model.layers.13', 'model.layers.14', 'model.layers.15', 'model.layers.16', 'model.layers.17', 'model.layers.18', 'model.layers.19', 'model.layers.20', 'model.layers.21', 'model.layers.22', 'model.layers.23', 'model.layers.24', 'model.layers.25', 'model.layers.26', 'model.layers.27', 'model.embed_tokens']\n",
      "[Debug] Trying to access layer: model.layers.0\n",
      "[Debug] Successfully found layer: model.layers.0\n",
      "[Debug] Trying to access layer: model.layers.1\n",
      "[Debug] Successfully found layer: model.layers.1\n",
      "[Debug] Trying to access layer: model.layers.2\n",
      "[Debug] Successfully found layer: model.layers.2\n",
      "[Debug] Trying to access layer: model.layers.3\n",
      "[Debug] Successfully found layer: model.layers.3\n",
      "[Debug] Trying to access layer: model.layers.4\n",
      "[Debug] Successfully found layer: model.layers.4\n",
      "[Debug] Trying to access layer: model.layers.5\n",
      "[Debug] Successfully found layer: model.layers.5\n",
      "[Debug] Trying to access layer: model.layers.6\n",
      "[Debug] Successfully found layer: model.layers.6\n",
      "[Debug] Trying to access layer: model.layers.7\n",
      "[Debug] Successfully found layer: model.layers.7\n",
      "[Debug] Trying to access layer: model.layers.8\n",
      "[Debug] Successfully found layer: model.layers.8\n",
      "[Debug] Trying to access layer: model.layers.9\n",
      "[Debug] Successfully found layer: model.layers.9\n",
      "[Debug] Trying to access layer: model.layers.10\n",
      "[Debug] Successfully found layer: model.layers.10\n",
      "[Debug] Trying to access layer: model.layers.11\n",
      "[Debug] Successfully found layer: model.layers.11\n",
      "[Debug] Trying to access layer: model.layers.12\n",
      "[Debug] Successfully found layer: model.layers.12\n",
      "[Debug] Trying to access layer: model.layers.13\n",
      "[Debug] Successfully found layer: model.layers.13\n",
      "[Debug] Trying to access layer: model.layers.14\n",
      "[Debug] Successfully found layer: model.layers.14\n",
      "[Debug] Trying to access layer: model.layers.15\n",
      "[Debug] Successfully found layer: model.layers.15\n",
      "[Debug] Trying to access layer: model.layers.16\n",
      "[Debug] Successfully found layer: model.layers.16\n",
      "[Debug] Trying to access layer: model.layers.17\n",
      "[Debug] Successfully found layer: model.layers.17\n",
      "[Debug] Trying to access layer: model.layers.18\n",
      "[Debug] Successfully found layer: model.layers.18\n",
      "[Debug] Trying to access layer: model.layers.19\n",
      "[Debug] Successfully found layer: model.layers.19\n",
      "[Debug] Trying to access layer: model.layers.20\n",
      "[Debug] Successfully found layer: model.layers.20\n",
      "[Debug] Trying to access layer: model.layers.21\n",
      "[Debug] Successfully found layer: model.layers.21\n",
      "[Debug] Trying to access layer: model.layers.22\n",
      "[Debug] Successfully found layer: model.layers.22\n",
      "[Debug] Trying to access layer: model.layers.23\n",
      "[Debug] Successfully found layer: model.layers.23\n",
      "[Debug] Trying to access layer: model.layers.24\n",
      "[Debug] Successfully found layer: model.layers.24\n",
      "[Debug] Trying to access layer: model.layers.25\n",
      "[Debug] Successfully found layer: model.layers.25\n",
      "[Debug] Trying to access layer: model.layers.26\n",
      "[Debug] Successfully found layer: model.layers.26\n",
      "[Debug] Trying to access layer: model.layers.27\n",
      "[Debug] Successfully found layer: model.layers.27\n",
      "[Debug] Trying to access layer: model.embed_tokens\n",
      "[Debug] Successfully found layer: model.embed_tokens\n",
      "[Hook] Layer Embedding(128256, 3072, padding_idx=128004) received input (tensor([[128000,  22427,    661,    315,    264,    468,    318,   3368,  32666,\n",
      "            320,  31255,      8,  16807,   5424,    315,  53125,    315,    264,\n",
      "            468,    318,   3368,  32666,    574,    304,  23393,    323,  20037,\n",
      "            709,    389,   6664,    220,    845,     11,    220,   1049,     24,\n",
      "             13]]),) and output tensor([[[-0.0010, -0.0007, -0.0046,  ..., -0.0014, -0.0020,  0.0020],\n",
      "         [-0.0222,  0.0134, -0.0069,  ..., -0.0128,  0.0258,  0.0019],\n",
      "         [-0.0190,  0.0284,  0.0396,  ..., -0.0309,  0.0654,  0.0103],\n",
      "         ...,\n",
      "         [ 0.0177, -0.0129, -0.0012,  ..., -0.0232, -0.0143,  0.0322],\n",
      "         [-0.0110, -0.0026,  0.0137,  ...,  0.0043,  0.0042,  0.0145],\n",
      "         [ 0.0093, -0.0031,  0.0278,  ...,  0.0117, -0.0084,  0.0096]]])\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[-0.0010, -0.0007, -0.0046,  ..., -0.0014, -0.0020,  0.0020],\n",
      "         [-0.0222,  0.0134, -0.0069,  ..., -0.0128,  0.0258,  0.0019],\n",
      "         [-0.0190,  0.0284,  0.0396,  ..., -0.0309,  0.0654,  0.0103],\n",
      "         ...,\n",
      "         [ 0.0177, -0.0129, -0.0012,  ..., -0.0232, -0.0143,  0.0322],\n",
      "         [-0.0110, -0.0026,  0.0137,  ...,  0.0043,  0.0042,  0.0145],\n",
      "         [ 0.0093, -0.0031,  0.0278,  ...,  0.0117, -0.0084,  0.0096]]]),) and output (tensor([[[ 442.9990,  239.9993,  302.9954,  ..., -154.0014,  -15.0020,\n",
      "           185.0020],\n",
      "         [ 155.9778,    2.0134,   37.9931,  ..., -213.0128,   38.0258,\n",
      "            12.0019],\n",
      "         [ 174.9810,  126.0284,  153.0396,  ...,  -79.0309,  -62.9346,\n",
      "           -52.9897],\n",
      "         ...,\n",
      "         [  22.0177, -135.0129, -193.0012,  ..., -411.0232,  300.9857,\n",
      "          -477.9678],\n",
      "         [  60.9890, -183.0026, -165.9863,  ...,   -0.9957,  159.0042,\n",
      "            -8.9855],\n",
      "         [ -39.9907, -125.0031,   40.0278,  ..., -119.9883,  -24.0084,\n",
      "          -356.9904]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[ 442.9990,  239.9993,  302.9954,  ..., -154.0014,  -15.0020,\n",
      "           185.0020],\n",
      "         [ 155.9778,    2.0134,   37.9931,  ..., -213.0128,   38.0258,\n",
      "            12.0019],\n",
      "         [ 174.9810,  126.0284,  153.0396,  ...,  -79.0309,  -62.9346,\n",
      "           -52.9897],\n",
      "         ...,\n",
      "         [  22.0177, -135.0129, -193.0012,  ..., -411.0232,  300.9857,\n",
      "          -477.9678],\n",
      "         [  60.9890, -183.0026, -165.9863,  ...,   -0.9957,  159.0042,\n",
      "            -8.9855],\n",
      "         [ -39.9907, -125.0031,   40.0278,  ..., -119.9883,  -24.0084,\n",
      "          -356.9904]]]),) and output (tensor([[[  439.9990,  -144.0007,  -410.0046,  ...,  -765.0015,\n",
      "           1193.9979, -1027.9980],\n",
      "         [  665.9778,  -467.9866,   749.9930,  ...,  -894.0128,\n",
      "            894.0258,   -90.9981],\n",
      "         [ -357.0190,   938.0284,   473.0396,  ...,  -473.0309,\n",
      "           -132.9346,   370.0103],\n",
      "         ...,\n",
      "         [-1388.9823,  -388.0129,   373.9988,  ...,  -624.0232,\n",
      "           1088.9856,  -892.9678],\n",
      "         [ -572.0110,   -35.0026,   268.0137,  ...,   665.0043,\n",
      "           1309.0042,  -475.9855],\n",
      "         [ -717.9907,    12.9969, -1144.9722,  ...,   362.0117,\n",
      "           -143.0084,   177.0096]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[  439.9990,  -144.0007,  -410.0046,  ...,  -765.0015,\n",
      "           1193.9979, -1027.9980],\n",
      "         [  665.9778,  -467.9866,   749.9930,  ...,  -894.0128,\n",
      "            894.0258,   -90.9981],\n",
      "         [ -357.0190,   938.0284,   473.0396,  ...,  -473.0309,\n",
      "           -132.9346,   370.0103],\n",
      "         ...,\n",
      "         [-1388.9823,  -388.0129,   373.9988,  ...,  -624.0232,\n",
      "           1088.9856,  -892.9678],\n",
      "         [ -572.0110,   -35.0026,   268.0137,  ...,   665.0043,\n",
      "           1309.0042,  -475.9855],\n",
      "         [ -717.9907,    12.9969, -1144.9722,  ...,   362.0117,\n",
      "           -143.0084,   177.0096]]]),) and output (tensor([[[  551.9990,   993.9993, -1790.0046,  ...,   445.9985,\n",
      "          -1386.0021, -1245.9980],\n",
      "         [ 5037.9775, -3194.9866, -2938.0068,  ...,   888.9872,\n",
      "          -1888.9742,   -87.9980],\n",
      "         [-1269.0190,  -576.9716, -1811.9604,  ..., -2260.0308,\n",
      "          -5295.9346,   296.0103],\n",
      "         ...,\n",
      "         [-1567.9823,  -105.0129,  1574.9988,  ..., -3282.0232,\n",
      "           1671.9856,  1036.0322],\n",
      "         [ -247.0110, -3040.0024,  1659.0137,  ...,  1246.0043,\n",
      "            714.0042,    69.0145],\n",
      "         [-3940.9907,  2983.9971, -1578.9722,  ...,  1530.0117,\n",
      "           -247.0084,  1289.0096]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[  551.9990,   993.9993, -1790.0046,  ...,   445.9985,\n",
      "          -1386.0021, -1245.9980],\n",
      "         [ 5037.9775, -3194.9866, -2938.0068,  ...,   888.9872,\n",
      "          -1888.9742,   -87.9980],\n",
      "         [-1269.0190,  -576.9716, -1811.9604,  ..., -2260.0308,\n",
      "          -5295.9346,   296.0103],\n",
      "         ...,\n",
      "         [-1567.9823,  -105.0129,  1574.9988,  ..., -3282.0232,\n",
      "           1671.9856,  1036.0322],\n",
      "         [ -247.0110, -3040.0024,  1659.0137,  ...,  1246.0043,\n",
      "            714.0042,    69.0145],\n",
      "         [-3940.9907,  2983.9971, -1578.9722,  ...,  1530.0117,\n",
      "           -247.0084,  1289.0096]]]),) and output (tensor([[[ 2131.9990,  -737.0007, -1071.0046,  ..., -2781.0015,\n",
      "           -885.0021,    86.0020],\n",
      "         [   99.9775, -5800.9863, -3441.0068,  ..., -3700.0127,\n",
      "          -4256.9741, -4437.9980],\n",
      "         [-3328.0190, -7426.9717,  -114.9604,  ...,   710.9692,\n",
      "          -4544.9346,  1644.0103],\n",
      "         ...,\n",
      "         [-2684.9824,   891.9871,  3589.9988,  ..., -4542.0234,\n",
      "           -615.0144, -2204.9678],\n",
      "         [-1354.0110, -4575.0024,  1973.0137,  ..., -1264.9956,\n",
      "           2037.0042,  1831.0144],\n",
      "         [-4885.9907,   -51.0029, -3393.9722,  ..., -6562.9883,\n",
      "          -1169.0083,   222.0096]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[ 2131.9990,  -737.0007, -1071.0046,  ..., -2781.0015,\n",
      "           -885.0021,    86.0020],\n",
      "         [   99.9775, -5800.9863, -3441.0068,  ..., -3700.0127,\n",
      "          -4256.9741, -4437.9980],\n",
      "         [-3328.0190, -7426.9717,  -114.9604,  ...,   710.9692,\n",
      "          -4544.9346,  1644.0103],\n",
      "         ...,\n",
      "         [-2684.9824,   891.9871,  3589.9988,  ..., -4542.0234,\n",
      "           -615.0144, -2204.9678],\n",
      "         [-1354.0110, -4575.0024,  1973.0137,  ..., -1264.9956,\n",
      "           2037.0042,  1831.0144],\n",
      "         [-4885.9907,   -51.0029, -3393.9722,  ..., -6562.9883,\n",
      "          -1169.0083,   222.0096]]]),) and output (tensor([[[ 3420.9990,   760.9993, -5376.0049,  ..., -4193.0015,\n",
      "           2860.9980, -4080.9980],\n",
      "         [ 6267.9775, -6616.9863, -4304.0068,  ..., -1258.0127,\n",
      "          -4175.9741, -2734.9980],\n",
      "         [-1300.0190, -8233.9717, -7640.9604,  ...,  -918.0308,\n",
      "          -7515.9346,  2706.0103],\n",
      "         ...,\n",
      "         [-1256.9824, -5143.0127, 10342.9990,  ...,   890.9766,\n",
      "           -674.0144, -4669.9678],\n",
      "         [ 6988.9893, -2129.0024,  2118.0137,  ...,  2038.0044,\n",
      "           1868.0042, -2085.9856],\n",
      "         [-8715.9902,  2277.9971, -4733.9722,  ..., -8197.9883,\n",
      "          -3957.0083,   638.0096]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[ 3420.9990,   760.9993, -5376.0049,  ..., -4193.0015,\n",
      "           2860.9980, -4080.9980],\n",
      "         [ 6267.9775, -6616.9863, -4304.0068,  ..., -1258.0127,\n",
      "          -4175.9741, -2734.9980],\n",
      "         [-1300.0190, -8233.9717, -7640.9604,  ...,  -918.0308,\n",
      "          -7515.9346,  2706.0103],\n",
      "         ...,\n",
      "         [-1256.9824, -5143.0127, 10342.9990,  ...,   890.9766,\n",
      "           -674.0144, -4669.9678],\n",
      "         [ 6988.9893, -2129.0024,  2118.0137,  ...,  2038.0044,\n",
      "           1868.0042, -2085.9856],\n",
      "         [-8715.9902,  2277.9971, -4733.9722,  ..., -8197.9883,\n",
      "          -3957.0083,   638.0096]]]),) and output (tensor([[[  2742.9990,  -1512.0007, -10493.0049,  ...,  -9945.0020,\n",
      "           -2017.0020,  -1802.9980],\n",
      "         [  7079.9775,  -9780.9863,  -2815.0068,  ...,  -3641.0127,\n",
      "           -6551.9741,   2099.0020],\n",
      "         [ -8200.0195, -11965.9717, -11877.9609,  ...,  -5825.0308,\n",
      "           -2543.9346,   5152.0103],\n",
      "         ...,\n",
      "         [ -3357.9824,  -7336.0127,   8416.9990,  ...,   3052.9766,\n",
      "            5841.9854,  -3616.9678],\n",
      "         [  4514.9893,  -3865.0024,   -160.9863,  ...,    843.0044,\n",
      "            4023.0042,   5322.0146],\n",
      "         [ -8077.9902,   4633.9971,  -1985.9722,  ..., -12032.9883,\n",
      "           -2463.0083,   1308.0096]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[  2742.9990,  -1512.0007, -10493.0049,  ...,  -9945.0020,\n",
      "           -2017.0020,  -1802.9980],\n",
      "         [  7079.9775,  -9780.9863,  -2815.0068,  ...,  -3641.0127,\n",
      "           -6551.9741,   2099.0020],\n",
      "         [ -8200.0195, -11965.9717, -11877.9609,  ...,  -5825.0308,\n",
      "           -2543.9346,   5152.0103],\n",
      "         ...,\n",
      "         [ -3357.9824,  -7336.0127,   8416.9990,  ...,   3052.9766,\n",
      "            5841.9854,  -3616.9678],\n",
      "         [  4514.9893,  -3865.0024,   -160.9863,  ...,    843.0044,\n",
      "            4023.0042,   5322.0146],\n",
      "         [ -8077.9902,   4633.9971,  -1985.9722,  ..., -12032.9883,\n",
      "           -2463.0083,   1308.0096]]]),) and output (tensor([[[  3032.9990,  -3420.0007, -10291.0049,  ...,  -6511.0020,\n",
      "           -1953.0020,   2001.0020],\n",
      "         [ 12735.9775, -14176.9863,   1116.9932,  ...,  -1123.0127,\n",
      "           -1113.9741,   1708.0020],\n",
      "         [-10610.0195, -12843.9717, -14480.9609,  ...,  -5669.0308,\n",
      "           -4753.9346,   1630.0103],\n",
      "         ...,\n",
      "         [ -3738.9824,  -9976.0127,   7003.9990,  ...,  12278.9766,\n",
      "            2913.9854,  -4073.9678],\n",
      "         [  4130.9893,  -1126.0024,   1040.0137,  ...,  -2850.9956,\n",
      "            4151.0039,   3962.0146],\n",
      "         [ -9470.9902,   7604.9971,  -2891.9722,  ...,  -4582.9883,\n",
      "            -198.0083,  -7652.9902]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[  3032.9990,  -3420.0007, -10291.0049,  ...,  -6511.0020,\n",
      "           -1953.0020,   2001.0020],\n",
      "         [ 12735.9775, -14176.9863,   1116.9932,  ...,  -1123.0127,\n",
      "           -1113.9741,   1708.0020],\n",
      "         [-10610.0195, -12843.9717, -14480.9609,  ...,  -5669.0308,\n",
      "           -4753.9346,   1630.0103],\n",
      "         ...,\n",
      "         [ -3738.9824,  -9976.0127,   7003.9990,  ...,  12278.9766,\n",
      "            2913.9854,  -4073.9678],\n",
      "         [  4130.9893,  -1126.0024,   1040.0137,  ...,  -2850.9956,\n",
      "            4151.0039,   3962.0146],\n",
      "         [ -9470.9902,   7604.9971,  -2891.9722,  ...,  -4582.9883,\n",
      "            -198.0083,  -7652.9902]]]),) and output (tensor([[[  3132.9990,  -1970.0007, -11544.0049,  ...,  -5780.0020,\n",
      "           -2486.0020,  -1921.9980],\n",
      "         [  9630.9775, -20526.9863,    480.9932,  ...,  -1956.0127,\n",
      "           -1465.9741,   1294.0020],\n",
      "         [-10635.0195, -15366.9717, -19133.9609,  ...,  -7768.0308,\n",
      "            -607.9346,   1833.0103],\n",
      "         ...,\n",
      "         [ -7342.9824, -14931.0127,   3749.9990,  ...,  12301.9766,\n",
      "            1921.9854,  -9227.9678],\n",
      "         [  -837.0107,  -1525.0024,   1966.0137,  ...,  -1556.9956,\n",
      "             -38.9961,   4293.0146],\n",
      "         [ -1796.9902,  10478.9971,  -5549.9722,  ...,    924.0117,\n",
      "            -387.0083,  -6671.9902]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[  3132.9990,  -1970.0007, -11544.0049,  ...,  -5780.0020,\n",
      "           -2486.0020,  -1921.9980],\n",
      "         [  9630.9775, -20526.9863,    480.9932,  ...,  -1956.0127,\n",
      "           -1465.9741,   1294.0020],\n",
      "         [-10635.0195, -15366.9717, -19133.9609,  ...,  -7768.0308,\n",
      "            -607.9346,   1833.0103],\n",
      "         ...,\n",
      "         [ -7342.9824, -14931.0127,   3749.9990,  ...,  12301.9766,\n",
      "            1921.9854,  -9227.9678],\n",
      "         [  -837.0107,  -1525.0024,   1966.0137,  ...,  -1556.9956,\n",
      "             -38.9961,   4293.0146],\n",
      "         [ -1796.9902,  10478.9971,  -5549.9722,  ...,    924.0117,\n",
      "            -387.0083,  -6671.9902]]]),) and output (tensor([[[  1046.9990,   2403.9993, -14486.0049,  ..., -10922.0020,\n",
      "           -1419.0020,  -2983.9980],\n",
      "         [ 11742.9775, -22418.9863,   1497.9932,  ...,  -2966.0127,\n",
      "            2475.0259,    951.0020],\n",
      "         [-14093.0195, -10124.9717, -18588.9609,  ...,  -6136.0308,\n",
      "           -3280.9346,     46.0103],\n",
      "         ...,\n",
      "         [-13129.9824,  -8585.0127,   3724.9990,  ...,  11066.9766,\n",
      "             639.9854, -15835.9678],\n",
      "         [ -1148.0107,   3503.9976,   2495.0137,  ...,   2272.0044,\n",
      "            1782.0039,    110.0146],\n",
      "         [ -1146.9902,  12415.9971,  -1897.9722,  ...,   1992.0117,\n",
      "           -2895.0083, -12581.9902]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[  1046.9990,   2403.9993, -14486.0049,  ..., -10922.0020,\n",
      "           -1419.0020,  -2983.9980],\n",
      "         [ 11742.9775, -22418.9863,   1497.9932,  ...,  -2966.0127,\n",
      "            2475.0259,    951.0020],\n",
      "         [-14093.0195, -10124.9717, -18588.9609,  ...,  -6136.0308,\n",
      "           -3280.9346,     46.0103],\n",
      "         ...,\n",
      "         [-13129.9824,  -8585.0127,   3724.9990,  ...,  11066.9766,\n",
      "             639.9854, -15835.9678],\n",
      "         [ -1148.0107,   3503.9976,   2495.0137,  ...,   2272.0044,\n",
      "            1782.0039,    110.0146],\n",
      "         [ -1146.9902,  12415.9971,  -1897.9722,  ...,   1992.0117,\n",
      "           -2895.0083, -12581.9902]]]),) and output (tensor([[[ -3767.0010,   5307.9990, -14221.0049,  ..., -15035.0020,\n",
      "           -3630.0020,   4168.0020],\n",
      "         [  7867.9775, -18616.9863,   1263.9932,  ...,  -3211.0127,\n",
      "            1676.0259,   -370.9980],\n",
      "         [-13852.0195, -16212.9717, -16731.9609,  ...,  -9988.0312,\n",
      "           -3265.9346,   -587.9897],\n",
      "         ...,\n",
      "         [-19121.9824, -13981.0127,   2889.9990,  ...,   9211.9766,\n",
      "             693.9854, -10435.9678],\n",
      "         [ -2980.0107,  -1725.0024,   1291.0137,  ...,  -3092.9956,\n",
      "           11031.0039,  -4740.9854],\n",
      "         [   -59.9902,   9172.9971,  -4426.9722,  ...,  -1942.9883,\n",
      "           -6914.0083, -13851.9902]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[ -3767.0010,   5307.9990, -14221.0049,  ..., -15035.0020,\n",
      "           -3630.0020,   4168.0020],\n",
      "         [  7867.9775, -18616.9863,   1263.9932,  ...,  -3211.0127,\n",
      "            1676.0259,   -370.9980],\n",
      "         [-13852.0195, -16212.9717, -16731.9609,  ...,  -9988.0312,\n",
      "           -3265.9346,   -587.9897],\n",
      "         ...,\n",
      "         [-19121.9824, -13981.0127,   2889.9990,  ...,   9211.9766,\n",
      "             693.9854, -10435.9678],\n",
      "         [ -2980.0107,  -1725.0024,   1291.0137,  ...,  -3092.9956,\n",
      "           11031.0039,  -4740.9854],\n",
      "         [   -59.9902,   9172.9971,  -4426.9722,  ...,  -1942.9883,\n",
      "           -6914.0083, -13851.9902]]]),) and output (tensor([[[ -1326.0010,   1223.9990, -16643.0039,  ..., -13240.0020,\n",
      "           -5677.0020,   9639.0020],\n",
      "         [  6338.9775, -13794.9863,    356.9932,  ...,  -2816.0127,\n",
      "            1646.0259,   2088.0020],\n",
      "         [-12407.0195, -14815.9717, -16433.9609,  ...,  -8575.0312,\n",
      "            2943.0654,  -6386.9897],\n",
      "         ...,\n",
      "         [-17967.9824,  -7229.0127,    327.9990,  ...,   8258.9766,\n",
      "            1175.9854, -12307.9678],\n",
      "         [ -2866.0107,  -2855.0024,   1923.0137,  ...,  -3949.9956,\n",
      "           11410.0039, -10534.9854],\n",
      "         [  1468.0098,   5503.9971,   -916.9722,  ...,  -3911.9883,\n",
      "          -11595.0078, -16059.9902]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[ -1326.0010,   1223.9990, -16643.0039,  ..., -13240.0020,\n",
      "           -5677.0020,   9639.0020],\n",
      "         [  6338.9775, -13794.9863,    356.9932,  ...,  -2816.0127,\n",
      "            1646.0259,   2088.0020],\n",
      "         [-12407.0195, -14815.9717, -16433.9609,  ...,  -8575.0312,\n",
      "            2943.0654,  -6386.9897],\n",
      "         ...,\n",
      "         [-17967.9824,  -7229.0127,    327.9990,  ...,   8258.9766,\n",
      "            1175.9854, -12307.9678],\n",
      "         [ -2866.0107,  -2855.0024,   1923.0137,  ...,  -3949.9956,\n",
      "           11410.0039, -10534.9854],\n",
      "         [  1468.0098,   5503.9971,   -916.9722,  ...,  -3911.9883,\n",
      "          -11595.0078, -16059.9902]]]),) and output (tensor([[[ -5120.0010,   2246.9990, -15993.0039,  ..., -19818.0020,\n",
      "           -9385.0020,  11924.0020],\n",
      "         [  9131.9775, -11166.9863,   1865.9932,  ...,  -6919.0127,\n",
      "            4146.0259,   2321.0020],\n",
      "         [-11999.0195, -14552.9717, -14164.9609,  ...,  -1017.0312,\n",
      "            8647.0654,  -4153.9897],\n",
      "         ...,\n",
      "         [-18918.9824,  -3251.0127,    918.9990,  ...,  13615.9766,\n",
      "           10858.9854,  -7995.9678],\n",
      "         [   495.9893,    834.9976,  -1005.9863,  ...,  -4669.9956,\n",
      "           14075.0039,  -8411.9854],\n",
      "         [  1931.0098,   3026.9971,   -667.9722,  ...,  -4737.9883,\n",
      "          -15392.0078, -17346.9902]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[ -5120.0010,   2246.9990, -15993.0039,  ..., -19818.0020,\n",
      "           -9385.0020,  11924.0020],\n",
      "         [  9131.9775, -11166.9863,   1865.9932,  ...,  -6919.0127,\n",
      "            4146.0259,   2321.0020],\n",
      "         [-11999.0195, -14552.9717, -14164.9609,  ...,  -1017.0312,\n",
      "            8647.0654,  -4153.9897],\n",
      "         ...,\n",
      "         [-18918.9824,  -3251.0127,    918.9990,  ...,  13615.9766,\n",
      "           10858.9854,  -7995.9678],\n",
      "         [   495.9893,    834.9976,  -1005.9863,  ...,  -4669.9956,\n",
      "           14075.0039,  -8411.9854],\n",
      "         [  1931.0098,   3026.9971,   -667.9722,  ...,  -4737.9883,\n",
      "          -15392.0078, -17346.9902]]]),) and output (tensor([[[ -6685.0010,  -1278.0010, -15553.0039,  ..., -19846.0020,\n",
      "          -10418.0020,  13376.0020],\n",
      "         [  6165.9775, -14153.9863,   5177.9932,  ...,  -2036.0127,\n",
      "           -2030.9741,   2600.0020],\n",
      "         [ -6449.0195, -17201.9727, -17383.9609,  ...,  -2013.0312,\n",
      "           14500.0654,  -7363.9897],\n",
      "         ...,\n",
      "         [-17687.9824,   1689.9873,  -1148.0010,  ...,  18463.9766,\n",
      "           12248.9854,  -9986.9678],\n",
      "         [  3954.9893,  -4081.0024,    409.0137,  ...,  -3881.9956,\n",
      "           15604.0039,   1136.0146],\n",
      "         [  -408.9902,   2283.9971,  -3586.9722,  ...,  -7161.9883,\n",
      "          -20202.0078, -22364.9902]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[ -6685.0010,  -1278.0010, -15553.0039,  ..., -19846.0020,\n",
      "          -10418.0020,  13376.0020],\n",
      "         [  6165.9775, -14153.9863,   5177.9932,  ...,  -2036.0127,\n",
      "           -2030.9741,   2600.0020],\n",
      "         [ -6449.0195, -17201.9727, -17383.9609,  ...,  -2013.0312,\n",
      "           14500.0654,  -7363.9897],\n",
      "         ...,\n",
      "         [-17687.9824,   1689.9873,  -1148.0010,  ...,  18463.9766,\n",
      "           12248.9854,  -9986.9678],\n",
      "         [  3954.9893,  -4081.0024,    409.0137,  ...,  -3881.9956,\n",
      "           15604.0039,   1136.0146],\n",
      "         [  -408.9902,   2283.9971,  -3586.9722,  ...,  -7161.9883,\n",
      "          -20202.0078, -22364.9902]]]),) and output (tensor([[[ -8422.0010,    339.9990, -15289.0039,  ..., -20884.0020,\n",
      "           -6908.0020,  11014.0020],\n",
      "         [  6666.9775,  -9419.9863,   7552.9932,  ...,  -4602.0127,\n",
      "           -2225.9741,  -1549.9980],\n",
      "         [ -7976.0195, -17305.9727, -20817.9609,  ..., -10431.0312,\n",
      "           10274.0654,  -9717.9902],\n",
      "         ...,\n",
      "         [-17563.9824,   8949.9873,  -4592.0010,  ...,  15648.9766,\n",
      "           20746.9844,  -9449.9678],\n",
      "         [ 12868.9893,  -8456.0020,  -1714.9863,  ...,  -2268.9956,\n",
      "           20808.0039,   1783.0146],\n",
      "         [ -1971.9902,  -1584.0029,   3921.0278,  ...,   1195.0117,\n",
      "          -20866.0078, -19119.9902]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[ -8422.0010,    339.9990, -15289.0039,  ..., -20884.0020,\n",
      "           -6908.0020,  11014.0020],\n",
      "         [  6666.9775,  -9419.9863,   7552.9932,  ...,  -4602.0127,\n",
      "           -2225.9741,  -1549.9980],\n",
      "         [ -7976.0195, -17305.9727, -20817.9609,  ..., -10431.0312,\n",
      "           10274.0654,  -9717.9902],\n",
      "         ...,\n",
      "         [-17563.9824,   8949.9873,  -4592.0010,  ...,  15648.9766,\n",
      "           20746.9844,  -9449.9678],\n",
      "         [ 12868.9893,  -8456.0020,  -1714.9863,  ...,  -2268.9956,\n",
      "           20808.0039,   1783.0146],\n",
      "         [ -1971.9902,  -1584.0029,   3921.0278,  ...,   1195.0117,\n",
      "          -20866.0078, -19119.9902]]]),) and output (tensor([[[-10989.0010,   7034.9990, -13964.0039,  ..., -23968.0020,\n",
      "           -3749.0020,  11730.0020],\n",
      "         [  7558.9775,  -3919.9863,   5860.9932,  ...,   4449.9873,\n",
      "             -52.9741,   4204.0020],\n",
      "         [ -3683.0195, -17374.9727, -21586.9609,  ..., -13872.0312,\n",
      "            9388.0654, -15402.9902],\n",
      "         ...,\n",
      "         [-17065.9824,  12594.9873,  -5128.0010,  ...,  17222.9766,\n",
      "           19678.9844, -10633.9678],\n",
      "         [ 15655.9893,  -5564.0020,   -596.9863,  ...,   1663.0044,\n",
      "           17068.0039,  10195.0146],\n",
      "         [  1234.0098,  -5843.0029,   8691.0273,  ...,   1658.0117,\n",
      "          -22606.0078, -19799.9902]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[-10989.0010,   7034.9990, -13964.0039,  ..., -23968.0020,\n",
      "           -3749.0020,  11730.0020],\n",
      "         [  7558.9775,  -3919.9863,   5860.9932,  ...,   4449.9873,\n",
      "             -52.9741,   4204.0020],\n",
      "         [ -3683.0195, -17374.9727, -21586.9609,  ..., -13872.0312,\n",
      "            9388.0654, -15402.9902],\n",
      "         ...,\n",
      "         [-17065.9824,  12594.9873,  -5128.0010,  ...,  17222.9766,\n",
      "           19678.9844, -10633.9678],\n",
      "         [ 15655.9893,  -5564.0020,   -596.9863,  ...,   1663.0044,\n",
      "           17068.0039,  10195.0146],\n",
      "         [  1234.0098,  -5843.0029,   8691.0273,  ...,   1658.0117,\n",
      "          -22606.0078, -19799.9902]]]),) and output (tensor([[[ -7159.0010,   6621.9990, -10967.0039,  ..., -17477.0020,\n",
      "           -2510.0020,  11888.0020],\n",
      "         [ 12647.9775,    222.0137,   7051.9932,  ...,   5383.9873,\n",
      "            1608.0259,   4288.0020],\n",
      "         [ -1821.0195, -18259.9727, -16526.9609,  ..., -12780.0312,\n",
      "           14310.0654, -22781.9902],\n",
      "         ...,\n",
      "         [-12874.9824,   8794.9873,  -9279.0010,  ...,  15470.9766,\n",
      "           25691.9844,  -5730.9678],\n",
      "         [ 13160.9893,  -8752.0020,  -1508.9863,  ...,   3928.0044,\n",
      "           18659.0039,   8083.0146],\n",
      "         [   991.0098,  -5765.0029,   5485.0273,  ...,  -1243.9883,\n",
      "          -26025.0078, -21420.9902]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[ -7159.0010,   6621.9990, -10967.0039,  ..., -17477.0020,\n",
      "           -2510.0020,  11888.0020],\n",
      "         [ 12647.9775,    222.0137,   7051.9932,  ...,   5383.9873,\n",
      "            1608.0259,   4288.0020],\n",
      "         [ -1821.0195, -18259.9727, -16526.9609,  ..., -12780.0312,\n",
      "           14310.0654, -22781.9902],\n",
      "         ...,\n",
      "         [-12874.9824,   8794.9873,  -9279.0010,  ...,  15470.9766,\n",
      "           25691.9844,  -5730.9678],\n",
      "         [ 13160.9893,  -8752.0020,  -1508.9863,  ...,   3928.0044,\n",
      "           18659.0039,   8083.0146],\n",
      "         [   991.0098,  -5765.0029,   5485.0273,  ...,  -1243.9883,\n",
      "          -26025.0078, -21420.9902]]]),) and output (tensor([[[ -4822.0010,   3124.9990,  -3885.0039,  ..., -19007.0020,\n",
      "            -551.0020,  16285.0020],\n",
      "         [ 10952.9775,   3526.0137,  12195.9932,  ...,   8343.9873,\n",
      "            -265.9741,   3983.0020],\n",
      "         [ -7270.0195, -16716.9727, -18033.9609,  ..., -22151.0312,\n",
      "           13070.0654, -22510.9902],\n",
      "         ...,\n",
      "         [ -6717.9824,   8815.9873, -10781.0010,  ...,  10734.9766,\n",
      "           22399.9844,  -8057.9678],\n",
      "         [  9102.9893, -12527.0020,  -3075.9863,  ...,   7515.0044,\n",
      "           19280.0039,  11943.0146],\n",
      "         [  1496.0098,   1863.9971,  15020.0273,  ...,  -3612.9883,\n",
      "          -21241.0078, -24011.9902]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[ -4822.0010,   3124.9990,  -3885.0039,  ..., -19007.0020,\n",
      "            -551.0020,  16285.0020],\n",
      "         [ 10952.9775,   3526.0137,  12195.9932,  ...,   8343.9873,\n",
      "            -265.9741,   3983.0020],\n",
      "         [ -7270.0195, -16716.9727, -18033.9609,  ..., -22151.0312,\n",
      "           13070.0654, -22510.9902],\n",
      "         ...,\n",
      "         [ -6717.9824,   8815.9873, -10781.0010,  ...,  10734.9766,\n",
      "           22399.9844,  -8057.9678],\n",
      "         [  9102.9893, -12527.0020,  -3075.9863,  ...,   7515.0044,\n",
      "           19280.0039,  11943.0146],\n",
      "         [  1496.0098,   1863.9971,  15020.0273,  ...,  -3612.9883,\n",
      "          -21241.0078, -24011.9902]]]),) and output (tensor([[[ -7235.0010,   6583.9990,  -4212.0039,  ..., -19104.0020,\n",
      "            3087.9980,  17084.0020],\n",
      "         [  7384.9775,   8910.0137,  15526.9932,  ...,   7064.9873,\n",
      "             438.0259,   5139.0020],\n",
      "         [-11261.0195, -19523.9727, -17499.9609,  ..., -26022.0312,\n",
      "           11087.0654, -17329.9902],\n",
      "         ...,\n",
      "         [-14067.9824,  18552.9883, -15302.0010,  ...,   6823.9766,\n",
      "           23774.9844,  -5468.9678],\n",
      "         [  9271.9893,  -9190.0020,    518.0137,  ...,  19876.0039,\n",
      "           24880.0039,  12246.0146],\n",
      "         [  6735.0098,   2885.9971,  15159.0273,  ...,  -7353.9883,\n",
      "          -22071.0078, -20954.9902]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[ -7235.0010,   6583.9990,  -4212.0039,  ..., -19104.0020,\n",
      "            3087.9980,  17084.0020],\n",
      "         [  7384.9775,   8910.0137,  15526.9932,  ...,   7064.9873,\n",
      "             438.0259,   5139.0020],\n",
      "         [-11261.0195, -19523.9727, -17499.9609,  ..., -26022.0312,\n",
      "           11087.0654, -17329.9902],\n",
      "         ...,\n",
      "         [-14067.9824,  18552.9883, -15302.0010,  ...,   6823.9766,\n",
      "           23774.9844,  -5468.9678],\n",
      "         [  9271.9893,  -9190.0020,    518.0137,  ...,  19876.0039,\n",
      "           24880.0039,  12246.0146],\n",
      "         [  6735.0098,   2885.9971,  15159.0273,  ...,  -7353.9883,\n",
      "          -22071.0078, -20954.9902]]]),) and output (tensor([[[ -5586.0010,   7645.9990,   -505.0039,  ..., -15346.0020,\n",
      "            -623.0020,  12986.0020],\n",
      "         [  9132.9775,   5957.0137,  13741.9932,  ...,   6370.9873,\n",
      "           -1920.9741,   2601.0020],\n",
      "         [-14773.0195, -27980.9727, -20750.9609,  ..., -28858.0312,\n",
      "            8624.0654, -14562.9902],\n",
      "         ...,\n",
      "         [-15224.9824,  20558.9883, -15213.0010,  ...,  11920.9766,\n",
      "           22900.9844,  -7289.9678],\n",
      "         [ 11861.9893, -12850.0020,  -2451.9863,  ...,  21478.0039,\n",
      "           27971.0039,  12355.0146],\n",
      "         [  8186.0098,  -1137.0029,  18347.0273,  ...,  -9847.9883,\n",
      "          -28852.0078, -21240.9902]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[ -5586.0010,   7645.9990,   -505.0039,  ..., -15346.0020,\n",
      "            -623.0020,  12986.0020],\n",
      "         [  9132.9775,   5957.0137,  13741.9932,  ...,   6370.9873,\n",
      "           -1920.9741,   2601.0020],\n",
      "         [-14773.0195, -27980.9727, -20750.9609,  ..., -28858.0312,\n",
      "            8624.0654, -14562.9902],\n",
      "         ...,\n",
      "         [-15224.9824,  20558.9883, -15213.0010,  ...,  11920.9766,\n",
      "           22900.9844,  -7289.9678],\n",
      "         [ 11861.9893, -12850.0020,  -2451.9863,  ...,  21478.0039,\n",
      "           27971.0039,  12355.0146],\n",
      "         [  8186.0098,  -1137.0029,  18347.0273,  ...,  -9847.9883,\n",
      "          -28852.0078, -21240.9902]]]),) and output (tensor([[[ -8916.0010,   2362.9990,  -4833.0039,  ..., -12075.0020,\n",
      "           -1124.0020,  12438.0020],\n",
      "         [  6938.9775,   5847.0137,  11594.9932,  ...,   5196.9873,\n",
      "           -3973.9741,  10397.0020],\n",
      "         [-10310.0195, -22182.9727, -21698.9609,  ..., -26565.0312,\n",
      "           10787.0654, -13057.9902],\n",
      "         ...,\n",
      "         [-19196.9824,  21648.9883, -18240.0000,  ...,   9369.9766,\n",
      "           31160.9844,  -9596.9678],\n",
      "         [ 20236.9883, -12332.0020,  -6281.9863,  ...,  12890.0039,\n",
      "           25081.0039,  16109.0146],\n",
      "         [  8455.0098,  -1421.0029,  15193.0273,  ...,  -1979.9883,\n",
      "          -30351.0078, -22489.9902]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[ -8916.0010,   2362.9990,  -4833.0039,  ..., -12075.0020,\n",
      "           -1124.0020,  12438.0020],\n",
      "         [  6938.9775,   5847.0137,  11594.9932,  ...,   5196.9873,\n",
      "           -3973.9741,  10397.0020],\n",
      "         [-10310.0195, -22182.9727, -21698.9609,  ..., -26565.0312,\n",
      "           10787.0654, -13057.9902],\n",
      "         ...,\n",
      "         [-19196.9824,  21648.9883, -18240.0000,  ...,   9369.9766,\n",
      "           31160.9844,  -9596.9678],\n",
      "         [ 20236.9883, -12332.0020,  -6281.9863,  ...,  12890.0039,\n",
      "           25081.0039,  16109.0146],\n",
      "         [  8455.0098,  -1421.0029,  15193.0273,  ...,  -1979.9883,\n",
      "          -30351.0078, -22489.9902]]]),) and output (tensor([[[-20230.0000,   7877.9990,    161.9961,  ..., -10250.0020,\n",
      "             676.9980,   7095.0020],\n",
      "         [  4999.9775,   5350.0137,  16454.9922,  ...,   7486.9873,\n",
      "            3628.0259,  15039.0020],\n",
      "         [-10343.0195, -18492.9727, -20014.9609,  ..., -34562.0312,\n",
      "           17005.0664, -17498.9902],\n",
      "         ...,\n",
      "         [-19093.9824,  22091.9883, -15157.0000,  ...,  12082.9766,\n",
      "           33661.9844,  -7893.9678],\n",
      "         [ 24533.9883,  -4717.0020,  -3741.9863,  ...,  19571.0039,\n",
      "           31390.0039,    -66.9854],\n",
      "         [ 16841.0098,  -2077.0029,  13639.0273,  ...,   6313.0117,\n",
      "          -21326.0078, -25032.9902]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[-20230.0000,   7877.9990,    161.9961,  ..., -10250.0020,\n",
      "             676.9980,   7095.0020],\n",
      "         [  4999.9775,   5350.0137,  16454.9922,  ...,   7486.9873,\n",
      "            3628.0259,  15039.0020],\n",
      "         [-10343.0195, -18492.9727, -20014.9609,  ..., -34562.0312,\n",
      "           17005.0664, -17498.9902],\n",
      "         ...,\n",
      "         [-19093.9824,  22091.9883, -15157.0000,  ...,  12082.9766,\n",
      "           33661.9844,  -7893.9678],\n",
      "         [ 24533.9883,  -4717.0020,  -3741.9863,  ...,  19571.0039,\n",
      "           31390.0039,    -66.9854],\n",
      "         [ 16841.0098,  -2077.0029,  13639.0273,  ...,   6313.0117,\n",
      "          -21326.0078, -25032.9902]]]),) and output (tensor([[[-25670.0000,  13293.9990,   9099.9961,  ...,  -6547.0020,\n",
      "           -5658.0020,   4192.0020],\n",
      "         [  8235.9775,   8258.0137,  18413.9922,  ...,   4626.9873,\n",
      "             417.0259,  13003.0020],\n",
      "         [ -7655.0195, -23634.9727, -19993.9609,  ..., -40077.0312,\n",
      "           17800.0664, -25011.9902],\n",
      "         ...,\n",
      "         [-26909.9824,  22717.9883, -10765.0000,  ...,  12572.9766,\n",
      "           27851.9844,  -4442.9678],\n",
      "         [ 25620.9883,  -8090.0020,  -5540.9863,  ...,  12694.0039,\n",
      "           33793.0039,   3377.0146],\n",
      "         [ 18309.0098,  -8856.0029,  16301.0273,  ...,   4033.0117,\n",
      "          -22160.0078, -40662.9922]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[-25670.0000,  13293.9990,   9099.9961,  ...,  -6547.0020,\n",
      "           -5658.0020,   4192.0020],\n",
      "         [  8235.9775,   8258.0137,  18413.9922,  ...,   4626.9873,\n",
      "             417.0259,  13003.0020],\n",
      "         [ -7655.0195, -23634.9727, -19993.9609,  ..., -40077.0312,\n",
      "           17800.0664, -25011.9902],\n",
      "         ...,\n",
      "         [-26909.9824,  22717.9883, -10765.0000,  ...,  12572.9766,\n",
      "           27851.9844,  -4442.9678],\n",
      "         [ 25620.9883,  -8090.0020,  -5540.9863,  ...,  12694.0039,\n",
      "           33793.0039,   3377.0146],\n",
      "         [ 18309.0098,  -8856.0029,  16301.0273,  ...,   4033.0117,\n",
      "          -22160.0078, -40662.9922]]]),) and output (tensor([[[-25652.0000,  13180.9990,  15728.9961,  ...,  -2831.0020,\n",
      "             158.9980,   2204.0020],\n",
      "         [  8272.9775,   3594.0137,   9039.9922,  ..., -11330.0127,\n",
      "          -10555.9746,  23741.0020],\n",
      "         [-10240.0195, -25989.9727, -16425.9609,  ..., -39754.0312,\n",
      "           14863.0664, -20156.9902],\n",
      "         ...,\n",
      "         [-34196.9844,  21520.9883, -13197.0000,  ...,  19814.9766,\n",
      "           24352.9844,  -6187.9678],\n",
      "         [ 25234.9883, -10224.0020,  -3008.9863,  ...,  18922.0039,\n",
      "           39131.0039,   6863.0146],\n",
      "         [ 20288.0098,  -6202.0029,  10264.0273,  ...,  -1201.9883,\n",
      "          -19782.0078, -37811.9922]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[-25652.0000,  13180.9990,  15728.9961,  ...,  -2831.0020,\n",
      "             158.9980,   2204.0020],\n",
      "         [  8272.9775,   3594.0137,   9039.9922,  ..., -11330.0127,\n",
      "          -10555.9746,  23741.0020],\n",
      "         [-10240.0195, -25989.9727, -16425.9609,  ..., -39754.0312,\n",
      "           14863.0664, -20156.9902],\n",
      "         ...,\n",
      "         [-34196.9844,  21520.9883, -13197.0000,  ...,  19814.9766,\n",
      "           24352.9844,  -6187.9678],\n",
      "         [ 25234.9883, -10224.0020,  -3008.9863,  ...,  18922.0039,\n",
      "           39131.0039,   6863.0146],\n",
      "         [ 20288.0098,  -6202.0029,  10264.0273,  ...,  -1201.9883,\n",
      "          -19782.0078, -37811.9922]]]),) and output (tensor([[[-14791.0000,  15299.9990,  16129.9961,  ...,   3187.9980,\n",
      "           -2552.0020,   5626.0020],\n",
      "         [ 11460.9775,   8723.0137,   4509.9922,  ..., -23714.0117,\n",
      "           -9519.9746,  29864.0020],\n",
      "         [-12932.0195, -19513.9727, -14983.9609,  ..., -37288.0312,\n",
      "           22020.0664, -13645.9902],\n",
      "         ...,\n",
      "         [-35914.9844,  22471.9883, -12112.0000,  ...,  19219.9766,\n",
      "           33018.9844,  -4354.9678],\n",
      "         [ 19049.9883,  -9728.0020,   1174.0137,  ...,  18250.0039,\n",
      "           30198.0039,   6613.0146],\n",
      "         [ 21701.0098,  -3767.0029,  13707.0273,  ...,   3850.0117,\n",
      "          -16722.0078, -31670.9922]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[-14791.0000,  15299.9990,  16129.9961,  ...,   3187.9980,\n",
      "           -2552.0020,   5626.0020],\n",
      "         [ 11460.9775,   8723.0137,   4509.9922,  ..., -23714.0117,\n",
      "           -9519.9746,  29864.0020],\n",
      "         [-12932.0195, -19513.9727, -14983.9609,  ..., -37288.0312,\n",
      "           22020.0664, -13645.9902],\n",
      "         ...,\n",
      "         [-35914.9844,  22471.9883, -12112.0000,  ...,  19219.9766,\n",
      "           33018.9844,  -4354.9678],\n",
      "         [ 19049.9883,  -9728.0020,   1174.0137,  ...,  18250.0039,\n",
      "           30198.0039,   6613.0146],\n",
      "         [ 21701.0098,  -3767.0029,  13707.0273,  ...,   3850.0117,\n",
      "          -16722.0078, -31670.9922]]]),) and output (tensor([[[-13284.0000,  26639.0000,   7467.9961,  ...,   4113.9980,\n",
      "           -6608.0020,   5336.0020],\n",
      "         [  9773.9775,   5673.0137,   9599.9922,  ..., -24564.0117,\n",
      "           -5040.9746,  35377.0000],\n",
      "         [-12450.0195, -25159.9727, -16075.9609,  ..., -38928.0312,\n",
      "           29195.0664, -17176.9902],\n",
      "         ...,\n",
      "         [-39520.9844,  16896.9883, -13290.0000,  ...,  13282.9766,\n",
      "           26751.9844,   2054.0322],\n",
      "         [ 21468.9883, -10055.0020,   6829.0137,  ...,  13759.0039,\n",
      "           45467.0039,   6445.0146],\n",
      "         [ 32727.0098,  -4483.0029,  12455.0273,  ...,  -1003.9883,\n",
      "          -16532.0078, -35153.9922]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[-13284.0000,  26639.0000,   7467.9961,  ...,   4113.9980,\n",
      "           -6608.0020,   5336.0020],\n",
      "         [  9773.9775,   5673.0137,   9599.9922,  ..., -24564.0117,\n",
      "           -5040.9746,  35377.0000],\n",
      "         [-12450.0195, -25159.9727, -16075.9609,  ..., -38928.0312,\n",
      "           29195.0664, -17176.9902],\n",
      "         ...,\n",
      "         [-39520.9844,  16896.9883, -13290.0000,  ...,  13282.9766,\n",
      "           26751.9844,   2054.0322],\n",
      "         [ 21468.9883, -10055.0020,   6829.0137,  ...,  13759.0039,\n",
      "           45467.0039,   6445.0146],\n",
      "         [ 32727.0098,  -4483.0029,  12455.0273,  ...,  -1003.9883,\n",
      "          -16532.0078, -35153.9922]]]),) and output (tensor([[[-21532.0000,  32796.0000,  16281.9961,  ...,  18208.9980,\n",
      "           -4282.0020,   5756.0020],\n",
      "         [ 11500.9775,  -2711.9863,  12284.9922,  ..., -36244.0117,\n",
      "            -737.9746,  46439.0000],\n",
      "         [ -8681.0195, -23963.9727, -16444.9609,  ..., -34682.0312,\n",
      "           27810.0664, -16761.9902],\n",
      "         ...,\n",
      "         [-44012.9844,   9430.9883,  -7829.0000,  ...,   5970.9766,\n",
      "           35997.9844,  -7645.9678],\n",
      "         [ 15284.9883,  -5876.0020,  18104.0137,  ...,  16613.0039,\n",
      "           42304.0039,  -1162.9854],\n",
      "         [ 27647.0098,  -9666.0029,   9086.0273,  ...,  13308.0117,\n",
      "          -19253.0078, -34883.9922]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[-21532.0000,  32796.0000,  16281.9961,  ...,  18208.9980,\n",
      "           -4282.0020,   5756.0020],\n",
      "         [ 11500.9775,  -2711.9863,  12284.9922,  ..., -36244.0117,\n",
      "            -737.9746,  46439.0000],\n",
      "         [ -8681.0195, -23963.9727, -16444.9609,  ..., -34682.0312,\n",
      "           27810.0664, -16761.9902],\n",
      "         ...,\n",
      "         [-44012.9844,   9430.9883,  -7829.0000,  ...,   5970.9766,\n",
      "           35997.9844,  -7645.9678],\n",
      "         [ 15284.9883,  -5876.0020,  18104.0137,  ...,  16613.0039,\n",
      "           42304.0039,  -1162.9854],\n",
      "         [ 27647.0098,  -9666.0029,   9086.0273,  ...,  13308.0117,\n",
      "          -19253.0078, -34883.9922]]]),) and output (tensor([[[-24082.0000,  33387.0000,  15374.9961,  ...,  18359.9980,\n",
      "           -7500.0020,  11519.0020],\n",
      "         [ 14488.9775,  11514.0137,   6122.9922,  ..., -27411.0117,\n",
      "           -4019.9746,  33732.0000],\n",
      "         [-12836.0195, -15086.9727, -11061.9609,  ..., -23308.0312,\n",
      "           18615.0664, -17822.9902],\n",
      "         ...,\n",
      "         [-26865.9844,  11455.9883,  -8183.0000,  ...,   1177.9766,\n",
      "           26517.9844, -13318.9678],\n",
      "         [  5890.9883,  -1322.0020,  17148.0137,  ...,  21474.0039,\n",
      "           41850.0039,   -218.9854],\n",
      "         [ 20944.0098, -10228.0029,   2350.0273,  ...,  13398.0117,\n",
      "          -17412.0078, -34333.9922]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[-24082.0000,  33387.0000,  15374.9961,  ...,  18359.9980,\n",
      "           -7500.0020,  11519.0020],\n",
      "         [ 14488.9775,  11514.0137,   6122.9922,  ..., -27411.0117,\n",
      "           -4019.9746,  33732.0000],\n",
      "         [-12836.0195, -15086.9727, -11061.9609,  ..., -23308.0312,\n",
      "           18615.0664, -17822.9902],\n",
      "         ...,\n",
      "         [-26865.9844,  11455.9883,  -8183.0000,  ...,   1177.9766,\n",
      "           26517.9844, -13318.9678],\n",
      "         [  5890.9883,  -1322.0020,  17148.0137,  ...,  21474.0039,\n",
      "           41850.0039,   -218.9854],\n",
      "         [ 20944.0098, -10228.0029,   2350.0273,  ...,  13398.0117,\n",
      "          -17412.0078, -34333.9922]]]),) and output (tensor([[[-14869.0000,  32284.0000,   3742.9961,  ...,  12458.9980,\n",
      "           -5452.0020,   7590.0020],\n",
      "         [ 10825.9775,  11313.0137,   9898.9922,  ..., -16812.0117,\n",
      "           -7449.9746,  38094.0000],\n",
      "         [ -9054.0195, -11233.9727,   -957.9609,  ..., -31579.0312,\n",
      "           12749.0664, -24540.9902],\n",
      "         ...,\n",
      "         [-20038.9844,  27189.9883,  -8624.0000,  ...,   6365.9766,\n",
      "           17262.9844, -12302.9678],\n",
      "         [  5037.9883,   1354.9980,  20749.0137,  ...,  21344.0039,\n",
      "           43084.0039,   1463.0146],\n",
      "         [ 20425.0098, -14692.0029,    436.0273,  ...,  10179.0117,\n",
      "          -16696.0078, -29271.9922]]]),)\n",
      "[Debug] Layer names being passed: ['model.layers.0', 'model.layers.1', 'model.layers.2', 'model.layers.3', 'model.layers.4', 'model.layers.5', 'model.layers.6', 'model.layers.7', 'model.layers.8', 'model.layers.9', 'model.layers.10', 'model.layers.11', 'model.layers.12', 'model.layers.13', 'model.layers.14', 'model.layers.15', 'model.layers.16', 'model.layers.17', 'model.layers.18', 'model.layers.19', 'model.layers.20', 'model.layers.21', 'model.layers.22', 'model.layers.23', 'model.layers.24', 'model.layers.25', 'model.layers.26', 'model.layers.27', 'model.embed_tokens']\n",
      "[Debug] Trying to access layer: model.layers.0\n",
      "[Debug] Successfully found layer: model.layers.0\n",
      "[Debug] Trying to access layer: model.layers.1\n",
      "[Debug] Successfully found layer: model.layers.1\n",
      "[Debug] Trying to access layer: model.layers.2\n",
      "[Debug] Successfully found layer: model.layers.2\n",
      "[Debug] Trying to access layer: model.layers.3\n",
      "[Debug] Successfully found layer: model.layers.3\n",
      "[Debug] Trying to access layer: model.layers.4\n",
      "[Debug] Successfully found layer: model.layers.4\n",
      "[Debug] Trying to access layer: model.layers.5\n",
      "[Debug] Successfully found layer: model.layers.5\n",
      "[Debug] Trying to access layer: model.layers.6\n",
      "[Debug] Successfully found layer: model.layers.6\n",
      "[Debug] Trying to access layer: model.layers.7\n",
      "[Debug] Successfully found layer: model.layers.7\n",
      "[Debug] Trying to access layer: model.layers.8\n",
      "[Debug] Successfully found layer: model.layers.8\n",
      "[Debug] Trying to access layer: model.layers.9\n",
      "[Debug] Successfully found layer: model.layers.9\n",
      "[Debug] Trying to access layer: model.layers.10\n",
      "[Debug] Successfully found layer: model.layers.10\n",
      "[Debug] Trying to access layer: model.layers.11\n",
      "[Debug] Successfully found layer: model.layers.11\n",
      "[Debug] Trying to access layer: model.layers.12\n",
      "[Debug] Successfully found layer: model.layers.12\n",
      "[Debug] Trying to access layer: model.layers.13\n",
      "[Debug] Successfully found layer: model.layers.13\n",
      "[Debug] Trying to access layer: model.layers.14\n",
      "[Debug] Successfully found layer: model.layers.14\n",
      "[Debug] Trying to access layer: model.layers.15\n",
      "[Debug] Successfully found layer: model.layers.15\n",
      "[Debug] Trying to access layer: model.layers.16\n",
      "[Debug] Successfully found layer: model.layers.16\n",
      "[Debug] Trying to access layer: model.layers.17\n",
      "[Debug] Successfully found layer: model.layers.17\n",
      "[Debug] Trying to access layer: model.layers.18\n",
      "[Debug] Successfully found layer: model.layers.18\n",
      "[Debug] Trying to access layer: model.layers.19\n",
      "[Debug] Successfully found layer: model.layers.19\n",
      "[Debug] Trying to access layer: model.layers.20\n",
      "[Debug] Successfully found layer: model.layers.20\n",
      "[Debug] Trying to access layer: model.layers.21\n",
      "[Debug] Successfully found layer: model.layers.21\n",
      "[Debug] Trying to access layer: model.layers.22\n",
      "[Debug] Successfully found layer: model.layers.22\n",
      "[Debug] Trying to access layer: model.layers.23\n",
      "[Debug] Successfully found layer: model.layers.23\n",
      "[Debug] Trying to access layer: model.layers.24\n",
      "[Debug] Successfully found layer: model.layers.24\n",
      "[Debug] Trying to access layer: model.layers.25\n",
      "[Debug] Successfully found layer: model.layers.25\n",
      "[Debug] Trying to access layer: model.layers.26\n",
      "[Debug] Successfully found layer: model.layers.26\n",
      "[Debug] Trying to access layer: model.layers.27\n",
      "[Debug] Successfully found layer: model.layers.27\n",
      "[Debug] Trying to access layer: model.embed_tokens\n",
      "[Debug] Successfully found layer: model.embed_tokens\n",
      "[Hook] Layer Embedding(128256, 3072, padding_idx=128004) received input (tensor([[128000,   3513,  12019,    315,    279,  16642,  13944,    578,   4632,\n",
      "            596,  44682,   6376,     11,    330,   3957,    273,    409,  15274,\n",
      "          17440,     66,    295,    498,   3967,    311,   1202,  11062,    439,\n",
      "            279,  16488,  66513,     11,    574,  14948,    555,   3892,  25181,\n",
      "            323,   9678,  20543,  10977,  21699,    555,  58097,     11,  90127,\n",
      "            323,  16448,   9581,   5990,    304,  29974,    596,  10335,    265,\n",
      "           6098,    818,  60347,     11,   1455,  35146,    279,  19019,   2781,\n",
      "           3785,  55365,    409,  20263,  15274,     13,   1102,    574,  42508,\n",
      "            304,  10335,    265,   6098,    818,  60347,   6424,   9995,    797,\n",
      "            332,   8032,     20,     60]]),) and output tensor([[[-0.0010, -0.0007, -0.0046,  ..., -0.0014, -0.0020,  0.0020],\n",
      "         [ 0.0084,  0.0048,  0.0079,  ..., -0.0003, -0.0199,  0.0056],\n",
      "         [-0.0457,  0.0110, -0.0255,  ..., -0.0031, -0.0065, -0.0383],\n",
      "         ...,\n",
      "         [ 0.0145, -0.0280, -0.0145,  ...,  0.0136, -0.0054, -0.0027],\n",
      "         [ 0.0240,  0.0083,  0.0192,  ..., -0.0125, -0.0205,  0.0031],\n",
      "         [-0.0045, -0.0086,  0.0126,  ...,  0.0082,  0.0127,  0.0007]]])\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[-0.0010, -0.0007, -0.0046,  ..., -0.0014, -0.0020,  0.0020],\n",
      "         [ 0.0084,  0.0048,  0.0079,  ..., -0.0003, -0.0199,  0.0056],\n",
      "         [-0.0457,  0.0110, -0.0255,  ..., -0.0031, -0.0065, -0.0383],\n",
      "         ...,\n",
      "         [ 0.0145, -0.0280, -0.0145,  ...,  0.0136, -0.0054, -0.0027],\n",
      "         [ 0.0240,  0.0083,  0.0192,  ..., -0.0125, -0.0205,  0.0031],\n",
      "         [-0.0045, -0.0086,  0.0126,  ...,  0.0082,  0.0127,  0.0007]]]),) and output (tensor([[[ 442.9990,  239.9993,  302.9954,  ..., -154.0014,  -15.0020,\n",
      "           185.0020],\n",
      "         [ 171.0084,    1.0048,   84.0079,  ..., -121.0003,   90.9801,\n",
      "            50.0056],\n",
      "         [ 160.9543,   65.0110,  230.9745,  ..., -454.0031,  429.9935,\n",
      "           204.9617],\n",
      "         ...,\n",
      "         [ 138.0145,  114.9720, -371.0145,  ..., -176.9864, -187.0054,\n",
      "          -113.0027],\n",
      "         [  82.0240,   52.0083, -213.9808,  ...,  -43.0125,  -59.0205,\n",
      "           -32.9969],\n",
      "         [  44.9955,  146.9914, -154.9874,  ..., -171.9918,  -33.9873,\n",
      "          -127.9993]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[ 442.9990,  239.9993,  302.9954,  ..., -154.0014,  -15.0020,\n",
      "           185.0020],\n",
      "         [ 171.0084,    1.0048,   84.0079,  ..., -121.0003,   90.9801,\n",
      "            50.0056],\n",
      "         [ 160.9543,   65.0110,  230.9745,  ..., -454.0031,  429.9935,\n",
      "           204.9617],\n",
      "         ...,\n",
      "         [ 138.0145,  114.9720, -371.0145,  ..., -176.9864, -187.0054,\n",
      "          -113.0027],\n",
      "         [  82.0240,   52.0083, -213.9808,  ...,  -43.0125,  -59.0205,\n",
      "           -32.9969],\n",
      "         [  44.9955,  146.9914, -154.9874,  ..., -171.9918,  -33.9873,\n",
      "          -127.9993]]]),) and output (tensor([[[  439.9990,  -144.0007,  -410.0046,  ...,  -765.0015,\n",
      "           1193.9979, -1027.9980],\n",
      "         [  362.0084,  -273.9952,   -54.9921,  ..., -1307.0002,\n",
      "           -151.0199,   784.0056],\n",
      "         [ -617.0457,  -288.9890,   165.9745,  ..., -1089.0032,\n",
      "            755.9935,   459.9617],\n",
      "         ...,\n",
      "         [-1402.9855,   948.9720,  1414.9855,  ...,   459.0136,\n",
      "            707.9946,  -670.0027],\n",
      "         [ -828.9760,    51.0083,  -477.9808,  ...,  -918.0125,\n",
      "            229.9795,   301.0031],\n",
      "         [  495.9955,  -780.0086,  -167.9874,  ...,    44.0082,\n",
      "            -46.9873,   621.0007]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[  439.9990,  -144.0007,  -410.0046,  ...,  -765.0015,\n",
      "           1193.9979, -1027.9980],\n",
      "         [  362.0084,  -273.9952,   -54.9921,  ..., -1307.0002,\n",
      "           -151.0199,   784.0056],\n",
      "         [ -617.0457,  -288.9890,   165.9745,  ..., -1089.0032,\n",
      "            755.9935,   459.9617],\n",
      "         ...,\n",
      "         [-1402.9855,   948.9720,  1414.9855,  ...,   459.0136,\n",
      "            707.9946,  -670.0027],\n",
      "         [ -828.9760,    51.0083,  -477.9808,  ...,  -918.0125,\n",
      "            229.9795,   301.0031],\n",
      "         [  495.9955,  -780.0086,  -167.9874,  ...,    44.0082,\n",
      "            -46.9873,   621.0007]]]),) and output (tensor([[[  551.9990,   993.9993, -1790.0046,  ...,   445.9985,\n",
      "          -1386.0021, -1245.9980],\n",
      "         [-1497.9917,  2510.0049,   352.0079,  ..., -1700.0002,\n",
      "          -1179.0199, -1620.9944],\n",
      "         [   62.9543, -5307.9893,  2026.9745,  ...,  -837.0032,\n",
      "           2124.9937,  2291.9617],\n",
      "         ...,\n",
      "         [-1528.9855, -1711.0280, -1890.0145,  ...,  -206.9864,\n",
      "             32.9946,  -699.0027],\n",
      "         [ 5049.0239,  1639.0083, -1223.9808,  ...,  3736.9875,\n",
      "           1061.9795,  -286.9968],\n",
      "         [  523.9955,   211.9914,   603.0126,  ..., -1782.9917,\n",
      "           -274.9873,  1273.0007]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[  551.9990,   993.9993, -1790.0046,  ...,   445.9985,\n",
      "          -1386.0021, -1245.9980],\n",
      "         [-1497.9917,  2510.0049,   352.0079,  ..., -1700.0002,\n",
      "          -1179.0199, -1620.9944],\n",
      "         [   62.9543, -5307.9893,  2026.9745,  ...,  -837.0032,\n",
      "           2124.9937,  2291.9617],\n",
      "         ...,\n",
      "         [-1528.9855, -1711.0280, -1890.0145,  ...,  -206.9864,\n",
      "             32.9946,  -699.0027],\n",
      "         [ 5049.0239,  1639.0083, -1223.9808,  ...,  3736.9875,\n",
      "           1061.9795,  -286.9968],\n",
      "         [  523.9955,   211.9914,   603.0126,  ..., -1782.9917,\n",
      "           -274.9873,  1273.0007]]]),) and output (tensor([[[  2131.9990,   -737.0007,  -1071.0046,  ...,  -2781.0015,\n",
      "            -885.0021,     86.0020],\n",
      "         [ -8056.9917,   4013.0049,     68.0079,  ...,  -3447.0002,\n",
      "             904.9801,  -7395.9941],\n",
      "         [  3655.9543, -11824.9893,   1236.9745,  ...,  -7099.0029,\n",
      "            5117.9937,   7028.9619],\n",
      "         ...,\n",
      "         [ -1970.9855,  -3868.0278,    462.9855,  ...,  -4826.9863,\n",
      "            2210.9946,  -6389.0029],\n",
      "         [  9130.0234,   -107.9917,   -182.9808,  ...,  -3067.0125,\n",
      "            1067.9795,  -2062.9968],\n",
      "         [ -1691.0045,  -3482.0085,   2458.0127,  ...,  -2278.9917,\n",
      "           -4038.9873,  -1583.9993]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[  2131.9990,   -737.0007,  -1071.0046,  ...,  -2781.0015,\n",
      "            -885.0021,     86.0020],\n",
      "         [ -8056.9917,   4013.0049,     68.0079,  ...,  -3447.0002,\n",
      "             904.9801,  -7395.9941],\n",
      "         [  3655.9543, -11824.9893,   1236.9745,  ...,  -7099.0029,\n",
      "            5117.9937,   7028.9619],\n",
      "         ...,\n",
      "         [ -1970.9855,  -3868.0278,    462.9855,  ...,  -4826.9863,\n",
      "            2210.9946,  -6389.0029],\n",
      "         [  9130.0234,   -107.9917,   -182.9808,  ...,  -3067.0125,\n",
      "            1067.9795,  -2062.9968],\n",
      "         [ -1691.0045,  -3482.0085,   2458.0127,  ...,  -2278.9917,\n",
      "           -4038.9873,  -1583.9993]]]),) and output (tensor([[[  3420.9990,    760.9993,  -5376.0049,  ...,  -4193.0015,\n",
      "            2860.9980,  -4080.9980],\n",
      "         [ -6403.9917,  -3593.9951,  -3886.9922,  ...,  -1701.0002,\n",
      "             646.9801,  -9393.9941],\n",
      "         [  -189.0457, -19370.9883,   1559.9745,  ...,  -8538.0029,\n",
      "           10585.9941,  12762.9619],\n",
      "         ...,\n",
      "         [   366.0145,  -3799.0278,   1398.9855,  ...,  -4003.9863,\n",
      "            1511.9946,  -4779.0029],\n",
      "         [ 22537.0234,  -2390.9917,  -3026.9810,  ...,  -6118.0127,\n",
      "            2285.9795,   4496.0029],\n",
      "         [ -3528.0044,  -5411.0088,   2763.0127,  ...,  -5763.9917,\n",
      "           -4429.9873,   2332.0007]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[  3420.9990,    760.9993,  -5376.0049,  ...,  -4193.0015,\n",
      "            2860.9980,  -4080.9980],\n",
      "         [ -6403.9917,  -3593.9951,  -3886.9922,  ...,  -1701.0002,\n",
      "             646.9801,  -9393.9941],\n",
      "         [  -189.0457, -19370.9883,   1559.9745,  ...,  -8538.0029,\n",
      "           10585.9941,  12762.9619],\n",
      "         ...,\n",
      "         [   366.0145,  -3799.0278,   1398.9855,  ...,  -4003.9863,\n",
      "            1511.9946,  -4779.0029],\n",
      "         [ 22537.0234,  -2390.9917,  -3026.9810,  ...,  -6118.0127,\n",
      "            2285.9795,   4496.0029],\n",
      "         [ -3528.0044,  -5411.0088,   2763.0127,  ...,  -5763.9917,\n",
      "           -4429.9873,   2332.0007]]]),) and output (tensor([[[  2742.9990,  -1512.0007, -10493.0049,  ...,  -9945.0020,\n",
      "           -2017.0020,  -1802.9980],\n",
      "         [  2852.0083,  -7325.9951,  -4867.9922,  ...,   -510.0002,\n",
      "            -598.0199, -15251.9941],\n",
      "         [   715.9543, -30702.9883,   5439.9746,  ..., -13968.0029,\n",
      "            7790.9941,  21467.9609],\n",
      "         ...,\n",
      "         [ -1080.9855,   2117.9722,   -655.0145,  ...,  -2901.9863,\n",
      "            1821.9946,  -1152.0029],\n",
      "         [ 26809.0234,    600.0083,  -6621.9810,  ..., -11914.0127,\n",
      "            -568.0205,   5304.0029],\n",
      "         [-11505.0039,  -2155.0088,  -1937.9873,  ...,  -2665.9917,\n",
      "           -5640.9873,   6093.0010]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[  2742.9990,  -1512.0007, -10493.0049,  ...,  -9945.0020,\n",
      "           -2017.0020,  -1802.9980],\n",
      "         [  2852.0083,  -7325.9951,  -4867.9922,  ...,   -510.0002,\n",
      "            -598.0199, -15251.9941],\n",
      "         [   715.9543, -30702.9883,   5439.9746,  ..., -13968.0029,\n",
      "            7790.9941,  21467.9609],\n",
      "         ...,\n",
      "         [ -1080.9855,   2117.9722,   -655.0145,  ...,  -2901.9863,\n",
      "            1821.9946,  -1152.0029],\n",
      "         [ 26809.0234,    600.0083,  -6621.9810,  ..., -11914.0127,\n",
      "            -568.0205,   5304.0029],\n",
      "         [-11505.0039,  -2155.0088,  -1937.9873,  ...,  -2665.9917,\n",
      "           -5640.9873,   6093.0010]]]),) and output (tensor([[[  3032.9990,  -3420.0007, -10291.0049,  ...,  -6511.0020,\n",
      "           -1953.0020,   2001.0020],\n",
      "         [  9672.0078,  -7809.9951,  -7298.9922,  ...,   1595.9998,\n",
      "             957.9801, -24773.9941],\n",
      "         [  1632.9543, -32355.9883,    693.9746,  ..., -14104.0029,\n",
      "           -2606.0059,  21009.9609],\n",
      "         ...,\n",
      "         [ -1642.9855,   3291.9722,   -964.0145,  ...,  -3016.9863,\n",
      "           -7385.0054,  -2043.0029],\n",
      "         [ 31557.0234,  -1815.9917,  -8651.9805,  ..., -12577.0127,\n",
      "             689.9795,   3513.0029],\n",
      "         [-10495.0039,   -527.0088,  -3312.9873,  ...,   1596.0083,\n",
      "           -4074.9873,   7785.0010]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[  3032.9990,  -3420.0007, -10291.0049,  ...,  -6511.0020,\n",
      "           -1953.0020,   2001.0020],\n",
      "         [  9672.0078,  -7809.9951,  -7298.9922,  ...,   1595.9998,\n",
      "             957.9801, -24773.9941],\n",
      "         [  1632.9543, -32355.9883,    693.9746,  ..., -14104.0029,\n",
      "           -2606.0059,  21009.9609],\n",
      "         ...,\n",
      "         [ -1642.9855,   3291.9722,   -964.0145,  ...,  -3016.9863,\n",
      "           -7385.0054,  -2043.0029],\n",
      "         [ 31557.0234,  -1815.9917,  -8651.9805,  ..., -12577.0127,\n",
      "             689.9795,   3513.0029],\n",
      "         [-10495.0039,   -527.0088,  -3312.9873,  ...,   1596.0083,\n",
      "           -4074.9873,   7785.0010]]]),) and output (tensor([[[ 3.1330e+03, -1.9700e+03, -1.1544e+04,  ..., -5.7800e+03,\n",
      "          -2.4860e+03, -1.9220e+03],\n",
      "         [ 5.4150e+03, -1.3784e+04, -9.1810e+03,  ..., -1.5000e+01,\n",
      "           5.8500e+03, -2.8819e+04],\n",
      "         [-5.4570e+03, -3.7198e+04,  3.9590e+03,  ..., -4.3210e+03,\n",
      "          -8.9101e+02,  2.4898e+04],\n",
      "         ...,\n",
      "         [-6.0160e+03,  7.0197e+02, -1.5270e+03,  ...,  3.2001e+02,\n",
      "          -7.3420e+03,  1.3100e+03],\n",
      "         [ 3.5113e+04,  1.6340e+03, -8.0330e+03,  ..., -1.8780e+04,\n",
      "           6.8790e+03,  2.7060e+03],\n",
      "         [-1.2581e+04, -1.3880e+03, -2.6410e+03,  ...,  1.4980e+03,\n",
      "          -2.5200e+03,  1.3823e+04]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[ 3.1330e+03, -1.9700e+03, -1.1544e+04,  ..., -5.7800e+03,\n",
      "          -2.4860e+03, -1.9220e+03],\n",
      "         [ 5.4150e+03, -1.3784e+04, -9.1810e+03,  ..., -1.5000e+01,\n",
      "           5.8500e+03, -2.8819e+04],\n",
      "         [-5.4570e+03, -3.7198e+04,  3.9590e+03,  ..., -4.3210e+03,\n",
      "          -8.9101e+02,  2.4898e+04],\n",
      "         ...,\n",
      "         [-6.0160e+03,  7.0197e+02, -1.5270e+03,  ...,  3.2001e+02,\n",
      "          -7.3420e+03,  1.3100e+03],\n",
      "         [ 3.5113e+04,  1.6340e+03, -8.0330e+03,  ..., -1.8780e+04,\n",
      "           6.8790e+03,  2.7060e+03],\n",
      "         [-1.2581e+04, -1.3880e+03, -2.6410e+03,  ...,  1.4980e+03,\n",
      "          -2.5200e+03,  1.3823e+04]]]),) and output (tensor([[[  1046.9990,   2403.9993, -14486.0049,  ..., -10922.0020,\n",
      "           -1419.0020,  -2983.9980],\n",
      "         [  2380.0078, -14676.9951, -10035.9922,  ...,  -6276.0000,\n",
      "            7950.9800, -24247.9941],\n",
      "         [ -8153.0459, -47733.9883,  -1972.0254,  ...,  -3190.0029,\n",
      "            1208.9941,  27017.9609],\n",
      "         ...,\n",
      "         [ -5743.9854,   3435.9722,  -1988.0145,  ...,  -3077.9863,\n",
      "           -9959.0059,  -3442.0029],\n",
      "         [ 39563.0234,  -2891.9917,  -7764.9805,  ..., -22333.0117,\n",
      "           -1861.0205,   1979.0029],\n",
      "         [-11039.0039,  -2411.0088,  -4297.9873,  ...,    508.0083,\n",
      "           -1613.9873,  19990.0000]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[  1046.9990,   2403.9993, -14486.0049,  ..., -10922.0020,\n",
      "           -1419.0020,  -2983.9980],\n",
      "         [  2380.0078, -14676.9951, -10035.9922,  ...,  -6276.0000,\n",
      "            7950.9800, -24247.9941],\n",
      "         [ -8153.0459, -47733.9883,  -1972.0254,  ...,  -3190.0029,\n",
      "            1208.9941,  27017.9609],\n",
      "         ...,\n",
      "         [ -5743.9854,   3435.9722,  -1988.0145,  ...,  -3077.9863,\n",
      "           -9959.0059,  -3442.0029],\n",
      "         [ 39563.0234,  -2891.9917,  -7764.9805,  ..., -22333.0117,\n",
      "           -1861.0205,   1979.0029],\n",
      "         [-11039.0039,  -2411.0088,  -4297.9873,  ...,    508.0083,\n",
      "           -1613.9873,  19990.0000]]]),) and output (tensor([[[ -3767.0010,   5307.9990, -14221.0049,  ..., -15035.0020,\n",
      "           -3630.0020,   4168.0020],\n",
      "         [  1800.0078, -17411.9961,  -8172.9922,  ...,  -7775.0000,\n",
      "            8627.9805, -25465.9941],\n",
      "         [ -6167.0459, -55425.9883,  -4020.0254,  ...,  -6568.0029,\n",
      "           -1770.0059,  33934.9609],\n",
      "         ...,\n",
      "         [ -9608.9854,   7685.9722,  -3920.0146,  ...,  -4117.9863,\n",
      "          -11258.0059,    498.9971],\n",
      "         [ 43254.0234, -10878.9922,  -2188.9805,  ..., -18616.0117,\n",
      "            5738.9795,   1314.0029],\n",
      "         [-10054.0039,  -6693.0088,  -7372.9873,  ...,  -4996.9917,\n",
      "           -3428.9873,  21002.0000]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[ -3767.0010,   5307.9990, -14221.0049,  ..., -15035.0020,\n",
      "           -3630.0020,   4168.0020],\n",
      "         [  1800.0078, -17411.9961,  -8172.9922,  ...,  -7775.0000,\n",
      "            8627.9805, -25465.9941],\n",
      "         [ -6167.0459, -55425.9883,  -4020.0254,  ...,  -6568.0029,\n",
      "           -1770.0059,  33934.9609],\n",
      "         ...,\n",
      "         [ -9608.9854,   7685.9722,  -3920.0146,  ...,  -4117.9863,\n",
      "          -11258.0059,    498.9971],\n",
      "         [ 43254.0234, -10878.9922,  -2188.9805,  ..., -18616.0117,\n",
      "            5738.9795,   1314.0029],\n",
      "         [-10054.0039,  -6693.0088,  -7372.9873,  ...,  -4996.9917,\n",
      "           -3428.9873,  21002.0000]]]),) and output (tensor([[[ -1326.0010,   1223.9990, -16643.0039,  ..., -13240.0020,\n",
      "           -5677.0020,   9639.0020],\n",
      "         [  -555.9922, -24943.9961, -10545.9922,  ...,  -4717.0000,\n",
      "            5277.9805, -18184.9941],\n",
      "         [ -4512.0459, -66007.9844,  -2811.0254,  ..., -12662.0029,\n",
      "            1487.9941,  39958.9609],\n",
      "         ...,\n",
      "         [ -6127.9854,  12685.9727,  -6795.0146,  ...,  -5212.9863,\n",
      "          -11672.0059,  -6289.0029],\n",
      "         [ 51997.0234,  -9448.9922,  -1649.9805,  ..., -19671.0117,\n",
      "            9122.9795,  -1765.9971],\n",
      "         [-10799.0039,  -6184.0088,  -6680.9873,  ...,  -7540.9917,\n",
      "            1924.0127,  23389.0000]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[ -1326.0010,   1223.9990, -16643.0039,  ..., -13240.0020,\n",
      "           -5677.0020,   9639.0020],\n",
      "         [  -555.9922, -24943.9961, -10545.9922,  ...,  -4717.0000,\n",
      "            5277.9805, -18184.9941],\n",
      "         [ -4512.0459, -66007.9844,  -2811.0254,  ..., -12662.0029,\n",
      "            1487.9941,  39958.9609],\n",
      "         ...,\n",
      "         [ -6127.9854,  12685.9727,  -6795.0146,  ...,  -5212.9863,\n",
      "          -11672.0059,  -6289.0029],\n",
      "         [ 51997.0234,  -9448.9922,  -1649.9805,  ..., -19671.0117,\n",
      "            9122.9795,  -1765.9971],\n",
      "         [-10799.0039,  -6184.0088,  -6680.9873,  ...,  -7540.9917,\n",
      "            1924.0127,  23389.0000]]]),) and output (tensor([[[ -5120.0010,   2246.9990, -15993.0039,  ..., -19818.0020,\n",
      "           -9385.0020,  11924.0020],\n",
      "         [  1917.0078, -27692.9961, -11207.9922,  ...,  -5022.0000,\n",
      "            5118.9805, -16664.9941],\n",
      "         [-11042.0459, -72832.9844,   1843.9746,  ..., -12288.0029,\n",
      "             882.9941,  38796.9609],\n",
      "         ...,\n",
      "         [  -532.9854,  12619.9727,  -6769.0146,  ...,    195.0137,\n",
      "          -15033.0059, -12306.0029],\n",
      "         [ 55875.0234,  -6176.9922,   -760.9805,  ..., -16182.0117,\n",
      "            7148.9795,  -1491.9971],\n",
      "         [-11957.0039, -10333.0088,  -8171.9873,  ...,  -7461.9917,\n",
      "            1485.0127,  20685.0000]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[ -5120.0010,   2246.9990, -15993.0039,  ..., -19818.0020,\n",
      "           -9385.0020,  11924.0020],\n",
      "         [  1917.0078, -27692.9961, -11207.9922,  ...,  -5022.0000,\n",
      "            5118.9805, -16664.9941],\n",
      "         [-11042.0459, -72832.9844,   1843.9746,  ..., -12288.0029,\n",
      "             882.9941,  38796.9609],\n",
      "         ...,\n",
      "         [  -532.9854,  12619.9727,  -6769.0146,  ...,    195.0137,\n",
      "          -15033.0059, -12306.0029],\n",
      "         [ 55875.0234,  -6176.9922,   -760.9805,  ..., -16182.0117,\n",
      "            7148.9795,  -1491.9971],\n",
      "         [-11957.0039, -10333.0088,  -8171.9873,  ...,  -7461.9917,\n",
      "            1485.0127,  20685.0000]]]),) and output (tensor([[[ -6685.0010,  -1278.0010, -15553.0039,  ..., -19846.0020,\n",
      "          -10418.0020,  13376.0020],\n",
      "         [ -1193.9922, -34994.9961,  -6808.9922,  ...,  -3046.0000,\n",
      "            4466.9805, -13257.9941],\n",
      "         [ -7287.0459, -87891.9844,   1416.9746,  ...,  -8411.0029,\n",
      "            1150.9941,  38277.9609],\n",
      "         ...,\n",
      "         [ -6683.9854,  10182.9727,  -9501.0146,  ...,  -4786.9863,\n",
      "          -16029.0059, -15174.0029],\n",
      "         [ 55009.0234,  -4667.9922,  -1005.9805,  ..., -12990.0117,\n",
      "            4345.9795,   1500.0029],\n",
      "         [ -9396.0039, -13269.0088,  -7663.9873,  ...,  -6379.9917,\n",
      "           -1297.9873,  27018.0000]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[ -6685.0010,  -1278.0010, -15553.0039,  ..., -19846.0020,\n",
      "          -10418.0020,  13376.0020],\n",
      "         [ -1193.9922, -34994.9961,  -6808.9922,  ...,  -3046.0000,\n",
      "            4466.9805, -13257.9941],\n",
      "         [ -7287.0459, -87891.9844,   1416.9746,  ...,  -8411.0029,\n",
      "            1150.9941,  38277.9609],\n",
      "         ...,\n",
      "         [ -6683.9854,  10182.9727,  -9501.0146,  ...,  -4786.9863,\n",
      "          -16029.0059, -15174.0029],\n",
      "         [ 55009.0234,  -4667.9922,  -1005.9805,  ..., -12990.0117,\n",
      "            4345.9795,   1500.0029],\n",
      "         [ -9396.0039, -13269.0088,  -7663.9873,  ...,  -6379.9917,\n",
      "           -1297.9873,  27018.0000]]]),) and output (tensor([[[  -8422.0010,     339.9990,  -15289.0039,  ...,  -20884.0020,\n",
      "            -6908.0020,   11014.0020],\n",
      "         [   -562.9922,  -38993.9961,   -4962.9922,  ...,     572.0000,\n",
      "             3769.9805,  -15556.9941],\n",
      "         [ -13569.0459, -103569.9844,    2132.9746,  ...,   -8455.0029,\n",
      "             3028.9941,   41108.9609],\n",
      "         ...,\n",
      "         [ -10571.9854,   11878.9727,  -12308.0146,  ...,   -1909.9863,\n",
      "           -21673.0059,  -23021.0039],\n",
      "         [  63373.0234,   -1152.9922,   -4485.9805,  ...,  -10611.0117,\n",
      "             4901.9795,     937.0029],\n",
      "         [  -6499.0039,   -9800.0088,   -8466.9873,  ...,  -10382.9922,\n",
      "              856.0127,   21256.0000]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[  -8422.0010,     339.9990,  -15289.0039,  ...,  -20884.0020,\n",
      "            -6908.0020,   11014.0020],\n",
      "         [   -562.9922,  -38993.9961,   -4962.9922,  ...,     572.0000,\n",
      "             3769.9805,  -15556.9941],\n",
      "         [ -13569.0459, -103569.9844,    2132.9746,  ...,   -8455.0029,\n",
      "             3028.9941,   41108.9609],\n",
      "         ...,\n",
      "         [ -10571.9854,   11878.9727,  -12308.0146,  ...,   -1909.9863,\n",
      "           -21673.0059,  -23021.0039],\n",
      "         [  63373.0234,   -1152.9922,   -4485.9805,  ...,  -10611.0117,\n",
      "             4901.9795,     937.0029],\n",
      "         [  -6499.0039,   -9800.0088,   -8466.9873,  ...,  -10382.9922,\n",
      "              856.0127,   21256.0000]]]),) and output (tensor([[[-1.0989e+04,  7.0350e+03, -1.3964e+04,  ..., -2.3968e+04,\n",
      "          -3.7490e+03,  1.1730e+04],\n",
      "         [-1.7599e+02, -4.0262e+04, -7.6040e+03,  ..., -2.2060e+03,\n",
      "          -5.9610e+03, -2.1108e+04],\n",
      "         [-1.4405e+04, -1.1528e+05,  3.4430e+03,  ..., -7.5010e+03,\n",
      "           3.0420e+03,  4.1454e+04],\n",
      "         ...,\n",
      "         [-6.9550e+03,  1.2886e+04, -1.6196e+04,  ...,  1.2230e+03,\n",
      "          -2.4072e+04, -2.5658e+04],\n",
      "         [ 6.8212e+04, -1.4799e+02, -1.0860e+03,  ..., -6.9290e+03,\n",
      "           1.5940e+03, -7.0997e+01],\n",
      "         [-2.7580e+03, -7.4420e+03, -7.9380e+03,  ..., -7.4070e+03,\n",
      "          -3.6430e+03,  2.3717e+04]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[-1.0989e+04,  7.0350e+03, -1.3964e+04,  ..., -2.3968e+04,\n",
      "          -3.7490e+03,  1.1730e+04],\n",
      "         [-1.7599e+02, -4.0262e+04, -7.6040e+03,  ..., -2.2060e+03,\n",
      "          -5.9610e+03, -2.1108e+04],\n",
      "         [-1.4405e+04, -1.1528e+05,  3.4430e+03,  ..., -7.5010e+03,\n",
      "           3.0420e+03,  4.1454e+04],\n",
      "         ...,\n",
      "         [-6.9550e+03,  1.2886e+04, -1.6196e+04,  ...,  1.2230e+03,\n",
      "          -2.4072e+04, -2.5658e+04],\n",
      "         [ 6.8212e+04, -1.4799e+02, -1.0860e+03,  ..., -6.9290e+03,\n",
      "           1.5940e+03, -7.0997e+01],\n",
      "         [-2.7580e+03, -7.4420e+03, -7.9380e+03,  ..., -7.4070e+03,\n",
      "          -3.6430e+03,  2.3717e+04]]]),) and output (tensor([[[  -7159.0010,    6621.9990,  -10967.0039,  ...,  -17477.0020,\n",
      "            -2510.0020,   11888.0020],\n",
      "         [  -1909.9922,  -48481.9961,  -11608.9922,  ...,   -1863.0000,\n",
      "            -3446.0195,  -23710.9941],\n",
      "         [ -11478.0459, -118230.9844,    2575.9746,  ...,  -12445.0029,\n",
      "              336.9941,   46589.9609],\n",
      "         ...,\n",
      "         [  -3021.9854,   12825.9727,  -14950.0146,  ...,     295.0137,\n",
      "           -29367.0059,  -29483.0039],\n",
      "         [  73731.0234,   -6510.9922,    4806.0195,  ...,  -11948.0117,\n",
      "              309.9795,    1059.0029],\n",
      "         [  -7149.0039,   -8431.0088,   -7928.9873,  ...,   -3625.9922,\n",
      "            -5602.9873,   17202.0000]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[  -7159.0010,    6621.9990,  -10967.0039,  ...,  -17477.0020,\n",
      "            -2510.0020,   11888.0020],\n",
      "         [  -1909.9922,  -48481.9961,  -11608.9922,  ...,   -1863.0000,\n",
      "            -3446.0195,  -23710.9941],\n",
      "         [ -11478.0459, -118230.9844,    2575.9746,  ...,  -12445.0029,\n",
      "              336.9941,   46589.9609],\n",
      "         ...,\n",
      "         [  -3021.9854,   12825.9727,  -14950.0146,  ...,     295.0137,\n",
      "           -29367.0059,  -29483.0039],\n",
      "         [  73731.0234,   -6510.9922,    4806.0195,  ...,  -11948.0117,\n",
      "              309.9795,    1059.0029],\n",
      "         [  -7149.0039,   -8431.0088,   -7928.9873,  ...,   -3625.9922,\n",
      "            -5602.9873,   17202.0000]]]),) and output (tensor([[[  -4822.0010,    3124.9990,   -3885.0039,  ...,  -19007.0020,\n",
      "             -551.0020,   16285.0020],\n",
      "         [    753.0078,  -48829.9961,  -10954.9922,  ...,     337.0000,\n",
      "             1577.9805,  -26795.9941],\n",
      "         [ -13984.0459, -131511.9844,    7003.9746,  ...,  -14658.0029,\n",
      "              888.9941,   55087.9609],\n",
      "         ...,\n",
      "         [  -6180.9854,   16126.9727,  -16920.0156,  ...,   -4578.9863,\n",
      "           -36187.0078,  -31501.0039],\n",
      "         [  82911.0234,  -15533.9922,    7705.0195,  ...,  -12300.0117,\n",
      "             3394.9795,    6813.0029],\n",
      "         [  -9245.0039,  -13596.0088,   -5936.9873,  ...,    -174.9922,\n",
      "            -9164.9873,   18406.0000]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[  -4822.0010,    3124.9990,   -3885.0039,  ...,  -19007.0020,\n",
      "             -551.0020,   16285.0020],\n",
      "         [    753.0078,  -48829.9961,  -10954.9922,  ...,     337.0000,\n",
      "             1577.9805,  -26795.9941],\n",
      "         [ -13984.0459, -131511.9844,    7003.9746,  ...,  -14658.0029,\n",
      "              888.9941,   55087.9609],\n",
      "         ...,\n",
      "         [  -6180.9854,   16126.9727,  -16920.0156,  ...,   -4578.9863,\n",
      "           -36187.0078,  -31501.0039],\n",
      "         [  82911.0234,  -15533.9922,    7705.0195,  ...,  -12300.0117,\n",
      "             3394.9795,    6813.0029],\n",
      "         [  -9245.0039,  -13596.0088,   -5936.9873,  ...,    -174.9922,\n",
      "            -9164.9873,   18406.0000]]]),) and output (tensor([[[-7.2350e+03,  6.5840e+03, -4.2120e+03,  ..., -1.9104e+04,\n",
      "           3.0880e+03,  1.7084e+04],\n",
      "         [ 7.8008e+01, -4.1993e+04, -6.5940e+03,  ..., -2.3960e+03,\n",
      "           1.1530e+03, -2.2870e+04],\n",
      "         [-5.5360e+03, -1.3676e+05,  4.3030e+03,  ..., -2.1177e+04,\n",
      "          -1.2120e+03,  5.8051e+04],\n",
      "         ...,\n",
      "         [-1.2339e+04,  1.9877e+04, -1.1600e+04,  ..., -6.9330e+03,\n",
      "          -3.8047e+04, -4.1446e+04],\n",
      "         [ 8.8927e+04, -1.4114e+04,  4.7170e+03,  ..., -1.3966e+04,\n",
      "           1.1309e+04,  5.9690e+03],\n",
      "         [-9.4490e+03, -1.8221e+04, -6.0320e+03,  ..., -8.7699e+02,\n",
      "          -9.2980e+03,  2.1535e+04]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[-7.2350e+03,  6.5840e+03, -4.2120e+03,  ..., -1.9104e+04,\n",
      "           3.0880e+03,  1.7084e+04],\n",
      "         [ 7.8008e+01, -4.1993e+04, -6.5940e+03,  ..., -2.3960e+03,\n",
      "           1.1530e+03, -2.2870e+04],\n",
      "         [-5.5360e+03, -1.3676e+05,  4.3030e+03,  ..., -2.1177e+04,\n",
      "          -1.2120e+03,  5.8051e+04],\n",
      "         ...,\n",
      "         [-1.2339e+04,  1.9877e+04, -1.1600e+04,  ..., -6.9330e+03,\n",
      "          -3.8047e+04, -4.1446e+04],\n",
      "         [ 8.8927e+04, -1.4114e+04,  4.7170e+03,  ..., -1.3966e+04,\n",
      "           1.1309e+04,  5.9690e+03],\n",
      "         [-9.4490e+03, -1.8221e+04, -6.0320e+03,  ..., -8.7699e+02,\n",
      "          -9.2980e+03,  2.1535e+04]]]),) and output (tensor([[[  -5586.0010,    7645.9990,    -505.0039,  ...,  -15346.0020,\n",
      "             -623.0020,   12986.0020],\n",
      "         [    993.0078,  -47705.9961,   -6923.9922,  ...,    1261.0000,\n",
      "             1979.9805,  -25003.9941],\n",
      "         [  -5611.0459, -133864.9844,    1411.9746,  ...,  -24507.0039,\n",
      "            -1617.0059,   63886.9609],\n",
      "         ...,\n",
      "         [  -7829.9854,   14883.9727,  -13723.0156,  ...,   -7635.9863,\n",
      "           -39171.0078,  -47415.0039],\n",
      "         [  95457.0234,  -16872.9922,    6554.0195,  ...,  -17152.0117,\n",
      "            10341.9795,    1502.0029],\n",
      "         [ -11592.0039,  -14577.0078,   -5287.9873,  ...,    4641.0078,\n",
      "            -4435.9873,   22467.0000]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[  -5586.0010,    7645.9990,    -505.0039,  ...,  -15346.0020,\n",
      "             -623.0020,   12986.0020],\n",
      "         [    993.0078,  -47705.9961,   -6923.9922,  ...,    1261.0000,\n",
      "             1979.9805,  -25003.9941],\n",
      "         [  -5611.0459, -133864.9844,    1411.9746,  ...,  -24507.0039,\n",
      "            -1617.0059,   63886.9609],\n",
      "         ...,\n",
      "         [  -7829.9854,   14883.9727,  -13723.0156,  ...,   -7635.9863,\n",
      "           -39171.0078,  -47415.0039],\n",
      "         [  95457.0234,  -16872.9922,    6554.0195,  ...,  -17152.0117,\n",
      "            10341.9795,    1502.0029],\n",
      "         [ -11592.0039,  -14577.0078,   -5287.9873,  ...,    4641.0078,\n",
      "            -4435.9873,   22467.0000]]]),) and output (tensor([[[  -8916.0010,    2362.9990,   -4833.0039,  ...,  -12075.0020,\n",
      "            -1124.0020,   12438.0020],\n",
      "         [   -570.9922,  -56677.9961,     840.0078,  ...,    2053.0000,\n",
      "             -555.0195,  -27424.9941],\n",
      "         [   3817.9541, -141455.9844,    1955.9746,  ...,  -31223.0039,\n",
      "             2338.9941,   61195.9609],\n",
      "         ...,\n",
      "         [ -12075.9854,   14232.9727,  -12003.0156,  ...,    -659.9863,\n",
      "           -44777.0078,  -49950.0039],\n",
      "         [ 106388.0234,  -17958.9922,    9139.0195,  ...,  -16381.0117,\n",
      "             8501.9795,    1694.0029],\n",
      "         [ -15566.0039,  -12448.0078,   -3830.9873,  ...,    3985.0078,\n",
      "            -6114.9873,   23556.0000]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[  -8916.0010,    2362.9990,   -4833.0039,  ...,  -12075.0020,\n",
      "            -1124.0020,   12438.0020],\n",
      "         [   -570.9922,  -56677.9961,     840.0078,  ...,    2053.0000,\n",
      "             -555.0195,  -27424.9941],\n",
      "         [   3817.9541, -141455.9844,    1955.9746,  ...,  -31223.0039,\n",
      "             2338.9941,   61195.9609],\n",
      "         ...,\n",
      "         [ -12075.9854,   14232.9727,  -12003.0156,  ...,    -659.9863,\n",
      "           -44777.0078,  -49950.0039],\n",
      "         [ 106388.0234,  -17958.9922,    9139.0195,  ...,  -16381.0117,\n",
      "             8501.9795,    1694.0029],\n",
      "         [ -15566.0039,  -12448.0078,   -3830.9873,  ...,    3985.0078,\n",
      "            -6114.9873,   23556.0000]]]),) and output (tensor([[[ -20230.0000,    7877.9990,     161.9961,  ...,  -10250.0020,\n",
      "              676.9980,    7095.0020],\n",
      "         [  -6002.9922,  -56215.9961,    -693.9922,  ...,   -2204.0000,\n",
      "             4647.9805,  -39246.9922],\n",
      "         [  -2028.0459, -149027.9844,    2860.9746,  ...,  -35436.0039,\n",
      "             7888.9941,   56771.9609],\n",
      "         ...,\n",
      "         [ -13715.9854,   13492.9727,  -15694.0156,  ...,    1120.0137,\n",
      "           -53038.0078,  -54205.0039],\n",
      "         [ 102579.0234,  -16793.9922,    3772.0195,  ...,  -21042.0117,\n",
      "             3668.9795,    1345.0029],\n",
      "         [ -14783.0039,  -13537.0078,   -5331.9873,  ...,    4070.0078,\n",
      "            -5747.9873,   28241.0000]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[ -20230.0000,    7877.9990,     161.9961,  ...,  -10250.0020,\n",
      "              676.9980,    7095.0020],\n",
      "         [  -6002.9922,  -56215.9961,    -693.9922,  ...,   -2204.0000,\n",
      "             4647.9805,  -39246.9922],\n",
      "         [  -2028.0459, -149027.9844,    2860.9746,  ...,  -35436.0039,\n",
      "             7888.9941,   56771.9609],\n",
      "         ...,\n",
      "         [ -13715.9854,   13492.9727,  -15694.0156,  ...,    1120.0137,\n",
      "           -53038.0078,  -54205.0039],\n",
      "         [ 102579.0234,  -16793.9922,    3772.0195,  ...,  -21042.0117,\n",
      "             3668.9795,    1345.0029],\n",
      "         [ -14783.0039,  -13537.0078,   -5331.9873,  ...,    4070.0078,\n",
      "            -5747.9873,   28241.0000]]]),) and output (tensor([[[ -25670.0000,   13293.9990,    9099.9961,  ...,   -6547.0020,\n",
      "            -5658.0020,    4192.0020],\n",
      "         [  -5423.9922,  -65970.0000,    2838.0078,  ...,   -4899.0000,\n",
      "             1832.9805,  -45713.9922],\n",
      "         [  -8664.0459, -150666.9844,   12218.9746,  ...,  -35349.0039,\n",
      "             3369.9941,   62778.9609],\n",
      "         ...,\n",
      "         [ -16586.9844,   19067.9727,  -16246.0156,  ...,   -9470.9863,\n",
      "           -53187.0078,  -55888.0039],\n",
      "         [ 106205.0234,   -7840.9922,   -3791.9805,  ...,  -22931.0117,\n",
      "             4809.9795,   -3388.9971],\n",
      "         [ -12330.0039,  -10912.0078,   -5934.9873,  ...,   -3517.9922,\n",
      "            -5237.9873,   35941.0000]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[ -25670.0000,   13293.9990,    9099.9961,  ...,   -6547.0020,\n",
      "            -5658.0020,    4192.0020],\n",
      "         [  -5423.9922,  -65970.0000,    2838.0078,  ...,   -4899.0000,\n",
      "             1832.9805,  -45713.9922],\n",
      "         [  -8664.0459, -150666.9844,   12218.9746,  ...,  -35349.0039,\n",
      "             3369.9941,   62778.9609],\n",
      "         ...,\n",
      "         [ -16586.9844,   19067.9727,  -16246.0156,  ...,   -9470.9863,\n",
      "           -53187.0078,  -55888.0039],\n",
      "         [ 106205.0234,   -7840.9922,   -3791.9805,  ...,  -22931.0117,\n",
      "             4809.9795,   -3388.9971],\n",
      "         [ -12330.0039,  -10912.0078,   -5934.9873,  ...,   -3517.9922,\n",
      "            -5237.9873,   35941.0000]]]),) and output (tensor([[[ -25652.0000,   13180.9990,   15728.9961,  ...,   -2831.0020,\n",
      "              158.9980,    2204.0020],\n",
      "         [ -11700.9922,  -69036.0000,    3018.0078,  ...,   -4105.0000,\n",
      "             -178.0195,  -50441.9922],\n",
      "         [  -7018.0459, -146392.9844,   11906.9746,  ...,  -29429.0039,\n",
      "             2365.9941,   65382.9609],\n",
      "         ...,\n",
      "         [ -10142.9844,   25755.9727,  -11021.0156,  ...,  -19325.9863,\n",
      "           -45859.0078,  -62170.0039],\n",
      "         [ 105761.0234,   -9910.9922,   -2121.9805,  ...,  -20719.0117,\n",
      "              957.9795,   -9045.9971],\n",
      "         [ -16503.0039,   -7104.0078,   -7950.9873,  ...,    2537.0078,\n",
      "            -3360.9873,   35734.0000]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[ -25652.0000,   13180.9990,   15728.9961,  ...,   -2831.0020,\n",
      "              158.9980,    2204.0020],\n",
      "         [ -11700.9922,  -69036.0000,    3018.0078,  ...,   -4105.0000,\n",
      "             -178.0195,  -50441.9922],\n",
      "         [  -7018.0459, -146392.9844,   11906.9746,  ...,  -29429.0039,\n",
      "             2365.9941,   65382.9609],\n",
      "         ...,\n",
      "         [ -10142.9844,   25755.9727,  -11021.0156,  ...,  -19325.9863,\n",
      "           -45859.0078,  -62170.0039],\n",
      "         [ 105761.0234,   -9910.9922,   -2121.9805,  ...,  -20719.0117,\n",
      "              957.9795,   -9045.9971],\n",
      "         [ -16503.0039,   -7104.0078,   -7950.9873,  ...,    2537.0078,\n",
      "            -3360.9873,   35734.0000]]]),) and output (tensor([[[ -14791.0000,   15299.9990,   16129.9961,  ...,    3187.9980,\n",
      "            -2552.0020,    5626.0020],\n",
      "         [  -9747.9922,  -66667.0000,    1343.0078,  ...,     886.0000,\n",
      "             -923.0195,  -62355.9922],\n",
      "         [  -8232.0459, -143104.9844,   18370.9746,  ...,  -27367.0039,\n",
      "            10916.9941,   71587.9609],\n",
      "         ...,\n",
      "         [  -6239.9844,   19766.9727,   -6841.0156,  ...,  -25368.9863,\n",
      "           -46460.0078,  -59313.0039],\n",
      "         [ 105963.0234,  -21528.9922,   -7747.9805,  ...,  -23648.0117,\n",
      "            -7239.0205,   -6226.9971],\n",
      "         [ -14915.0039,   -3441.0078,   -9091.9873,  ...,   11766.0078,\n",
      "            -5605.9873,   36216.0000]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[ -14791.0000,   15299.9990,   16129.9961,  ...,    3187.9980,\n",
      "            -2552.0020,    5626.0020],\n",
      "         [  -9747.9922,  -66667.0000,    1343.0078,  ...,     886.0000,\n",
      "             -923.0195,  -62355.9922],\n",
      "         [  -8232.0459, -143104.9844,   18370.9746,  ...,  -27367.0039,\n",
      "            10916.9941,   71587.9609],\n",
      "         ...,\n",
      "         [  -6239.9844,   19766.9727,   -6841.0156,  ...,  -25368.9863,\n",
      "           -46460.0078,  -59313.0039],\n",
      "         [ 105963.0234,  -21528.9922,   -7747.9805,  ...,  -23648.0117,\n",
      "            -7239.0205,   -6226.9971],\n",
      "         [ -14915.0039,   -3441.0078,   -9091.9873,  ...,   11766.0078,\n",
      "            -5605.9873,   36216.0000]]]),) and output (tensor([[[ -13284.0000,   26639.0000,    7467.9961,  ...,    4113.9980,\n",
      "            -6608.0020,    5336.0020],\n",
      "         [ -14180.9922,  -65506.0000,   -7108.9922,  ...,    -263.0000,\n",
      "             4520.9805,  -65131.9922],\n",
      "         [  -7018.0459, -136480.9844,   13338.9746,  ...,  -26890.0039,\n",
      "            14040.9941,   63403.9609],\n",
      "         ...,\n",
      "         [  -8213.9844,   18176.9727,   -7251.0156,  ...,  -20141.9863,\n",
      "           -57529.0078,  -60948.0039],\n",
      "         [ 104630.0234,   -9809.9922,   -7420.9805,  ...,  -15600.0117,\n",
      "            -8325.0205,     896.0029],\n",
      "         [  -6676.0039,    1800.9922,   -7013.9873,  ...,    9365.0078,\n",
      "            -3372.9873,   42415.0000]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[ -13284.0000,   26639.0000,    7467.9961,  ...,    4113.9980,\n",
      "            -6608.0020,    5336.0020],\n",
      "         [ -14180.9922,  -65506.0000,   -7108.9922,  ...,    -263.0000,\n",
      "             4520.9805,  -65131.9922],\n",
      "         [  -7018.0459, -136480.9844,   13338.9746,  ...,  -26890.0039,\n",
      "            14040.9941,   63403.9609],\n",
      "         ...,\n",
      "         [  -8213.9844,   18176.9727,   -7251.0156,  ...,  -20141.9863,\n",
      "           -57529.0078,  -60948.0039],\n",
      "         [ 104630.0234,   -9809.9922,   -7420.9805,  ...,  -15600.0117,\n",
      "            -8325.0205,     896.0029],\n",
      "         [  -6676.0039,    1800.9922,   -7013.9873,  ...,    9365.0078,\n",
      "            -3372.9873,   42415.0000]]]),) and output (tensor([[[ -21532.0000,   32796.0000,   16281.9961,  ...,   18208.9980,\n",
      "            -4282.0020,    5756.0020],\n",
      "         [  -8255.9922,  -60926.0000,   -6662.9922,  ...,    -201.0000,\n",
      "             5009.9805,  -65415.9922],\n",
      "         [  -5499.0459, -139157.9844,   17108.9746,  ...,  -30073.0039,\n",
      "            18998.9941,   74661.9609],\n",
      "         ...,\n",
      "         [  -7021.9844,   19041.9727,   -7398.0156,  ...,  -17642.9863,\n",
      "           -50061.0078,  -70532.0000],\n",
      "         [  98796.0234,   -7356.9922,  -11213.9805,  ...,  -27014.0117,\n",
      "            -9569.0205,   -3958.9971],\n",
      "         [  -7155.0039,    7837.9922,   -6757.9873,  ...,   14211.0078,\n",
      "            -6408.9873,   43949.0000]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[ -21532.0000,   32796.0000,   16281.9961,  ...,   18208.9980,\n",
      "            -4282.0020,    5756.0020],\n",
      "         [  -8255.9922,  -60926.0000,   -6662.9922,  ...,    -201.0000,\n",
      "             5009.9805,  -65415.9922],\n",
      "         [  -5499.0459, -139157.9844,   17108.9746,  ...,  -30073.0039,\n",
      "            18998.9941,   74661.9609],\n",
      "         ...,\n",
      "         [  -7021.9844,   19041.9727,   -7398.0156,  ...,  -17642.9863,\n",
      "           -50061.0078,  -70532.0000],\n",
      "         [  98796.0234,   -7356.9922,  -11213.9805,  ...,  -27014.0117,\n",
      "            -9569.0205,   -3958.9971],\n",
      "         [  -7155.0039,    7837.9922,   -6757.9873,  ...,   14211.0078,\n",
      "            -6408.9873,   43949.0000]]]),) and output (tensor([[[ -24082.0000,   33387.0000,   15374.9961,  ...,   18359.9980,\n",
      "            -7500.0020,   11519.0020],\n",
      "         [   -811.9922,  -54034.0000,   -5530.9922,  ...,    2853.0000,\n",
      "            -1854.0195,  -67861.9922],\n",
      "         [  -2758.0459, -132231.9844,   28572.9746,  ...,  -31373.0039,\n",
      "            25096.9941,   76011.9609],\n",
      "         ...,\n",
      "         [   1016.0156,   19774.9727,  -16991.0156,  ...,  -20869.9863,\n",
      "           -41828.0078,  -60768.0000],\n",
      "         [ 103362.0234,   -6429.9922,   -9289.9805,  ...,  -26744.0117,\n",
      "           -16239.0205,   -8312.9971],\n",
      "         [  -8486.0039,    8972.9922,   -4059.9873,  ...,   13610.0078,\n",
      "           -13941.9873,   31912.0000]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[ -24082.0000,   33387.0000,   15374.9961,  ...,   18359.9980,\n",
      "            -7500.0020,   11519.0020],\n",
      "         [   -811.9922,  -54034.0000,   -5530.9922,  ...,    2853.0000,\n",
      "            -1854.0195,  -67861.9922],\n",
      "         [  -2758.0459, -132231.9844,   28572.9746,  ...,  -31373.0039,\n",
      "            25096.9941,   76011.9609],\n",
      "         ...,\n",
      "         [   1016.0156,   19774.9727,  -16991.0156,  ...,  -20869.9863,\n",
      "           -41828.0078,  -60768.0000],\n",
      "         [ 103362.0234,   -6429.9922,   -9289.9805,  ...,  -26744.0117,\n",
      "           -16239.0205,   -8312.9971],\n",
      "         [  -8486.0039,    8972.9922,   -4059.9873,  ...,   13610.0078,\n",
      "           -13941.9873,   31912.0000]]]),) and output (tensor([[[ -14869.0000,   32284.0000,    3742.9961,  ...,   12458.9980,\n",
      "            -5452.0020,    7590.0020],\n",
      "         [  -1988.9922,  -54261.0000,   -5569.9922,  ...,    1613.0000,\n",
      "             4664.9805,  -70247.9922],\n",
      "         [  -4679.0459, -132676.9844,   28661.9746,  ...,  -38261.0039,\n",
      "            18233.9941,   71163.9609],\n",
      "         ...,\n",
      "         [  -4671.9844,   19985.9727,  -13426.0156,  ...,  -22155.9863,\n",
      "           -45027.0078,  -58762.0000],\n",
      "         [ 104668.0234,  -11957.9922,  -21401.9805,  ...,  -19352.0117,\n",
      "           -17982.0195,   -2874.9971],\n",
      "         [  -5578.0039,    2991.9922,     191.0127,  ...,   11816.0078,\n",
      "           -19548.9883,   25502.0000]]]),)\n",
      "[Debug] Layer names being passed: ['model.layers.0', 'model.layers.1', 'model.layers.2', 'model.layers.3', 'model.layers.4', 'model.layers.5', 'model.layers.6', 'model.layers.7', 'model.layers.8', 'model.layers.9', 'model.layers.10', 'model.layers.11', 'model.layers.12', 'model.layers.13', 'model.layers.14', 'model.layers.15', 'model.layers.16', 'model.layers.17', 'model.layers.18', 'model.layers.19', 'model.layers.20', 'model.layers.21', 'model.layers.22', 'model.layers.23', 'model.layers.24', 'model.layers.25', 'model.layers.26', 'model.layers.27', 'model.embed_tokens']\n",
      "[Debug] Trying to access layer: model.layers.0\n",
      "[Debug] Successfully found layer: model.layers.0\n",
      "[Debug] Trying to access layer: model.layers.1\n",
      "[Debug] Successfully found layer: model.layers.1\n",
      "[Debug] Trying to access layer: model.layers.2\n",
      "[Debug] Successfully found layer: model.layers.2\n",
      "[Debug] Trying to access layer: model.layers.3\n",
      "[Debug] Successfully found layer: model.layers.3\n",
      "[Debug] Trying to access layer: model.layers.4\n",
      "[Debug] Successfully found layer: model.layers.4\n",
      "[Debug] Trying to access layer: model.layers.5\n",
      "[Debug] Successfully found layer: model.layers.5\n",
      "[Debug] Trying to access layer: model.layers.6\n",
      "[Debug] Successfully found layer: model.layers.6\n",
      "[Debug] Trying to access layer: model.layers.7\n",
      "[Debug] Successfully found layer: model.layers.7\n",
      "[Debug] Trying to access layer: model.layers.8\n",
      "[Debug] Successfully found layer: model.layers.8\n",
      "[Debug] Trying to access layer: model.layers.9\n",
      "[Debug] Successfully found layer: model.layers.9\n",
      "[Debug] Trying to access layer: model.layers.10\n",
      "[Debug] Successfully found layer: model.layers.10\n",
      "[Debug] Trying to access layer: model.layers.11\n",
      "[Debug] Successfully found layer: model.layers.11\n",
      "[Debug] Trying to access layer: model.layers.12\n",
      "[Debug] Successfully found layer: model.layers.12\n",
      "[Debug] Trying to access layer: model.layers.13\n",
      "[Debug] Successfully found layer: model.layers.13\n",
      "[Debug] Trying to access layer: model.layers.14\n",
      "[Debug] Successfully found layer: model.layers.14\n",
      "[Debug] Trying to access layer: model.layers.15\n",
      "[Debug] Successfully found layer: model.layers.15\n",
      "[Debug] Trying to access layer: model.layers.16\n",
      "[Debug] Successfully found layer: model.layers.16\n",
      "[Debug] Trying to access layer: model.layers.17\n",
      "[Debug] Successfully found layer: model.layers.17\n",
      "[Debug] Trying to access layer: model.layers.18\n",
      "[Debug] Successfully found layer: model.layers.18\n",
      "[Debug] Trying to access layer: model.layers.19\n",
      "[Debug] Successfully found layer: model.layers.19\n",
      "[Debug] Trying to access layer: model.layers.20\n",
      "[Debug] Successfully found layer: model.layers.20\n",
      "[Debug] Trying to access layer: model.layers.21\n",
      "[Debug] Successfully found layer: model.layers.21\n",
      "[Debug] Trying to access layer: model.layers.22\n",
      "[Debug] Successfully found layer: model.layers.22\n",
      "[Debug] Trying to access layer: model.layers.23\n",
      "[Debug] Successfully found layer: model.layers.23\n",
      "[Debug] Trying to access layer: model.layers.24\n",
      "[Debug] Successfully found layer: model.layers.24\n",
      "[Debug] Trying to access layer: model.layers.25\n",
      "[Debug] Successfully found layer: model.layers.25\n",
      "[Debug] Trying to access layer: model.layers.26\n",
      "[Debug] Successfully found layer: model.layers.26\n",
      "[Debug] Trying to access layer: model.layers.27\n",
      "[Debug] Successfully found layer: model.layers.27\n",
      "[Debug] Trying to access layer: model.embed_tokens\n",
      "[Debug] Successfully found layer: model.embed_tokens\n",
      "[Hook] Layer Embedding(128256, 3072, padding_idx=128004) received input (tensor([[128000,     44,    980,  61023,    386,    980,    285,   3145,  12446,\n",
      "            304,  11427,  44104,     82,    323,  16700,  86497,    320,  21470,\n",
      "            439,  10913,  13936,     11,    477,    279,   4892,   3778,   8681,\n",
      "          63911,    570,   4314,  22484,    617,  35901,   1027,  16917,  28160,\n",
      "            555,   4027,    323,  44611,  10696,    354,  65916,    505,  44304,\n",
      "           1778,    439,  61495,    323,   9578,  56741,     82,     13,   1102,\n",
      "            574,  13240,    430,    304,    220,   1049,     18,     11,   1193,\n",
      "            220,    975,    311,    220,   1627,   3346,    315,  16763,   1974,\n",
      "          61951,   2103,  14958,    304,    264,  12309,   5933,   1614,    320,\n",
      "           9210,    374,     11,    814,   1051,    539,   1511,    369,  30029,\n",
      "           4245,    311,    279,  48111,    315,    279,    362,  35174,    570,\n",
      "          63388,    750,     11,    814,   4097,   4056,     22,      4,    315,\n",
      "          10054,  12862,   4363,   3158,     13,   1666,    279,   1917,    596,\n",
      "           1455,  17744,  43024,  27331,  17614,   2015,     11,    279,    386,\n",
      "            980,    285,   3145,   4097,    832,    315,    279,    810,  47379,\n",
      "           3062,  17614,  10373,     13]]),) and output tensor([[[-9.9945e-04, -6.5613e-04, -4.6387e-03,  ..., -1.4267e-03,\n",
      "          -2.0447e-03,  1.9684e-03],\n",
      "         [-4.7607e-03, -2.3682e-02, -2.6398e-03,  ...,  4.8218e-03,\n",
      "           3.5889e-02,  4.5395e-04],\n",
      "         [-3.1471e-04, -2.9419e-02,  4.5166e-03,  ..., -1.1597e-02,\n",
      "          -2.5269e-02, -1.6556e-03],\n",
      "         ...,\n",
      "         [ 2.8687e-03, -1.0559e-02,  8.7891e-03,  ...,  1.4099e-02,\n",
      "           3.6377e-02, -1.6113e-02],\n",
      "         [-1.0498e-02,  2.6733e-02, -7.9632e-05,  ..., -6.3965e-02,\n",
      "           1.4801e-03, -3.1494e-02],\n",
      "         [ 9.3384e-03, -3.0670e-03,  2.7832e-02,  ...,  1.1658e-02,\n",
      "          -8.3618e-03,  9.6436e-03]]])\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[-9.9945e-04, -6.5613e-04, -4.6387e-03,  ..., -1.4267e-03,\n",
      "          -2.0447e-03,  1.9684e-03],\n",
      "         [-4.7607e-03, -2.3682e-02, -2.6398e-03,  ...,  4.8218e-03,\n",
      "           3.5889e-02,  4.5395e-04],\n",
      "         [-3.1471e-04, -2.9419e-02,  4.5166e-03,  ..., -1.1597e-02,\n",
      "          -2.5269e-02, -1.6556e-03],\n",
      "         ...,\n",
      "         [ 2.8687e-03, -1.0559e-02,  8.7891e-03,  ...,  1.4099e-02,\n",
      "           3.6377e-02, -1.6113e-02],\n",
      "         [-1.0498e-02,  2.6733e-02, -7.9632e-05,  ..., -6.3965e-02,\n",
      "           1.4801e-03, -3.1494e-02],\n",
      "         [ 9.3384e-03, -3.0670e-03,  2.7832e-02,  ...,  1.1658e-02,\n",
      "          -8.3618e-03,  9.6436e-03]]]),) and output (tensor([[[ 442.9990,  239.9993,  302.9954,  ..., -154.0014,  -15.0020,\n",
      "           185.0020],\n",
      "         [  26.9952,   76.9763,  -76.0026,  ...,  -37.9952,  164.0359,\n",
      "            95.0005],\n",
      "         [ 413.9997, -145.0294,  605.0045,  ...,  -79.0116,  178.9747,\n",
      "          -193.0016],\n",
      "         ...,\n",
      "         [ 104.0029, -331.0106, -120.9912,  ..., -708.9859,  159.0364,\n",
      "          -555.0161],\n",
      "         [-203.0105,  -60.9733,  446.9999,  ..., -400.0640,  175.0015,\n",
      "            41.9685],\n",
      "         [-235.9907,   36.9969,   54.0278,  ..., -352.9883,   13.9916,\n",
      "           -56.9904]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[ 442.9990,  239.9993,  302.9954,  ..., -154.0014,  -15.0020,\n",
      "           185.0020],\n",
      "         [  26.9952,   76.9763,  -76.0026,  ...,  -37.9952,  164.0359,\n",
      "            95.0005],\n",
      "         [ 413.9997, -145.0294,  605.0045,  ...,  -79.0116,  178.9747,\n",
      "          -193.0016],\n",
      "         ...,\n",
      "         [ 104.0029, -331.0106, -120.9912,  ..., -708.9859,  159.0364,\n",
      "          -555.0161],\n",
      "         [-203.0105,  -60.9733,  446.9999,  ..., -400.0640,  175.0015,\n",
      "            41.9685],\n",
      "         [-235.9907,   36.9969,   54.0278,  ..., -352.9883,   13.9916,\n",
      "           -56.9904]]]),) and output (tensor([[[  439.9990,  -144.0007,  -410.0046,  ...,  -765.0015,\n",
      "           1193.9979, -1027.9980],\n",
      "         [  220.9952,  -388.0237,   702.9974,  ...,  -844.9952,\n",
      "           1507.0359,   306.0005],\n",
      "         [ -482.0003,  -514.0294,   830.0045,  ...,  -673.0116,\n",
      "           1086.9747,  -689.0016],\n",
      "         ...,\n",
      "         [  419.0029,  -361.0106,   125.0088,  ..., -1742.9858,\n",
      "            796.0364, -1445.0161],\n",
      "         [ -580.0105,   240.0267,   682.9999,  ...,  -207.0640,\n",
      "            424.0015,   472.9685],\n",
      "         [   23.0093,    74.9969,   941.0278,  ...,  -422.9883,\n",
      "             72.9916,   -68.9904]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[  439.9990,  -144.0007,  -410.0046,  ...,  -765.0015,\n",
      "           1193.9979, -1027.9980],\n",
      "         [  220.9952,  -388.0237,   702.9974,  ...,  -844.9952,\n",
      "           1507.0359,   306.0005],\n",
      "         [ -482.0003,  -514.0294,   830.0045,  ...,  -673.0116,\n",
      "           1086.9747,  -689.0016],\n",
      "         ...,\n",
      "         [  419.0029,  -361.0106,   125.0088,  ..., -1742.9858,\n",
      "            796.0364, -1445.0161],\n",
      "         [ -580.0105,   240.0267,   682.9999,  ...,  -207.0640,\n",
      "            424.0015,   472.9685],\n",
      "         [   23.0093,    74.9969,   941.0278,  ...,  -422.9883,\n",
      "             72.9916,   -68.9904]]]),) and output (tensor([[[  551.9990,   993.9993, -1790.0046,  ...,   445.9985,\n",
      "          -1386.0021, -1245.9980],\n",
      "         [ 1796.9952,  1737.9763, -2023.0027,  ...,   197.0048,\n",
      "           -209.9641,  2407.0005],\n",
      "         [  210.9997, -4656.0293, -1857.9955,  ..., -1262.0116,\n",
      "            446.9747,  -266.0016],\n",
      "         ...,\n",
      "         [ 2994.0029,  -811.0106,  -908.9912,  ...,    46.0142,\n",
      "            458.0364,   309.9839],\n",
      "         [-3771.0105,  1200.0267,   671.9999,  ...,  -547.0640,\n",
      "           4683.0015,  1130.9685],\n",
      "         [  732.0093, -3625.0032,  -183.9722,  ..., -3038.9883,\n",
      "           1117.9917, -1376.9904]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[  551.9990,   993.9993, -1790.0046,  ...,   445.9985,\n",
      "          -1386.0021, -1245.9980],\n",
      "         [ 1796.9952,  1737.9763, -2023.0027,  ...,   197.0048,\n",
      "           -209.9641,  2407.0005],\n",
      "         [  210.9997, -4656.0293, -1857.9955,  ..., -1262.0116,\n",
      "            446.9747,  -266.0016],\n",
      "         ...,\n",
      "         [ 2994.0029,  -811.0106,  -908.9912,  ...,    46.0142,\n",
      "            458.0364,   309.9839],\n",
      "         [-3771.0105,  1200.0267,   671.9999,  ...,  -547.0640,\n",
      "           4683.0015,  1130.9685],\n",
      "         [  732.0093, -3625.0032,  -183.9722,  ..., -3038.9883,\n",
      "           1117.9917, -1376.9904]]]),) and output (tensor([[[ 2131.9990,  -737.0007, -1071.0046,  ..., -2781.0015,\n",
      "           -885.0021,    86.0020],\n",
      "         [ 1475.9951,   461.9763,  2809.9973,  ...,  1510.0049,\n",
      "          -3148.9641,  2569.0005],\n",
      "         [ 3875.9998, -6565.0293,  2758.0044,  ..., -4235.0117,\n",
      "            808.9747, -4141.0015],\n",
      "         ...,\n",
      "         [ 5995.0029,    62.9894, -3985.9912,  ...,   981.0142,\n",
      "           3804.0364,  -783.0161],\n",
      "         [-2413.0105, -6411.9731,  4420.0000,  ..., -4057.0640,\n",
      "           6858.0015,  3095.9685],\n",
      "         [  327.0093, -9256.0029,  2340.0278,  ..., -7073.9883,\n",
      "           -242.0083, -2333.9902]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[ 2131.9990,  -737.0007, -1071.0046,  ..., -2781.0015,\n",
      "           -885.0021,    86.0020],\n",
      "         [ 1475.9951,   461.9763,  2809.9973,  ...,  1510.0049,\n",
      "          -3148.9641,  2569.0005],\n",
      "         [ 3875.9998, -6565.0293,  2758.0044,  ..., -4235.0117,\n",
      "            808.9747, -4141.0015],\n",
      "         ...,\n",
      "         [ 5995.0029,    62.9894, -3985.9912,  ...,   981.0142,\n",
      "           3804.0364,  -783.0161],\n",
      "         [-2413.0105, -6411.9731,  4420.0000,  ..., -4057.0640,\n",
      "           6858.0015,  3095.9685],\n",
      "         [  327.0093, -9256.0029,  2340.0278,  ..., -7073.9883,\n",
      "           -242.0083, -2333.9902]]]),) and output (tensor([[[  3420.9990,    760.9993,  -5376.0049,  ...,  -4193.0015,\n",
      "            2860.9980,  -4080.9980],\n",
      "         [  2521.9951,   1510.9763,   3527.9973,  ...,   2051.0049,\n",
      "           -4710.9639,   7049.0005],\n",
      "         [  2312.9998,  -3988.0293,  -1008.9956,  ...,  -3149.0117,\n",
      "           -1506.0253,  -6455.0015],\n",
      "         ...,\n",
      "         [   582.0029,   1937.9895,  -5162.9912,  ...,   4360.0142,\n",
      "            -279.9639,  -3754.0161],\n",
      "         [ -1506.0105,  -7420.9731,   2450.0000,  ...,  -6660.0640,\n",
      "             647.0015,   7914.9688],\n",
      "         [  1011.0093, -16419.0039,     63.0278,  ...,  -5985.9883,\n",
      "            1784.9917,    858.0098]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[  3420.9990,    760.9993,  -5376.0049,  ...,  -4193.0015,\n",
      "            2860.9980,  -4080.9980],\n",
      "         [  2521.9951,   1510.9763,   3527.9973,  ...,   2051.0049,\n",
      "           -4710.9639,   7049.0005],\n",
      "         [  2312.9998,  -3988.0293,  -1008.9956,  ...,  -3149.0117,\n",
      "           -1506.0253,  -6455.0015],\n",
      "         ...,\n",
      "         [   582.0029,   1937.9895,  -5162.9912,  ...,   4360.0142,\n",
      "            -279.9639,  -3754.0161],\n",
      "         [ -1506.0105,  -7420.9731,   2450.0000,  ...,  -6660.0640,\n",
      "             647.0015,   7914.9688],\n",
      "         [  1011.0093, -16419.0039,     63.0278,  ...,  -5985.9883,\n",
      "            1784.9917,    858.0098]]]),) and output (tensor([[[  2742.9990,  -1512.0007, -10493.0049,  ...,  -9945.0020,\n",
      "           -2017.0020,  -1802.9980],\n",
      "         [ -5931.0049,   6279.9766,   5435.9971,  ...,   1248.0049,\n",
      "           -8979.9639,   9619.0000],\n",
      "         [ -1059.0002,  -9016.0293,  -1153.9956,  ...,  -2396.0117,\n",
      "           -5463.0254,  -1636.0015],\n",
      "         ...,\n",
      "         [  2220.0029,   5536.9893,  -6034.9912,  ...,   2523.0142,\n",
      "           -3990.9639,  -5882.0161],\n",
      "         [ -3710.0105,  -8266.9727,   2341.0000,  ...,  -4827.0640,\n",
      "           -4336.9985,  13815.9688],\n",
      "         [  3991.0093, -24762.0039,  -2745.9722,  ...,  -8134.9883,\n",
      "            1623.9917,   5878.0098]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[  2742.9990,  -1512.0007, -10493.0049,  ...,  -9945.0020,\n",
      "           -2017.0020,  -1802.9980],\n",
      "         [ -5931.0049,   6279.9766,   5435.9971,  ...,   1248.0049,\n",
      "           -8979.9639,   9619.0000],\n",
      "         [ -1059.0002,  -9016.0293,  -1153.9956,  ...,  -2396.0117,\n",
      "           -5463.0254,  -1636.0015],\n",
      "         ...,\n",
      "         [  2220.0029,   5536.9893,  -6034.9912,  ...,   2523.0142,\n",
      "           -3990.9639,  -5882.0161],\n",
      "         [ -3710.0105,  -8266.9727,   2341.0000,  ...,  -4827.0640,\n",
      "           -4336.9985,  13815.9688],\n",
      "         [  3991.0093, -24762.0039,  -2745.9722,  ...,  -8134.9883,\n",
      "            1623.9917,   5878.0098]]]),) and output (tensor([[[  3032.9990,  -3420.0007, -10291.0049,  ...,  -6511.0020,\n",
      "           -1953.0020,   2001.0020],\n",
      "         [ -4943.0049,   6703.9766,   5085.9971,  ...,   1859.0049,\n",
      "           -7161.9639,  10211.0000],\n",
      "         [  4880.0000,  -6534.0293,  -1355.9956,  ...,   1123.9883,\n",
      "           -5953.0254,   3038.9985],\n",
      "         ...,\n",
      "         [  5967.0029,   7257.9893,  -6548.9912,  ...,   -626.9858,\n",
      "            1076.0361,  -3831.0161],\n",
      "         [  7620.9893, -14156.9727,   1752.0000,  ..., -10413.0645,\n",
      "           -6889.9985,  10231.9688],\n",
      "         [  1795.0093, -28198.0039,  -2929.9722,  ...,  -7597.9883,\n",
      "            5912.9917,   2960.0098]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[  3032.9990,  -3420.0007, -10291.0049,  ...,  -6511.0020,\n",
      "           -1953.0020,   2001.0020],\n",
      "         [ -4943.0049,   6703.9766,   5085.9971,  ...,   1859.0049,\n",
      "           -7161.9639,  10211.0000],\n",
      "         [  4880.0000,  -6534.0293,  -1355.9956,  ...,   1123.9883,\n",
      "           -5953.0254,   3038.9985],\n",
      "         ...,\n",
      "         [  5967.0029,   7257.9893,  -6548.9912,  ...,   -626.9858,\n",
      "            1076.0361,  -3831.0161],\n",
      "         [  7620.9893, -14156.9727,   1752.0000,  ..., -10413.0645,\n",
      "           -6889.9985,  10231.9688],\n",
      "         [  1795.0093, -28198.0039,  -2929.9722,  ...,  -7597.9883,\n",
      "            5912.9917,   2960.0098]]]),) and output (tensor([[[  3132.9990,  -1970.0007, -11544.0049,  ...,  -5780.0020,\n",
      "           -2486.0020,  -1921.9980],\n",
      "         [ -3024.0049,   8740.9766,   6901.9971,  ...,   1647.0049,\n",
      "          -11966.9639,   9179.0000],\n",
      "         [  3204.0000,  -8863.0293,   1693.0044,  ...,  -1081.0117,\n",
      "           -1606.0254,    805.9985],\n",
      "         ...,\n",
      "         [  6871.0029,   1012.9893,  -7118.9912,  ...,  -2353.9858,\n",
      "            5155.0361,  -2378.0161],\n",
      "         [  5969.9893, -13051.9727,   1939.0000,  ...,  -6841.0645,\n",
      "           -5227.9985,  15273.9688],\n",
      "         [  -459.9907, -32783.0039,  -4579.9722,  ...,  -7989.9883,\n",
      "            2043.9917,    125.0098]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[  3132.9990,  -1970.0007, -11544.0049,  ...,  -5780.0020,\n",
      "           -2486.0020,  -1921.9980],\n",
      "         [ -3024.0049,   8740.9766,   6901.9971,  ...,   1647.0049,\n",
      "          -11966.9639,   9179.0000],\n",
      "         [  3204.0000,  -8863.0293,   1693.0044,  ...,  -1081.0117,\n",
      "           -1606.0254,    805.9985],\n",
      "         ...,\n",
      "         [  6871.0029,   1012.9893,  -7118.9912,  ...,  -2353.9858,\n",
      "            5155.0361,  -2378.0161],\n",
      "         [  5969.9893, -13051.9727,   1939.0000,  ...,  -6841.0645,\n",
      "           -5227.9985,  15273.9688],\n",
      "         [  -459.9907, -32783.0039,  -4579.9722,  ...,  -7989.9883,\n",
      "            2043.9917,    125.0098]]]),) and output (tensor([[[  1046.9990,   2403.9993, -14486.0049,  ..., -10922.0020,\n",
      "           -1419.0020,  -2983.9980],\n",
      "         [ -3449.0049,  11674.9766,   6179.9971,  ...,    484.0049,\n",
      "          -10049.9639,   5954.0000],\n",
      "         [  1708.0000,  -9187.0293,   1622.0044,  ...,  -2747.0117,\n",
      "            3523.9746,   1292.9985],\n",
      "         ...,\n",
      "         [  4654.0029,  -3586.0107,  -4794.9912,  ...,  -1665.9858,\n",
      "            7201.0361,  -1985.0161],\n",
      "         [  4088.9893, -14438.9727,  -1184.0000,  ...,  -7835.0645,\n",
      "           -4923.9985,  13898.9688],\n",
      "         [ -1031.9907, -43022.0039,  -4871.9722,  ..., -10518.9883,\n",
      "            -559.0083,  -3283.9902]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[  1046.9990,   2403.9993, -14486.0049,  ..., -10922.0020,\n",
      "           -1419.0020,  -2983.9980],\n",
      "         [ -3449.0049,  11674.9766,   6179.9971,  ...,    484.0049,\n",
      "          -10049.9639,   5954.0000],\n",
      "         [  1708.0000,  -9187.0293,   1622.0044,  ...,  -2747.0117,\n",
      "            3523.9746,   1292.9985],\n",
      "         ...,\n",
      "         [  4654.0029,  -3586.0107,  -4794.9912,  ...,  -1665.9858,\n",
      "            7201.0361,  -1985.0161],\n",
      "         [  4088.9893, -14438.9727,  -1184.0000,  ...,  -7835.0645,\n",
      "           -4923.9985,  13898.9688],\n",
      "         [ -1031.9907, -43022.0039,  -4871.9722,  ..., -10518.9883,\n",
      "            -559.0083,  -3283.9902]]]),) and output (tensor([[[ -3767.0010,   5307.9990, -14221.0049,  ..., -15035.0020,\n",
      "           -3630.0020,   4168.0020],\n",
      "         [   -79.0049,  10795.9766,   5633.9971,  ...,   2268.0049,\n",
      "           -9457.9639,  -1043.0000],\n",
      "         [  -973.0000,  -7084.0293,   1167.0044,  ...,  -1998.0117,\n",
      "            9544.9746,  -3839.0015],\n",
      "         ...,\n",
      "         [ -2290.9971,  -4601.0107,  -4849.9912,  ...,   2014.0142,\n",
      "            6420.0361,  -2154.0161],\n",
      "         [  8902.9893, -16748.9727,  -3542.0000,  ...,  -5764.0645,\n",
      "           -3831.9985,   9790.9688],\n",
      "         [   159.0093, -40180.0039,  -4954.9722,  ...,  -7144.9883,\n",
      "           -6777.0083,  -5641.9902]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[ -3767.0010,   5307.9990, -14221.0049,  ..., -15035.0020,\n",
      "           -3630.0020,   4168.0020],\n",
      "         [   -79.0049,  10795.9766,   5633.9971,  ...,   2268.0049,\n",
      "           -9457.9639,  -1043.0000],\n",
      "         [  -973.0000,  -7084.0293,   1167.0044,  ...,  -1998.0117,\n",
      "            9544.9746,  -3839.0015],\n",
      "         ...,\n",
      "         [ -2290.9971,  -4601.0107,  -4849.9912,  ...,   2014.0142,\n",
      "            6420.0361,  -2154.0161],\n",
      "         [  8902.9893, -16748.9727,  -3542.0000,  ...,  -5764.0645,\n",
      "           -3831.9985,   9790.9688],\n",
      "         [   159.0093, -40180.0039,  -4954.9722,  ...,  -7144.9883,\n",
      "           -6777.0083,  -5641.9902]]]),) and output (tensor([[[-1.3260e+03,  1.2240e+03, -1.6643e+04,  ..., -1.3240e+04,\n",
      "          -5.6770e+03,  9.6390e+03],\n",
      "         [-8.8000e+02,  1.5023e+04,  3.9240e+03,  ...,  1.2440e+03,\n",
      "          -7.4900e+03, -3.3760e+03],\n",
      "         [ 7.8480e+03, -5.2400e+03,  3.2720e+03,  ...,  1.4310e+03,\n",
      "           8.5830e+03, -1.1921e+04],\n",
      "         ...,\n",
      "         [ 9.0400e+02, -4.2610e+03, -8.7360e+03,  ...,  9.6620e+03,\n",
      "           1.4540e+03, -3.0016e+01],\n",
      "         [ 9.6080e+03, -1.5743e+04, -1.7800e+03,  ..., -1.2592e+04,\n",
      "          -1.8120e+03,  1.2540e+04],\n",
      "         [ 1.3920e+03, -4.7386e+04, -6.4900e+03,  ..., -8.2840e+03,\n",
      "          -5.1070e+03,  3.3970e+03]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[-1.3260e+03,  1.2240e+03, -1.6643e+04,  ..., -1.3240e+04,\n",
      "          -5.6770e+03,  9.6390e+03],\n",
      "         [-8.8000e+02,  1.5023e+04,  3.9240e+03,  ...,  1.2440e+03,\n",
      "          -7.4900e+03, -3.3760e+03],\n",
      "         [ 7.8480e+03, -5.2400e+03,  3.2720e+03,  ...,  1.4310e+03,\n",
      "           8.5830e+03, -1.1921e+04],\n",
      "         ...,\n",
      "         [ 9.0400e+02, -4.2610e+03, -8.7360e+03,  ...,  9.6620e+03,\n",
      "           1.4540e+03, -3.0016e+01],\n",
      "         [ 9.6080e+03, -1.5743e+04, -1.7800e+03,  ..., -1.2592e+04,\n",
      "          -1.8120e+03,  1.2540e+04],\n",
      "         [ 1.3920e+03, -4.7386e+04, -6.4900e+03,  ..., -8.2840e+03,\n",
      "          -5.1070e+03,  3.3970e+03]]]),) and output (tensor([[[ -5120.0010,   2246.9990, -15993.0039,  ..., -19818.0020,\n",
      "           -9385.0020,  11924.0020],\n",
      "         [ -3643.0049,  11434.9766,   1209.9971,  ...,  -4180.9951,\n",
      "           -2536.9639,  -7865.0000],\n",
      "         [  6877.0000,   -664.0293,   4092.0044,  ...,   1241.9883,\n",
      "           11896.9746, -16041.0020],\n",
      "         ...,\n",
      "         [  1196.0029, -10670.0107, -11856.9912,  ...,   5572.0137,\n",
      "            3102.0361,   1451.9839],\n",
      "         [  9385.9893, -26109.9727,  -4036.0000,  ..., -11380.0645,\n",
      "            2521.0015,  17498.9688],\n",
      "         [ -1333.9907, -50417.0039,  -8705.9727,  ...,  -4007.9883,\n",
      "          -10034.0078,   3377.0098]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[ -5120.0010,   2246.9990, -15993.0039,  ..., -19818.0020,\n",
      "           -9385.0020,  11924.0020],\n",
      "         [ -3643.0049,  11434.9766,   1209.9971,  ...,  -4180.9951,\n",
      "           -2536.9639,  -7865.0000],\n",
      "         [  6877.0000,   -664.0293,   4092.0044,  ...,   1241.9883,\n",
      "           11896.9746, -16041.0020],\n",
      "         ...,\n",
      "         [  1196.0029, -10670.0107, -11856.9912,  ...,   5572.0137,\n",
      "            3102.0361,   1451.9839],\n",
      "         [  9385.9893, -26109.9727,  -4036.0000,  ..., -11380.0645,\n",
      "            2521.0015,  17498.9688],\n",
      "         [ -1333.9907, -50417.0039,  -8705.9727,  ...,  -4007.9883,\n",
      "          -10034.0078,   3377.0098]]]),) and output (tensor([[[ -6685.0010,  -1278.0010, -15553.0039,  ..., -19846.0020,\n",
      "          -10418.0020,  13376.0020],\n",
      "         [ -6020.0049,  10412.9766,   6085.9971,  ...,   3321.0049,\n",
      "           -3993.9639, -11325.0000],\n",
      "         [  2583.0000,  -2083.0293,   4945.0044,  ...,   5527.9883,\n",
      "            9971.9746, -20848.0020],\n",
      "         ...,\n",
      "         [  4298.0029, -10709.0107, -10498.9912,  ...,   5832.0137,\n",
      "             743.0361,  -2674.0161],\n",
      "         [ 15117.9893, -28975.9727,  -5129.0000,  ...,  -7253.0645,\n",
      "            3824.0015,  22300.9688],\n",
      "         [ -3333.9907, -59797.0039, -12843.9727,  ...,  -1755.9883,\n",
      "          -10663.0078,   5444.0098]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[ -6685.0010,  -1278.0010, -15553.0039,  ..., -19846.0020,\n",
      "          -10418.0020,  13376.0020],\n",
      "         [ -6020.0049,  10412.9766,   6085.9971,  ...,   3321.0049,\n",
      "           -3993.9639, -11325.0000],\n",
      "         [  2583.0000,  -2083.0293,   4945.0044,  ...,   5527.9883,\n",
      "            9971.9746, -20848.0020],\n",
      "         ...,\n",
      "         [  4298.0029, -10709.0107, -10498.9912,  ...,   5832.0137,\n",
      "             743.0361,  -2674.0161],\n",
      "         [ 15117.9893, -28975.9727,  -5129.0000,  ...,  -7253.0645,\n",
      "            3824.0015,  22300.9688],\n",
      "         [ -3333.9907, -59797.0039, -12843.9727,  ...,  -1755.9883,\n",
      "          -10663.0078,   5444.0098]]]),) and output (tensor([[[ -8422.0010,    339.9990, -15289.0039,  ..., -20884.0020,\n",
      "           -6908.0020,  11014.0020],\n",
      "         [ -8142.0049,   8727.9766,  11995.9971,  ...,  -3562.9951,\n",
      "           -7239.9639, -13825.0000],\n",
      "         [ -4623.0000,  -1405.0293,   3421.0044,  ...,   5569.9883,\n",
      "           11201.9746, -18350.0020],\n",
      "         ...,\n",
      "         [  4277.0029, -17019.0117, -10310.9912,  ...,   8612.0137,\n",
      "             293.0361,  -7518.0161],\n",
      "         [ 14919.9893, -28861.9727,  -5237.0000,  ...,  -2892.0645,\n",
      "            5262.0015,  15008.9688],\n",
      "         [ -4053.9907, -69367.0000, -11975.9727,  ...,  -1578.9883,\n",
      "          -15944.0078,   3145.0098]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[ -8422.0010,    339.9990, -15289.0039,  ..., -20884.0020,\n",
      "           -6908.0020,  11014.0020],\n",
      "         [ -8142.0049,   8727.9766,  11995.9971,  ...,  -3562.9951,\n",
      "           -7239.9639, -13825.0000],\n",
      "         [ -4623.0000,  -1405.0293,   3421.0044,  ...,   5569.9883,\n",
      "           11201.9746, -18350.0020],\n",
      "         ...,\n",
      "         [  4277.0029, -17019.0117, -10310.9912,  ...,   8612.0137,\n",
      "             293.0361,  -7518.0161],\n",
      "         [ 14919.9893, -28861.9727,  -5237.0000,  ...,  -2892.0645,\n",
      "            5262.0015,  15008.9688],\n",
      "         [ -4053.9907, -69367.0000, -11975.9727,  ...,  -1578.9883,\n",
      "          -15944.0078,   3145.0098]]]),) and output (tensor([[[-10989.0010,   7034.9990, -13964.0039,  ..., -23968.0020,\n",
      "           -3749.0020,  11730.0020],\n",
      "         [-10188.0049,  15473.9766,  15971.9971,  ...,  -8616.9951,\n",
      "           -6636.9639,  -9302.0000],\n",
      "         [ -4861.0000,  -7979.0293,   2454.0044,  ...,    948.9883,\n",
      "           12691.9746, -18184.0020],\n",
      "         ...,\n",
      "         [  1463.0029, -13675.0117, -14547.9912,  ...,   9118.0137,\n",
      "            -932.9639,  -3358.0161],\n",
      "         [ 20606.9883, -22265.9727,  -3774.0000,  ...,  -1915.0645,\n",
      "            4641.0015,   8215.9688],\n",
      "         [ -6006.9907, -81046.0000, -15455.9727,  ...,    218.0117,\n",
      "          -18512.0078,    110.0098]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[-10989.0010,   7034.9990, -13964.0039,  ..., -23968.0020,\n",
      "           -3749.0020,  11730.0020],\n",
      "         [-10188.0049,  15473.9766,  15971.9971,  ...,  -8616.9951,\n",
      "           -6636.9639,  -9302.0000],\n",
      "         [ -4861.0000,  -7979.0293,   2454.0044,  ...,    948.9883,\n",
      "           12691.9746, -18184.0020],\n",
      "         ...,\n",
      "         [  1463.0029, -13675.0117, -14547.9912,  ...,   9118.0137,\n",
      "            -932.9639,  -3358.0161],\n",
      "         [ 20606.9883, -22265.9727,  -3774.0000,  ...,  -1915.0645,\n",
      "            4641.0015,   8215.9688],\n",
      "         [ -6006.9907, -81046.0000, -15455.9727,  ...,    218.0117,\n",
      "          -18512.0078,    110.0098]]]),) and output (tensor([[[-7.1590e+03,  6.6220e+03, -1.0967e+04,  ..., -1.7477e+04,\n",
      "          -2.5100e+03,  1.1888e+04],\n",
      "         [-9.8970e+03,  1.3941e+04,  1.2938e+04,  ..., -1.1841e+04,\n",
      "          -5.2480e+03, -1.0575e+04],\n",
      "         [-4.7620e+03, -1.1336e+04,  5.3180e+03,  ...,  3.7999e+02,\n",
      "           1.7751e+04, -1.8872e+04],\n",
      "         ...,\n",
      "         [ 4.4500e+02, -1.1087e+04, -1.2524e+04,  ...,  1.3619e+04,\n",
      "           4.9370e+03, -1.3130e+03],\n",
      "         [ 2.4206e+04, -1.1291e+04,  5.0000e+01,  ..., -8.5406e+02,\n",
      "           6.9820e+03,  3.4960e+03],\n",
      "         [-4.9810e+03, -8.5365e+04, -1.0985e+04,  ...,  1.3030e+03,\n",
      "          -1.9123e+04, -5.9430e+03]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[-7.1590e+03,  6.6220e+03, -1.0967e+04,  ..., -1.7477e+04,\n",
      "          -2.5100e+03,  1.1888e+04],\n",
      "         [-9.8970e+03,  1.3941e+04,  1.2938e+04,  ..., -1.1841e+04,\n",
      "          -5.2480e+03, -1.0575e+04],\n",
      "         [-4.7620e+03, -1.1336e+04,  5.3180e+03,  ...,  3.7999e+02,\n",
      "           1.7751e+04, -1.8872e+04],\n",
      "         ...,\n",
      "         [ 4.4500e+02, -1.1087e+04, -1.2524e+04,  ...,  1.3619e+04,\n",
      "           4.9370e+03, -1.3130e+03],\n",
      "         [ 2.4206e+04, -1.1291e+04,  5.0000e+01,  ..., -8.5406e+02,\n",
      "           6.9820e+03,  3.4960e+03],\n",
      "         [-4.9810e+03, -8.5365e+04, -1.0985e+04,  ...,  1.3030e+03,\n",
      "          -1.9123e+04, -5.9430e+03]]]),) and output (tensor([[[ -4822.0010,   3124.9990,  -3885.0039,  ..., -19007.0020,\n",
      "            -551.0020,  16285.0020],\n",
      "         [-11918.0049,   9649.9766,  12527.9971,  ...,  -9677.9951,\n",
      "            -934.9639,  -3449.0000],\n",
      "         [ -3182.0000, -13942.0293,   5580.0044,  ...,   -847.0117,\n",
      "           17941.9746, -18368.0020],\n",
      "         ...,\n",
      "         [ -2306.9971, -11294.0117, -12529.9912,  ...,  17666.0137,\n",
      "            6473.0361,   1651.9839],\n",
      "         [ 21922.9883, -13793.9727,   -441.0000,  ...,  -5756.0645,\n",
      "           11092.0020,   1346.9688],\n",
      "         [  -948.9907, -93229.0000, -13095.9727,  ...,   7404.0117,\n",
      "          -15584.0078,   -562.9902]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[ -4822.0010,   3124.9990,  -3885.0039,  ..., -19007.0020,\n",
      "            -551.0020,  16285.0020],\n",
      "         [-11918.0049,   9649.9766,  12527.9971,  ...,  -9677.9951,\n",
      "            -934.9639,  -3449.0000],\n",
      "         [ -3182.0000, -13942.0293,   5580.0044,  ...,   -847.0117,\n",
      "           17941.9746, -18368.0020],\n",
      "         ...,\n",
      "         [ -2306.9971, -11294.0117, -12529.9912,  ...,  17666.0137,\n",
      "            6473.0361,   1651.9839],\n",
      "         [ 21922.9883, -13793.9727,   -441.0000,  ...,  -5756.0645,\n",
      "           11092.0020,   1346.9688],\n",
      "         [  -948.9907, -93229.0000, -13095.9727,  ...,   7404.0117,\n",
      "          -15584.0078,   -562.9902]]]),) and output (tensor([[[  -7235.0010,    6583.9990,   -4212.0039,  ...,  -19104.0020,\n",
      "             3087.9980,   17084.0020],\n",
      "         [  -9636.0049,    5123.9766,   15258.9971,  ...,  -12867.9951,\n",
      "            -2725.9639,    -935.0000],\n",
      "         [  -4007.0000,  -11164.0293,    7750.0044,  ...,    6441.9883,\n",
      "            20508.9746,  -12914.0020],\n",
      "         ...,\n",
      "         [  -7655.9971,   -8767.0117,   -7850.9912,  ...,   22846.0137,\n",
      "             5099.0361,   -1140.0161],\n",
      "         [  25950.9883,  -10551.9727,    7004.0000,  ...,   -8489.0645,\n",
      "             2026.0020,   -2444.0312],\n",
      "         [   -164.9907, -101108.0000,  -18735.9727,  ...,    8190.0117,\n",
      "           -14177.0078,   -5619.9902]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[  -7235.0010,    6583.9990,   -4212.0039,  ...,  -19104.0020,\n",
      "             3087.9980,   17084.0020],\n",
      "         [  -9636.0049,    5123.9766,   15258.9971,  ...,  -12867.9951,\n",
      "            -2725.9639,    -935.0000],\n",
      "         [  -4007.0000,  -11164.0293,    7750.0044,  ...,    6441.9883,\n",
      "            20508.9746,  -12914.0020],\n",
      "         ...,\n",
      "         [  -7655.9971,   -8767.0117,   -7850.9912,  ...,   22846.0137,\n",
      "             5099.0361,   -1140.0161],\n",
      "         [  25950.9883,  -10551.9727,    7004.0000,  ...,   -8489.0645,\n",
      "             2026.0020,   -2444.0312],\n",
      "         [   -164.9907, -101108.0000,  -18735.9727,  ...,    8190.0117,\n",
      "           -14177.0078,   -5619.9902]]]),) and output (tensor([[[  -5586.0010,    7645.9990,    -505.0039,  ...,  -15346.0020,\n",
      "             -623.0020,   12986.0020],\n",
      "         [  -7550.0049,     681.9766,   22731.9961,  ...,  -16195.9951,\n",
      "           -12005.9639,    7598.0000],\n",
      "         [  -2045.0000,  -13252.0293,   10603.0039,  ...,    2840.9883,\n",
      "            15304.9746,  -15233.0020],\n",
      "         ...,\n",
      "         [  -2145.9971,  -12128.0117,   -6586.9912,  ...,   21153.0137,\n",
      "             8849.0361,    2657.9839],\n",
      "         [  23417.9883,   -6135.9727,    4468.0000,  ...,  -16393.0645,\n",
      "            -1251.9980,   -3438.0312],\n",
      "         [   6395.0093, -101671.0000,  -12944.9727,  ...,    5444.0117,\n",
      "           -19699.0078,   -6803.9902]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[  -5586.0010,    7645.9990,    -505.0039,  ...,  -15346.0020,\n",
      "             -623.0020,   12986.0020],\n",
      "         [  -7550.0049,     681.9766,   22731.9961,  ...,  -16195.9951,\n",
      "           -12005.9639,    7598.0000],\n",
      "         [  -2045.0000,  -13252.0293,   10603.0039,  ...,    2840.9883,\n",
      "            15304.9746,  -15233.0020],\n",
      "         ...,\n",
      "         [  -2145.9971,  -12128.0117,   -6586.9912,  ...,   21153.0137,\n",
      "             8849.0361,    2657.9839],\n",
      "         [  23417.9883,   -6135.9727,    4468.0000,  ...,  -16393.0645,\n",
      "            -1251.9980,   -3438.0312],\n",
      "         [   6395.0093, -101671.0000,  -12944.9727,  ...,    5444.0117,\n",
      "           -19699.0078,   -6803.9902]]]),) and output (tensor([[[  -8916.0010,    2362.9990,   -4833.0039,  ...,  -12075.0020,\n",
      "            -1124.0020,   12438.0020],\n",
      "         [ -11204.0049,    -240.0234,   24101.9961,  ...,  -17567.9961,\n",
      "           -17910.9648,   13496.0000],\n",
      "         [  -7753.0000,  -12764.0293,    8176.0039,  ...,    3545.9883,\n",
      "            17159.9746,   -9250.0020],\n",
      "         ...,\n",
      "         [  -8257.9971,   -9197.0117,   -5693.9912,  ...,   21693.0137,\n",
      "            11130.0361,    5276.9839],\n",
      "         [  19844.9883,   -3224.9727,   -2986.0000,  ...,  -16304.0645,\n",
      "             9210.0020,   -3131.0312],\n",
      "         [   7250.0098, -102645.0000,  -11423.9727,  ...,    4792.0117,\n",
      "           -11441.0078,   -4615.9902]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[  -8916.0010,    2362.9990,   -4833.0039,  ...,  -12075.0020,\n",
      "            -1124.0020,   12438.0020],\n",
      "         [ -11204.0049,    -240.0234,   24101.9961,  ...,  -17567.9961,\n",
      "           -17910.9648,   13496.0000],\n",
      "         [  -7753.0000,  -12764.0293,    8176.0039,  ...,    3545.9883,\n",
      "            17159.9746,   -9250.0020],\n",
      "         ...,\n",
      "         [  -8257.9971,   -9197.0117,   -5693.9912,  ...,   21693.0137,\n",
      "            11130.0361,    5276.9839],\n",
      "         [  19844.9883,   -3224.9727,   -2986.0000,  ...,  -16304.0645,\n",
      "             9210.0020,   -3131.0312],\n",
      "         [   7250.0098, -102645.0000,  -11423.9727,  ...,    4792.0117,\n",
      "           -11441.0078,   -4615.9902]]]),) and output (tensor([[[ -20230.0000,    7877.9990,     161.9961,  ...,  -10250.0020,\n",
      "              676.9980,    7095.0020],\n",
      "         [ -15231.0049,   -3364.0234,   15384.9961,  ...,  -20400.9961,\n",
      "           -24216.9648,    8217.0000],\n",
      "         [  -5434.0000,  -13980.0293,    8151.0039,  ...,    6306.9883,\n",
      "            20004.9746,   -7727.0020],\n",
      "         ...,\n",
      "         [  -7976.9971,  -19267.0117,   -5070.9912,  ...,   17383.0137,\n",
      "             4475.0361,    6938.9839],\n",
      "         [  24863.9883,    -626.9727,   -2257.0000,  ...,  -20429.0645,\n",
      "            12060.0020,  -10199.0312],\n",
      "         [  14116.0098, -105308.0000,  -13093.9727,  ...,    4930.0117,\n",
      "           -12533.0078,  -13062.9902]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[ -20230.0000,    7877.9990,     161.9961,  ...,  -10250.0020,\n",
      "              676.9980,    7095.0020],\n",
      "         [ -15231.0049,   -3364.0234,   15384.9961,  ...,  -20400.9961,\n",
      "           -24216.9648,    8217.0000],\n",
      "         [  -5434.0000,  -13980.0293,    8151.0039,  ...,    6306.9883,\n",
      "            20004.9746,   -7727.0020],\n",
      "         ...,\n",
      "         [  -7976.9971,  -19267.0117,   -5070.9912,  ...,   17383.0137,\n",
      "             4475.0361,    6938.9839],\n",
      "         [  24863.9883,    -626.9727,   -2257.0000,  ...,  -20429.0645,\n",
      "            12060.0020,  -10199.0312],\n",
      "         [  14116.0098, -105308.0000,  -13093.9727,  ...,    4930.0117,\n",
      "           -12533.0078,  -13062.9902]]]),) and output (tensor([[[ -25670.0000,   13293.9990,    9099.9961,  ...,   -6547.0020,\n",
      "            -5658.0020,    4192.0020],\n",
      "         [  -5505.0049,     631.9766,   13679.9961,  ...,  -16705.9961,\n",
      "           -30276.9648,    4857.0000],\n",
      "         [ -10866.0000,  -13619.0293,    9096.0039,  ...,   10890.9883,\n",
      "            17827.9746,   -6814.0020],\n",
      "         ...,\n",
      "         [ -14232.9971,  -16153.0117,    2956.0088,  ...,   18461.0137,\n",
      "             6115.0361,   -2184.0156],\n",
      "         [  25006.9883,   -6082.9727,   -4058.0000,  ...,  -23979.0645,\n",
      "            11662.0020,  -12984.0312],\n",
      "         [  15010.0098, -107739.0000,  -14630.9727,  ...,    5544.0117,\n",
      "            -9715.0078,  -15957.9902]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[ -25670.0000,   13293.9990,    9099.9961,  ...,   -6547.0020,\n",
      "            -5658.0020,    4192.0020],\n",
      "         [  -5505.0049,     631.9766,   13679.9961,  ...,  -16705.9961,\n",
      "           -30276.9648,    4857.0000],\n",
      "         [ -10866.0000,  -13619.0293,    9096.0039,  ...,   10890.9883,\n",
      "            17827.9746,   -6814.0020],\n",
      "         ...,\n",
      "         [ -14232.9971,  -16153.0117,    2956.0088,  ...,   18461.0137,\n",
      "             6115.0361,   -2184.0156],\n",
      "         [  25006.9883,   -6082.9727,   -4058.0000,  ...,  -23979.0645,\n",
      "            11662.0020,  -12984.0312],\n",
      "         [  15010.0098, -107739.0000,  -14630.9727,  ...,    5544.0117,\n",
      "            -9715.0078,  -15957.9902]]]),) and output (tensor([[[ -25652.0000,   13180.9990,   15728.9961,  ...,   -2831.0020,\n",
      "              158.9980,    2204.0020],\n",
      "         [ -16142.0049,   -7830.0234,   13539.9961,  ...,  -20450.9961,\n",
      "           -36550.9648,    7397.0000],\n",
      "         [  -3971.0000,   -8771.0293,   15013.0039,  ...,   11485.9883,\n",
      "            20570.9746,   -3549.0020],\n",
      "         ...,\n",
      "         [ -20043.9961,  -15745.0117,   10767.0088,  ...,   24686.0137,\n",
      "             1882.0361,   -1239.0156],\n",
      "         [  19269.9883,   -9104.9727,  -11098.0000,  ...,  -35706.0625,\n",
      "            14347.0020,  -10890.0312],\n",
      "         [  12204.0098, -111585.0000,  -11499.9727,  ...,   -2665.9883,\n",
      "            -7100.0078,  -14466.9902]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[ -25652.0000,   13180.9990,   15728.9961,  ...,   -2831.0020,\n",
      "              158.9980,    2204.0020],\n",
      "         [ -16142.0049,   -7830.0234,   13539.9961,  ...,  -20450.9961,\n",
      "           -36550.9648,    7397.0000],\n",
      "         [  -3971.0000,   -8771.0293,   15013.0039,  ...,   11485.9883,\n",
      "            20570.9746,   -3549.0020],\n",
      "         ...,\n",
      "         [ -20043.9961,  -15745.0117,   10767.0088,  ...,   24686.0137,\n",
      "             1882.0361,   -1239.0156],\n",
      "         [  19269.9883,   -9104.9727,  -11098.0000,  ...,  -35706.0625,\n",
      "            14347.0020,  -10890.0312],\n",
      "         [  12204.0098, -111585.0000,  -11499.9727,  ...,   -2665.9883,\n",
      "            -7100.0078,  -14466.9902]]]),) and output (tensor([[[ -14791.0000,   15299.9990,   16129.9961,  ...,    3187.9980,\n",
      "            -2552.0020,    5626.0020],\n",
      "         [ -13962.0049,  -12683.0234,    5945.9961,  ...,  -18723.9961,\n",
      "           -28874.9648,    -687.0000],\n",
      "         [ -13815.0000,  -12600.0293,    8512.0039,  ...,   11020.9883,\n",
      "            19257.9746,   -2247.0020],\n",
      "         ...,\n",
      "         [ -21772.9961,   -6474.0117,   14937.0088,  ...,   16465.0137,\n",
      "             -265.9639,     165.9844],\n",
      "         [  23629.9883,   -6857.9727,  -10298.0000,  ...,  -39984.0625,\n",
      "             8720.0020,  -18664.0312],\n",
      "         [  11216.0098, -121377.0000,   -9976.9727,  ...,    1080.0117,\n",
      "            -5514.0078,  -20755.9902]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[ -14791.0000,   15299.9990,   16129.9961,  ...,    3187.9980,\n",
      "            -2552.0020,    5626.0020],\n",
      "         [ -13962.0049,  -12683.0234,    5945.9961,  ...,  -18723.9961,\n",
      "           -28874.9648,    -687.0000],\n",
      "         [ -13815.0000,  -12600.0293,    8512.0039,  ...,   11020.9883,\n",
      "            19257.9746,   -2247.0020],\n",
      "         ...,\n",
      "         [ -21772.9961,   -6474.0117,   14937.0088,  ...,   16465.0137,\n",
      "             -265.9639,     165.9844],\n",
      "         [  23629.9883,   -6857.9727,  -10298.0000,  ...,  -39984.0625,\n",
      "             8720.0020,  -18664.0312],\n",
      "         [  11216.0098, -121377.0000,   -9976.9727,  ...,    1080.0117,\n",
      "            -5514.0078,  -20755.9902]]]),) and output (tensor([[[ -13284.0000,   26639.0000,    7467.9961,  ...,    4113.9980,\n",
      "            -6608.0020,    5336.0020],\n",
      "         [ -17661.0039,   -6421.0234,    2240.9961,  ...,  -16307.9961,\n",
      "           -29735.9648,    4234.0000],\n",
      "         [ -16184.0000,  -10595.0293,    6141.0039,  ...,    5239.9883,\n",
      "            17326.9746,   -3736.0020],\n",
      "         ...,\n",
      "         [ -19954.9961,   -1526.0117,   19377.0078,  ...,   16807.0137,\n",
      "            -1227.9639,    2459.9844],\n",
      "         [  23318.9883,   -4620.9727,   -9284.0000,  ...,  -32813.0625,\n",
      "             8092.0020,  -23510.0312],\n",
      "         [   2954.0098, -126563.0000,   -3141.9727,  ...,   -4953.9883,\n",
      "            -4685.0078,  -17432.9902]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[ -13284.0000,   26639.0000,    7467.9961,  ...,    4113.9980,\n",
      "            -6608.0020,    5336.0020],\n",
      "         [ -17661.0039,   -6421.0234,    2240.9961,  ...,  -16307.9961,\n",
      "           -29735.9648,    4234.0000],\n",
      "         [ -16184.0000,  -10595.0293,    6141.0039,  ...,    5239.9883,\n",
      "            17326.9746,   -3736.0020],\n",
      "         ...,\n",
      "         [ -19954.9961,   -1526.0117,   19377.0078,  ...,   16807.0137,\n",
      "            -1227.9639,    2459.9844],\n",
      "         [  23318.9883,   -4620.9727,   -9284.0000,  ...,  -32813.0625,\n",
      "             8092.0020,  -23510.0312],\n",
      "         [   2954.0098, -126563.0000,   -3141.9727,  ...,   -4953.9883,\n",
      "            -4685.0078,  -17432.9902]]]),) and output (tensor([[[ -21532.0000,   32796.0000,   16281.9961,  ...,   18208.9980,\n",
      "            -4282.0020,    5756.0020],\n",
      "         [ -28917.0039,   -3136.0234,    2959.9961,  ...,  -17359.9961,\n",
      "           -36045.9648,    5309.0000],\n",
      "         [ -22258.0000,  -15799.0293,    3997.0039,  ...,   12624.9883,\n",
      "            16993.9746,   -9666.0020],\n",
      "         ...,\n",
      "         [ -25815.9961,    5815.9883,   18986.0078,  ...,   14236.0137,\n",
      "             1840.0361,    1351.9844],\n",
      "         [  22714.9883,  -18355.9727,   -6540.0000,  ...,  -36502.0625,\n",
      "            13980.0020,  -36309.0312],\n",
      "         [  -1712.9902, -130358.0000,   -3573.9727,  ...,   -1943.9883,\n",
      "            -7976.0078,  -13413.9902]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[ -21532.0000,   32796.0000,   16281.9961,  ...,   18208.9980,\n",
      "            -4282.0020,    5756.0020],\n",
      "         [ -28917.0039,   -3136.0234,    2959.9961,  ...,  -17359.9961,\n",
      "           -36045.9648,    5309.0000],\n",
      "         [ -22258.0000,  -15799.0293,    3997.0039,  ...,   12624.9883,\n",
      "            16993.9746,   -9666.0020],\n",
      "         ...,\n",
      "         [ -25815.9961,    5815.9883,   18986.0078,  ...,   14236.0137,\n",
      "             1840.0361,    1351.9844],\n",
      "         [  22714.9883,  -18355.9727,   -6540.0000,  ...,  -36502.0625,\n",
      "            13980.0020,  -36309.0312],\n",
      "         [  -1712.9902, -130358.0000,   -3573.9727,  ...,   -1943.9883,\n",
      "            -7976.0078,  -13413.9902]]]),) and output (tensor([[[ -24082.0000,   33387.0000,   15374.9961,  ...,   18359.9980,\n",
      "            -7500.0020,   11519.0020],\n",
      "         [ -28678.0039,     472.9766,    5041.9961,  ...,  -14083.9961,\n",
      "           -29974.9648,    9608.0000],\n",
      "         [ -27856.0000,  -23186.0293,    4068.0039,  ...,   12834.9883,\n",
      "             6904.9746,    2288.9980],\n",
      "         ...,\n",
      "         [ -28526.9961,   -2200.0117,   18216.0078,  ...,   14386.0137,\n",
      "             3351.0361,   -8384.0156],\n",
      "         [  21951.9883,  -17989.9727,   -9585.0000,  ...,  -30873.0625,\n",
      "            20689.0020,  -32748.0312],\n",
      "         [  -6615.9902, -123412.0000,   -3796.9727,  ...,   -6211.9883,\n",
      "            -5479.0078,  -14925.9902]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[ -24082.0000,   33387.0000,   15374.9961,  ...,   18359.9980,\n",
      "            -7500.0020,   11519.0020],\n",
      "         [ -28678.0039,     472.9766,    5041.9961,  ...,  -14083.9961,\n",
      "           -29974.9648,    9608.0000],\n",
      "         [ -27856.0000,  -23186.0293,    4068.0039,  ...,   12834.9883,\n",
      "             6904.9746,    2288.9980],\n",
      "         ...,\n",
      "         [ -28526.9961,   -2200.0117,   18216.0078,  ...,   14386.0137,\n",
      "             3351.0361,   -8384.0156],\n",
      "         [  21951.9883,  -17989.9727,   -9585.0000,  ...,  -30873.0625,\n",
      "            20689.0020,  -32748.0312],\n",
      "         [  -6615.9902, -123412.0000,   -3796.9727,  ...,   -6211.9883,\n",
      "            -5479.0078,  -14925.9902]]]),) and output (tensor([[[ -14869.0000,   32284.0000,    3742.9961,  ...,   12458.9980,\n",
      "            -5452.0020,    7590.0020],\n",
      "         [ -26070.0039,    3497.9766,    1560.9961,  ...,   -6998.9961,\n",
      "           -29992.9648,    -858.0000],\n",
      "         [ -26953.0000,  -21560.0293,    9854.0039,  ...,   18467.9883,\n",
      "              698.9746,   -3406.0020],\n",
      "         ...,\n",
      "         [ -27954.9961,   -7409.0117,   24502.0078,  ...,   14780.0137,\n",
      "            -1759.9639,  -13634.0156],\n",
      "         [  24487.9883,  -11765.9727,   -5101.0000,  ...,  -33587.0625,\n",
      "            19623.0020,  -31881.0312],\n",
      "         [ -19519.9902, -126697.0000,   -2012.9727,  ...,   -3549.9883,\n",
      "            -8901.0078,  -22440.9902]]]),)\n",
      "[Debug] Layer names being passed: ['model.layers.0', 'model.layers.1', 'model.layers.2', 'model.layers.3', 'model.layers.4', 'model.layers.5', 'model.layers.6', 'model.layers.7', 'model.layers.8', 'model.layers.9', 'model.layers.10', 'model.layers.11', 'model.layers.12', 'model.layers.13', 'model.layers.14', 'model.layers.15', 'model.layers.16', 'model.layers.17', 'model.layers.18', 'model.layers.19', 'model.layers.20', 'model.layers.21', 'model.layers.22', 'model.layers.23', 'model.layers.24', 'model.layers.25', 'model.layers.26', 'model.layers.27', 'model.embed_tokens']\n",
      "[Debug] Trying to access layer: model.layers.0\n",
      "[Debug] Successfully found layer: model.layers.0\n",
      "[Debug] Trying to access layer: model.layers.1\n",
      "[Debug] Successfully found layer: model.layers.1\n",
      "[Debug] Trying to access layer: model.layers.2\n",
      "[Debug] Successfully found layer: model.layers.2\n",
      "[Debug] Trying to access layer: model.layers.3\n",
      "[Debug] Successfully found layer: model.layers.3\n",
      "[Debug] Trying to access layer: model.layers.4\n",
      "[Debug] Successfully found layer: model.layers.4\n",
      "[Debug] Trying to access layer: model.layers.5\n",
      "[Debug] Successfully found layer: model.layers.5\n",
      "[Debug] Trying to access layer: model.layers.6\n",
      "[Debug] Successfully found layer: model.layers.6\n",
      "[Debug] Trying to access layer: model.layers.7\n",
      "[Debug] Successfully found layer: model.layers.7\n",
      "[Debug] Trying to access layer: model.layers.8\n",
      "[Debug] Successfully found layer: model.layers.8\n",
      "[Debug] Trying to access layer: model.layers.9\n",
      "[Debug] Successfully found layer: model.layers.9\n",
      "[Debug] Trying to access layer: model.layers.10\n",
      "[Debug] Successfully found layer: model.layers.10\n",
      "[Debug] Trying to access layer: model.layers.11\n",
      "[Debug] Successfully found layer: model.layers.11\n",
      "[Debug] Trying to access layer: model.layers.12\n",
      "[Debug] Successfully found layer: model.layers.12\n",
      "[Debug] Trying to access layer: model.layers.13\n",
      "[Debug] Successfully found layer: model.layers.13\n",
      "[Debug] Trying to access layer: model.layers.14\n",
      "[Debug] Successfully found layer: model.layers.14\n",
      "[Debug] Trying to access layer: model.layers.15\n",
      "[Debug] Successfully found layer: model.layers.15\n",
      "[Debug] Trying to access layer: model.layers.16\n",
      "[Debug] Successfully found layer: model.layers.16\n",
      "[Debug] Trying to access layer: model.layers.17\n",
      "[Debug] Successfully found layer: model.layers.17\n",
      "[Debug] Trying to access layer: model.layers.18\n",
      "[Debug] Successfully found layer: model.layers.18\n",
      "[Debug] Trying to access layer: model.layers.19\n",
      "[Debug] Successfully found layer: model.layers.19\n",
      "[Debug] Trying to access layer: model.layers.20\n",
      "[Debug] Successfully found layer: model.layers.20\n",
      "[Debug] Trying to access layer: model.layers.21\n",
      "[Debug] Successfully found layer: model.layers.21\n",
      "[Debug] Trying to access layer: model.layers.22\n",
      "[Debug] Successfully found layer: model.layers.22\n",
      "[Debug] Trying to access layer: model.layers.23\n",
      "[Debug] Successfully found layer: model.layers.23\n",
      "[Debug] Trying to access layer: model.layers.24\n",
      "[Debug] Successfully found layer: model.layers.24\n",
      "[Debug] Trying to access layer: model.layers.25\n",
      "[Debug] Successfully found layer: model.layers.25\n",
      "[Debug] Trying to access layer: model.layers.26\n",
      "[Debug] Successfully found layer: model.layers.26\n",
      "[Debug] Trying to access layer: model.layers.27\n",
      "[Debug] Successfully found layer: model.layers.27\n",
      "[Debug] Trying to access layer: model.embed_tokens\n",
      "[Debug] Successfully found layer: model.embed_tokens\n",
      "[Hook] Layer Embedding(128256, 3072, padding_idx=128004) received input (tensor([[128000,     37,  11975,    596,   5492,    369,  77533,   3367,  23323,\n",
      "          14583,  72785,    268,  50178,    279,   4101,   1306,  48810,   1403,\n",
      "          12875,    505,    459,  10065,  23756,    323,  19486,    279,   7434,\n",
      "            311,  51052,   4885,     13,    578,   1501,   1176,  85170,    389,\n",
      "          69530,   8304,    389,   6287,    220,   1032,     11,    220,   1049,\n",
      "             19,     11,    439,    264,    220,   1954,  24401,  12707,   4632,\n",
      "             13,   1952,   6287,    220,    508,     11,    433,   6137,   1202,\n",
      "           4725,   1629,    315,  17510,   4791,   7716,  16938,  24401,  18243,\n",
      "            389,  80523,     11,    520,    220,     22,   9012,     13,    578,\n",
      "           4101,   8220,   1202,   1629,    389,   3297,    220,     18,     11,\n",
      "            220,   1049,     24,     11,    449,    264,   2860,    315,   4848,\n",
      "          15956,    323,  71049,  86703,  18243,     13,  14583,  72785,    268,\n",
      "           2163,  69530,   8304,  20193,   1306,    279,   4101,   9670,     13,\n",
      "            432,    261,  11099,    617,  43087,    389,   2577,  26429,    526,\n",
      "            505,   6287,    220,    806,     11,    220,    679,     17,    311,\n",
      "           6841,    220,     18,     11,    220,    679,     18,    323,   1578,\n",
      "            505,   5651,    220,     16,     11,    220,    679,     19,    311,\n",
      "           5936,    220,     18,     11,    220,    679,     22,     13]]),) and output tensor([[[-0.0010, -0.0007, -0.0046,  ..., -0.0014, -0.0020,  0.0020],\n",
      "         [ 0.0049, -0.0220, -0.0010,  ...,  0.0149,  0.0195,  0.0210],\n",
      "         [ 0.0530,  0.0087, -0.0042,  ...,  0.0056,  0.0081,  0.0043],\n",
      "         ...,\n",
      "         [ 0.0240, -0.0150, -0.0019,  ..., -0.0222, -0.0195,  0.0127],\n",
      "         [ 0.0079,  0.0114,  0.0211,  ...,  0.0079, -0.0139, -0.0129],\n",
      "         [ 0.0093, -0.0031,  0.0278,  ...,  0.0117, -0.0084,  0.0096]]])\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[-0.0010, -0.0007, -0.0046,  ..., -0.0014, -0.0020,  0.0020],\n",
      "         [ 0.0049, -0.0220, -0.0010,  ...,  0.0149,  0.0195,  0.0210],\n",
      "         [ 0.0530,  0.0087, -0.0042,  ...,  0.0056,  0.0081,  0.0043],\n",
      "         ...,\n",
      "         [ 0.0240, -0.0150, -0.0019,  ..., -0.0222, -0.0195,  0.0127],\n",
      "         [ 0.0079,  0.0114,  0.0211,  ...,  0.0079, -0.0139, -0.0129],\n",
      "         [ 0.0093, -0.0031,  0.0278,  ...,  0.0117, -0.0084,  0.0096]]]),) and output (tensor([[[ 442.9990,  239.9993,  302.9954,  ..., -154.0014,  -15.0020,\n",
      "           185.0020],\n",
      "         [  29.0049,   87.9780, -109.0010,  ...,  -95.9851,   35.0195,\n",
      "            24.0210],\n",
      "         [ 107.0530,  190.0087, -149.0042,  ..., -213.9944,  -22.9919,\n",
      "          -318.9957],\n",
      "         ...,\n",
      "         [   3.0240,  -30.0150, -190.0019,  ..., -192.0222,    8.9805,\n",
      "          -191.9873],\n",
      "         [ 159.0079,  -97.9886,  -87.9789,  ...,  -75.9921,   32.9861,\n",
      "           166.9871],\n",
      "         [  -2.9907,  -12.0031,  -59.9722,  ...,   -9.9883,  129.9916,\n",
      "           -91.9904]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[ 442.9990,  239.9993,  302.9954,  ..., -154.0014,  -15.0020,\n",
      "           185.0020],\n",
      "         [  29.0049,   87.9780, -109.0010,  ...,  -95.9851,   35.0195,\n",
      "            24.0210],\n",
      "         [ 107.0530,  190.0087, -149.0042,  ..., -213.9944,  -22.9919,\n",
      "          -318.9957],\n",
      "         ...,\n",
      "         [   3.0240,  -30.0150, -190.0019,  ..., -192.0222,    8.9805,\n",
      "          -191.9873],\n",
      "         [ 159.0079,  -97.9886,  -87.9789,  ...,  -75.9921,   32.9861,\n",
      "           166.9871],\n",
      "         [  -2.9907,  -12.0031,  -59.9722,  ...,   -9.9883,  129.9916,\n",
      "           -91.9904]]]),) and output (tensor([[[  439.9990,  -144.0007,  -410.0046,  ...,  -765.0015,\n",
      "           1193.9979, -1027.9980],\n",
      "         [ -104.9951,  -619.0220,  -160.0010,  ...,  -452.9851,\n",
      "            994.0195,  -306.9790],\n",
      "         [  369.0530,  1387.0087,  -332.0042,  ...,  -371.9944,\n",
      "           1316.0081,  -155.9957],\n",
      "         ...,\n",
      "         [ -906.9760, -1005.0150,  -419.0019,  ...,   -85.0222,\n",
      "           -658.0195, -3948.9873],\n",
      "         [-1039.9921,   -29.9886,  1458.0211,  ...,  -514.9921,\n",
      "            493.9861, -1418.0129],\n",
      "         [ -315.9907,  -784.0031,  -377.9722,  ..., -1332.9883,\n",
      "            -35.0084, -1054.9904]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[  439.9990,  -144.0007,  -410.0046,  ...,  -765.0015,\n",
      "           1193.9979, -1027.9980],\n",
      "         [ -104.9951,  -619.0220,  -160.0010,  ...,  -452.9851,\n",
      "            994.0195,  -306.9790],\n",
      "         [  369.0530,  1387.0087,  -332.0042,  ...,  -371.9944,\n",
      "           1316.0081,  -155.9957],\n",
      "         ...,\n",
      "         [ -906.9760, -1005.0150,  -419.0019,  ...,   -85.0222,\n",
      "           -658.0195, -3948.9873],\n",
      "         [-1039.9921,   -29.9886,  1458.0211,  ...,  -514.9921,\n",
      "            493.9861, -1418.0129],\n",
      "         [ -315.9907,  -784.0031,  -377.9722,  ..., -1332.9883,\n",
      "            -35.0084, -1054.9904]]]),) and output (tensor([[[   551.9990,    993.9993,  -1790.0046,  ...,    445.9985,\n",
      "           -1386.0021,  -1245.9980],\n",
      "         [  1254.0049,    451.9780,    488.9990,  ...,  -1745.9851,\n",
      "            -655.9805,  -4584.9790],\n",
      "         [  2968.0530,   1423.0088,   3216.9958,  ...,  -3752.9944,\n",
      "            5948.0078,    151.0043],\n",
      "         ...,\n",
      "         [  -413.9761,  -1453.0150,  -3301.0020,  ...,  -3178.0222,\n",
      "           -2237.0195, -15658.9873],\n",
      "         [ -2515.9922,   2395.0115,   1570.0211,  ...,  -3869.9922,\n",
      "            1008.9861,  -1369.0129],\n",
      "         [ -2327.9907,  -3483.0029,  -4449.9722,  ...,   1209.0117,\n",
      "            -452.0084,   2352.0098]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[   551.9990,    993.9993,  -1790.0046,  ...,    445.9985,\n",
      "           -1386.0021,  -1245.9980],\n",
      "         [  1254.0049,    451.9780,    488.9990,  ...,  -1745.9851,\n",
      "            -655.9805,  -4584.9790],\n",
      "         [  2968.0530,   1423.0088,   3216.9958,  ...,  -3752.9944,\n",
      "            5948.0078,    151.0043],\n",
      "         ...,\n",
      "         [  -413.9761,  -1453.0150,  -3301.0020,  ...,  -3178.0222,\n",
      "           -2237.0195, -15658.9873],\n",
      "         [ -2515.9922,   2395.0115,   1570.0211,  ...,  -3869.9922,\n",
      "            1008.9861,  -1369.0129],\n",
      "         [ -2327.9907,  -3483.0029,  -4449.9722,  ...,   1209.0117,\n",
      "            -452.0084,   2352.0098]]]),) and output (tensor([[[ 2.1320e+03, -7.3700e+02, -1.0710e+03,  ..., -2.7810e+03,\n",
      "          -8.8500e+02,  8.6002e+01],\n",
      "         [ 2.5990e+03, -4.3810e+03,  3.7030e+03,  ..., -3.5710e+03,\n",
      "           3.4190e+03, -5.8090e+03],\n",
      "         [-1.9439e+03,  4.6820e+03,  2.0130e+03,  ..., -2.9200e+03,\n",
      "           2.8210e+03, -4.2630e+03],\n",
      "         ...,\n",
      "         [ 2.4030e+03, -5.4030e+03, -4.4850e+03,  ..., -2.6250e+03,\n",
      "          -2.4790e+03, -1.7421e+04],\n",
      "         [-5.0360e+03,  1.9740e+03,  3.8500e+03,  ..., -3.9800e+03,\n",
      "           2.1300e+03, -5.4410e+03],\n",
      "         [-3.7590e+03, -2.9050e+03, -2.7430e+03,  ...,  4.0117e+00,\n",
      "          -1.5760e+03, -1.3120e+03]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[ 2.1320e+03, -7.3700e+02, -1.0710e+03,  ..., -2.7810e+03,\n",
      "          -8.8500e+02,  8.6002e+01],\n",
      "         [ 2.5990e+03, -4.3810e+03,  3.7030e+03,  ..., -3.5710e+03,\n",
      "           3.4190e+03, -5.8090e+03],\n",
      "         [-1.9439e+03,  4.6820e+03,  2.0130e+03,  ..., -2.9200e+03,\n",
      "           2.8210e+03, -4.2630e+03],\n",
      "         ...,\n",
      "         [ 2.4030e+03, -5.4030e+03, -4.4850e+03,  ..., -2.6250e+03,\n",
      "          -2.4790e+03, -1.7421e+04],\n",
      "         [-5.0360e+03,  1.9740e+03,  3.8500e+03,  ..., -3.9800e+03,\n",
      "           2.1300e+03, -5.4410e+03],\n",
      "         [-3.7590e+03, -2.9050e+03, -2.7430e+03,  ...,  4.0117e+00,\n",
      "          -1.5760e+03, -1.3120e+03]]]),) and output (tensor([[[  3420.9990,    760.9993,  -5376.0049,  ...,  -4193.0015,\n",
      "            2860.9980,  -4080.9980],\n",
      "         [  4988.0049,  -4478.0220,   3592.9990,  ...,  -8177.9854,\n",
      "             365.0195,  -4302.9790],\n",
      "         [ -3320.9470,  -1185.9912,  -5910.0039,  ...,   3218.0056,\n",
      "            -915.9922,   -466.9956],\n",
      "         ...,\n",
      "         [ -2184.9761,  -7548.0151,  -6007.0020,  ...,   -802.0222,\n",
      "           -4209.0195, -21265.9883],\n",
      "         [ -2038.9922,   -204.9885,   6036.0210,  ...,   -417.9922,\n",
      "            1270.9861, -11163.0127],\n",
      "         [ -8994.9902,  -2416.0029,  -8180.9722,  ...,   4066.0117,\n",
      "           -4906.0083,  -4826.9902]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[  3420.9990,    760.9993,  -5376.0049,  ...,  -4193.0015,\n",
      "            2860.9980,  -4080.9980],\n",
      "         [  4988.0049,  -4478.0220,   3592.9990,  ...,  -8177.9854,\n",
      "             365.0195,  -4302.9790],\n",
      "         [ -3320.9470,  -1185.9912,  -5910.0039,  ...,   3218.0056,\n",
      "            -915.9922,   -466.9956],\n",
      "         ...,\n",
      "         [ -2184.9761,  -7548.0151,  -6007.0020,  ...,   -802.0222,\n",
      "           -4209.0195, -21265.9883],\n",
      "         [ -2038.9922,   -204.9885,   6036.0210,  ...,   -417.9922,\n",
      "            1270.9861, -11163.0127],\n",
      "         [ -8994.9902,  -2416.0029,  -8180.9722,  ...,   4066.0117,\n",
      "           -4906.0083,  -4826.9902]]]),) and output (tensor([[[  2742.9990,  -1512.0007, -10493.0049,  ...,  -9945.0020,\n",
      "           -2017.0020,  -1802.9980],\n",
      "         [  4510.0049,  -4868.0220,   3688.9990,  ...,  -9181.9854,\n",
      "           -2034.9805,  -8314.9785],\n",
      "         [ -6513.9473,  -3173.9912,  -9882.0039,  ...,  -2172.9944,\n",
      "           -1939.9922,   1786.0044],\n",
      "         ...,\n",
      "         [ -5236.9761,  -7362.0156,  -4145.0020,  ...,  -1776.0222,\n",
      "           -7049.0195, -24688.9883],\n",
      "         [   458.0078,  -3370.9885,   3188.0210,  ...,   4275.0078,\n",
      "            8205.9863, -15363.0127],\n",
      "         [ -8347.9902,  -8498.0029,  -7144.9727,  ...,   6462.0117,\n",
      "          -10217.0078,  -5217.9902]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[  2742.9990,  -1512.0007, -10493.0049,  ...,  -9945.0020,\n",
      "           -2017.0020,  -1802.9980],\n",
      "         [  4510.0049,  -4868.0220,   3688.9990,  ...,  -9181.9854,\n",
      "           -2034.9805,  -8314.9785],\n",
      "         [ -6513.9473,  -3173.9912,  -9882.0039,  ...,  -2172.9944,\n",
      "           -1939.9922,   1786.0044],\n",
      "         ...,\n",
      "         [ -5236.9761,  -7362.0156,  -4145.0020,  ...,  -1776.0222,\n",
      "           -7049.0195, -24688.9883],\n",
      "         [   458.0078,  -3370.9885,   3188.0210,  ...,   4275.0078,\n",
      "            8205.9863, -15363.0127],\n",
      "         [ -8347.9902,  -8498.0029,  -7144.9727,  ...,   6462.0117,\n",
      "          -10217.0078,  -5217.9902]]]),) and output (tensor([[[  3032.9990,  -3420.0007, -10291.0049,  ...,  -6511.0020,\n",
      "           -1953.0020,   2001.0020],\n",
      "         [  5934.0049,  -7990.0220,   6269.9990,  ...,  -5440.9854,\n",
      "           -5281.9805,  -7309.9785],\n",
      "         [ -4760.9473,  -5884.9912, -10201.0039,  ...,   1284.0056,\n",
      "           -3866.9922,   1039.0044],\n",
      "         ...,\n",
      "         [ -1912.9761,  -4926.0156,  -5558.0020,  ...,  -3289.0222,\n",
      "           -9652.0195, -29257.9883],\n",
      "         [ -3651.9922,   -737.9885,   3395.0210,  ...,   4185.0078,\n",
      "           12891.9863, -28893.0117],\n",
      "         [ -6183.9902,  -5724.0029,  -3971.9727,  ...,   9975.0117,\n",
      "          -14472.0078,  -5574.9902]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[  3032.9990,  -3420.0007, -10291.0049,  ...,  -6511.0020,\n",
      "           -1953.0020,   2001.0020],\n",
      "         [  5934.0049,  -7990.0220,   6269.9990,  ...,  -5440.9854,\n",
      "           -5281.9805,  -7309.9785],\n",
      "         [ -4760.9473,  -5884.9912, -10201.0039,  ...,   1284.0056,\n",
      "           -3866.9922,   1039.0044],\n",
      "         ...,\n",
      "         [ -1912.9761,  -4926.0156,  -5558.0020,  ...,  -3289.0222,\n",
      "           -9652.0195, -29257.9883],\n",
      "         [ -3651.9922,   -737.9885,   3395.0210,  ...,   4185.0078,\n",
      "           12891.9863, -28893.0117],\n",
      "         [ -6183.9902,  -5724.0029,  -3971.9727,  ...,   9975.0117,\n",
      "          -14472.0078,  -5574.9902]]]),) and output (tensor([[[  3132.9990,  -1970.0007, -11544.0049,  ...,  -5780.0020,\n",
      "           -2486.0020,  -1921.9980],\n",
      "         [  8491.0049,  -6173.0220,   2976.9990,  ...,  -7562.9854,\n",
      "             410.0195,  -4479.9785],\n",
      "         [ -2439.9473,  -9928.9912,  -7922.0039,  ...,   6199.0059,\n",
      "           -3875.9922,  -1131.9956],\n",
      "         ...,\n",
      "         [  1259.0239,  -1518.0156,  -4946.0020,  ...,  -3790.0222,\n",
      "           -7659.0195, -32755.9883],\n",
      "         [ -1572.9922,  -3462.9885,   3070.0210,  ...,   -716.9922,\n",
      "           14050.9863, -32639.0117],\n",
      "         [ -7685.9902, -11118.0029,  -3263.9727,  ...,  13373.0117,\n",
      "          -15242.0078,  -3171.9902]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[  3132.9990,  -1970.0007, -11544.0049,  ...,  -5780.0020,\n",
      "           -2486.0020,  -1921.9980],\n",
      "         [  8491.0049,  -6173.0220,   2976.9990,  ...,  -7562.9854,\n",
      "             410.0195,  -4479.9785],\n",
      "         [ -2439.9473,  -9928.9912,  -7922.0039,  ...,   6199.0059,\n",
      "           -3875.9922,  -1131.9956],\n",
      "         ...,\n",
      "         [  1259.0239,  -1518.0156,  -4946.0020,  ...,  -3790.0222,\n",
      "           -7659.0195, -32755.9883],\n",
      "         [ -1572.9922,  -3462.9885,   3070.0210,  ...,   -716.9922,\n",
      "           14050.9863, -32639.0117],\n",
      "         [ -7685.9902, -11118.0029,  -3263.9727,  ...,  13373.0117,\n",
      "          -15242.0078,  -3171.9902]]]),) and output (tensor([[[  1046.9990,   2403.9993, -14486.0049,  ..., -10922.0020,\n",
      "           -1419.0020,  -2983.9980],\n",
      "         [  4518.0049, -12306.0215,   2060.9990,  ...,  -5020.9854,\n",
      "           -2871.9805,  -2788.9785],\n",
      "         [ -9049.9473, -13423.9912,  -4604.0039,  ...,   3603.0059,\n",
      "           -3194.9922,  -1973.9956],\n",
      "         ...,\n",
      "         [  2008.0239,  -2785.0156,  -5820.0020,  ...,  -1332.0225,\n",
      "           -7192.0195, -36007.9883],\n",
      "         [    58.0078,  -3395.9883,   1266.0210,  ...,  -3781.9922,\n",
      "           17200.9863, -40932.0117],\n",
      "         [ -7150.9902, -13292.0029,  -7588.9727,  ...,  11558.0117,\n",
      "          -17904.0078,   1263.0098]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[  1046.9990,   2403.9993, -14486.0049,  ..., -10922.0020,\n",
      "           -1419.0020,  -2983.9980],\n",
      "         [  4518.0049, -12306.0215,   2060.9990,  ...,  -5020.9854,\n",
      "           -2871.9805,  -2788.9785],\n",
      "         [ -9049.9473, -13423.9912,  -4604.0039,  ...,   3603.0059,\n",
      "           -3194.9922,  -1973.9956],\n",
      "         ...,\n",
      "         [  2008.0239,  -2785.0156,  -5820.0020,  ...,  -1332.0225,\n",
      "           -7192.0195, -36007.9883],\n",
      "         [    58.0078,  -3395.9883,   1266.0210,  ...,  -3781.9922,\n",
      "           17200.9863, -40932.0117],\n",
      "         [ -7150.9902, -13292.0029,  -7588.9727,  ...,  11558.0117,\n",
      "          -17904.0078,   1263.0098]]]),) and output (tensor([[[ -3767.0010,   5307.9990, -14221.0049,  ..., -15035.0020,\n",
      "           -3630.0020,   4168.0020],\n",
      "         [   631.0049, -10027.0215,   2024.9990,  ...,  -4073.9854,\n",
      "           -3042.9805,  -6636.9785],\n",
      "         [-10239.9473, -15398.9912,  -1091.0039,  ...,   2837.0059,\n",
      "           -6292.9922,  -3454.9956],\n",
      "         ...,\n",
      "         [  6107.0239,  -9404.0156,  -7371.0020,  ...,   2832.9775,\n",
      "          -10065.0195, -40739.9883],\n",
      "         [   899.0078,  -5547.9883,   -511.9790,  ...,  -6256.9922,\n",
      "           16008.9863, -41810.0117],\n",
      "         [ -2987.9902,  -7242.0029,  -7161.9727,  ...,   8528.0117,\n",
      "          -15934.0078,  -3656.9902]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[ -3767.0010,   5307.9990, -14221.0049,  ..., -15035.0020,\n",
      "           -3630.0020,   4168.0020],\n",
      "         [   631.0049, -10027.0215,   2024.9990,  ...,  -4073.9854,\n",
      "           -3042.9805,  -6636.9785],\n",
      "         [-10239.9473, -15398.9912,  -1091.0039,  ...,   2837.0059,\n",
      "           -6292.9922,  -3454.9956],\n",
      "         ...,\n",
      "         [  6107.0239,  -9404.0156,  -7371.0020,  ...,   2832.9775,\n",
      "          -10065.0195, -40739.9883],\n",
      "         [   899.0078,  -5547.9883,   -511.9790,  ...,  -6256.9922,\n",
      "           16008.9863, -41810.0117],\n",
      "         [ -2987.9902,  -7242.0029,  -7161.9727,  ...,   8528.0117,\n",
      "          -15934.0078,  -3656.9902]]]),) and output (tensor([[[ -1326.0010,   1223.9990, -16643.0039,  ..., -13240.0020,\n",
      "           -5677.0020,   9639.0020],\n",
      "         [  7631.0049, -12423.0215,    -58.0010,  ...,  -3201.9854,\n",
      "           -3528.9805,  -8901.9785],\n",
      "         [ -8885.9473, -15356.9912,   1274.9961,  ...,    433.0059,\n",
      "           -3035.9922,  -3389.9956],\n",
      "         ...,\n",
      "         [  7358.0239, -10214.0156,  -5500.0020,  ...,    616.9775,\n",
      "          -10563.0195, -44224.9883],\n",
      "         [  1291.0078,  -5851.9883,    165.0210,  ..., -11274.9922,\n",
      "           15039.9863, -50219.0117],\n",
      "         [ -5697.9902,   -901.0029, -10577.9727,  ...,  11027.0117,\n",
      "          -16064.0078,  -2740.9902]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[ -1326.0010,   1223.9990, -16643.0039,  ..., -13240.0020,\n",
      "           -5677.0020,   9639.0020],\n",
      "         [  7631.0049, -12423.0215,    -58.0010,  ...,  -3201.9854,\n",
      "           -3528.9805,  -8901.9785],\n",
      "         [ -8885.9473, -15356.9912,   1274.9961,  ...,    433.0059,\n",
      "           -3035.9922,  -3389.9956],\n",
      "         ...,\n",
      "         [  7358.0239, -10214.0156,  -5500.0020,  ...,    616.9775,\n",
      "          -10563.0195, -44224.9883],\n",
      "         [  1291.0078,  -5851.9883,    165.0210,  ..., -11274.9922,\n",
      "           15039.9863, -50219.0117],\n",
      "         [ -5697.9902,   -901.0029, -10577.9727,  ...,  11027.0117,\n",
      "          -16064.0078,  -2740.9902]]]),) and output (tensor([[[ -5120.0010,   2246.9990, -15993.0039,  ..., -19818.0020,\n",
      "           -9385.0020,  11924.0020],\n",
      "         [  7455.0049,  -8807.0215,   1540.9990,  ...,  -7628.9854,\n",
      "           -5788.9805,  -3534.9785],\n",
      "         [ -6062.9473,  -9901.9912,   4678.9961,  ...,   -280.9941,\n",
      "           -7949.9922,  -3456.9956],\n",
      "         ...,\n",
      "         [ 14098.0234, -13960.0156,  -7938.0020,  ...,    969.9775,\n",
      "          -10947.0195, -50811.9883],\n",
      "         [  6370.0078,   1792.0117,   3325.0210,  ..., -13835.9922,\n",
      "           20004.9863, -53640.0117],\n",
      "         [ -7379.9902,   -498.0029, -10979.9727,  ...,   9638.0117,\n",
      "          -15668.0078,  -2744.9902]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[ -5120.0010,   2246.9990, -15993.0039,  ..., -19818.0020,\n",
      "           -9385.0020,  11924.0020],\n",
      "         [  7455.0049,  -8807.0215,   1540.9990,  ...,  -7628.9854,\n",
      "           -5788.9805,  -3534.9785],\n",
      "         [ -6062.9473,  -9901.9912,   4678.9961,  ...,   -280.9941,\n",
      "           -7949.9922,  -3456.9956],\n",
      "         ...,\n",
      "         [ 14098.0234, -13960.0156,  -7938.0020,  ...,    969.9775,\n",
      "          -10947.0195, -50811.9883],\n",
      "         [  6370.0078,   1792.0117,   3325.0210,  ..., -13835.9922,\n",
      "           20004.9863, -53640.0117],\n",
      "         [ -7379.9902,   -498.0029, -10979.9727,  ...,   9638.0117,\n",
      "          -15668.0078,  -2744.9902]]]),) and output (tensor([[[ -6685.0010,  -1278.0010, -15553.0039,  ..., -19846.0020,\n",
      "          -10418.0020,  13376.0020],\n",
      "         [  2879.0049,  -8990.0215,   1430.9990,  ...,  -6373.9854,\n",
      "           -7662.9805,  -2659.9785],\n",
      "         [-13483.9473,  -9622.9912,   5033.9961,  ...,  -5010.9941,\n",
      "            -419.9922,   5144.0044],\n",
      "         ...,\n",
      "         [ 19498.0234, -15758.0156,  -9709.0020,  ...,  -1714.0225,\n",
      "          -10847.0195, -54502.9883],\n",
      "         [ 13720.0078,   1204.0117,  -4785.9790,  ..., -15587.9922,\n",
      "           12238.9863, -58633.0117],\n",
      "         [ -3472.9902,  -1587.0029, -13137.9727,  ...,  12259.0117,\n",
      "          -16708.0078,   1798.0098]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[ -6685.0010,  -1278.0010, -15553.0039,  ..., -19846.0020,\n",
      "          -10418.0020,  13376.0020],\n",
      "         [  2879.0049,  -8990.0215,   1430.9990,  ...,  -6373.9854,\n",
      "           -7662.9805,  -2659.9785],\n",
      "         [-13483.9473,  -9622.9912,   5033.9961,  ...,  -5010.9941,\n",
      "            -419.9922,   5144.0044],\n",
      "         ...,\n",
      "         [ 19498.0234, -15758.0156,  -9709.0020,  ...,  -1714.0225,\n",
      "          -10847.0195, -54502.9883],\n",
      "         [ 13720.0078,   1204.0117,  -4785.9790,  ..., -15587.9922,\n",
      "           12238.9863, -58633.0117],\n",
      "         [ -3472.9902,  -1587.0029, -13137.9727,  ...,  12259.0117,\n",
      "          -16708.0078,   1798.0098]]]),) and output (tensor([[[ -8422.0010,    339.9990, -15289.0039,  ..., -20884.0020,\n",
      "           -6908.0020,  11014.0020],\n",
      "         [   -87.9951, -15491.0215,   -158.0010,  ...,  -7859.9854,\n",
      "          -10565.9805,   -489.9785],\n",
      "         [-11614.9473,  -9847.9912,   1276.9961,  ...,  -6202.9941,\n",
      "            3091.0078,   9901.0039],\n",
      "         ...,\n",
      "         [ 24330.0234, -17131.0156,  -8459.0020,  ...,   -937.0225,\n",
      "          -12978.0195, -61847.9883],\n",
      "         [ 14237.0078,    885.0117,  -7105.9790,  ..., -17832.9922,\n",
      "            9951.9863, -63879.0117],\n",
      "         [  3637.0098,    848.9971, -13878.9727,  ...,  17134.0117,\n",
      "          -19909.0078,  -2593.9902]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[ -8422.0010,    339.9990, -15289.0039,  ..., -20884.0020,\n",
      "           -6908.0020,  11014.0020],\n",
      "         [   -87.9951, -15491.0215,   -158.0010,  ...,  -7859.9854,\n",
      "          -10565.9805,   -489.9785],\n",
      "         [-11614.9473,  -9847.9912,   1276.9961,  ...,  -6202.9941,\n",
      "            3091.0078,   9901.0039],\n",
      "         ...,\n",
      "         [ 24330.0234, -17131.0156,  -8459.0020,  ...,   -937.0225,\n",
      "          -12978.0195, -61847.9883],\n",
      "         [ 14237.0078,    885.0117,  -7105.9790,  ..., -17832.9922,\n",
      "            9951.9863, -63879.0117],\n",
      "         [  3637.0098,    848.9971, -13878.9727,  ...,  17134.0117,\n",
      "          -19909.0078,  -2593.9902]]]),) and output (tensor([[[-10989.0010,   7034.9990, -13964.0039,  ..., -23968.0020,\n",
      "           -3749.0020,  11730.0020],\n",
      "         [   -82.9951, -17347.0215,   1929.9990,  ..., -13217.9854,\n",
      "           -8092.9805,   3369.0215],\n",
      "         [ -6598.9473, -17167.9922,   1846.9961,  ...,    687.0059,\n",
      "             918.0078,   3422.0039],\n",
      "         ...,\n",
      "         [ 23295.0234, -14669.0156,  -6750.0020,  ...,  -2926.0225,\n",
      "           -8414.0195, -67084.9844],\n",
      "         [ 19152.0078,   2139.0117,  -7437.9790,  ..., -22351.9922,\n",
      "            2568.9863, -69203.0156],\n",
      "         [  4346.0098,  -3078.0029, -12797.9727,  ...,  16490.0117,\n",
      "          -19907.0078,   1577.0098]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[-10989.0010,   7034.9990, -13964.0039,  ..., -23968.0020,\n",
      "           -3749.0020,  11730.0020],\n",
      "         [   -82.9951, -17347.0215,   1929.9990,  ..., -13217.9854,\n",
      "           -8092.9805,   3369.0215],\n",
      "         [ -6598.9473, -17167.9922,   1846.9961,  ...,    687.0059,\n",
      "             918.0078,   3422.0039],\n",
      "         ...,\n",
      "         [ 23295.0234, -14669.0156,  -6750.0020,  ...,  -2926.0225,\n",
      "           -8414.0195, -67084.9844],\n",
      "         [ 19152.0078,   2139.0117,  -7437.9790,  ..., -22351.9922,\n",
      "            2568.9863, -69203.0156],\n",
      "         [  4346.0098,  -3078.0029, -12797.9727,  ...,  16490.0117,\n",
      "          -19907.0078,   1577.0098]]]),) and output (tensor([[[ -7159.0010,   6621.9990, -10967.0039,  ..., -17477.0020,\n",
      "           -2510.0020,  11888.0020],\n",
      "         [ -5250.9951,  -8749.0215,    235.9990,  ..., -11870.9854,\n",
      "          -13695.9805,   1407.0215],\n",
      "         [ -4392.9473, -10644.9922,  -1808.0039,  ...,   1217.0059,\n",
      "            3620.0078,  -1234.9961],\n",
      "         ...,\n",
      "         [ 18352.0234, -14572.0156,  -3875.0020,  ...,  -8227.0225,\n",
      "           -4840.0195, -71809.9844],\n",
      "         [ 27938.0078,  -2323.9883,  -9908.9785,  ..., -28772.9922,\n",
      "            4361.9863, -72280.0156],\n",
      "         [  5637.0098,  -1116.0029, -12898.9727,  ...,  20796.0117,\n",
      "          -20861.0078,  -1314.9902]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[ -7159.0010,   6621.9990, -10967.0039,  ..., -17477.0020,\n",
      "           -2510.0020,  11888.0020],\n",
      "         [ -5250.9951,  -8749.0215,    235.9990,  ..., -11870.9854,\n",
      "          -13695.9805,   1407.0215],\n",
      "         [ -4392.9473, -10644.9922,  -1808.0039,  ...,   1217.0059,\n",
      "            3620.0078,  -1234.9961],\n",
      "         ...,\n",
      "         [ 18352.0234, -14572.0156,  -3875.0020,  ...,  -8227.0225,\n",
      "           -4840.0195, -71809.9844],\n",
      "         [ 27938.0078,  -2323.9883,  -9908.9785,  ..., -28772.9922,\n",
      "            4361.9863, -72280.0156],\n",
      "         [  5637.0098,  -1116.0029, -12898.9727,  ...,  20796.0117,\n",
      "          -20861.0078,  -1314.9902]]]),) and output (tensor([[[ -4822.0010,   3124.9990,  -3885.0039,  ..., -19007.0020,\n",
      "            -551.0020,  16285.0020],\n",
      "         [   700.0049, -12783.0215,   4554.9990,  ..., -11473.9854,\n",
      "          -13687.9805,  -1507.9785],\n",
      "         [ -7238.9473, -11307.9922,  -4520.0039,  ...,   -242.9941,\n",
      "            -535.9922,    569.0039],\n",
      "         ...,\n",
      "         [ 16020.0234, -13801.0156, -11011.0020,  ..., -12858.0225,\n",
      "           -6046.0195, -67778.9844],\n",
      "         [ 29258.0078,  -6989.9883,  -5365.9785,  ..., -33594.9922,\n",
      "           -4797.0137, -75015.0156],\n",
      "         [  5426.0098,  -2457.0029,  -8989.9727,  ...,  18878.0117,\n",
      "          -19658.0078,  -8322.9902]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[ -4822.0010,   3124.9990,  -3885.0039,  ..., -19007.0020,\n",
      "            -551.0020,  16285.0020],\n",
      "         [   700.0049, -12783.0215,   4554.9990,  ..., -11473.9854,\n",
      "          -13687.9805,  -1507.9785],\n",
      "         [ -7238.9473, -11307.9922,  -4520.0039,  ...,   -242.9941,\n",
      "            -535.9922,    569.0039],\n",
      "         ...,\n",
      "         [ 16020.0234, -13801.0156, -11011.0020,  ..., -12858.0225,\n",
      "           -6046.0195, -67778.9844],\n",
      "         [ 29258.0078,  -6989.9883,  -5365.9785,  ..., -33594.9922,\n",
      "           -4797.0137, -75015.0156],\n",
      "         [  5426.0098,  -2457.0029,  -8989.9727,  ...,  18878.0117,\n",
      "          -19658.0078,  -8322.9902]]]),) and output (tensor([[[ -7235.0010,   6583.9990,  -4212.0039,  ..., -19104.0020,\n",
      "            3087.9980,  17084.0020],\n",
      "         [ -6323.9951, -11114.0215,   4982.9990,  ..., -11964.9854,\n",
      "          -12380.9805,  -3056.9785],\n",
      "         [   272.0527,  -9643.9922,  -3025.0039,  ...,   1106.0059,\n",
      "            9826.0078,  -5400.9961],\n",
      "         ...,\n",
      "         [ 17012.0234, -18298.0156,  -6023.0020,  ...,  -3285.0225,\n",
      "           -4921.0195, -64651.9844],\n",
      "         [ 32875.0078,  -8937.9883,  -5340.9785,  ..., -27684.9922,\n",
      "           -7651.0137, -75089.0156],\n",
      "         [ 14012.0098,    -76.0029,  -9661.9727,  ...,  21451.0117,\n",
      "          -16944.0078,  -4053.9902]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[ -7235.0010,   6583.9990,  -4212.0039,  ..., -19104.0020,\n",
      "            3087.9980,  17084.0020],\n",
      "         [ -6323.9951, -11114.0215,   4982.9990,  ..., -11964.9854,\n",
      "          -12380.9805,  -3056.9785],\n",
      "         [   272.0527,  -9643.9922,  -3025.0039,  ...,   1106.0059,\n",
      "            9826.0078,  -5400.9961],\n",
      "         ...,\n",
      "         [ 17012.0234, -18298.0156,  -6023.0020,  ...,  -3285.0225,\n",
      "           -4921.0195, -64651.9844],\n",
      "         [ 32875.0078,  -8937.9883,  -5340.9785,  ..., -27684.9922,\n",
      "           -7651.0137, -75089.0156],\n",
      "         [ 14012.0098,    -76.0029,  -9661.9727,  ...,  21451.0117,\n",
      "          -16944.0078,  -4053.9902]]]),) and output (tensor([[[-5.5860e+03,  7.6460e+03, -5.0500e+02,  ..., -1.5346e+04,\n",
      "          -6.2300e+02,  1.2986e+04],\n",
      "         [-1.3479e+04, -1.1484e+04,  5.7190e+03,  ..., -1.5679e+04,\n",
      "          -1.3016e+04,  1.0870e+03],\n",
      "         [ 4.8041e+03, -1.0740e+04, -7.8600e+03,  ...,  2.4600e+03,\n",
      "           5.0330e+03, -1.2820e+03],\n",
      "         ...,\n",
      "         [ 1.9744e+04, -1.8161e+04, -1.1430e+03,  ...,  3.5098e+02,\n",
      "           5.5798e+02, -6.9444e+04],\n",
      "         [ 2.9745e+04, -7.5760e+03, -1.0269e+04,  ..., -2.7981e+04,\n",
      "           8.8199e+02, -8.1283e+04],\n",
      "         [ 2.0319e+04,  5.3997e+01, -9.0540e+03,  ...,  1.8606e+04,\n",
      "          -2.0537e+04, -2.8970e+03]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[-5.5860e+03,  7.6460e+03, -5.0500e+02,  ..., -1.5346e+04,\n",
      "          -6.2300e+02,  1.2986e+04],\n",
      "         [-1.3479e+04, -1.1484e+04,  5.7190e+03,  ..., -1.5679e+04,\n",
      "          -1.3016e+04,  1.0870e+03],\n",
      "         [ 4.8041e+03, -1.0740e+04, -7.8600e+03,  ...,  2.4600e+03,\n",
      "           5.0330e+03, -1.2820e+03],\n",
      "         ...,\n",
      "         [ 1.9744e+04, -1.8161e+04, -1.1430e+03,  ...,  3.5098e+02,\n",
      "           5.5798e+02, -6.9444e+04],\n",
      "         [ 2.9745e+04, -7.5760e+03, -1.0269e+04,  ..., -2.7981e+04,\n",
      "           8.8199e+02, -8.1283e+04],\n",
      "         [ 2.0319e+04,  5.3997e+01, -9.0540e+03,  ...,  1.8606e+04,\n",
      "          -2.0537e+04, -2.8970e+03]]]),) and output (tensor([[[ -8916.0010,   2362.9990,  -4833.0039,  ..., -12075.0020,\n",
      "           -1124.0020,  12438.0020],\n",
      "         [-14169.9951,  -7040.0215,  10431.9990,  ..., -11690.9844,\n",
      "          -10572.9805,  -3691.9785],\n",
      "         [  -873.9473, -18527.9922,  -2503.0039,  ...,  -1136.9941,\n",
      "            6735.0078,   6712.0039],\n",
      "         ...,\n",
      "         [  9054.0234, -22350.0156,  -2420.0020,  ...,   6475.9775,\n",
      "            3897.9805, -72433.9844],\n",
      "         [ 22680.0078,  -7226.9883, -10153.9785,  ..., -22138.9922,\n",
      "            6743.9863, -87740.0156],\n",
      "         [ 17443.0098,   4225.9971,  -9918.9727,  ...,  16178.0117,\n",
      "          -20652.0078,   1466.0098]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[ -8916.0010,   2362.9990,  -4833.0039,  ..., -12075.0020,\n",
      "           -1124.0020,  12438.0020],\n",
      "         [-14169.9951,  -7040.0215,  10431.9990,  ..., -11690.9844,\n",
      "          -10572.9805,  -3691.9785],\n",
      "         [  -873.9473, -18527.9922,  -2503.0039,  ...,  -1136.9941,\n",
      "            6735.0078,   6712.0039],\n",
      "         ...,\n",
      "         [  9054.0234, -22350.0156,  -2420.0020,  ...,   6475.9775,\n",
      "            3897.9805, -72433.9844],\n",
      "         [ 22680.0078,  -7226.9883, -10153.9785,  ..., -22138.9922,\n",
      "            6743.9863, -87740.0156],\n",
      "         [ 17443.0098,   4225.9971,  -9918.9727,  ...,  16178.0117,\n",
      "          -20652.0078,   1466.0098]]]),) and output (tensor([[[-20230.0000,   7877.9990,    161.9961,  ..., -10250.0020,\n",
      "             676.9980,   7095.0020],\n",
      "         [-17392.9961,   -408.0215,  15190.9990,  ...,  -2463.9844,\n",
      "          -11106.9805, -12616.9785],\n",
      "         [ -7380.9473, -21057.9922,   -598.0039,  ...,   2544.0059,\n",
      "            9941.0078,   8030.0039],\n",
      "         ...,\n",
      "         [  6402.0234, -19615.0156,  -7655.0020,  ...,   4820.9775,\n",
      "            4638.9805, -74647.9844],\n",
      "         [ 23117.0078,  -9452.9883, -15052.9785,  ..., -16789.9922,\n",
      "           13007.9863, -87068.0156],\n",
      "         [ 22724.0098,   1234.9971,  -7774.9727,  ...,  21371.0117,\n",
      "          -22424.0078,   1614.0098]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[-20230.0000,   7877.9990,    161.9961,  ..., -10250.0020,\n",
      "             676.9980,   7095.0020],\n",
      "         [-17392.9961,   -408.0215,  15190.9990,  ...,  -2463.9844,\n",
      "          -11106.9805, -12616.9785],\n",
      "         [ -7380.9473, -21057.9922,   -598.0039,  ...,   2544.0059,\n",
      "            9941.0078,   8030.0039],\n",
      "         ...,\n",
      "         [  6402.0234, -19615.0156,  -7655.0020,  ...,   4820.9775,\n",
      "            4638.9805, -74647.9844],\n",
      "         [ 23117.0078,  -9452.9883, -15052.9785,  ..., -16789.9922,\n",
      "           13007.9863, -87068.0156],\n",
      "         [ 22724.0098,   1234.9971,  -7774.9727,  ...,  21371.0117,\n",
      "          -22424.0078,   1614.0098]]]),) and output (tensor([[[-25670.0000,  13293.9990,   9099.9961,  ...,  -6547.0020,\n",
      "           -5658.0020,   4192.0020],\n",
      "         [-18991.9961,  -6067.0215,  22923.0000,  ...,   3081.0156,\n",
      "          -21643.9805, -11096.9785],\n",
      "         [ -6973.9473, -17975.9922,   3733.9961,  ...,   -777.9941,\n",
      "           10567.0078,  12894.0039],\n",
      "         ...,\n",
      "         [  9907.0234, -12174.0156,  -2643.0020,  ...,   4182.9775,\n",
      "            2119.9805, -79239.9844],\n",
      "         [ 16845.0078, -10166.9883, -10430.9785,  ..., -20689.9922,\n",
      "           15687.9863, -90998.0156],\n",
      "         [ 26218.0098,  -4952.0029,  -2369.9727,  ...,  18734.0117,\n",
      "          -21519.0078,   4776.0098]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[-25670.0000,  13293.9990,   9099.9961,  ...,  -6547.0020,\n",
      "           -5658.0020,   4192.0020],\n",
      "         [-18991.9961,  -6067.0215,  22923.0000,  ...,   3081.0156,\n",
      "          -21643.9805, -11096.9785],\n",
      "         [ -6973.9473, -17975.9922,   3733.9961,  ...,   -777.9941,\n",
      "           10567.0078,  12894.0039],\n",
      "         ...,\n",
      "         [  9907.0234, -12174.0156,  -2643.0020,  ...,   4182.9775,\n",
      "            2119.9805, -79239.9844],\n",
      "         [ 16845.0078, -10166.9883, -10430.9785,  ..., -20689.9922,\n",
      "           15687.9863, -90998.0156],\n",
      "         [ 26218.0098,  -4952.0029,  -2369.9727,  ...,  18734.0117,\n",
      "          -21519.0078,   4776.0098]]]),) and output (tensor([[[-25652.0000,  13180.9990,  15728.9961,  ...,  -2831.0020,\n",
      "             158.9980,   2204.0020],\n",
      "         [-12008.9961,  -8323.0215,  26575.0000,  ...,   3950.0156,\n",
      "          -27214.9805,  -9753.9785],\n",
      "         [ -6821.9473, -20592.9922,   6937.9961,  ...,   6370.0059,\n",
      "           14332.0078,  10386.0039],\n",
      "         ...,\n",
      "         [  8020.0234,  -5994.0156,  -6199.0020,  ...,   -520.0225,\n",
      "           -5367.0195, -82159.9844],\n",
      "         [ 19399.0078,  -9637.9883, -16250.9785,  ..., -24782.9922,\n",
      "           11490.9863, -97101.0156],\n",
      "         [ 25770.0098,  -9923.0029,  -1818.9727,  ...,   8842.0117,\n",
      "          -20498.0078,   3150.0098]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[-25652.0000,  13180.9990,  15728.9961,  ...,  -2831.0020,\n",
      "             158.9980,   2204.0020],\n",
      "         [-12008.9961,  -8323.0215,  26575.0000,  ...,   3950.0156,\n",
      "          -27214.9805,  -9753.9785],\n",
      "         [ -6821.9473, -20592.9922,   6937.9961,  ...,   6370.0059,\n",
      "           14332.0078,  10386.0039],\n",
      "         ...,\n",
      "         [  8020.0234,  -5994.0156,  -6199.0020,  ...,   -520.0225,\n",
      "           -5367.0195, -82159.9844],\n",
      "         [ 19399.0078,  -9637.9883, -16250.9785,  ..., -24782.9922,\n",
      "           11490.9863, -97101.0156],\n",
      "         [ 25770.0098,  -9923.0029,  -1818.9727,  ...,   8842.0117,\n",
      "          -20498.0078,   3150.0098]]]),) and output (tensor([[[-14791.0000,  15299.9990,  16129.9961,  ...,   3187.9980,\n",
      "           -2552.0020,   5626.0020],\n",
      "         [-21330.9961,  -7214.0215,  30384.0000,  ...,  10800.0156,\n",
      "          -38325.9805, -15092.9785],\n",
      "         [-10571.9473, -21857.9922,   8969.9961,  ...,   9585.0059,\n",
      "           15763.0078,   1380.0039],\n",
      "         ...,\n",
      "         [ -1537.9766, -12832.0156,  -4694.0020,  ...,   1518.9775,\n",
      "           -9214.0195, -78322.9844],\n",
      "         [ 21006.0078, -10058.9883, -13898.9785,  ..., -27569.9922,\n",
      "            7057.9863, -94856.0156],\n",
      "         [ 27518.0098,   -893.0029,   6046.0273,  ...,   9194.0117,\n",
      "          -16476.0078,    495.0098]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[-14791.0000,  15299.9990,  16129.9961,  ...,   3187.9980,\n",
      "           -2552.0020,   5626.0020],\n",
      "         [-21330.9961,  -7214.0215,  30384.0000,  ...,  10800.0156,\n",
      "          -38325.9805, -15092.9785],\n",
      "         [-10571.9473, -21857.9922,   8969.9961,  ...,   9585.0059,\n",
      "           15763.0078,   1380.0039],\n",
      "         ...,\n",
      "         [ -1537.9766, -12832.0156,  -4694.0020,  ...,   1518.9775,\n",
      "           -9214.0195, -78322.9844],\n",
      "         [ 21006.0078, -10058.9883, -13898.9785,  ..., -27569.9922,\n",
      "            7057.9863, -94856.0156],\n",
      "         [ 27518.0098,   -893.0029,   6046.0273,  ...,   9194.0117,\n",
      "          -16476.0078,    495.0098]]]),) and output (tensor([[[-13284.0000,  26639.0000,   7467.9961,  ...,   4113.9980,\n",
      "           -6608.0020,   5336.0020],\n",
      "         [-23595.9961,  -2898.0215,  29935.0000,  ...,  15192.0156,\n",
      "          -29745.9805, -15388.9785],\n",
      "         [-11250.9473, -21802.9922,  12577.9961,  ...,  15433.0059,\n",
      "           23477.0078,  -1775.9961],\n",
      "         ...,\n",
      "         [  1054.0234,  -7510.0156,  -5553.0020,  ...,  -7836.0225,\n",
      "          -18010.0195, -71645.9844],\n",
      "         [ 25988.0078, -13103.9883, -10751.9785,  ..., -29072.9922,\n",
      "           13946.9863, -92020.0156],\n",
      "         [ 24676.0098,   8977.9971,   1640.0273,  ...,  12220.0117,\n",
      "          -17810.0078,   -436.9902]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[-13284.0000,  26639.0000,   7467.9961,  ...,   4113.9980,\n",
      "           -6608.0020,   5336.0020],\n",
      "         [-23595.9961,  -2898.0215,  29935.0000,  ...,  15192.0156,\n",
      "          -29745.9805, -15388.9785],\n",
      "         [-11250.9473, -21802.9922,  12577.9961,  ...,  15433.0059,\n",
      "           23477.0078,  -1775.9961],\n",
      "         ...,\n",
      "         [  1054.0234,  -7510.0156,  -5553.0020,  ...,  -7836.0225,\n",
      "          -18010.0195, -71645.9844],\n",
      "         [ 25988.0078, -13103.9883, -10751.9785,  ..., -29072.9922,\n",
      "           13946.9863, -92020.0156],\n",
      "         [ 24676.0098,   8977.9971,   1640.0273,  ...,  12220.0117,\n",
      "          -17810.0078,   -436.9902]]]),) and output (tensor([[[-21532.0000,  32796.0000,  16281.9961,  ...,  18208.9980,\n",
      "           -4282.0020,   5756.0020],\n",
      "         [-29637.9961,   1672.9785,  38232.0000,  ...,  14186.0156,\n",
      "          -23440.9805, -13056.9785],\n",
      "         [-17387.9473, -16267.9922,  13589.9961,  ...,  15536.0059,\n",
      "           20481.0078, -12103.9961],\n",
      "         ...,\n",
      "         [  -917.9766, -16985.0156,  -6310.0020,  ..., -13094.0225,\n",
      "           -9059.0195, -71287.9844],\n",
      "         [ 25875.0078, -19989.9883, -10451.9785,  ..., -35003.9922,\n",
      "           23903.9863, -84706.0156],\n",
      "         [ 25360.0098,   3489.9971,  -1515.9727,  ...,  14756.0117,\n",
      "          -19840.0078,   1954.0098]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[-21532.0000,  32796.0000,  16281.9961,  ...,  18208.9980,\n",
      "           -4282.0020,   5756.0020],\n",
      "         [-29637.9961,   1672.9785,  38232.0000,  ...,  14186.0156,\n",
      "          -23440.9805, -13056.9785],\n",
      "         [-17387.9473, -16267.9922,  13589.9961,  ...,  15536.0059,\n",
      "           20481.0078, -12103.9961],\n",
      "         ...,\n",
      "         [  -917.9766, -16985.0156,  -6310.0020,  ..., -13094.0225,\n",
      "           -9059.0195, -71287.9844],\n",
      "         [ 25875.0078, -19989.9883, -10451.9785,  ..., -35003.9922,\n",
      "           23903.9863, -84706.0156],\n",
      "         [ 25360.0098,   3489.9971,  -1515.9727,  ...,  14756.0117,\n",
      "          -19840.0078,   1954.0098]]]),) and output (tensor([[[-24082.0000,  33387.0000,  15374.9961,  ...,  18359.9980,\n",
      "           -7500.0020,  11519.0020],\n",
      "         [-37772.9961,    783.9785,  34366.0000,  ...,   7611.0156,\n",
      "          -22385.9805, -15464.9785],\n",
      "         [-16037.9473, -20105.9922,  10459.9961,  ...,  20794.0059,\n",
      "           20374.0078,  -8168.9961],\n",
      "         ...,\n",
      "         [ -5758.9766, -12372.0156,  -5105.0020,  ..., -22738.0234,\n",
      "           -3546.0195, -61827.9844],\n",
      "         [ 27547.0078, -10272.9883,   -733.9785,  ..., -41397.9922,\n",
      "           19998.9863, -91068.0156],\n",
      "         [ 27349.0098, -14993.0029,  -4890.9727,  ...,  14975.0117,\n",
      "          -16700.0078,    697.0098]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[-24082.0000,  33387.0000,  15374.9961,  ...,  18359.9980,\n",
      "           -7500.0020,  11519.0020],\n",
      "         [-37772.9961,    783.9785,  34366.0000,  ...,   7611.0156,\n",
      "          -22385.9805, -15464.9785],\n",
      "         [-16037.9473, -20105.9922,  10459.9961,  ...,  20794.0059,\n",
      "           20374.0078,  -8168.9961],\n",
      "         ...,\n",
      "         [ -5758.9766, -12372.0156,  -5105.0020,  ..., -22738.0234,\n",
      "           -3546.0195, -61827.9844],\n",
      "         [ 27547.0078, -10272.9883,   -733.9785,  ..., -41397.9922,\n",
      "           19998.9863, -91068.0156],\n",
      "         [ 27349.0098, -14993.0029,  -4890.9727,  ...,  14975.0117,\n",
      "          -16700.0078,    697.0098]]]),) and output (tensor([[[-14869.0000,  32284.0000,   3742.9961,  ...,  12458.9980,\n",
      "           -5452.0020,   7590.0020],\n",
      "         [-36060.9961,  10441.9785,  38235.0000,  ...,   8722.0156,\n",
      "          -23819.9805, -13255.9785],\n",
      "         [-16774.9473, -24553.9922,   4223.9961,  ...,  13912.0059,\n",
      "           30515.0078, -10878.9961],\n",
      "         ...,\n",
      "         [ -6182.9766, -16806.0156,  -8836.0020,  ..., -15826.0234,\n",
      "           -5557.0195, -61698.9844],\n",
      "         [ 24347.0078,  -9002.9883,   4292.0215,  ..., -33671.9922,\n",
      "           20163.9863, -83011.0156],\n",
      "         [ 16639.0098, -17468.0039,  -9048.9727,  ...,  24923.0117,\n",
      "          -27925.0078,   2151.0098]]]),)\n",
      "[Debug] Layer names being passed: ['model.layers.0', 'model.layers.1', 'model.layers.2', 'model.layers.3', 'model.layers.4', 'model.layers.5', 'model.layers.6', 'model.layers.7', 'model.layers.8', 'model.layers.9', 'model.layers.10', 'model.layers.11', 'model.layers.12', 'model.layers.13', 'model.layers.14', 'model.layers.15', 'model.layers.16', 'model.layers.17', 'model.layers.18', 'model.layers.19', 'model.layers.20', 'model.layers.21', 'model.layers.22', 'model.layers.23', 'model.layers.24', 'model.layers.25', 'model.layers.26', 'model.layers.27', 'model.embed_tokens']\n",
      "[Debug] Trying to access layer: model.layers.0\n",
      "[Debug] Successfully found layer: model.layers.0\n",
      "[Debug] Trying to access layer: model.layers.1\n",
      "[Debug] Successfully found layer: model.layers.1\n",
      "[Debug] Trying to access layer: model.layers.2\n",
      "[Debug] Successfully found layer: model.layers.2\n",
      "[Debug] Trying to access layer: model.layers.3\n",
      "[Debug] Successfully found layer: model.layers.3\n",
      "[Debug] Trying to access layer: model.layers.4\n",
      "[Debug] Successfully found layer: model.layers.4\n",
      "[Debug] Trying to access layer: model.layers.5\n",
      "[Debug] Successfully found layer: model.layers.5\n",
      "[Debug] Trying to access layer: model.layers.6\n",
      "[Debug] Successfully found layer: model.layers.6\n",
      "[Debug] Trying to access layer: model.layers.7\n",
      "[Debug] Successfully found layer: model.layers.7\n",
      "[Debug] Trying to access layer: model.layers.8\n",
      "[Debug] Successfully found layer: model.layers.8\n",
      "[Debug] Trying to access layer: model.layers.9\n",
      "[Debug] Successfully found layer: model.layers.9\n",
      "[Debug] Trying to access layer: model.layers.10\n",
      "[Debug] Successfully found layer: model.layers.10\n",
      "[Debug] Trying to access layer: model.layers.11\n",
      "[Debug] Successfully found layer: model.layers.11\n",
      "[Debug] Trying to access layer: model.layers.12\n",
      "[Debug] Successfully found layer: model.layers.12\n",
      "[Debug] Trying to access layer: model.layers.13\n",
      "[Debug] Successfully found layer: model.layers.13\n",
      "[Debug] Trying to access layer: model.layers.14\n",
      "[Debug] Successfully found layer: model.layers.14\n",
      "[Debug] Trying to access layer: model.layers.15\n",
      "[Debug] Successfully found layer: model.layers.15\n",
      "[Debug] Trying to access layer: model.layers.16\n",
      "[Debug] Successfully found layer: model.layers.16\n",
      "[Debug] Trying to access layer: model.layers.17\n",
      "[Debug] Successfully found layer: model.layers.17\n",
      "[Debug] Trying to access layer: model.layers.18\n",
      "[Debug] Successfully found layer: model.layers.18\n",
      "[Debug] Trying to access layer: model.layers.19\n",
      "[Debug] Successfully found layer: model.layers.19\n",
      "[Debug] Trying to access layer: model.layers.20\n",
      "[Debug] Successfully found layer: model.layers.20\n",
      "[Debug] Trying to access layer: model.layers.21\n",
      "[Debug] Successfully found layer: model.layers.21\n",
      "[Debug] Trying to access layer: model.layers.22\n",
      "[Debug] Successfully found layer: model.layers.22\n",
      "[Debug] Trying to access layer: model.layers.23\n",
      "[Debug] Successfully found layer: model.layers.23\n",
      "[Debug] Trying to access layer: model.layers.24\n",
      "[Debug] Successfully found layer: model.layers.24\n",
      "[Debug] Trying to access layer: model.layers.25\n",
      "[Debug] Successfully found layer: model.layers.25\n",
      "[Debug] Trying to access layer: model.layers.26\n",
      "[Debug] Successfully found layer: model.layers.26\n",
      "[Debug] Trying to access layer: model.layers.27\n",
      "[Debug] Successfully found layer: model.layers.27\n",
      "[Debug] Trying to access layer: model.embed_tokens\n",
      "[Debug] Successfully found layer: model.embed_tokens\n",
      "[Hook] Layer Embedding(128256, 3072, padding_idx=128004) received input (tensor([[128000,   9207,    315,    279,   3723,   4273,    304,    279,  23315,\n",
      "           5111,    578,   3560,    315,    279,   3723,   4273,    304,    279,\n",
      "          23315,   5111,   6137,   1306,   4435,   5111,   8105,    323,  81700,\n",
      "           1139,   2539,  15507,   2391,    279,  23315,   5111,    505,    220,\n",
      "           6280,     20,    311,    220,   4468,     20,     13]]),) and output tensor([[[-9.9945e-04, -6.5613e-04, -4.6387e-03,  ..., -1.4267e-03,\n",
      "          -2.0447e-03,  1.9684e-03],\n",
      "         [-3.6621e-04, -4.2725e-02,  2.5482e-03,  ...,  7.8735e-03,\n",
      "          -5.6744e-05,  2.2217e-02],\n",
      "         [ 4.4556e-03, -1.0925e-02,  5.2643e-04,  ...,  2.3071e-02,\n",
      "           2.1851e-02,  6.5002e-03],\n",
      "         ...,\n",
      "         [ 1.1108e-02, -3.4668e-02, -3.7994e-03,  ..., -1.4099e-02,\n",
      "          -1.4771e-02,  2.4902e-02],\n",
      "         [ 2.4048e-02,  8.3008e-03,  1.9165e-02,  ..., -1.2512e-02,\n",
      "          -2.0508e-02,  3.1433e-03],\n",
      "         [ 9.3384e-03, -3.0670e-03,  2.7832e-02,  ...,  1.1658e-02,\n",
      "          -8.3618e-03,  9.6436e-03]]])\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[-9.9945e-04, -6.5613e-04, -4.6387e-03,  ..., -1.4267e-03,\n",
      "          -2.0447e-03,  1.9684e-03],\n",
      "         [-3.6621e-04, -4.2725e-02,  2.5482e-03,  ...,  7.8735e-03,\n",
      "          -5.6744e-05,  2.2217e-02],\n",
      "         [ 4.4556e-03, -1.0925e-02,  5.2643e-04,  ...,  2.3071e-02,\n",
      "           2.1851e-02,  6.5002e-03],\n",
      "         ...,\n",
      "         [ 1.1108e-02, -3.4668e-02, -3.7994e-03,  ..., -1.4099e-02,\n",
      "          -1.4771e-02,  2.4902e-02],\n",
      "         [ 2.4048e-02,  8.3008e-03,  1.9165e-02,  ..., -1.2512e-02,\n",
      "          -2.0508e-02,  3.1433e-03],\n",
      "         [ 9.3384e-03, -3.0670e-03,  2.7832e-02,  ...,  1.1658e-02,\n",
      "          -8.3618e-03,  9.6436e-03]]]),) and output (tensor([[[ 442.9990,  239.9993,  302.9954,  ..., -154.0014,  -15.0020,\n",
      "           185.0020],\n",
      "         [ 141.9996,   37.9573,  -55.9975,  ..., -157.9921,  -78.0001,\n",
      "           112.0222],\n",
      "         [  28.0045,  -32.0109,  -77.9995,  ...,  -39.9769,  -62.9781,\n",
      "           247.0065],\n",
      "         ...,\n",
      "         [ 128.0111,  112.9653,   -6.0038,  ...,  111.9859, -156.0148,\n",
      "          -210.9751],\n",
      "         [ 438.0240,   69.0083, -168.9808,  ..., -263.0125,   -1.0205,\n",
      "          -365.9969],\n",
      "         [ -19.9907,  216.9969,   57.0278,  ..., -148.9883,  -65.0084,\n",
      "           165.0096]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[ 442.9990,  239.9993,  302.9954,  ..., -154.0014,  -15.0020,\n",
      "           185.0020],\n",
      "         [ 141.9996,   37.9573,  -55.9975,  ..., -157.9921,  -78.0001,\n",
      "           112.0222],\n",
      "         [  28.0045,  -32.0109,  -77.9995,  ...,  -39.9769,  -62.9781,\n",
      "           247.0065],\n",
      "         ...,\n",
      "         [ 128.0111,  112.9653,   -6.0038,  ...,  111.9859, -156.0148,\n",
      "          -210.9751],\n",
      "         [ 438.0240,   69.0083, -168.9808,  ..., -263.0125,   -1.0205,\n",
      "          -365.9969],\n",
      "         [ -19.9907,  216.9969,   57.0278,  ..., -148.9883,  -65.0084,\n",
      "           165.0096]]]),) and output (tensor([[[  439.9990,  -144.0007,  -410.0046,  ...,  -765.0015,\n",
      "           1193.9979, -1027.9980],\n",
      "         [  496.9996,  -499.0427,   113.0025,  ...,  -619.9921,\n",
      "            996.9999,   378.0222],\n",
      "         [  871.0045,   521.9891,   102.0005,  ...,   -65.9769,\n",
      "            479.0219,   268.0065],\n",
      "         ...,\n",
      "         [ -419.9889,   779.9653,  -864.0038,  ...,  -442.0141,\n",
      "           -642.0148,   486.0249],\n",
      "         [ 1484.0240,  -735.9917,  -957.9808,  ...,   191.9875,\n",
      "            967.9795, -2108.9968],\n",
      "         [  399.0093,   607.9969,  -240.9722,  ...,  -456.9883,\n",
      "           -455.0084,   -29.9904]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[  439.9990,  -144.0007,  -410.0046,  ...,  -765.0015,\n",
      "           1193.9979, -1027.9980],\n",
      "         [  496.9996,  -499.0427,   113.0025,  ...,  -619.9921,\n",
      "            996.9999,   378.0222],\n",
      "         [  871.0045,   521.9891,   102.0005,  ...,   -65.9769,\n",
      "            479.0219,   268.0065],\n",
      "         ...,\n",
      "         [ -419.9889,   779.9653,  -864.0038,  ...,  -442.0141,\n",
      "           -642.0148,   486.0249],\n",
      "         [ 1484.0240,  -735.9917,  -957.9808,  ...,   191.9875,\n",
      "            967.9795, -2108.9968],\n",
      "         [  399.0093,   607.9969,  -240.9722,  ...,  -456.9883,\n",
      "           -455.0084,   -29.9904]]]),) and output (tensor([[[  551.9990,   993.9993, -1790.0046,  ...,   445.9985,\n",
      "          -1386.0021, -1245.9980],\n",
      "         [ 3319.9995, -1356.0427,  2922.0024,  ...,   -56.9921,\n",
      "          -2551.0000,  2403.0222],\n",
      "         [ 2733.0044,  -898.0109,  2518.0005,  ..., -1695.9769,\n",
      "          -1311.9781,   542.0065],\n",
      "         ...,\n",
      "         [ -131.9889, -1604.0347, -1258.0038,  ...,  2957.9858,\n",
      "           -508.0148,   977.0249],\n",
      "         [ 5212.0239, -1000.9917,   167.0192,  ..., -3715.0125,\n",
      "           -197.0205, -3069.9968],\n",
      "         [ 2059.0093,  -265.0031,   286.0278,  ..., -1403.9883,\n",
      "           -542.0084,  2751.0098]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[  551.9990,   993.9993, -1790.0046,  ...,   445.9985,\n",
      "          -1386.0021, -1245.9980],\n",
      "         [ 3319.9995, -1356.0427,  2922.0024,  ...,   -56.9921,\n",
      "          -2551.0000,  2403.0222],\n",
      "         [ 2733.0044,  -898.0109,  2518.0005,  ..., -1695.9769,\n",
      "          -1311.9781,   542.0065],\n",
      "         ...,\n",
      "         [ -131.9889, -1604.0347, -1258.0038,  ...,  2957.9858,\n",
      "           -508.0148,   977.0249],\n",
      "         [ 5212.0239, -1000.9917,   167.0192,  ..., -3715.0125,\n",
      "           -197.0205, -3069.9968],\n",
      "         [ 2059.0093,  -265.0031,   286.0278,  ..., -1403.9883,\n",
      "           -542.0084,  2751.0098]]]),) and output (tensor([[[ 2131.9990,  -737.0007, -1071.0046,  ..., -2781.0015,\n",
      "           -885.0021,    86.0020],\n",
      "         [ 2823.9995,  -863.0427,  5625.0024,  ...,   445.0079,\n",
      "          -4279.0000,   756.0222],\n",
      "         [ 1690.0044,  4604.9893,  4517.0005,  ...,  2166.0229,\n",
      "           1562.0219,  -333.9935],\n",
      "         ...,\n",
      "         [-3065.9888,  1831.9653, -4837.0039,  ...,  2075.9858,\n",
      "           -122.0146,  5515.0249],\n",
      "         [-2165.9761,  1485.0083, -3497.9810,  ..., -1473.0125,\n",
      "          -1427.0205, -5122.9971],\n",
      "         [ 1287.0093,  2087.9971, -2692.9722,  ..., -1756.9883,\n",
      "           2687.9917,  6584.0098]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[ 2131.9990,  -737.0007, -1071.0046,  ..., -2781.0015,\n",
      "           -885.0021,    86.0020],\n",
      "         [ 2823.9995,  -863.0427,  5625.0024,  ...,   445.0079,\n",
      "          -4279.0000,   756.0222],\n",
      "         [ 1690.0044,  4604.9893,  4517.0005,  ...,  2166.0229,\n",
      "           1562.0219,  -333.9935],\n",
      "         ...,\n",
      "         [-3065.9888,  1831.9653, -4837.0039,  ...,  2075.9858,\n",
      "           -122.0146,  5515.0249],\n",
      "         [-2165.9761,  1485.0083, -3497.9810,  ..., -1473.0125,\n",
      "          -1427.0205, -5122.9971],\n",
      "         [ 1287.0093,  2087.9971, -2692.9722,  ..., -1756.9883,\n",
      "           2687.9917,  6584.0098]]]),) and output (tensor([[[ 3420.9990,   760.9993, -5376.0049,  ..., -4193.0015,\n",
      "           2860.9980, -4080.9980],\n",
      "         [ -199.0005,  -103.0427,  5501.0024,  ...,  -163.9921,\n",
      "           4213.0000,  -374.9778],\n",
      "         [  134.0044,  6263.9893,  3115.0005,  ...,  7801.0229,\n",
      "          -1287.9781,  1159.0065],\n",
      "         ...,\n",
      "         [-3684.9888,   217.9653, -4836.0039,  ...,  2476.9858,\n",
      "             32.9854, -5593.9751],\n",
      "         [-7353.9761, -1199.9917,  -577.9810,  ...,  3162.9875,\n",
      "          -5996.0205, -4733.9971],\n",
      "         [-2220.9907, -1527.0029, -4223.9722,  ..., -3707.9883,\n",
      "           2341.9917,  3078.0098]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[ 3420.9990,   760.9993, -5376.0049,  ..., -4193.0015,\n",
      "           2860.9980, -4080.9980],\n",
      "         [ -199.0005,  -103.0427,  5501.0024,  ...,  -163.9921,\n",
      "           4213.0000,  -374.9778],\n",
      "         [  134.0044,  6263.9893,  3115.0005,  ...,  7801.0229,\n",
      "          -1287.9781,  1159.0065],\n",
      "         ...,\n",
      "         [-3684.9888,   217.9653, -4836.0039,  ...,  2476.9858,\n",
      "             32.9854, -5593.9751],\n",
      "         [-7353.9761, -1199.9917,  -577.9810,  ...,  3162.9875,\n",
      "          -5996.0205, -4733.9971],\n",
      "         [-2220.9907, -1527.0029, -4223.9722,  ..., -3707.9883,\n",
      "           2341.9917,  3078.0098]]]),) and output (tensor([[[  2742.9990,  -1512.0007, -10493.0049,  ...,  -9945.0020,\n",
      "           -2017.0020,  -1802.9980],\n",
      "         [ -4106.0005,   3345.9573,   2834.0024,  ...,  -3760.9922,\n",
      "            9742.0000,   1466.0222],\n",
      "         [ -3695.9956,   2661.9893,   -681.9995,  ...,   4159.0229,\n",
      "           -1823.9780,   -145.9935],\n",
      "         ...,\n",
      "         [ -5400.9888,   1349.9653,  -5009.0039,  ...,    722.9858,\n",
      "            1296.9854,  -2861.9751],\n",
      "         [-12998.9766,  -7752.9917,    451.0190,  ...,   6865.9873,\n",
      "          -14991.0205,  -5636.9971],\n",
      "         [  -238.9907,  -6811.0029,  -8093.9722,  ...,  -3444.9883,\n",
      "            3544.9917,   3206.0098]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[  2742.9990,  -1512.0007, -10493.0049,  ...,  -9945.0020,\n",
      "           -2017.0020,  -1802.9980],\n",
      "         [ -4106.0005,   3345.9573,   2834.0024,  ...,  -3760.9922,\n",
      "            9742.0000,   1466.0222],\n",
      "         [ -3695.9956,   2661.9893,   -681.9995,  ...,   4159.0229,\n",
      "           -1823.9780,   -145.9935],\n",
      "         ...,\n",
      "         [ -5400.9888,   1349.9653,  -5009.0039,  ...,    722.9858,\n",
      "            1296.9854,  -2861.9751],\n",
      "         [-12998.9766,  -7752.9917,    451.0190,  ...,   6865.9873,\n",
      "          -14991.0205,  -5636.9971],\n",
      "         [  -238.9907,  -6811.0029,  -8093.9722,  ...,  -3444.9883,\n",
      "            3544.9917,   3206.0098]]]),) and output (tensor([[[  3032.9990,  -3420.0007, -10291.0049,  ...,  -6511.0020,\n",
      "           -1953.0020,   2001.0020],\n",
      "         [ -4307.0005,   3869.9573,  -3131.9976,  ...,    238.0078,\n",
      "           12074.0000,  -1509.9778],\n",
      "         [ -1990.9956,   7143.9893,  -6151.9995,  ...,   3566.0229,\n",
      "             645.0220,   1054.0065],\n",
      "         ...,\n",
      "         [ -5077.9888,  -1850.0347,  -7844.0039,  ...,   2901.9858,\n",
      "            4524.9854,  -8302.9746],\n",
      "         [-12900.9766,  -3295.9917,  -1283.9810,  ...,   5285.9873,\n",
      "          -11104.0205,  -8929.9971],\n",
      "         [ 10617.0098,  -8842.0029,  -5713.9722,  ..., -10399.9883,\n",
      "           -1340.0083,   6028.0098]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[  3032.9990,  -3420.0007, -10291.0049,  ...,  -6511.0020,\n",
      "           -1953.0020,   2001.0020],\n",
      "         [ -4307.0005,   3869.9573,  -3131.9976,  ...,    238.0078,\n",
      "           12074.0000,  -1509.9778],\n",
      "         [ -1990.9956,   7143.9893,  -6151.9995,  ...,   3566.0229,\n",
      "             645.0220,   1054.0065],\n",
      "         ...,\n",
      "         [ -5077.9888,  -1850.0347,  -7844.0039,  ...,   2901.9858,\n",
      "            4524.9854,  -8302.9746],\n",
      "         [-12900.9766,  -3295.9917,  -1283.9810,  ...,   5285.9873,\n",
      "          -11104.0205,  -8929.9971],\n",
      "         [ 10617.0098,  -8842.0029,  -5713.9722,  ..., -10399.9883,\n",
      "           -1340.0083,   6028.0098]]]),) and output (tensor([[[  3132.9990,  -1970.0007, -11544.0049,  ...,  -5780.0020,\n",
      "           -2486.0020,  -1921.9980],\n",
      "         [-10673.0000,   4707.9570,  -7473.9976,  ...,  -5031.9922,\n",
      "           17574.0000,   2512.0222],\n",
      "         [  1201.0044,   2432.9893,  -6701.9995,  ...,   7663.0229,\n",
      "            2371.0220,   3051.0063],\n",
      "         ...,\n",
      "         [ -3365.9888,  -5972.0347,  -7464.0039,  ...,   1592.9858,\n",
      "            3685.9854,  -4664.9746],\n",
      "         [ -6141.9766,  -2472.9917,  -1296.9810,  ...,   7570.9873,\n",
      "          -12035.0205,  -8433.9971],\n",
      "         [ 13385.0098, -15902.0029,  -3262.9722,  ...,  -8470.9883,\n",
      "            1965.9917,   5590.0098]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[  3132.9990,  -1970.0007, -11544.0049,  ...,  -5780.0020,\n",
      "           -2486.0020,  -1921.9980],\n",
      "         [-10673.0000,   4707.9570,  -7473.9976,  ...,  -5031.9922,\n",
      "           17574.0000,   2512.0222],\n",
      "         [  1201.0044,   2432.9893,  -6701.9995,  ...,   7663.0229,\n",
      "            2371.0220,   3051.0063],\n",
      "         ...,\n",
      "         [ -3365.9888,  -5972.0347,  -7464.0039,  ...,   1592.9858,\n",
      "            3685.9854,  -4664.9746],\n",
      "         [ -6141.9766,  -2472.9917,  -1296.9810,  ...,   7570.9873,\n",
      "          -12035.0205,  -8433.9971],\n",
      "         [ 13385.0098, -15902.0029,  -3262.9722,  ...,  -8470.9883,\n",
      "            1965.9917,   5590.0098]]]),) and output (tensor([[[  1046.9990,   2403.9993, -14486.0049,  ..., -10922.0020,\n",
      "           -1419.0020,  -2983.9980],\n",
      "         [-14724.0000,    935.9570,  -6076.9976,  ...,  -3751.9922,\n",
      "            6741.0000,  -2292.9778],\n",
      "         [  4006.0044,   2308.9893,  -4776.9995,  ...,   4035.0229,\n",
      "           -4732.9780,   1459.0063],\n",
      "         ...,\n",
      "         [ -3890.9888,  -4782.0347, -10277.0039,  ...,   2115.9858,\n",
      "            9970.9854,  -2756.9746],\n",
      "         [ -4454.9766,  -3543.9917,  -3410.9810,  ...,   5855.9873,\n",
      "          -10492.0205, -13922.9971],\n",
      "         [ 13528.0098, -15023.0029,  -3497.9722,  ..., -13638.9883,\n",
      "             967.9917,   6192.0098]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[  1046.9990,   2403.9993, -14486.0049,  ..., -10922.0020,\n",
      "           -1419.0020,  -2983.9980],\n",
      "         [-14724.0000,    935.9570,  -6076.9976,  ...,  -3751.9922,\n",
      "            6741.0000,  -2292.9778],\n",
      "         [  4006.0044,   2308.9893,  -4776.9995,  ...,   4035.0229,\n",
      "           -4732.9780,   1459.0063],\n",
      "         ...,\n",
      "         [ -3890.9888,  -4782.0347, -10277.0039,  ...,   2115.9858,\n",
      "            9970.9854,  -2756.9746],\n",
      "         [ -4454.9766,  -3543.9917,  -3410.9810,  ...,   5855.9873,\n",
      "          -10492.0205, -13922.9971],\n",
      "         [ 13528.0098, -15023.0029,  -3497.9722,  ..., -13638.9883,\n",
      "             967.9917,   6192.0098]]]),) and output (tensor([[[ -3767.0010,   5307.9990, -14221.0049,  ..., -15035.0020,\n",
      "           -3630.0020,   4168.0020],\n",
      "         [ -7590.0000,   6853.9570, -10271.9980,  ...,  -3343.9922,\n",
      "            1403.0000,  -2670.9778],\n",
      "         [  2813.0044,   9572.9893,  -7808.9995,  ...,   -761.9771,\n",
      "           -5645.9780,   2537.0063],\n",
      "         ...,\n",
      "         [   126.0112,  -6743.0347,  -9742.0039,  ...,     73.9858,\n",
      "            7639.9854,   4645.0254],\n",
      "         [ -4441.9766,   4208.0083,  -6595.9810,  ...,   5085.9873,\n",
      "           -4697.0205, -16768.9961],\n",
      "         [  7920.0098, -12846.0029,  -6688.9722,  ..., -11484.9883,\n",
      "           -4267.0083,  13332.0098]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[ -3767.0010,   5307.9990, -14221.0049,  ..., -15035.0020,\n",
      "           -3630.0020,   4168.0020],\n",
      "         [ -7590.0000,   6853.9570, -10271.9980,  ...,  -3343.9922,\n",
      "            1403.0000,  -2670.9778],\n",
      "         [  2813.0044,   9572.9893,  -7808.9995,  ...,   -761.9771,\n",
      "           -5645.9780,   2537.0063],\n",
      "         ...,\n",
      "         [   126.0112,  -6743.0347,  -9742.0039,  ...,     73.9858,\n",
      "            7639.9854,   4645.0254],\n",
      "         [ -4441.9766,   4208.0083,  -6595.9810,  ...,   5085.9873,\n",
      "           -4697.0205, -16768.9961],\n",
      "         [  7920.0098, -12846.0029,  -6688.9722,  ..., -11484.9883,\n",
      "           -4267.0083,  13332.0098]]]),) and output (tensor([[[ -1326.0010,   1223.9990, -16643.0039,  ..., -13240.0020,\n",
      "           -5677.0020,   9639.0020],\n",
      "         [-13823.0000,   4429.9570,  -8355.9980,  ...,  -6248.9922,\n",
      "            2529.0000,  -9421.9775],\n",
      "         [  -860.9956,  16181.9893,  -6776.9995,  ...,  -4503.9771,\n",
      "          -10435.9785,   1032.0063],\n",
      "         ...,\n",
      "         [  3795.0112,  -1902.0347, -11300.0039,  ...,  -3361.0142,\n",
      "            4271.9854,   2712.0254],\n",
      "         [  -709.9766,   9271.0078,  -4679.9810,  ...,   2017.9873,\n",
      "           -4868.0205, -19752.9961],\n",
      "         [  9310.0098, -11592.0029,  -5128.9722,  ..., -15145.9883,\n",
      "             894.9917,  16950.0098]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[ -1326.0010,   1223.9990, -16643.0039,  ..., -13240.0020,\n",
      "           -5677.0020,   9639.0020],\n",
      "         [-13823.0000,   4429.9570,  -8355.9980,  ...,  -6248.9922,\n",
      "            2529.0000,  -9421.9775],\n",
      "         [  -860.9956,  16181.9893,  -6776.9995,  ...,  -4503.9771,\n",
      "          -10435.9785,   1032.0063],\n",
      "         ...,\n",
      "         [  3795.0112,  -1902.0347, -11300.0039,  ...,  -3361.0142,\n",
      "            4271.9854,   2712.0254],\n",
      "         [  -709.9766,   9271.0078,  -4679.9810,  ...,   2017.9873,\n",
      "           -4868.0205, -19752.9961],\n",
      "         [  9310.0098, -11592.0029,  -5128.9722,  ..., -15145.9883,\n",
      "             894.9917,  16950.0098]]]),) and output (tensor([[[ -5120.0010,   2246.9990, -15993.0039,  ..., -19818.0020,\n",
      "           -9385.0020,  11924.0020],\n",
      "         [-12332.0000,   4631.9570,  -4793.9980,  ...,  -9717.9922,\n",
      "            1457.0000,  -6129.9775],\n",
      "         [ -6556.9956,  18053.9883,  -7565.9995,  ...,  -2063.9771,\n",
      "          -13615.9785,    727.0063],\n",
      "         ...,\n",
      "         [  3900.0112,  -6336.0347, -10247.0039,  ...,  -2266.0142,\n",
      "            5165.9854,   4592.0254],\n",
      "         [  3637.0234,   9716.0078,  -3929.9810,  ...,   2341.9873,\n",
      "           -7390.0205, -18450.9961],\n",
      "         [  6842.0098,  -9619.0029,  -5326.9722,  ..., -14607.9883,\n",
      "            1565.9917,  13678.0098]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[ -5120.0010,   2246.9990, -15993.0039,  ..., -19818.0020,\n",
      "           -9385.0020,  11924.0020],\n",
      "         [-12332.0000,   4631.9570,  -4793.9980,  ...,  -9717.9922,\n",
      "            1457.0000,  -6129.9775],\n",
      "         [ -6556.9956,  18053.9883,  -7565.9995,  ...,  -2063.9771,\n",
      "          -13615.9785,    727.0063],\n",
      "         ...,\n",
      "         [  3900.0112,  -6336.0347, -10247.0039,  ...,  -2266.0142,\n",
      "            5165.9854,   4592.0254],\n",
      "         [  3637.0234,   9716.0078,  -3929.9810,  ...,   2341.9873,\n",
      "           -7390.0205, -18450.9961],\n",
      "         [  6842.0098,  -9619.0029,  -5326.9722,  ..., -14607.9883,\n",
      "            1565.9917,  13678.0098]]]),) and output (tensor([[[ -6685.0010,  -1278.0010, -15553.0039,  ..., -19846.0020,\n",
      "          -10418.0020,  13376.0020],\n",
      "         [-14854.0000,   6731.9570,  -4390.9980,  ..., -15119.9922,\n",
      "            5425.0000, -11578.9775],\n",
      "         [ -9836.9961,  20229.9883,   -459.9995,  ...,  -2286.9771,\n",
      "          -10637.9785,   5452.0063],\n",
      "         ...,\n",
      "         [  3246.0112,  -9506.0352,  -4847.0039,  ...,  -3513.0142,\n",
      "            5221.9854,   7978.0254],\n",
      "         [  6106.0234,  10376.0078,  -6346.9810,  ...,  -2254.0127,\n",
      "            -816.0205, -22984.9961],\n",
      "         [  6881.0098, -14346.0029,  -4525.9722,  ..., -16596.9883,\n",
      "             986.9917,  11658.0098]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[ -6685.0010,  -1278.0010, -15553.0039,  ..., -19846.0020,\n",
      "          -10418.0020,  13376.0020],\n",
      "         [-14854.0000,   6731.9570,  -4390.9980,  ..., -15119.9922,\n",
      "            5425.0000, -11578.9775],\n",
      "         [ -9836.9961,  20229.9883,   -459.9995,  ...,  -2286.9771,\n",
      "          -10637.9785,   5452.0063],\n",
      "         ...,\n",
      "         [  3246.0112,  -9506.0352,  -4847.0039,  ...,  -3513.0142,\n",
      "            5221.9854,   7978.0254],\n",
      "         [  6106.0234,  10376.0078,  -6346.9810,  ...,  -2254.0127,\n",
      "            -816.0205, -22984.9961],\n",
      "         [  6881.0098, -14346.0029,  -4525.9722,  ..., -16596.9883,\n",
      "             986.9917,  11658.0098]]]),) and output (tensor([[[ -8422.0010,    339.9990, -15289.0039,  ..., -20884.0020,\n",
      "           -6908.0020,  11014.0020],\n",
      "         [ -7863.0000,   2901.9570,  -2435.9980,  ..., -13349.9922,\n",
      "           13836.0000,  -8792.9775],\n",
      "         [ -9576.9961,  28417.9883,    647.0005,  ...,  -5072.9771,\n",
      "           -7406.9785,   1417.0063],\n",
      "         ...,\n",
      "         [ -2019.9888, -13053.0352,  -9116.0039,  ...,  -4429.0142,\n",
      "            6426.9854,   7524.0254],\n",
      "         [  5436.0234,  13698.0078,  -5164.9810,  ...,  -5457.0127,\n",
      "           -3216.0205, -18384.9961],\n",
      "         [  5253.0098,  -9144.0029,  -8171.9722,  ...,  -9722.9883,\n",
      "             820.9917,  11703.0098]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[ -8422.0010,    339.9990, -15289.0039,  ..., -20884.0020,\n",
      "           -6908.0020,  11014.0020],\n",
      "         [ -7863.0000,   2901.9570,  -2435.9980,  ..., -13349.9922,\n",
      "           13836.0000,  -8792.9775],\n",
      "         [ -9576.9961,  28417.9883,    647.0005,  ...,  -5072.9771,\n",
      "           -7406.9785,   1417.0063],\n",
      "         ...,\n",
      "         [ -2019.9888, -13053.0352,  -9116.0039,  ...,  -4429.0142,\n",
      "            6426.9854,   7524.0254],\n",
      "         [  5436.0234,  13698.0078,  -5164.9810,  ...,  -5457.0127,\n",
      "           -3216.0205, -18384.9961],\n",
      "         [  5253.0098,  -9144.0029,  -8171.9722,  ...,  -9722.9883,\n",
      "             820.9917,  11703.0098]]]),) and output (tensor([[[-10989.0010,   7034.9990, -13964.0039,  ..., -23968.0020,\n",
      "           -3749.0020,  11730.0020],\n",
      "         [ -2836.0000,   7346.9570,  -5120.9980,  ..., -22120.9922,\n",
      "            8607.0000,  -6006.9775],\n",
      "         [-13445.9961,  35267.9883,   1654.0005,  ...,   1340.0229,\n",
      "          -16041.9785,  -5219.9937],\n",
      "         ...,\n",
      "         [  -612.9888, -13871.0352, -10241.0039,  ...,  -5465.0142,\n",
      "           11098.9854,   9785.0254],\n",
      "         [  2203.0234,  16338.0078,  -3087.9810,  ...,     62.9873,\n",
      "           -5503.0205, -18470.9961],\n",
      "         [ 10119.0098,  -6167.0029, -12931.9727,  ..., -11320.9883,\n",
      "           -2774.0083,  17837.0098]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[-10989.0010,   7034.9990, -13964.0039,  ..., -23968.0020,\n",
      "           -3749.0020,  11730.0020],\n",
      "         [ -2836.0000,   7346.9570,  -5120.9980,  ..., -22120.9922,\n",
      "            8607.0000,  -6006.9775],\n",
      "         [-13445.9961,  35267.9883,   1654.0005,  ...,   1340.0229,\n",
      "          -16041.9785,  -5219.9937],\n",
      "         ...,\n",
      "         [  -612.9888, -13871.0352, -10241.0039,  ...,  -5465.0142,\n",
      "           11098.9854,   9785.0254],\n",
      "         [  2203.0234,  16338.0078,  -3087.9810,  ...,     62.9873,\n",
      "           -5503.0205, -18470.9961],\n",
      "         [ 10119.0098,  -6167.0029, -12931.9727,  ..., -11320.9883,\n",
      "           -2774.0083,  17837.0098]]]),) and output (tensor([[[ -7159.0010,   6621.9990, -10967.0039,  ..., -17477.0020,\n",
      "           -2510.0020,  11888.0020],\n",
      "         [  -322.0000,   4806.9570,  -7939.9980,  ..., -16044.9922,\n",
      "            7126.0000,  -5586.9775],\n",
      "         [-17242.9961,  39388.9883,    324.0005,  ...,   1058.0229,\n",
      "           -9770.9785, -10837.9941],\n",
      "         ...,\n",
      "         [   999.0112, -12247.0352,  -6408.0039,  ...,   1359.9858,\n",
      "            9969.9854,   9790.0254],\n",
      "         [  -535.9766,  19892.0078,   -715.9810,  ...,   2334.9873,\n",
      "           -8405.0205, -21311.9961],\n",
      "         [ 17438.0098,  -9173.0029,  -7268.9727,  ..., -12230.9883,\n",
      "             273.9917,  13128.0098]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[ -7159.0010,   6621.9990, -10967.0039,  ..., -17477.0020,\n",
      "           -2510.0020,  11888.0020],\n",
      "         [  -322.0000,   4806.9570,  -7939.9980,  ..., -16044.9922,\n",
      "            7126.0000,  -5586.9775],\n",
      "         [-17242.9961,  39388.9883,    324.0005,  ...,   1058.0229,\n",
      "           -9770.9785, -10837.9941],\n",
      "         ...,\n",
      "         [   999.0112, -12247.0352,  -6408.0039,  ...,   1359.9858,\n",
      "            9969.9854,   9790.0254],\n",
      "         [  -535.9766,  19892.0078,   -715.9810,  ...,   2334.9873,\n",
      "           -8405.0205, -21311.9961],\n",
      "         [ 17438.0098,  -9173.0029,  -7268.9727,  ..., -12230.9883,\n",
      "             273.9917,  13128.0098]]]),) and output (tensor([[[-4.8220e+03,  3.1250e+03, -3.8850e+03,  ..., -1.9007e+04,\n",
      "          -5.5100e+02,  1.6285e+04],\n",
      "         [-1.9000e+01,  5.6540e+03, -6.5400e+03,  ..., -1.7204e+04,\n",
      "           8.8120e+03, -1.0886e+04],\n",
      "         [-1.1082e+04,  4.4057e+04,  4.3920e+03,  ..., -8.0230e+03,\n",
      "          -1.3491e+04, -1.5321e+04],\n",
      "         ...,\n",
      "         [ 6.0440e+03, -2.0223e+04, -1.2037e+04,  ..., -5.0001e+02,\n",
      "          -1.8401e+02,  1.2725e+04],\n",
      "         [ 4.3060e+03,  2.0013e+04,  2.6090e+03,  ...,  2.0060e+03,\n",
      "          -1.0333e+04, -1.6267e+04],\n",
      "         [ 1.6419e+04, -9.1190e+03, -1.2622e+04,  ..., -1.9536e+04,\n",
      "          -2.1010e+03,  1.7935e+04]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[-4.8220e+03,  3.1250e+03, -3.8850e+03,  ..., -1.9007e+04,\n",
      "          -5.5100e+02,  1.6285e+04],\n",
      "         [-1.9000e+01,  5.6540e+03, -6.5400e+03,  ..., -1.7204e+04,\n",
      "           8.8120e+03, -1.0886e+04],\n",
      "         [-1.1082e+04,  4.4057e+04,  4.3920e+03,  ..., -8.0230e+03,\n",
      "          -1.3491e+04, -1.5321e+04],\n",
      "         ...,\n",
      "         [ 6.0440e+03, -2.0223e+04, -1.2037e+04,  ..., -5.0001e+02,\n",
      "          -1.8401e+02,  1.2725e+04],\n",
      "         [ 4.3060e+03,  2.0013e+04,  2.6090e+03,  ...,  2.0060e+03,\n",
      "          -1.0333e+04, -1.6267e+04],\n",
      "         [ 1.6419e+04, -9.1190e+03, -1.2622e+04,  ..., -1.9536e+04,\n",
      "          -2.1010e+03,  1.7935e+04]]]),) and output (tensor([[[-7.2350e+03,  6.5840e+03, -4.2120e+03,  ..., -1.9104e+04,\n",
      "           3.0880e+03,  1.7084e+04],\n",
      "         [ 3.0510e+03,  4.6590e+03, -1.0018e+04,  ..., -2.2847e+04,\n",
      "           7.0700e+03,  5.0225e+00],\n",
      "         [-5.9000e+03,  5.4163e+04, -5.3000e+02,  ..., -1.0202e+04,\n",
      "          -1.1619e+04, -8.9400e+03],\n",
      "         ...,\n",
      "         [ 3.8250e+03, -2.1025e+04, -1.0241e+04,  ...,  2.0960e+03,\n",
      "          -4.4400e+03,  1.4011e+04],\n",
      "         [ 4.1120e+03,  2.0948e+04,  2.6290e+03,  ...,  1.2250e+03,\n",
      "          -1.7078e+04, -2.0275e+04],\n",
      "         [ 1.8584e+04, -9.3150e+03, -1.2941e+04,  ..., -2.2028e+04,\n",
      "          -1.8790e+03,  2.3966e+04]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[-7.2350e+03,  6.5840e+03, -4.2120e+03,  ..., -1.9104e+04,\n",
      "           3.0880e+03,  1.7084e+04],\n",
      "         [ 3.0510e+03,  4.6590e+03, -1.0018e+04,  ..., -2.2847e+04,\n",
      "           7.0700e+03,  5.0225e+00],\n",
      "         [-5.9000e+03,  5.4163e+04, -5.3000e+02,  ..., -1.0202e+04,\n",
      "          -1.1619e+04, -8.9400e+03],\n",
      "         ...,\n",
      "         [ 3.8250e+03, -2.1025e+04, -1.0241e+04,  ...,  2.0960e+03,\n",
      "          -4.4400e+03,  1.4011e+04],\n",
      "         [ 4.1120e+03,  2.0948e+04,  2.6290e+03,  ...,  1.2250e+03,\n",
      "          -1.7078e+04, -2.0275e+04],\n",
      "         [ 1.8584e+04, -9.3150e+03, -1.2941e+04,  ..., -2.2028e+04,\n",
      "          -1.8790e+03,  2.3966e+04]]]),) and output (tensor([[[ -5586.0010,   7645.9990,   -505.0039,  ..., -15346.0020,\n",
      "            -623.0020,  12986.0020],\n",
      "         [  7451.0000,   3928.9570,  -8299.9980,  ..., -30006.9922,\n",
      "            6531.0000,  -7711.9775],\n",
      "         [ -3128.9961,  60864.9883,   8587.0000,  ...,  -4719.9766,\n",
      "           -9475.9785,  -9875.9941],\n",
      "         ...,\n",
      "         [  7572.0112, -22593.0352,  -4411.0039,  ...,   1582.9858,\n",
      "           -6101.0146,  16358.0254],\n",
      "         [  2222.0234,  21519.0078,   4131.0190,  ...,   2655.9873,\n",
      "          -22538.0195, -20795.9961],\n",
      "         [ 15925.0098, -15699.0029, -12094.9727,  ..., -16352.9883,\n",
      "             296.9917,  20893.0098]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[ -5586.0010,   7645.9990,   -505.0039,  ..., -15346.0020,\n",
      "            -623.0020,  12986.0020],\n",
      "         [  7451.0000,   3928.9570,  -8299.9980,  ..., -30006.9922,\n",
      "            6531.0000,  -7711.9775],\n",
      "         [ -3128.9961,  60864.9883,   8587.0000,  ...,  -4719.9766,\n",
      "           -9475.9785,  -9875.9941],\n",
      "         ...,\n",
      "         [  7572.0112, -22593.0352,  -4411.0039,  ...,   1582.9858,\n",
      "           -6101.0146,  16358.0254],\n",
      "         [  2222.0234,  21519.0078,   4131.0190,  ...,   2655.9873,\n",
      "          -22538.0195, -20795.9961],\n",
      "         [ 15925.0098, -15699.0029, -12094.9727,  ..., -16352.9883,\n",
      "             296.9917,  20893.0098]]]),) and output (tensor([[[ -8916.0010,   2362.9990,  -4833.0039,  ..., -12075.0020,\n",
      "           -1124.0020,  12438.0020],\n",
      "         [  4753.0000,   7187.9570,   2816.0020,  ..., -30331.9922,\n",
      "           11626.0000, -13004.9775],\n",
      "         [ -3593.9961,  65602.9844,  12272.0000,  ...,   -651.9766,\n",
      "           -3862.9785,  -1493.9941],\n",
      "         ...,\n",
      "         [  3789.0112, -19237.0352,  -7938.0039,  ...,   9595.9863,\n",
      "          -11878.0146,  20077.0254],\n",
      "         [  4875.0234,  25536.0078,  -1244.9810,  ...,   3691.9873,\n",
      "          -24768.0195, -25381.9961],\n",
      "         [  9232.0098, -22243.0039, -16696.9727,  ...,  -7914.9883,\n",
      "            1242.9917,  13325.0098]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[ -8916.0010,   2362.9990,  -4833.0039,  ..., -12075.0020,\n",
      "           -1124.0020,  12438.0020],\n",
      "         [  4753.0000,   7187.9570,   2816.0020,  ..., -30331.9922,\n",
      "           11626.0000, -13004.9775],\n",
      "         [ -3593.9961,  65602.9844,  12272.0000,  ...,   -651.9766,\n",
      "           -3862.9785,  -1493.9941],\n",
      "         ...,\n",
      "         [  3789.0112, -19237.0352,  -7938.0039,  ...,   9595.9863,\n",
      "          -11878.0146,  20077.0254],\n",
      "         [  4875.0234,  25536.0078,  -1244.9810,  ...,   3691.9873,\n",
      "          -24768.0195, -25381.9961],\n",
      "         [  9232.0098, -22243.0039, -16696.9727,  ...,  -7914.9883,\n",
      "            1242.9917,  13325.0098]]]),) and output (tensor([[[-20230.0000,   7877.9990,    161.9961,  ..., -10250.0020,\n",
      "             676.9980,   7095.0020],\n",
      "         [ 10427.0000,   2397.9570,    144.0020,  ..., -36192.9922,\n",
      "           10917.0000, -14263.9775],\n",
      "         [ -4098.9961,  72505.9844,  20839.0000,  ...,  -4136.9766,\n",
      "           -4592.9785,  -6076.9941],\n",
      "         ...,\n",
      "         [ 10258.0117, -21858.0352,  -1337.0039,  ...,  15788.9863,\n",
      "          -19961.0156,  12757.0254],\n",
      "         [  9802.0234,  34055.0078,   1982.0190,  ...,  -3173.0127,\n",
      "          -17303.0195, -21170.9961],\n",
      "         [  7537.0098, -23907.0039, -12115.9727,  ...,  -6054.9883,\n",
      "           -6275.0083,  21042.0098]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[-20230.0000,   7877.9990,    161.9961,  ..., -10250.0020,\n",
      "             676.9980,   7095.0020],\n",
      "         [ 10427.0000,   2397.9570,    144.0020,  ..., -36192.9922,\n",
      "           10917.0000, -14263.9775],\n",
      "         [ -4098.9961,  72505.9844,  20839.0000,  ...,  -4136.9766,\n",
      "           -4592.9785,  -6076.9941],\n",
      "         ...,\n",
      "         [ 10258.0117, -21858.0352,  -1337.0039,  ...,  15788.9863,\n",
      "          -19961.0156,  12757.0254],\n",
      "         [  9802.0234,  34055.0078,   1982.0190,  ...,  -3173.0127,\n",
      "          -17303.0195, -21170.9961],\n",
      "         [  7537.0098, -23907.0039, -12115.9727,  ...,  -6054.9883,\n",
      "           -6275.0083,  21042.0098]]]),) and output (tensor([[[-2.5670e+04,  1.3294e+04,  9.1000e+03,  ..., -6.5470e+03,\n",
      "          -5.6580e+03,  4.1920e+03],\n",
      "         [ 1.2309e+04,  8.5460e+03,  1.6100e+02,  ..., -3.4665e+04,\n",
      "           1.0050e+04, -2.0001e+04],\n",
      "         [-1.9170e+03,  8.0185e+04,  2.3021e+04,  ..., -5.8430e+03,\n",
      "           2.3402e+02, -5.0900e+03],\n",
      "         ...,\n",
      "         [ 5.1800e+03, -2.3035e+04, -5.0004e+01,  ...,  1.1784e+04,\n",
      "          -1.9926e+04,  1.2219e+04],\n",
      "         [ 7.4000e+03,  3.8549e+04,  4.2290e+03,  ..., -7.7840e+03,\n",
      "          -1.7793e+04, -1.3520e+04],\n",
      "         [ 6.4720e+03, -2.2877e+04, -1.0904e+04,  ..., -2.5060e+03,\n",
      "          -6.0320e+03,  2.5141e+04]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[-2.5670e+04,  1.3294e+04,  9.1000e+03,  ..., -6.5470e+03,\n",
      "          -5.6580e+03,  4.1920e+03],\n",
      "         [ 1.2309e+04,  8.5460e+03,  1.6100e+02,  ..., -3.4665e+04,\n",
      "           1.0050e+04, -2.0001e+04],\n",
      "         [-1.9170e+03,  8.0185e+04,  2.3021e+04,  ..., -5.8430e+03,\n",
      "           2.3402e+02, -5.0900e+03],\n",
      "         ...,\n",
      "         [ 5.1800e+03, -2.3035e+04, -5.0004e+01,  ...,  1.1784e+04,\n",
      "          -1.9926e+04,  1.2219e+04],\n",
      "         [ 7.4000e+03,  3.8549e+04,  4.2290e+03,  ..., -7.7840e+03,\n",
      "          -1.7793e+04, -1.3520e+04],\n",
      "         [ 6.4720e+03, -2.2877e+04, -1.0904e+04,  ..., -2.5060e+03,\n",
      "          -6.0320e+03,  2.5141e+04]]]),) and output (tensor([[[-25652.0000,  13180.9990,  15728.9961,  ...,  -2831.0020,\n",
      "             158.9980,   2204.0020],\n",
      "         [  1827.0000,   9116.9570,  -2067.9980,  ..., -29706.9922,\n",
      "           11031.0000, -20054.9766],\n",
      "         [   813.0039,  73736.9844,  21834.0000,  ..., -13582.9766,\n",
      "           -1494.9785,  -8433.9941],\n",
      "         ...,\n",
      "         [  4463.0117, -30293.0352,    734.9961,  ...,  13510.9863,\n",
      "          -18620.0156,  13298.0254],\n",
      "         [ 10375.0234,  47850.0078,   1667.0190,  ...,  -2285.0127,\n",
      "          -18625.0195,  -8507.9961],\n",
      "         [ 13999.0098, -26691.0039, -10355.9727,  ...,   7375.0117,\n",
      "           -5017.0083,  23591.0098]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[-25652.0000,  13180.9990,  15728.9961,  ...,  -2831.0020,\n",
      "             158.9980,   2204.0020],\n",
      "         [  1827.0000,   9116.9570,  -2067.9980,  ..., -29706.9922,\n",
      "           11031.0000, -20054.9766],\n",
      "         [   813.0039,  73736.9844,  21834.0000,  ..., -13582.9766,\n",
      "           -1494.9785,  -8433.9941],\n",
      "         ...,\n",
      "         [  4463.0117, -30293.0352,    734.9961,  ...,  13510.9863,\n",
      "          -18620.0156,  13298.0254],\n",
      "         [ 10375.0234,  47850.0078,   1667.0190,  ...,  -2285.0127,\n",
      "          -18625.0195,  -8507.9961],\n",
      "         [ 13999.0098, -26691.0039, -10355.9727,  ...,   7375.0117,\n",
      "           -5017.0083,  23591.0098]]]),) and output (tensor([[[-14791.0000,  15299.9990,  16129.9961,  ...,   3187.9980,\n",
      "           -2552.0020,   5626.0020],\n",
      "         [  5440.0000,   8329.9570, -12930.9980,  ..., -29184.9922,\n",
      "           22023.0000, -26155.9766],\n",
      "         [  4763.0039,  72597.9844,  15432.0000,  ..., -11276.9766,\n",
      "            5956.0215, -11707.9941],\n",
      "         ...,\n",
      "         [  7432.0117, -33725.0352,    957.9961,  ...,  16459.9863,\n",
      "          -24826.0156,  13657.0254],\n",
      "         [ 17649.0234,  45821.0078,   7313.0190,  ...,  -5238.0127,\n",
      "          -19615.0195, -13437.9961],\n",
      "         [ 16020.0098, -28645.0039, -14619.9727,  ...,   3183.0117,\n",
      "           -5009.0083,  22000.0098]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[-14791.0000,  15299.9990,  16129.9961,  ...,   3187.9980,\n",
      "           -2552.0020,   5626.0020],\n",
      "         [  5440.0000,   8329.9570, -12930.9980,  ..., -29184.9922,\n",
      "           22023.0000, -26155.9766],\n",
      "         [  4763.0039,  72597.9844,  15432.0000,  ..., -11276.9766,\n",
      "            5956.0215, -11707.9941],\n",
      "         ...,\n",
      "         [  7432.0117, -33725.0352,    957.9961,  ...,  16459.9863,\n",
      "          -24826.0156,  13657.0254],\n",
      "         [ 17649.0234,  45821.0078,   7313.0190,  ...,  -5238.0127,\n",
      "          -19615.0195, -13437.9961],\n",
      "         [ 16020.0098, -28645.0039, -14619.9727,  ...,   3183.0117,\n",
      "           -5009.0083,  22000.0098]]]),) and output (tensor([[[-13284.0000,  26639.0000,   7467.9961,  ...,   4113.9980,\n",
      "           -6608.0020,   5336.0020],\n",
      "         [  6031.0000,   8257.9570, -12016.9980,  ..., -35265.9922,\n",
      "           22305.0000, -29519.9766],\n",
      "         [  5032.0039,  69907.9844,  17178.0000,  ...,  -4141.9766,\n",
      "            -863.9785, -15713.9941],\n",
      "         ...,\n",
      "         [  3499.0117, -31109.0352,   3095.9961,  ...,  18416.9863,\n",
      "          -36890.0156,   3250.0254],\n",
      "         [ 18584.0234,  51943.0078,   5021.0190,  ...,  -5706.0127,\n",
      "          -17450.0195, -12299.9961],\n",
      "         [ 18655.0098, -30866.0039, -15683.9727,  ...,   6526.0117,\n",
      "             -95.0083,  25298.0098]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[-13284.0000,  26639.0000,   7467.9961,  ...,   4113.9980,\n",
      "           -6608.0020,   5336.0020],\n",
      "         [  6031.0000,   8257.9570, -12016.9980,  ..., -35265.9922,\n",
      "           22305.0000, -29519.9766],\n",
      "         [  5032.0039,  69907.9844,  17178.0000,  ...,  -4141.9766,\n",
      "            -863.9785, -15713.9941],\n",
      "         ...,\n",
      "         [  3499.0117, -31109.0352,   3095.9961,  ...,  18416.9863,\n",
      "          -36890.0156,   3250.0254],\n",
      "         [ 18584.0234,  51943.0078,   5021.0190,  ...,  -5706.0127,\n",
      "          -17450.0195, -12299.9961],\n",
      "         [ 18655.0098, -30866.0039, -15683.9727,  ...,   6526.0117,\n",
      "             -95.0083,  25298.0098]]]),) and output (tensor([[[-21532.0000,  32796.0000,  16281.9961,  ...,  18208.9980,\n",
      "           -4282.0020,   5756.0020],\n",
      "         [ 13309.0000,   8674.9570,  -7505.9980,  ..., -34687.9922,\n",
      "           25454.0000, -24790.9766],\n",
      "         [ -5349.9961,  68554.9844,  17043.0000,  ...,   4507.0234,\n",
      "           -3543.9785,  -9910.9941],\n",
      "         ...,\n",
      "         [   528.0117, -17492.0352,   4126.9961,  ...,  22367.9863,\n",
      "          -25228.0156,   4735.0254],\n",
      "         [ 19647.0234,  50802.0078,   5225.0190,  ...,  -9184.0127,\n",
      "          -14551.0195,   1463.0039],\n",
      "         [ 20725.0098, -28020.0039, -19586.9727,  ...,   2993.0117,\n",
      "            -320.0083,  25777.0098]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[-21532.0000,  32796.0000,  16281.9961,  ...,  18208.9980,\n",
      "           -4282.0020,   5756.0020],\n",
      "         [ 13309.0000,   8674.9570,  -7505.9980,  ..., -34687.9922,\n",
      "           25454.0000, -24790.9766],\n",
      "         [ -5349.9961,  68554.9844,  17043.0000,  ...,   4507.0234,\n",
      "           -3543.9785,  -9910.9941],\n",
      "         ...,\n",
      "         [   528.0117, -17492.0352,   4126.9961,  ...,  22367.9863,\n",
      "          -25228.0156,   4735.0254],\n",
      "         [ 19647.0234,  50802.0078,   5225.0190,  ...,  -9184.0127,\n",
      "          -14551.0195,   1463.0039],\n",
      "         [ 20725.0098, -28020.0039, -19586.9727,  ...,   2993.0117,\n",
      "            -320.0083,  25777.0098]]]),) and output (tensor([[[-24082.0000,  33387.0000,  15374.9961,  ...,  18359.9980,\n",
      "           -7500.0020,  11519.0020],\n",
      "         [ 12314.0000,   8682.9570,  -2779.9980,  ..., -32942.9922,\n",
      "           27163.0000, -24007.9766],\n",
      "         [-14898.9961,  77391.9844,  25584.0000,  ...,   9266.0234,\n",
      "            2264.0215, -10120.9941],\n",
      "         ...,\n",
      "         [  1776.0117, -20469.0352,    507.9961,  ...,  22196.9863,\n",
      "          -25290.0156,   2885.0254],\n",
      "         [  8345.0234,  59481.0078,   4486.0190,  ...,  -7361.0127,\n",
      "          -16455.0195,  -1944.9961],\n",
      "         [ 16994.0098, -28052.0039, -11915.9727,  ...,   4572.0117,\n",
      "            4771.9917,  25297.0098]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[-24082.0000,  33387.0000,  15374.9961,  ...,  18359.9980,\n",
      "           -7500.0020,  11519.0020],\n",
      "         [ 12314.0000,   8682.9570,  -2779.9980,  ..., -32942.9922,\n",
      "           27163.0000, -24007.9766],\n",
      "         [-14898.9961,  77391.9844,  25584.0000,  ...,   9266.0234,\n",
      "            2264.0215, -10120.9941],\n",
      "         ...,\n",
      "         [  1776.0117, -20469.0352,    507.9961,  ...,  22196.9863,\n",
      "          -25290.0156,   2885.0254],\n",
      "         [  8345.0234,  59481.0078,   4486.0190,  ...,  -7361.0127,\n",
      "          -16455.0195,  -1944.9961],\n",
      "         [ 16994.0098, -28052.0039, -11915.9727,  ...,   4572.0117,\n",
      "            4771.9917,  25297.0098]]]),) and output (tensor([[[-14869.0000,  32284.0000,   3742.9961,  ...,  12458.9980,\n",
      "           -5452.0020,   7590.0020],\n",
      "         [  2025.0000,  12032.9570,  11589.0020,  ..., -26634.9922,\n",
      "           29828.0000, -27305.9766],\n",
      "         [ -8906.9961,  78297.9844,  37910.0000,  ...,  18057.0234,\n",
      "           -1450.9785, -11899.9941],\n",
      "         ...,\n",
      "         [   434.0117, -27429.0352,   7300.9961,  ...,  24831.9863,\n",
      "          -24331.0156,   8487.0254],\n",
      "         [  9713.0234,  51739.0078,   9177.0195,  ...,  -9471.0127,\n",
      "          -12662.0195, -12095.9961],\n",
      "         [ 14132.0098, -28500.0039, -10518.9727,  ...,   -698.9883,\n",
      "            5358.9917,  28607.0098]]]),)\n",
      "[Debug] Layer names being passed: ['model.layers.0', 'model.layers.1', 'model.layers.2', 'model.layers.3', 'model.layers.4', 'model.layers.5', 'model.layers.6', 'model.layers.7', 'model.layers.8', 'model.layers.9', 'model.layers.10', 'model.layers.11', 'model.layers.12', 'model.layers.13', 'model.layers.14', 'model.layers.15', 'model.layers.16', 'model.layers.17', 'model.layers.18', 'model.layers.19', 'model.layers.20', 'model.layers.21', 'model.layers.22', 'model.layers.23', 'model.layers.24', 'model.layers.25', 'model.layers.26', 'model.layers.27', 'model.embed_tokens']\n",
      "[Debug] Trying to access layer: model.layers.0\n",
      "[Debug] Successfully found layer: model.layers.0\n",
      "[Debug] Trying to access layer: model.layers.1\n",
      "[Debug] Successfully found layer: model.layers.1\n",
      "[Debug] Trying to access layer: model.layers.2\n",
      "[Debug] Successfully found layer: model.layers.2\n",
      "[Debug] Trying to access layer: model.layers.3\n",
      "[Debug] Successfully found layer: model.layers.3\n",
      "[Debug] Trying to access layer: model.layers.4\n",
      "[Debug] Successfully found layer: model.layers.4\n",
      "[Debug] Trying to access layer: model.layers.5\n",
      "[Debug] Successfully found layer: model.layers.5\n",
      "[Debug] Trying to access layer: model.layers.6\n",
      "[Debug] Successfully found layer: model.layers.6\n",
      "[Debug] Trying to access layer: model.layers.7\n",
      "[Debug] Successfully found layer: model.layers.7\n",
      "[Debug] Trying to access layer: model.layers.8\n",
      "[Debug] Successfully found layer: model.layers.8\n",
      "[Debug] Trying to access layer: model.layers.9\n",
      "[Debug] Successfully found layer: model.layers.9\n",
      "[Debug] Trying to access layer: model.layers.10\n",
      "[Debug] Successfully found layer: model.layers.10\n",
      "[Debug] Trying to access layer: model.layers.11\n",
      "[Debug] Successfully found layer: model.layers.11\n",
      "[Debug] Trying to access layer: model.layers.12\n",
      "[Debug] Successfully found layer: model.layers.12\n",
      "[Debug] Trying to access layer: model.layers.13\n",
      "[Debug] Successfully found layer: model.layers.13\n",
      "[Debug] Trying to access layer: model.layers.14\n",
      "[Debug] Successfully found layer: model.layers.14\n",
      "[Debug] Trying to access layer: model.layers.15\n",
      "[Debug] Successfully found layer: model.layers.15\n",
      "[Debug] Trying to access layer: model.layers.16\n",
      "[Debug] Successfully found layer: model.layers.16\n",
      "[Debug] Trying to access layer: model.layers.17\n",
      "[Debug] Successfully found layer: model.layers.17\n",
      "[Debug] Trying to access layer: model.layers.18\n",
      "[Debug] Successfully found layer: model.layers.18\n",
      "[Debug] Trying to access layer: model.layers.19\n",
      "[Debug] Successfully found layer: model.layers.19\n",
      "[Debug] Trying to access layer: model.layers.20\n",
      "[Debug] Successfully found layer: model.layers.20\n",
      "[Debug] Trying to access layer: model.layers.21\n",
      "[Debug] Successfully found layer: model.layers.21\n",
      "[Debug] Trying to access layer: model.layers.22\n",
      "[Debug] Successfully found layer: model.layers.22\n",
      "[Debug] Trying to access layer: model.layers.23\n",
      "[Debug] Successfully found layer: model.layers.23\n",
      "[Debug] Trying to access layer: model.layers.24\n",
      "[Debug] Successfully found layer: model.layers.24\n",
      "[Debug] Trying to access layer: model.layers.25\n",
      "[Debug] Successfully found layer: model.layers.25\n",
      "[Debug] Trying to access layer: model.layers.26\n",
      "[Debug] Successfully found layer: model.layers.26\n",
      "[Debug] Trying to access layer: model.layers.27\n",
      "[Debug] Successfully found layer: model.layers.27\n",
      "[Debug] Trying to access layer: model.embed_tokens\n",
      "[Debug] Successfully found layer: model.embed_tokens\n",
      "[Hook] Layer Embedding(128256, 3072, padding_idx=128004) received input (tensor([[128000,  25499,    315,    279,  10734,    315,   2418,    323,    315,\n",
      "            279,  47317,    578,  19476,    304,    279,  42021,   2586,    505,\n",
      "            279,  41903,    323,   5054,  22006,    315,    279,  92931,     11,\n",
      "           1778,    439,   3927,   2191,     11,    279,   3674,   5226,    439,\n",
      "          46820,   1534,    555,    279,  24983,  16023,  55475,  98989,   2933,\n",
      "             11,    323,    279,  25768,    315,  13736,  16948,  37588,    555,\n",
      "            279,  54007,    409,   9995,    288,    447,  26235,     13,   1666,\n",
      "            649,    387,   3970,    304,    279,  22755,     11,    279,   8753,\n",
      "          18489,    574,  17345,  28160,    555,    279,   5054,  19675,    315,\n",
      "            279,  92931,    323,  16565,    315,   3823,   3268,    439,    574,\n",
      "            279,    549,    815,     13,  42021,    315,  44177,    902,  53580,\n",
      "            433,    320,     19,   5887,    220,  11242,     21,    570]]),) and output tensor([[[-0.0010, -0.0007, -0.0046,  ..., -0.0014, -0.0020,  0.0020],\n",
      "         [ 0.0187, -0.0200, -0.0173,  ...,  0.0332, -0.0049,  0.0227],\n",
      "         [ 0.0045, -0.0109,  0.0005,  ...,  0.0231,  0.0219,  0.0065],\n",
      "         ...,\n",
      "         [-0.0303, -0.0140,  0.0011,  ..., -0.0004, -0.0031,  0.0238],\n",
      "         [ 0.0139,  0.0073,  0.0117,  ..., -0.0192,  0.0009, -0.0121],\n",
      "         [-0.0198, -0.0332,  0.0024,  ..., -0.0077,  0.0262,  0.0134]]])\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[-0.0010, -0.0007, -0.0046,  ..., -0.0014, -0.0020,  0.0020],\n",
      "         [ 0.0187, -0.0200, -0.0173,  ...,  0.0332, -0.0049,  0.0227],\n",
      "         [ 0.0045, -0.0109,  0.0005,  ...,  0.0231,  0.0219,  0.0065],\n",
      "         ...,\n",
      "         [-0.0303, -0.0140,  0.0011,  ..., -0.0004, -0.0031,  0.0238],\n",
      "         [ 0.0139,  0.0073,  0.0117,  ..., -0.0192,  0.0009, -0.0121],\n",
      "         [-0.0198, -0.0332,  0.0024,  ..., -0.0077,  0.0262,  0.0134]]]),) and output (tensor([[[ 442.9990,  239.9993,  302.9954,  ..., -154.0014,  -15.0020,\n",
      "           185.0020],\n",
      "         [ 266.0187,  201.9800,   77.9827,  ...,  -31.9668,   77.9951,\n",
      "           -29.9773],\n",
      "         [ -30.9955,  -88.0109, -233.9995,  ...,  -80.9769,   27.0219,\n",
      "            65.0065],\n",
      "         ...,\n",
      "         [  10.9697, -158.0140, -127.9989,  ..., -224.0004,   77.9969,\n",
      "            92.0238],\n",
      "         [ -35.9861,  -50.9927,  -98.9883,  ...,   18.9808,  -42.9991,\n",
      "            22.9879],\n",
      "         [ 303.9802,  508.9668, -283.9976,  ..., -179.0077, -294.9738,\n",
      "          -167.9866]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[ 442.9990,  239.9993,  302.9954,  ..., -154.0014,  -15.0020,\n",
      "           185.0020],\n",
      "         [ 266.0187,  201.9800,   77.9827,  ...,  -31.9668,   77.9951,\n",
      "           -29.9773],\n",
      "         [ -30.9955,  -88.0109, -233.9995,  ...,  -80.9769,   27.0219,\n",
      "            65.0065],\n",
      "         ...,\n",
      "         [  10.9697, -158.0140, -127.9989,  ..., -224.0004,   77.9969,\n",
      "            92.0238],\n",
      "         [ -35.9861,  -50.9927,  -98.9883,  ...,   18.9808,  -42.9991,\n",
      "            22.9879],\n",
      "         [ 303.9802,  508.9668, -283.9976,  ..., -179.0077, -294.9738,\n",
      "          -167.9866]]]),) and output (tensor([[[  439.9990,  -144.0007,  -410.0046,  ...,  -765.0015,\n",
      "           1193.9979, -1027.9980],\n",
      "         [  120.0187,  -389.0200,   395.9827,  ...,    46.0332,\n",
      "           1080.9951,  -450.9773],\n",
      "         [  322.0045,   394.9891,  -266.9995,  ...,  -741.9769,\n",
      "            242.0219,   -89.9935],\n",
      "         ...,\n",
      "         [  509.9697,    12.9860,  -197.9989,  ...,   821.9996,\n",
      "           -687.0031,  -284.9762],\n",
      "         [ -959.9861,  -383.9927,   182.0117,  ...,   508.9808,\n",
      "            -38.9991,   299.9879],\n",
      "         [  124.9802,   618.9668,  -181.9976,  ...,   -53.0077,\n",
      "            398.0262,   -60.9866]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[  439.9990,  -144.0007,  -410.0046,  ...,  -765.0015,\n",
      "           1193.9979, -1027.9980],\n",
      "         [  120.0187,  -389.0200,   395.9827,  ...,    46.0332,\n",
      "           1080.9951,  -450.9773],\n",
      "         [  322.0045,   394.9891,  -266.9995,  ...,  -741.9769,\n",
      "            242.0219,   -89.9935],\n",
      "         ...,\n",
      "         [  509.9697,    12.9860,  -197.9989,  ...,   821.9996,\n",
      "           -687.0031,  -284.9762],\n",
      "         [ -959.9861,  -383.9927,   182.0117,  ...,   508.9808,\n",
      "            -38.9991,   299.9879],\n",
      "         [  124.9802,   618.9668,  -181.9976,  ...,   -53.0077,\n",
      "            398.0262,   -60.9866]]]),) and output (tensor([[[  551.9990,   993.9993, -1790.0046,  ...,   445.9985,\n",
      "          -1386.0021, -1245.9980],\n",
      "         [-4676.9814, -1997.0200,  1571.9827,  ...,  -786.9668,\n",
      "          -2411.0049, -1899.9773],\n",
      "         [  747.0045,  3320.9890, -3425.9995,  ..., -1439.9769,\n",
      "          -1899.9781, -1472.9935],\n",
      "         ...,\n",
      "         [-2323.0303,  -120.0140,  4051.0010,  ...,  5151.9995,\n",
      "          -3675.0029, -2497.9761],\n",
      "         [ 1892.0139,  2074.0073,   -89.9883,  ...,   260.9808,\n",
      "           1214.0010,   325.9879],\n",
      "         [ -665.0198,  1823.9668,  1160.0024,  ..., -1445.0077,\n",
      "           -895.9738,  -242.9866]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[  551.9990,   993.9993, -1790.0046,  ...,   445.9985,\n",
      "          -1386.0021, -1245.9980],\n",
      "         [-4676.9814, -1997.0200,  1571.9827,  ...,  -786.9668,\n",
      "          -2411.0049, -1899.9773],\n",
      "         [  747.0045,  3320.9890, -3425.9995,  ..., -1439.9769,\n",
      "          -1899.9781, -1472.9935],\n",
      "         ...,\n",
      "         [-2323.0303,  -120.0140,  4051.0010,  ...,  5151.9995,\n",
      "          -3675.0029, -2497.9761],\n",
      "         [ 1892.0139,  2074.0073,   -89.9883,  ...,   260.9808,\n",
      "           1214.0010,   325.9879],\n",
      "         [ -665.0198,  1823.9668,  1160.0024,  ..., -1445.0077,\n",
      "           -895.9738,  -242.9866]]]),) and output (tensor([[[ 2.1320e+03, -7.3700e+02, -1.0710e+03,  ..., -2.7810e+03,\n",
      "          -8.8500e+02,  8.6002e+01],\n",
      "         [-3.8000e+03, -3.6310e+03,  5.1490e+03,  ...,  1.0360e+03,\n",
      "          -4.0780e+03,  5.9902e+02],\n",
      "         [-4.9300e+02,  6.6580e+03, -3.4590e+03,  ..., -3.2970e+03,\n",
      "          -1.6850e+03, -9.0030e+03],\n",
      "         ...,\n",
      "         [-9.0303e+00, -7.2001e+02,  7.6380e+03,  ...,  4.9290e+03,\n",
      "          -1.6070e+04, -2.8790e+03],\n",
      "         [-5.6399e+02,  9.1220e+03, -5.7199e+02,  ..., -4.1620e+03,\n",
      "           1.6800e+02,  1.9910e+03],\n",
      "         [ 4.2420e+03, -3.3410e+03, -8.7700e+02,  ..., -4.4800e+03,\n",
      "          -5.2480e+03, -1.3540e+03]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[ 2.1320e+03, -7.3700e+02, -1.0710e+03,  ..., -2.7810e+03,\n",
      "          -8.8500e+02,  8.6002e+01],\n",
      "         [-3.8000e+03, -3.6310e+03,  5.1490e+03,  ...,  1.0360e+03,\n",
      "          -4.0780e+03,  5.9902e+02],\n",
      "         [-4.9300e+02,  6.6580e+03, -3.4590e+03,  ..., -3.2970e+03,\n",
      "          -1.6850e+03, -9.0030e+03],\n",
      "         ...,\n",
      "         [-9.0303e+00, -7.2001e+02,  7.6380e+03,  ...,  4.9290e+03,\n",
      "          -1.6070e+04, -2.8790e+03],\n",
      "         [-5.6399e+02,  9.1220e+03, -5.7199e+02,  ..., -4.1620e+03,\n",
      "           1.6800e+02,  1.9910e+03],\n",
      "         [ 4.2420e+03, -3.3410e+03, -8.7700e+02,  ..., -4.4800e+03,\n",
      "          -5.2480e+03, -1.3540e+03]]]),) and output (tensor([[[  3420.9990,    760.9993,  -5376.0049,  ...,  -4193.0015,\n",
      "            2860.9980,  -4080.9980],\n",
      "         [ -2248.9814,   -349.0200,   7795.9824,  ...,   2925.0332,\n",
      "           -7357.0049,   4413.0225],\n",
      "         [ -3207.9956,   9912.9893,  -8746.0000,  ...,  -3152.9771,\n",
      "           -4238.9780, -14024.9932],\n",
      "         ...,\n",
      "         [  3368.9697,  -4658.0142,   7607.0010,  ...,   5767.9995,\n",
      "          -23419.0039,   1641.0239],\n",
      "         [ -3945.9861,  14966.0078,   -648.9883,  ..., -11814.0195,\n",
      "            -446.9990,  -1812.0122],\n",
      "         [ 12781.9805,  -3691.0332,  -2887.9976,  ...,  -4567.0078,\n",
      "           -9046.9736,   1611.0134]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[  3420.9990,    760.9993,  -5376.0049,  ...,  -4193.0015,\n",
      "            2860.9980,  -4080.9980],\n",
      "         [ -2248.9814,   -349.0200,   7795.9824,  ...,   2925.0332,\n",
      "           -7357.0049,   4413.0225],\n",
      "         [ -3207.9956,   9912.9893,  -8746.0000,  ...,  -3152.9771,\n",
      "           -4238.9780, -14024.9932],\n",
      "         ...,\n",
      "         [  3368.9697,  -4658.0142,   7607.0010,  ...,   5767.9995,\n",
      "          -23419.0039,   1641.0239],\n",
      "         [ -3945.9861,  14966.0078,   -648.9883,  ..., -11814.0195,\n",
      "            -446.9990,  -1812.0122],\n",
      "         [ 12781.9805,  -3691.0332,  -2887.9976,  ...,  -4567.0078,\n",
      "           -9046.9736,   1611.0134]]]),) and output (tensor([[[  2742.9990,  -1512.0007, -10493.0049,  ...,  -9945.0020,\n",
      "           -2017.0020,  -1802.9980],\n",
      "         [ -2249.9814,   3206.9800,   6916.9824,  ...,   6403.0332,\n",
      "           -7673.0049,   8355.0225],\n",
      "         [ -1764.9956,  11461.9893,  -9174.0000,  ...,  -5931.9771,\n",
      "           -5633.9780, -18375.9922],\n",
      "         ...,\n",
      "         [  5363.9697,  -4848.0142,  10176.0010,  ...,  13351.0000,\n",
      "          -19994.0039,   4407.0239],\n",
      "         [ -4943.9863,  18564.0078,  -3397.9883,  ..., -12342.0195,\n",
      "            4143.0010,  -3116.0122],\n",
      "         [ 23179.9805,  -1131.0332,  -2081.9976,  ...,  -2826.0078,\n",
      "           -8411.9736,   6499.0137]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[  2742.9990,  -1512.0007, -10493.0049,  ...,  -9945.0020,\n",
      "           -2017.0020,  -1802.9980],\n",
      "         [ -2249.9814,   3206.9800,   6916.9824,  ...,   6403.0332,\n",
      "           -7673.0049,   8355.0225],\n",
      "         [ -1764.9956,  11461.9893,  -9174.0000,  ...,  -5931.9771,\n",
      "           -5633.9780, -18375.9922],\n",
      "         ...,\n",
      "         [  5363.9697,  -4848.0142,  10176.0010,  ...,  13351.0000,\n",
      "          -19994.0039,   4407.0239],\n",
      "         [ -4943.9863,  18564.0078,  -3397.9883,  ..., -12342.0195,\n",
      "            4143.0010,  -3116.0122],\n",
      "         [ 23179.9805,  -1131.0332,  -2081.9976,  ...,  -2826.0078,\n",
      "           -8411.9736,   6499.0137]]]),) and output (tensor([[[  3032.9990,  -3420.0007, -10291.0049,  ...,  -6511.0020,\n",
      "           -1953.0020,   2001.0020],\n",
      "         [ -6872.9814,   5339.9800,   9071.9824,  ...,   8535.0332,\n",
      "           -9054.0049,  18727.0234],\n",
      "         [  3262.0044,   7517.9893, -15005.0000,  ...,  -3016.9771,\n",
      "           -9962.9785, -22976.9922],\n",
      "         ...,\n",
      "         [  1842.9697,  -6279.0142,  12673.0010,  ...,  14555.0000,\n",
      "          -28985.0039,   9297.0234],\n",
      "         [  -506.9863,  24932.0078,  -7516.9883,  ...,  -9325.0195,\n",
      "            1301.0010,  -2145.0122],\n",
      "         [ 24557.9805,  -2124.0332,   -711.9976,  ...,  -4198.0078,\n",
      "          -12831.9736,   8676.0137]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[  3032.9990,  -3420.0007, -10291.0049,  ...,  -6511.0020,\n",
      "           -1953.0020,   2001.0020],\n",
      "         [ -6872.9814,   5339.9800,   9071.9824,  ...,   8535.0332,\n",
      "           -9054.0049,  18727.0234],\n",
      "         [  3262.0044,   7517.9893, -15005.0000,  ...,  -3016.9771,\n",
      "           -9962.9785, -22976.9922],\n",
      "         ...,\n",
      "         [  1842.9697,  -6279.0142,  12673.0010,  ...,  14555.0000,\n",
      "          -28985.0039,   9297.0234],\n",
      "         [  -506.9863,  24932.0078,  -7516.9883,  ...,  -9325.0195,\n",
      "            1301.0010,  -2145.0122],\n",
      "         [ 24557.9805,  -2124.0332,   -711.9976,  ...,  -4198.0078,\n",
      "          -12831.9736,   8676.0137]]]),) and output (tensor([[[  3132.9990,  -1970.0007, -11544.0049,  ...,  -5780.0020,\n",
      "           -2486.0020,  -1921.9980],\n",
      "         [ -6798.9814,   1374.9800,   4837.9824,  ...,   2230.0332,\n",
      "           -5051.0049,  17656.0234],\n",
      "         [  7763.0044,   7032.9893, -20603.0000,  ...,    -73.9771,\n",
      "           -3282.9785, -23114.9922],\n",
      "         ...,\n",
      "         [  4396.9697,  -7426.0142,  12523.0010,  ...,  17336.0000,\n",
      "          -32507.0039,  11082.0234],\n",
      "         [   436.0137,  26971.0078,  -6618.9883,  ..., -14337.0195,\n",
      "             962.0010,  -5046.0122],\n",
      "         [ 28039.9805,  -6414.0332,  -4326.9976,  ...,  -8750.0078,\n",
      "          -11461.9736,  11742.0137]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[  3132.9990,  -1970.0007, -11544.0049,  ...,  -5780.0020,\n",
      "           -2486.0020,  -1921.9980],\n",
      "         [ -6798.9814,   1374.9800,   4837.9824,  ...,   2230.0332,\n",
      "           -5051.0049,  17656.0234],\n",
      "         [  7763.0044,   7032.9893, -20603.0000,  ...,    -73.9771,\n",
      "           -3282.9785, -23114.9922],\n",
      "         ...,\n",
      "         [  4396.9697,  -7426.0142,  12523.0010,  ...,  17336.0000,\n",
      "          -32507.0039,  11082.0234],\n",
      "         [   436.0137,  26971.0078,  -6618.9883,  ..., -14337.0195,\n",
      "             962.0010,  -5046.0122],\n",
      "         [ 28039.9805,  -6414.0332,  -4326.9976,  ...,  -8750.0078,\n",
      "          -11461.9736,  11742.0137]]]),) and output (tensor([[[  1046.9990,   2403.9993, -14486.0049,  ..., -10922.0020,\n",
      "           -1419.0020,  -2983.9980],\n",
      "         [ -6955.9814,    240.9800,   2844.9824,  ...,   3757.0332,\n",
      "           -6785.0049,  20549.0234],\n",
      "         [ 10558.0039,   5852.9893, -21341.0000,  ...,  -1634.9771,\n",
      "           -1273.9785, -27561.9922],\n",
      "         ...,\n",
      "         [  1788.9697,  -1737.0142,  14959.0010,  ...,  19589.0000,\n",
      "          -37409.0039,   5794.0234],\n",
      "         [   984.0137,  30392.0078,  -2065.9883,  ..., -15329.0195,\n",
      "            4594.0010, -11772.0117],\n",
      "         [ 32058.9805,  -3370.0332,  -7566.9976,  ...,  -5541.0078,\n",
      "          -10342.9736,  10160.0137]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[  1046.9990,   2403.9993, -14486.0049,  ..., -10922.0020,\n",
      "           -1419.0020,  -2983.9980],\n",
      "         [ -6955.9814,    240.9800,   2844.9824,  ...,   3757.0332,\n",
      "           -6785.0049,  20549.0234],\n",
      "         [ 10558.0039,   5852.9893, -21341.0000,  ...,  -1634.9771,\n",
      "           -1273.9785, -27561.9922],\n",
      "         ...,\n",
      "         [  1788.9697,  -1737.0142,  14959.0010,  ...,  19589.0000,\n",
      "          -37409.0039,   5794.0234],\n",
      "         [   984.0137,  30392.0078,  -2065.9883,  ..., -15329.0195,\n",
      "            4594.0010, -11772.0117],\n",
      "         [ 32058.9805,  -3370.0332,  -7566.9976,  ...,  -5541.0078,\n",
      "          -10342.9736,  10160.0137]]]),) and output (tensor([[[ -3767.0010,   5307.9990, -14221.0049,  ..., -15035.0020,\n",
      "           -3630.0020,   4168.0020],\n",
      "         [-10687.9814,   5013.9800,   2954.9824,  ...,   4129.0332,\n",
      "           -5432.0049,  18063.0234],\n",
      "         [  8638.0039,   -820.0107, -24289.0000,  ...,  -1852.9771,\n",
      "           -6446.9785, -28869.9922],\n",
      "         ...,\n",
      "         [  3499.9697,   1251.9858,  19462.0000,  ...,  15054.0000,\n",
      "          -46449.0039,   7335.0234],\n",
      "         [ -1590.9863,  31865.0078,  -2868.9883,  ..., -19194.0195,\n",
      "            5692.0010, -12076.0117],\n",
      "         [ 29354.9805,  -9140.0332,  -7099.9976,  ...,  -4763.0078,\n",
      "          -16228.9736,   2607.0137]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[ -3767.0010,   5307.9990, -14221.0049,  ..., -15035.0020,\n",
      "           -3630.0020,   4168.0020],\n",
      "         [-10687.9814,   5013.9800,   2954.9824,  ...,   4129.0332,\n",
      "           -5432.0049,  18063.0234],\n",
      "         [  8638.0039,   -820.0107, -24289.0000,  ...,  -1852.9771,\n",
      "           -6446.9785, -28869.9922],\n",
      "         ...,\n",
      "         [  3499.9697,   1251.9858,  19462.0000,  ...,  15054.0000,\n",
      "          -46449.0039,   7335.0234],\n",
      "         [ -1590.9863,  31865.0078,  -2868.9883,  ..., -19194.0195,\n",
      "            5692.0010, -12076.0117],\n",
      "         [ 29354.9805,  -9140.0332,  -7099.9976,  ...,  -4763.0078,\n",
      "          -16228.9736,   2607.0137]]]),) and output (tensor([[[ -1326.0010,   1223.9990, -16643.0039,  ..., -13240.0020,\n",
      "           -5677.0020,   9639.0020],\n",
      "         [-15667.9814,   6291.9800,   2134.9824,  ...,   3316.0332,\n",
      "           -6858.0049,  20188.0234],\n",
      "         [ 11345.0039,  -3516.0107, -23475.0000,  ...,  -1130.9771,\n",
      "           -6248.9785, -29286.9922],\n",
      "         ...,\n",
      "         [  7007.9697,   -948.0142,  15084.0000,  ...,  12946.0000,\n",
      "          -53996.0039,  12104.0234],\n",
      "         [ -6043.9863,  37779.0078,   -397.9883,  ..., -26485.0195,\n",
      "            2261.0010, -10358.0117],\n",
      "         [ 39125.9805, -12003.0332,  -5000.9976,  ...,  -9011.0078,\n",
      "          -15126.9736,   5361.0137]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[ -1326.0010,   1223.9990, -16643.0039,  ..., -13240.0020,\n",
      "           -5677.0020,   9639.0020],\n",
      "         [-15667.9814,   6291.9800,   2134.9824,  ...,   3316.0332,\n",
      "           -6858.0049,  20188.0234],\n",
      "         [ 11345.0039,  -3516.0107, -23475.0000,  ...,  -1130.9771,\n",
      "           -6248.9785, -29286.9922],\n",
      "         ...,\n",
      "         [  7007.9697,   -948.0142,  15084.0000,  ...,  12946.0000,\n",
      "          -53996.0039,  12104.0234],\n",
      "         [ -6043.9863,  37779.0078,   -397.9883,  ..., -26485.0195,\n",
      "            2261.0010, -10358.0117],\n",
      "         [ 39125.9805, -12003.0332,  -5000.9976,  ...,  -9011.0078,\n",
      "          -15126.9736,   5361.0137]]]),) and output (tensor([[[ -5120.0010,   2246.9990, -15993.0039,  ..., -19818.0020,\n",
      "           -9385.0020,  11924.0020],\n",
      "         [-18546.9805,   5376.9800,   4113.9824,  ...,   6731.0332,\n",
      "           -9587.0049,  19115.0234],\n",
      "         [  9087.0039,   -534.0107, -22586.0000,  ...,   -617.9771,\n",
      "           -6414.9785, -25891.9922],\n",
      "         ...,\n",
      "         [  7389.9697,   2599.9858,  12673.0000,  ...,   6849.0000,\n",
      "          -57470.0039,  11257.0234],\n",
      "         [ -7395.9863,  38563.0078,  -2854.9883,  ..., -28486.0195,\n",
      "            2235.0010,  -8156.0117],\n",
      "         [ 43003.9805,  -6469.0332,  -4527.9976,  ..., -12540.0078,\n",
      "           -9149.9736,   7941.0137]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[ -5120.0010,   2246.9990, -15993.0039,  ..., -19818.0020,\n",
      "           -9385.0020,  11924.0020],\n",
      "         [-18546.9805,   5376.9800,   4113.9824,  ...,   6731.0332,\n",
      "           -9587.0049,  19115.0234],\n",
      "         [  9087.0039,   -534.0107, -22586.0000,  ...,   -617.9771,\n",
      "           -6414.9785, -25891.9922],\n",
      "         ...,\n",
      "         [  7389.9697,   2599.9858,  12673.0000,  ...,   6849.0000,\n",
      "          -57470.0039,  11257.0234],\n",
      "         [ -7395.9863,  38563.0078,  -2854.9883,  ..., -28486.0195,\n",
      "            2235.0010,  -8156.0117],\n",
      "         [ 43003.9805,  -6469.0332,  -4527.9976,  ..., -12540.0078,\n",
      "           -9149.9736,   7941.0137]]]),) and output (tensor([[[ -6685.0010,  -1278.0010, -15553.0039,  ..., -19846.0020,\n",
      "          -10418.0020,  13376.0020],\n",
      "         [-19659.9805,   2987.9800,   6981.9824,  ...,   6549.0332,\n",
      "          -10637.0049,  23193.0234],\n",
      "         [  9820.0039,   5500.9893, -23653.0000,  ...,  -1295.9771,\n",
      "           -5557.9785, -28935.9922],\n",
      "         ...,\n",
      "         [  5746.9697,   2252.9858,  16818.0000,  ...,   5801.0000,\n",
      "          -60188.0039,  16664.0234],\n",
      "         [ -5194.9863,  46829.0078,  -3035.9883,  ..., -31473.0195,\n",
      "            5979.0010,  -6195.0117],\n",
      "         [ 46422.9805,  -9720.0332,  -5611.9976,  ..., -11233.0078,\n",
      "           -2754.9736,  14511.0137]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[ -6685.0010,  -1278.0010, -15553.0039,  ..., -19846.0020,\n",
      "          -10418.0020,  13376.0020],\n",
      "         [-19659.9805,   2987.9800,   6981.9824,  ...,   6549.0332,\n",
      "          -10637.0049,  23193.0234],\n",
      "         [  9820.0039,   5500.9893, -23653.0000,  ...,  -1295.9771,\n",
      "           -5557.9785, -28935.9922],\n",
      "         ...,\n",
      "         [  5746.9697,   2252.9858,  16818.0000,  ...,   5801.0000,\n",
      "          -60188.0039,  16664.0234],\n",
      "         [ -5194.9863,  46829.0078,  -3035.9883,  ..., -31473.0195,\n",
      "            5979.0010,  -6195.0117],\n",
      "         [ 46422.9805,  -9720.0332,  -5611.9976,  ..., -11233.0078,\n",
      "           -2754.9736,  14511.0137]]]),) and output (tensor([[[ -8422.0010,    339.9990, -15289.0039,  ..., -20884.0020,\n",
      "           -6908.0020,  11014.0020],\n",
      "         [-22246.9805,   2075.9800,   3116.9824,  ...,   6225.0332,\n",
      "           -2298.0049,  20802.0234],\n",
      "         [ 12320.0039,    472.9893, -23880.0000,  ...,   -225.9771,\n",
      "           -7338.9785, -31840.9922],\n",
      "         ...,\n",
      "         [  4341.9697,   6338.9858,  21452.0000,  ...,   3431.0000,\n",
      "          -67784.0000,  17012.0234],\n",
      "         [ -5821.9863,  51387.0078,  -3722.9883,  ..., -38816.0195,\n",
      "            6767.0010, -10691.0117],\n",
      "         [ 49755.9805,  -8055.0332,  -6545.9976,  ...,  -5112.0078,\n",
      "           -1336.9736,  14441.0137]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[ -8422.0010,    339.9990, -15289.0039,  ..., -20884.0020,\n",
      "           -6908.0020,  11014.0020],\n",
      "         [-22246.9805,   2075.9800,   3116.9824,  ...,   6225.0332,\n",
      "           -2298.0049,  20802.0234],\n",
      "         [ 12320.0039,    472.9893, -23880.0000,  ...,   -225.9771,\n",
      "           -7338.9785, -31840.9922],\n",
      "         ...,\n",
      "         [  4341.9697,   6338.9858,  21452.0000,  ...,   3431.0000,\n",
      "          -67784.0000,  17012.0234],\n",
      "         [ -5821.9863,  51387.0078,  -3722.9883,  ..., -38816.0195,\n",
      "            6767.0010, -10691.0117],\n",
      "         [ 49755.9805,  -8055.0332,  -6545.9976,  ...,  -5112.0078,\n",
      "           -1336.9736,  14441.0137]]]),) and output (tensor([[[-10989.0010,   7034.9990, -13964.0039,  ..., -23968.0020,\n",
      "           -3749.0020,  11730.0020],\n",
      "         [-20899.9805,   3301.9800,   3627.9824,  ...,   3212.0332,\n",
      "           -6394.0049,  15568.0234],\n",
      "         [ 10169.0039,   2043.9893, -21599.0000,  ...,  -6966.9771,\n",
      "            -880.9785, -33715.9922],\n",
      "         ...,\n",
      "         [  1277.9697,   6947.9858,  20530.0000,  ...,   8242.0000,\n",
      "          -74897.0000,  13550.0234],\n",
      "         [ -6080.9863,  52654.0078,  -6664.9883,  ..., -35777.0195,\n",
      "            4228.0010,  -8375.0117],\n",
      "         [ 58963.9805,  -5969.0332,  -6572.9976,  ...,  -1293.0078,\n",
      "           -2520.9736,  14753.0137]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[-10989.0010,   7034.9990, -13964.0039,  ..., -23968.0020,\n",
      "           -3749.0020,  11730.0020],\n",
      "         [-20899.9805,   3301.9800,   3627.9824,  ...,   3212.0332,\n",
      "           -6394.0049,  15568.0234],\n",
      "         [ 10169.0039,   2043.9893, -21599.0000,  ...,  -6966.9771,\n",
      "            -880.9785, -33715.9922],\n",
      "         ...,\n",
      "         [  1277.9697,   6947.9858,  20530.0000,  ...,   8242.0000,\n",
      "          -74897.0000,  13550.0234],\n",
      "         [ -6080.9863,  52654.0078,  -6664.9883,  ..., -35777.0195,\n",
      "            4228.0010,  -8375.0117],\n",
      "         [ 58963.9805,  -5969.0332,  -6572.9976,  ...,  -1293.0078,\n",
      "           -2520.9736,  14753.0137]]]),) and output (tensor([[[ -7159.0010,   6621.9990, -10967.0039,  ..., -17477.0020,\n",
      "           -2510.0020,  11888.0020],\n",
      "         [-13139.9805,   2340.9800,   3928.9824,  ...,   -592.9668,\n",
      "           -6459.0049,  12977.0234],\n",
      "         [  6303.0039,   1869.9893, -18287.0000,  ...,  -6194.9771,\n",
      "           -8623.9785, -38077.9922],\n",
      "         ...,\n",
      "         [ -4185.0303,   5070.9858,  20814.0000,  ...,   9921.0000,\n",
      "          -82307.0000,  11797.0234],\n",
      "         [ -8043.9863,  57987.0078,  -5718.9883,  ..., -45788.0195,\n",
      "            3576.0010, -11496.0117],\n",
      "         [ 59491.9805, -12415.0332,  -5316.9976,  ...,   1106.9922,\n",
      "           -3586.9736,  15623.0137]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[ -7159.0010,   6621.9990, -10967.0039,  ..., -17477.0020,\n",
      "           -2510.0020,  11888.0020],\n",
      "         [-13139.9805,   2340.9800,   3928.9824,  ...,   -592.9668,\n",
      "           -6459.0049,  12977.0234],\n",
      "         [  6303.0039,   1869.9893, -18287.0000,  ...,  -6194.9771,\n",
      "           -8623.9785, -38077.9922],\n",
      "         ...,\n",
      "         [ -4185.0303,   5070.9858,  20814.0000,  ...,   9921.0000,\n",
      "          -82307.0000,  11797.0234],\n",
      "         [ -8043.9863,  57987.0078,  -5718.9883,  ..., -45788.0195,\n",
      "            3576.0010, -11496.0117],\n",
      "         [ 59491.9805, -12415.0332,  -5316.9976,  ...,   1106.9922,\n",
      "           -3586.9736,  15623.0137]]]),) and output (tensor([[[ -4822.0010,   3124.9990,  -3885.0039,  ..., -19007.0020,\n",
      "            -551.0020,  16285.0020],\n",
      "         [-14074.9805,   5451.9800,   4658.9824,  ...,  -2389.9668,\n",
      "           -9142.0049,  15443.0234],\n",
      "         [  5586.0039,  -1742.0107, -20944.0000,  ...,  -5018.9771,\n",
      "          -11242.9785, -44480.9922],\n",
      "         ...,\n",
      "         [  1345.9697,   1923.9858,  25113.0000,  ...,  11913.0000,\n",
      "          -86203.0000,  10422.0234],\n",
      "         [ -3877.9863,  62173.0078,  -6257.9883,  ..., -42390.0195,\n",
      "           -3610.9990,  -6039.0117],\n",
      "         [ 59545.9805, -12917.0332,  -8445.9980,  ...,    276.9922,\n",
      "           -2714.9736,  19578.0137]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[ -4822.0010,   3124.9990,  -3885.0039,  ..., -19007.0020,\n",
      "            -551.0020,  16285.0020],\n",
      "         [-14074.9805,   5451.9800,   4658.9824,  ...,  -2389.9668,\n",
      "           -9142.0049,  15443.0234],\n",
      "         [  5586.0039,  -1742.0107, -20944.0000,  ...,  -5018.9771,\n",
      "          -11242.9785, -44480.9922],\n",
      "         ...,\n",
      "         [  1345.9697,   1923.9858,  25113.0000,  ...,  11913.0000,\n",
      "          -86203.0000,  10422.0234],\n",
      "         [ -3877.9863,  62173.0078,  -6257.9883,  ..., -42390.0195,\n",
      "           -3610.9990,  -6039.0117],\n",
      "         [ 59545.9805, -12917.0332,  -8445.9980,  ...,    276.9922,\n",
      "           -2714.9736,  19578.0137]]]),) and output (tensor([[[ -7235.0010,   6583.9990,  -4212.0039,  ..., -19104.0020,\n",
      "            3087.9980,  17084.0020],\n",
      "         [-17890.9805,    655.9800,   3028.9824,  ...,  -2720.9668,\n",
      "          -10573.0049,  11773.0234],\n",
      "         [  1562.0039,   1287.9893, -25349.0000,  ...,  -4591.9771,\n",
      "           -7499.9785, -50652.9922],\n",
      "         ...,\n",
      "         [  5489.9697,   4252.9858,  22421.0000,  ...,  12162.0000,\n",
      "          -86297.0000,  16082.0234],\n",
      "         [ -2299.9863,  69748.0078, -11499.9883,  ..., -38741.0195,\n",
      "            -158.9990,  -8362.0117],\n",
      "         [ 64871.9805, -14016.0332,  -6811.9980,  ...,   1023.9922,\n",
      "           -3230.9736,  23459.0137]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[ -7235.0010,   6583.9990,  -4212.0039,  ..., -19104.0020,\n",
      "            3087.9980,  17084.0020],\n",
      "         [-17890.9805,    655.9800,   3028.9824,  ...,  -2720.9668,\n",
      "          -10573.0049,  11773.0234],\n",
      "         [  1562.0039,   1287.9893, -25349.0000,  ...,  -4591.9771,\n",
      "           -7499.9785, -50652.9922],\n",
      "         ...,\n",
      "         [  5489.9697,   4252.9858,  22421.0000,  ...,  12162.0000,\n",
      "          -86297.0000,  16082.0234],\n",
      "         [ -2299.9863,  69748.0078, -11499.9883,  ..., -38741.0195,\n",
      "            -158.9990,  -8362.0117],\n",
      "         [ 64871.9805, -14016.0332,  -6811.9980,  ...,   1023.9922,\n",
      "           -3230.9736,  23459.0137]]]),) and output (tensor([[[ -5586.0010,   7645.9990,   -505.0039,  ..., -15346.0020,\n",
      "            -623.0020,  12986.0020],\n",
      "         [-23400.9805,  -2448.0200,   2593.9824,  ...,  -3714.9668,\n",
      "          -10047.0049,  13632.0234],\n",
      "         [  5559.0039,  -3627.0107, -19613.0000,  ...,  -2564.9771,\n",
      "           -1894.9785, -54487.9922],\n",
      "         ...,\n",
      "         [  5643.9697,   8908.9863,  20832.0000,  ...,  15739.0000,\n",
      "          -87275.0000,  10585.0234],\n",
      "         [ -3862.9863,  74686.0078, -11041.9883,  ..., -47648.0195,\n",
      "           -5257.9990, -11743.0117],\n",
      "         [ 65802.9844,  -8263.0332,  -5887.9980,  ...,   7863.9922,\n",
      "           -1223.9736,  26223.0137]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[ -5586.0010,   7645.9990,   -505.0039,  ..., -15346.0020,\n",
      "            -623.0020,  12986.0020],\n",
      "         [-23400.9805,  -2448.0200,   2593.9824,  ...,  -3714.9668,\n",
      "          -10047.0049,  13632.0234],\n",
      "         [  5559.0039,  -3627.0107, -19613.0000,  ...,  -2564.9771,\n",
      "           -1894.9785, -54487.9922],\n",
      "         ...,\n",
      "         [  5643.9697,   8908.9863,  20832.0000,  ...,  15739.0000,\n",
      "          -87275.0000,  10585.0234],\n",
      "         [ -3862.9863,  74686.0078, -11041.9883,  ..., -47648.0195,\n",
      "           -5257.9990, -11743.0117],\n",
      "         [ 65802.9844,  -8263.0332,  -5887.9980,  ...,   7863.9922,\n",
      "           -1223.9736,  26223.0137]]]),) and output (tensor([[[ -8916.0010,   2362.9990,  -4833.0039,  ..., -12075.0020,\n",
      "           -1124.0020,  12438.0020],\n",
      "         [-20663.9805,  -4859.0200,    423.9824,  ..., -16986.9668,\n",
      "           -7889.0049,   9659.0234],\n",
      "         [  4584.0039,  -4662.0107, -20352.0000,  ...,  -4031.9771,\n",
      "           -4606.9785, -56679.9922],\n",
      "         ...,\n",
      "         [ -3887.0303,   -402.0137,  17699.0000,  ...,  19729.0000,\n",
      "          -89002.0000,  18315.0234],\n",
      "         [ -2933.9863,  80330.0078,  -6082.9883,  ..., -41012.0195,\n",
      "            1688.0010,  -7384.0117],\n",
      "         [ 63819.9844,   1840.9668,  -4018.9980,  ...,  10102.9922,\n",
      "            1715.0264,  20768.0137]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[ -8916.0010,   2362.9990,  -4833.0039,  ..., -12075.0020,\n",
      "           -1124.0020,  12438.0020],\n",
      "         [-20663.9805,  -4859.0200,    423.9824,  ..., -16986.9668,\n",
      "           -7889.0049,   9659.0234],\n",
      "         [  4584.0039,  -4662.0107, -20352.0000,  ...,  -4031.9771,\n",
      "           -4606.9785, -56679.9922],\n",
      "         ...,\n",
      "         [ -3887.0303,   -402.0137,  17699.0000,  ...,  19729.0000,\n",
      "          -89002.0000,  18315.0234],\n",
      "         [ -2933.9863,  80330.0078,  -6082.9883,  ..., -41012.0195,\n",
      "            1688.0010,  -7384.0117],\n",
      "         [ 63819.9844,   1840.9668,  -4018.9980,  ...,  10102.9922,\n",
      "            1715.0264,  20768.0137]]]),) and output (tensor([[[-20230.0000,   7877.9990,    161.9961,  ..., -10250.0020,\n",
      "             676.9980,   7095.0020],\n",
      "         [-20350.9805,  -7156.0200,  -4920.0176,  ..., -12574.9668,\n",
      "           -9890.0049,   5036.0234],\n",
      "         [  8072.0039,   -543.0107, -16220.0000,  ...,  -7050.9771,\n",
      "          -11337.9785, -57687.9922],\n",
      "         ...,\n",
      "         [  2557.9697,  -7373.0137,  16857.0000,  ...,  26325.0000,\n",
      "          -85605.0000,  17159.0234],\n",
      "         [ -4162.9863,  88559.0078,  -2449.9883,  ..., -41503.0195,\n",
      "            9433.0010,  -2988.0117],\n",
      "         [ 67530.9844,   1267.9668,  -6492.9980,  ...,  15826.9922,\n",
      "            1360.0264,  11096.0137]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[-20230.0000,   7877.9990,    161.9961,  ..., -10250.0020,\n",
      "             676.9980,   7095.0020],\n",
      "         [-20350.9805,  -7156.0200,  -4920.0176,  ..., -12574.9668,\n",
      "           -9890.0049,   5036.0234],\n",
      "         [  8072.0039,   -543.0107, -16220.0000,  ...,  -7050.9771,\n",
      "          -11337.9785, -57687.9922],\n",
      "         ...,\n",
      "         [  2557.9697,  -7373.0137,  16857.0000,  ...,  26325.0000,\n",
      "          -85605.0000,  17159.0234],\n",
      "         [ -4162.9863,  88559.0078,  -2449.9883,  ..., -41503.0195,\n",
      "            9433.0010,  -2988.0117],\n",
      "         [ 67530.9844,   1267.9668,  -6492.9980,  ...,  15826.9922,\n",
      "            1360.0264,  11096.0137]]]),) and output (tensor([[[-25670.0000,  13293.9990,   9099.9961,  ...,  -6547.0020,\n",
      "           -5658.0020,   4192.0020],\n",
      "         [-23789.9805,  -8376.0195,  -9127.0176,  ..., -13106.9668,\n",
      "          -12549.0049,   1898.0234],\n",
      "         [  8408.0039,  -2146.0107, -23896.0000,  ...,  -7615.9771,\n",
      "           -8339.9785, -49972.9922],\n",
      "         ...,\n",
      "         [  7111.9697,  -7195.0137,  19156.0000,  ...,  28219.0000,\n",
      "          -84807.0000,  25113.0234],\n",
      "         [  4728.0137,  88467.0078,  -5798.9883,  ..., -43418.0195,\n",
      "            3355.0010,   7522.9883],\n",
      "         [ 64564.9844,  -1205.0332,  -5142.9980,  ...,  16512.9922,\n",
      "            3338.0264,   7261.0137]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[-25670.0000,  13293.9990,   9099.9961,  ...,  -6547.0020,\n",
      "           -5658.0020,   4192.0020],\n",
      "         [-23789.9805,  -8376.0195,  -9127.0176,  ..., -13106.9668,\n",
      "          -12549.0049,   1898.0234],\n",
      "         [  8408.0039,  -2146.0107, -23896.0000,  ...,  -7615.9771,\n",
      "           -8339.9785, -49972.9922],\n",
      "         ...,\n",
      "         [  7111.9697,  -7195.0137,  19156.0000,  ...,  28219.0000,\n",
      "          -84807.0000,  25113.0234],\n",
      "         [  4728.0137,  88467.0078,  -5798.9883,  ..., -43418.0195,\n",
      "            3355.0010,   7522.9883],\n",
      "         [ 64564.9844,  -1205.0332,  -5142.9980,  ...,  16512.9922,\n",
      "            3338.0264,   7261.0137]]]),) and output (tensor([[[-25652.0000,  13180.9990,  15728.9961,  ...,  -2831.0020,\n",
      "             158.9980,   2204.0020],\n",
      "         [-26234.9805,  -2991.0195,  -7631.0176,  ..., -19783.9668,\n",
      "          -16044.0049,   2032.0234],\n",
      "         [ 11591.0039,  -4773.0107, -27135.0000,  ..., -14532.9766,\n",
      "           -4675.9785, -50622.9922],\n",
      "         ...,\n",
      "         [  8193.9697,  -4365.0137,  20163.0000,  ...,  25199.0000,\n",
      "          -91545.0000,  25852.0234],\n",
      "         [  4730.0137,  88080.0078,   2968.0117,  ..., -43675.0195,\n",
      "             183.0010,   8233.9883],\n",
      "         [ 67602.9844,  -1545.0332,   1819.0020,  ...,  20039.9922,\n",
      "            5309.0264,   7817.0137]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[-25652.0000,  13180.9990,  15728.9961,  ...,  -2831.0020,\n",
      "             158.9980,   2204.0020],\n",
      "         [-26234.9805,  -2991.0195,  -7631.0176,  ..., -19783.9668,\n",
      "          -16044.0049,   2032.0234],\n",
      "         [ 11591.0039,  -4773.0107, -27135.0000,  ..., -14532.9766,\n",
      "           -4675.9785, -50622.9922],\n",
      "         ...,\n",
      "         [  8193.9697,  -4365.0137,  20163.0000,  ...,  25199.0000,\n",
      "          -91545.0000,  25852.0234],\n",
      "         [  4730.0137,  88080.0078,   2968.0117,  ..., -43675.0195,\n",
      "             183.0010,   8233.9883],\n",
      "         [ 67602.9844,  -1545.0332,   1819.0020,  ...,  20039.9922,\n",
      "            5309.0264,   7817.0137]]]),) and output (tensor([[[-14791.0000,  15299.9990,  16129.9961,  ...,   3187.9980,\n",
      "           -2552.0020,   5626.0020],\n",
      "         [-23081.9805,  -5532.0195, -12725.0176,  ..., -16646.9668,\n",
      "          -13242.0049,  -1892.9766],\n",
      "         [  8025.0039, -12112.0107, -28962.0000,  ...,  -8674.9766,\n",
      "           -5662.9785, -46946.9922],\n",
      "         ...,\n",
      "         [  5920.9697,  -9202.0137,  19969.0000,  ...,  25675.0000,\n",
      "          -90681.0000,  31935.0234],\n",
      "         [  7141.0137,  94620.0078,   8135.0117,  ..., -38272.0195,\n",
      "            1291.0010,  10228.9883],\n",
      "         [ 65380.9844,  -1286.0332,    415.0020,  ...,  25789.9922,\n",
      "            4448.0264,   3786.0137]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[-14791.0000,  15299.9990,  16129.9961,  ...,   3187.9980,\n",
      "           -2552.0020,   5626.0020],\n",
      "         [-23081.9805,  -5532.0195, -12725.0176,  ..., -16646.9668,\n",
      "          -13242.0049,  -1892.9766],\n",
      "         [  8025.0039, -12112.0107, -28962.0000,  ...,  -8674.9766,\n",
      "           -5662.9785, -46946.9922],\n",
      "         ...,\n",
      "         [  5920.9697,  -9202.0137,  19969.0000,  ...,  25675.0000,\n",
      "          -90681.0000,  31935.0234],\n",
      "         [  7141.0137,  94620.0078,   8135.0117,  ..., -38272.0195,\n",
      "            1291.0010,  10228.9883],\n",
      "         [ 65380.9844,  -1286.0332,    415.0020,  ...,  25789.9922,\n",
      "            4448.0264,   3786.0137]]]),) and output (tensor([[[-1.3284e+04,  2.6639e+04,  7.4680e+03,  ...,  4.1140e+03,\n",
      "          -6.6080e+03,  5.3360e+03],\n",
      "         [-2.6112e+04,  8.5598e+02, -1.2209e+04,  ..., -1.7332e+04,\n",
      "          -2.2258e+04, -2.4830e+03],\n",
      "         [ 1.1384e+04, -7.8240e+03, -2.7149e+04,  ...,  3.8023e+01,\n",
      "          -7.3470e+03, -4.4880e+04],\n",
      "         ...,\n",
      "         [ 1.4588e+04, -1.3450e+04,  1.8928e+04,  ...,  2.6154e+04,\n",
      "          -9.6995e+04,  2.5785e+04],\n",
      "         [ 1.0292e+04,  1.0374e+05,  3.5060e+03,  ..., -4.1486e+04,\n",
      "          -8.1380e+03,  8.3020e+03],\n",
      "         [ 6.4593e+04, -1.2730e+03, -4.1390e+03,  ...,  2.9287e+04,\n",
      "           8.1520e+03,  3.6340e+03]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[-1.3284e+04,  2.6639e+04,  7.4680e+03,  ...,  4.1140e+03,\n",
      "          -6.6080e+03,  5.3360e+03],\n",
      "         [-2.6112e+04,  8.5598e+02, -1.2209e+04,  ..., -1.7332e+04,\n",
      "          -2.2258e+04, -2.4830e+03],\n",
      "         [ 1.1384e+04, -7.8240e+03, -2.7149e+04,  ...,  3.8023e+01,\n",
      "          -7.3470e+03, -4.4880e+04],\n",
      "         ...,\n",
      "         [ 1.4588e+04, -1.3450e+04,  1.8928e+04,  ...,  2.6154e+04,\n",
      "          -9.6995e+04,  2.5785e+04],\n",
      "         [ 1.0292e+04,  1.0374e+05,  3.5060e+03,  ..., -4.1486e+04,\n",
      "          -8.1380e+03,  8.3020e+03],\n",
      "         [ 6.4593e+04, -1.2730e+03, -4.1390e+03,  ...,  2.9287e+04,\n",
      "           8.1520e+03,  3.6340e+03]]]),) and output (tensor([[[-21532.0000,  32796.0000,  16281.9961,  ...,  18208.9980,\n",
      "           -4282.0020,   5756.0020],\n",
      "         [-22805.9805,   -552.0195, -14012.0176,  ..., -16492.9668,\n",
      "          -21763.0039,  -5557.9766],\n",
      "         [  4727.0039, -14033.0107, -36073.0000,  ...,    412.0234,\n",
      "           -9579.9785, -41107.9922],\n",
      "         ...,\n",
      "         [ 13676.9697, -10605.0137,  17978.0000,  ...,  34638.0000,\n",
      "          -98858.0000,  30917.0234],\n",
      "         [ 10207.0137,  96514.0078,   2005.0117,  ..., -34915.0195,\n",
      "           -5070.9990,  19091.9883],\n",
      "         [ 63177.9844, -10086.0332,  -2939.9980,  ...,  26480.9922,\n",
      "             234.0264,   3956.0137]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[-21532.0000,  32796.0000,  16281.9961,  ...,  18208.9980,\n",
      "           -4282.0020,   5756.0020],\n",
      "         [-22805.9805,   -552.0195, -14012.0176,  ..., -16492.9668,\n",
      "          -21763.0039,  -5557.9766],\n",
      "         [  4727.0039, -14033.0107, -36073.0000,  ...,    412.0234,\n",
      "           -9579.9785, -41107.9922],\n",
      "         ...,\n",
      "         [ 13676.9697, -10605.0137,  17978.0000,  ...,  34638.0000,\n",
      "          -98858.0000,  30917.0234],\n",
      "         [ 10207.0137,  96514.0078,   2005.0117,  ..., -34915.0195,\n",
      "           -5070.9990,  19091.9883],\n",
      "         [ 63177.9844, -10086.0332,  -2939.9980,  ...,  26480.9922,\n",
      "             234.0264,   3956.0137]]]),) and output (tensor([[[-2.4082e+04,  3.3387e+04,  1.5375e+04,  ...,  1.8360e+04,\n",
      "          -7.5000e+03,  1.1519e+04],\n",
      "         [-1.2637e+04, -5.5250e+03, -1.4149e+04,  ..., -1.6555e+04,\n",
      "          -1.8162e+04, -1.3536e+04],\n",
      "         [-2.3996e+01, -1.0447e+04, -3.5654e+04,  ...,  6.1300e+03,\n",
      "          -6.6820e+03, -3.5979e+04],\n",
      "         ...,\n",
      "         [ 1.3084e+04, -1.4194e+04,  1.2491e+04,  ...,  3.8405e+04,\n",
      "          -9.8344e+04,  3.4244e+04],\n",
      "         [ 1.9174e+04,  9.6197e+04,  4.5140e+03,  ..., -2.8366e+04,\n",
      "          -7.2640e+03,  2.4272e+04],\n",
      "         [ 5.3238e+04, -1.7650e+03, -1.1060e+03,  ...,  2.6420e+04,\n",
      "           1.0980e+03,  9.6970e+03]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[-2.4082e+04,  3.3387e+04,  1.5375e+04,  ...,  1.8360e+04,\n",
      "          -7.5000e+03,  1.1519e+04],\n",
      "         [-1.2637e+04, -5.5250e+03, -1.4149e+04,  ..., -1.6555e+04,\n",
      "          -1.8162e+04, -1.3536e+04],\n",
      "         [-2.3996e+01, -1.0447e+04, -3.5654e+04,  ...,  6.1300e+03,\n",
      "          -6.6820e+03, -3.5979e+04],\n",
      "         ...,\n",
      "         [ 1.3084e+04, -1.4194e+04,  1.2491e+04,  ...,  3.8405e+04,\n",
      "          -9.8344e+04,  3.4244e+04],\n",
      "         [ 1.9174e+04,  9.6197e+04,  4.5140e+03,  ..., -2.8366e+04,\n",
      "          -7.2640e+03,  2.4272e+04],\n",
      "         [ 5.3238e+04, -1.7650e+03, -1.1060e+03,  ...,  2.6420e+04,\n",
      "           1.0980e+03,  9.6970e+03]]]),) and output (tensor([[[-14869.0000,  32284.0000,   3742.9961,  ...,  12458.9980,\n",
      "           -5452.0020,   7590.0020],\n",
      "         [-11633.9805, -13143.0195,  -8491.0176,  ..., -14613.9668,\n",
      "          -11631.0039, -20358.9766],\n",
      "         [ -3081.9961,  -6594.0107, -27815.0000,  ...,   7694.0234,\n",
      "           -1487.9785, -32588.9922],\n",
      "         ...,\n",
      "         [ 10173.9697, -11589.0137,  13750.0000,  ...,  37092.0000,\n",
      "          -90166.0000,  41284.0234],\n",
      "         [ 20210.0137,  92295.0078,  17903.0117,  ..., -32797.0195,\n",
      "           -8462.9990,  26264.9883],\n",
      "         [ 44198.9844, -15501.0332,  -2485.9980,  ...,  32542.9922,\n",
      "            1315.0264,  20552.0137]]]),)\n",
      "[Debug] Layer names being passed: ['model.layers.0', 'model.layers.1', 'model.layers.2', 'model.layers.3', 'model.layers.4', 'model.layers.5', 'model.layers.6', 'model.layers.7', 'model.layers.8', 'model.layers.9', 'model.layers.10', 'model.layers.11', 'model.layers.12', 'model.layers.13', 'model.layers.14', 'model.layers.15', 'model.layers.16', 'model.layers.17', 'model.layers.18', 'model.layers.19', 'model.layers.20', 'model.layers.21', 'model.layers.22', 'model.layers.23', 'model.layers.24', 'model.layers.25', 'model.layers.26', 'model.layers.27', 'model.embed_tokens']\n",
      "[Debug] Trying to access layer: model.layers.0\n",
      "[Debug] Successfully found layer: model.layers.0\n",
      "[Debug] Trying to access layer: model.layers.1\n",
      "[Debug] Successfully found layer: model.layers.1\n",
      "[Debug] Trying to access layer: model.layers.2\n",
      "[Debug] Successfully found layer: model.layers.2\n",
      "[Debug] Trying to access layer: model.layers.3\n",
      "[Debug] Successfully found layer: model.layers.3\n",
      "[Debug] Trying to access layer: model.layers.4\n",
      "[Debug] Successfully found layer: model.layers.4\n",
      "[Debug] Trying to access layer: model.layers.5\n",
      "[Debug] Successfully found layer: model.layers.5\n",
      "[Debug] Trying to access layer: model.layers.6\n",
      "[Debug] Successfully found layer: model.layers.6\n",
      "[Debug] Trying to access layer: model.layers.7\n",
      "[Debug] Successfully found layer: model.layers.7\n",
      "[Debug] Trying to access layer: model.layers.8\n",
      "[Debug] Successfully found layer: model.layers.8\n",
      "[Debug] Trying to access layer: model.layers.9\n",
      "[Debug] Successfully found layer: model.layers.9\n",
      "[Debug] Trying to access layer: model.layers.10\n",
      "[Debug] Successfully found layer: model.layers.10\n",
      "[Debug] Trying to access layer: model.layers.11\n",
      "[Debug] Successfully found layer: model.layers.11\n",
      "[Debug] Trying to access layer: model.layers.12\n",
      "[Debug] Successfully found layer: model.layers.12\n",
      "[Debug] Trying to access layer: model.layers.13\n",
      "[Debug] Successfully found layer: model.layers.13\n",
      "[Debug] Trying to access layer: model.layers.14\n",
      "[Debug] Successfully found layer: model.layers.14\n",
      "[Debug] Trying to access layer: model.layers.15\n",
      "[Debug] Successfully found layer: model.layers.15\n",
      "[Debug] Trying to access layer: model.layers.16\n",
      "[Debug] Successfully found layer: model.layers.16\n",
      "[Debug] Trying to access layer: model.layers.17\n",
      "[Debug] Successfully found layer: model.layers.17\n",
      "[Debug] Trying to access layer: model.layers.18\n",
      "[Debug] Successfully found layer: model.layers.18\n",
      "[Debug] Trying to access layer: model.layers.19\n",
      "[Debug] Successfully found layer: model.layers.19\n",
      "[Debug] Trying to access layer: model.layers.20\n",
      "[Debug] Successfully found layer: model.layers.20\n",
      "[Debug] Trying to access layer: model.layers.21\n",
      "[Debug] Successfully found layer: model.layers.21\n",
      "[Debug] Trying to access layer: model.layers.22\n",
      "[Debug] Successfully found layer: model.layers.22\n",
      "[Debug] Trying to access layer: model.layers.23\n",
      "[Debug] Successfully found layer: model.layers.23\n",
      "[Debug] Trying to access layer: model.layers.24\n",
      "[Debug] Successfully found layer: model.layers.24\n",
      "[Debug] Trying to access layer: model.layers.25\n",
      "[Debug] Successfully found layer: model.layers.25\n",
      "[Debug] Trying to access layer: model.layers.26\n",
      "[Debug] Successfully found layer: model.layers.26\n",
      "[Debug] Trying to access layer: model.layers.27\n",
      "[Debug] Successfully found layer: model.layers.27\n",
      "[Debug] Trying to access layer: model.embed_tokens\n",
      "[Debug] Successfully found layer: model.embed_tokens\n",
      "[Hook] Layer Embedding(128256, 3072, padding_idx=128004) received input (tensor([[128000,     42,   1003,  20531,  12324,    578,  50280,  12324,    374,\n",
      "            264,  52482,  12324,  15871,   1990,   6890,    323,  17076,     11,\n",
      "           3515,   3940,   1120,   1306,    279,  17071,    315,   6890,    304,\n",
      "            220,   6393,     22,     13,   5734,    706,    520,   3115,   6476,\n",
      "            264,   9099,   3560,   8032,     17,     60,   6890,    323,  17076,\n",
      "            617,  21095,   2380,  25981,    927,  50280,     11,   2737,    279,\n",
      "          76985,   9483,    587,  40422,  15317,    315,    220,   6393,     22,\n",
      "            323,    220,   5162,     20,     11,    439,   1664,    439,    279,\n",
      "            735,    867,    321,   5111,    315,    220,   2550,     24,     13,\n",
      "            578,   1403,   5961,    617,   1101,   1027,   6532,    304,   3892,\n",
      "          96380,  21168,    927,   2585,    315,    279,  12095,  46799,  96486,\n",
      "             13]]),) and output tensor([[[-0.0010, -0.0007, -0.0046,  ..., -0.0014, -0.0020,  0.0020],\n",
      "         [-0.0193,  0.0151,  0.0059,  ...,  0.0107,  0.0371,  0.0077],\n",
      "         [-0.0283, -0.0150,  0.0598,  ..., -0.0486, -0.0020, -0.0308],\n",
      "         ...,\n",
      "         [ 0.0220,  0.0016,  0.0282,  ...,  0.0118,  0.0156, -0.0008],\n",
      "         [ 0.0356,  0.0117, -0.0050,  ...,  0.0147, -0.0095, -0.0229],\n",
      "         [ 0.0093, -0.0031,  0.0278,  ...,  0.0117, -0.0084,  0.0096]]])\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[-0.0010, -0.0007, -0.0046,  ..., -0.0014, -0.0020,  0.0020],\n",
      "         [-0.0193,  0.0151,  0.0059,  ...,  0.0107,  0.0371,  0.0077],\n",
      "         [-0.0283, -0.0150,  0.0598,  ..., -0.0486, -0.0020, -0.0308],\n",
      "         ...,\n",
      "         [ 0.0220,  0.0016,  0.0282,  ...,  0.0118,  0.0156, -0.0008],\n",
      "         [ 0.0356,  0.0117, -0.0050,  ...,  0.0147, -0.0095, -0.0229],\n",
      "         [ 0.0093, -0.0031,  0.0278,  ...,  0.0117, -0.0084,  0.0096]]]),) and output (tensor([[[ 442.9990,  239.9993,  302.9954,  ..., -154.0014,  -15.0020,\n",
      "           185.0020],\n",
      "         [  -6.0193,  142.0151,  -83.9941,  ..., -204.9893,  -81.9629,\n",
      "           -24.9923],\n",
      "         [ -27.0283,  137.9850,  -46.9402,  ..., -200.0486,  203.9980,\n",
      "           -16.0308],\n",
      "         ...,\n",
      "         [-175.9780,  -44.9984,  126.0282,  ..., -147.9882,   35.0156,\n",
      "            35.9992],\n",
      "         [ 279.0356,  249.0117,  122.9950,  ...,  -70.9853,   32.9905,\n",
      "            59.9771],\n",
      "         [  22.0093, -108.0031,  665.0278,  ..., -975.9883,  -50.0084,\n",
      "          -167.9904]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[ 442.9990,  239.9993,  302.9954,  ..., -154.0014,  -15.0020,\n",
      "           185.0020],\n",
      "         [  -6.0193,  142.0151,  -83.9941,  ..., -204.9893,  -81.9629,\n",
      "           -24.9923],\n",
      "         [ -27.0283,  137.9850,  -46.9402,  ..., -200.0486,  203.9980,\n",
      "           -16.0308],\n",
      "         ...,\n",
      "         [-175.9780,  -44.9984,  126.0282,  ..., -147.9882,   35.0156,\n",
      "            35.9992],\n",
      "         [ 279.0356,  249.0117,  122.9950,  ...,  -70.9853,   32.9905,\n",
      "            59.9771],\n",
      "         [  22.0093, -108.0031,  665.0278,  ..., -975.9883,  -50.0084,\n",
      "          -167.9904]]]),) and output (tensor([[[  439.9990,  -144.0007,  -410.0046,  ...,  -765.0015,\n",
      "           1193.9979, -1027.9980],\n",
      "         [  526.9807,   221.0151,  -267.9941,  ...,   223.0107,\n",
      "           1205.0371,  -912.9923],\n",
      "         [  298.9717,   -12.0150,   340.0598,  ...,  -485.0486,\n",
      "            717.9980,   928.9692],\n",
      "         ...,\n",
      "         [ -254.9780,  -484.9984,  -707.9718,  ...,   377.0118,\n",
      "           1085.0156,  -122.0008],\n",
      "         [ 1081.0356,  -406.9883,  -166.0050,  ...,    40.0147,\n",
      "            366.9905,   621.9771],\n",
      "         [  572.0093,  -424.0031,   918.0278,  ..., -1329.9883,\n",
      "           -172.0084,  -484.9904]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[  439.9990,  -144.0007,  -410.0046,  ...,  -765.0015,\n",
      "           1193.9979, -1027.9980],\n",
      "         [  526.9807,   221.0151,  -267.9941,  ...,   223.0107,\n",
      "           1205.0371,  -912.9923],\n",
      "         [  298.9717,   -12.0150,   340.0598,  ...,  -485.0486,\n",
      "            717.9980,   928.9692],\n",
      "         ...,\n",
      "         [ -254.9780,  -484.9984,  -707.9718,  ...,   377.0118,\n",
      "           1085.0156,  -122.0008],\n",
      "         [ 1081.0356,  -406.9883,  -166.0050,  ...,    40.0147,\n",
      "            366.9905,   621.9771],\n",
      "         [  572.0093,  -424.0031,   918.0278,  ..., -1329.9883,\n",
      "           -172.0084,  -484.9904]]]),) and output (tensor([[[  551.9990,   993.9993, -1790.0046,  ...,   445.9985,\n",
      "          -1386.0021, -1245.9980],\n",
      "         [-1929.0193,     7.0151,  1414.0059,  ...,  3583.0107,\n",
      "           2235.0371, -2173.9922],\n",
      "         [ 3204.9717, -1034.0149,  2991.0598,  ..., -1581.0486,\n",
      "          -4153.0020, -1828.0308],\n",
      "         ...,\n",
      "         [ -169.9780,  -575.9984,  1019.0282,  ...,   663.0118,\n",
      "          -1222.9844,  2017.9993],\n",
      "         [ 4708.0356,  4512.0117,  -607.0050,  ...,  1201.0146,\n",
      "          -2979.0095,  -588.0229],\n",
      "         [  218.0093,   274.9969, -4220.9722,  ..., -2534.9883,\n",
      "           -398.0084,  2197.0098]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[  551.9990,   993.9993, -1790.0046,  ...,   445.9985,\n",
      "          -1386.0021, -1245.9980],\n",
      "         [-1929.0193,     7.0151,  1414.0059,  ...,  3583.0107,\n",
      "           2235.0371, -2173.9922],\n",
      "         [ 3204.9717, -1034.0149,  2991.0598,  ..., -1581.0486,\n",
      "          -4153.0020, -1828.0308],\n",
      "         ...,\n",
      "         [ -169.9780,  -575.9984,  1019.0282,  ...,   663.0118,\n",
      "          -1222.9844,  2017.9993],\n",
      "         [ 4708.0356,  4512.0117,  -607.0050,  ...,  1201.0146,\n",
      "          -2979.0095,  -588.0229],\n",
      "         [  218.0093,   274.9969, -4220.9722,  ..., -2534.9883,\n",
      "           -398.0084,  2197.0098]]]),) and output (tensor([[[ 2131.9990,  -737.0007, -1071.0046,  ..., -2781.0015,\n",
      "           -885.0021,    86.0020],\n",
      "         [-5526.0195, -3369.9849,   750.0059,  ...,  3286.0107,\n",
      "            485.0371, -1318.9922],\n",
      "         [  477.9717,  -106.0149,  2113.0598,  ..., -2147.0486,\n",
      "          -7798.0020, -1383.0308],\n",
      "         ...,\n",
      "         [-1388.9780, -2848.9985,  1190.0282,  ...,  -551.9882,\n",
      "           2586.0156, -1209.0007],\n",
      "         [ 7639.0356,  5803.0117,  -939.0050,  ..., -3709.9854,\n",
      "          -3768.0095,  -467.0229],\n",
      "         [-4798.9907,  1577.9969, -4678.9722,  ...,  2077.0117,\n",
      "           4246.9917,   398.0098]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[ 2131.9990,  -737.0007, -1071.0046,  ..., -2781.0015,\n",
      "           -885.0021,    86.0020],\n",
      "         [-5526.0195, -3369.9849,   750.0059,  ...,  3286.0107,\n",
      "            485.0371, -1318.9922],\n",
      "         [  477.9717,  -106.0149,  2113.0598,  ..., -2147.0486,\n",
      "          -7798.0020, -1383.0308],\n",
      "         ...,\n",
      "         [-1388.9780, -2848.9985,  1190.0282,  ...,  -551.9882,\n",
      "           2586.0156, -1209.0007],\n",
      "         [ 7639.0356,  5803.0117,  -939.0050,  ..., -3709.9854,\n",
      "          -3768.0095,  -467.0229],\n",
      "         [-4798.9907,  1577.9969, -4678.9722,  ...,  2077.0117,\n",
      "           4246.9917,   398.0098]]]),) and output (tensor([[[  3420.9990,    760.9993,  -5376.0049,  ...,  -4193.0015,\n",
      "            2860.9980,  -4080.9980],\n",
      "         [-10442.0195,  -7851.9849,   1836.0059,  ...,   6827.0107,\n",
      "            -465.9629,    -19.9922],\n",
      "         [  3602.9717,  -1325.0149,   6879.0596,  ...,  -4925.0488,\n",
      "           -9176.0020,  -1696.0308],\n",
      "         ...,\n",
      "         [  -629.9780,  -1495.9985,    977.0282,  ...,   2387.0117,\n",
      "            6645.0156,   1098.9993],\n",
      "         [ 13080.0352,   4118.0117,    568.9950,  ...,   2456.0146,\n",
      "           -9880.0098,   2097.9771],\n",
      "         [ -2909.9907,   6890.9971,  -2715.9722,  ...,   4169.0117,\n",
      "            5085.9917,    436.0098]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[  3420.9990,    760.9993,  -5376.0049,  ...,  -4193.0015,\n",
      "            2860.9980,  -4080.9980],\n",
      "         [-10442.0195,  -7851.9849,   1836.0059,  ...,   6827.0107,\n",
      "            -465.9629,    -19.9922],\n",
      "         [  3602.9717,  -1325.0149,   6879.0596,  ...,  -4925.0488,\n",
      "           -9176.0020,  -1696.0308],\n",
      "         ...,\n",
      "         [  -629.9780,  -1495.9985,    977.0282,  ...,   2387.0117,\n",
      "            6645.0156,   1098.9993],\n",
      "         [ 13080.0352,   4118.0117,    568.9950,  ...,   2456.0146,\n",
      "           -9880.0098,   2097.9771],\n",
      "         [ -2909.9907,   6890.9971,  -2715.9722,  ...,   4169.0117,\n",
      "            5085.9917,    436.0098]]]),) and output (tensor([[[  2742.9990,  -1512.0007, -10493.0049,  ...,  -9945.0020,\n",
      "           -2017.0020,  -1802.9980],\n",
      "         [ -6508.0195, -11818.9844,   -596.9941,  ...,  11654.0107,\n",
      "           -5392.9629,   8567.0078],\n",
      "         [  5544.9717,   4997.9854,   6011.0596,  ...,  -7394.0488,\n",
      "           -8991.0020,    104.9692],\n",
      "         ...,\n",
      "         [   -41.9780, -11904.9980,   1194.0282,  ...,   1132.0117,\n",
      "            9597.0156,  -1752.0007],\n",
      "         [ 14873.0352,   2155.0117,   1590.9950,  ...,   9485.0146,\n",
      "          -19129.0098,   6086.9771],\n",
      "         [  3028.0093,   9095.9971,  -6041.9722,  ...,   3219.0117,\n",
      "            -255.0083,  -5992.9902]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[  2742.9990,  -1512.0007, -10493.0049,  ...,  -9945.0020,\n",
      "           -2017.0020,  -1802.9980],\n",
      "         [ -6508.0195, -11818.9844,   -596.9941,  ...,  11654.0107,\n",
      "           -5392.9629,   8567.0078],\n",
      "         [  5544.9717,   4997.9854,   6011.0596,  ...,  -7394.0488,\n",
      "           -8991.0020,    104.9692],\n",
      "         ...,\n",
      "         [   -41.9780, -11904.9980,   1194.0282,  ...,   1132.0117,\n",
      "            9597.0156,  -1752.0007],\n",
      "         [ 14873.0352,   2155.0117,   1590.9950,  ...,   9485.0146,\n",
      "          -19129.0098,   6086.9771],\n",
      "         [  3028.0093,   9095.9971,  -6041.9722,  ...,   3219.0117,\n",
      "            -255.0083,  -5992.9902]]]),) and output (tensor([[[  3032.9990,  -3420.0007, -10291.0049,  ...,  -6511.0020,\n",
      "           -1953.0020,   2001.0020],\n",
      "         [ -4683.0195,  -7768.9844,  -4447.9941,  ...,  10891.0107,\n",
      "           -5775.9629,   5599.0078],\n",
      "         [  3309.9717,   6326.9854,   4549.0596,  ...,  -5587.0488,\n",
      "           -7786.0020,  -1097.0308],\n",
      "         ...,\n",
      "         [ -2947.9780, -12770.9980,   1359.0282,  ...,  -1508.9883,\n",
      "            4039.0156,   -404.0007],\n",
      "         [ 16815.0352,   -370.9883,   3836.9951,  ...,   9720.0146,\n",
      "          -30001.0098,    601.9771],\n",
      "         [  3705.0093,   9853.9971,  -4792.9722,  ...,   4993.0117,\n",
      "           -3071.0083,  -3946.9902]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[  3032.9990,  -3420.0007, -10291.0049,  ...,  -6511.0020,\n",
      "           -1953.0020,   2001.0020],\n",
      "         [ -4683.0195,  -7768.9844,  -4447.9941,  ...,  10891.0107,\n",
      "           -5775.9629,   5599.0078],\n",
      "         [  3309.9717,   6326.9854,   4549.0596,  ...,  -5587.0488,\n",
      "           -7786.0020,  -1097.0308],\n",
      "         ...,\n",
      "         [ -2947.9780, -12770.9980,   1359.0282,  ...,  -1508.9883,\n",
      "            4039.0156,   -404.0007],\n",
      "         [ 16815.0352,   -370.9883,   3836.9951,  ...,   9720.0146,\n",
      "          -30001.0098,    601.9771],\n",
      "         [  3705.0093,   9853.9971,  -4792.9722,  ...,   4993.0117,\n",
      "           -3071.0083,  -3946.9902]]]),) and output (tensor([[[  3132.9990,  -1970.0007, -11544.0049,  ...,  -5780.0020,\n",
      "           -2486.0020,  -1921.9980],\n",
      "         [ -6640.0195,  -6576.9844,  -1920.9941,  ...,  15018.0107,\n",
      "           -3838.9629,   9351.0078],\n",
      "         [  5831.9717,   9834.9854,   5457.0596,  ...,  -3880.0488,\n",
      "           -6437.0020,   1953.9692],\n",
      "         ...,\n",
      "         [ -8464.9785, -12470.9980,    340.0282,  ...,   3425.0117,\n",
      "            3664.0156,   1584.9993],\n",
      "         [ 13700.0352,   2103.0117,   2438.9951,  ...,   6525.0146,\n",
      "          -27238.0098,   -704.0229],\n",
      "         [  4716.0093,  12243.9971,  -5578.9722,  ...,   6926.0117,\n",
      "           -1199.0083,  -6260.9902]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[  3132.9990,  -1970.0007, -11544.0049,  ...,  -5780.0020,\n",
      "           -2486.0020,  -1921.9980],\n",
      "         [ -6640.0195,  -6576.9844,  -1920.9941,  ...,  15018.0107,\n",
      "           -3838.9629,   9351.0078],\n",
      "         [  5831.9717,   9834.9854,   5457.0596,  ...,  -3880.0488,\n",
      "           -6437.0020,   1953.9692],\n",
      "         ...,\n",
      "         [ -8464.9785, -12470.9980,    340.0282,  ...,   3425.0117,\n",
      "            3664.0156,   1584.9993],\n",
      "         [ 13700.0352,   2103.0117,   2438.9951,  ...,   6525.0146,\n",
      "          -27238.0098,   -704.0229],\n",
      "         [  4716.0093,  12243.9971,  -5578.9722,  ...,   6926.0117,\n",
      "           -1199.0083,  -6260.9902]]]),) and output (tensor([[[  1046.9990,   2403.9993, -14486.0049,  ..., -10922.0020,\n",
      "           -1419.0020,  -2983.9980],\n",
      "         [-11039.0195,  -4615.9844,  -9097.9941,  ...,  20989.0117,\n",
      "            1785.0371,  12549.0078],\n",
      "         [   495.9717,   9521.9854,   5449.0596,  ...,  -8905.0488,\n",
      "           -6685.0020,   5359.9692],\n",
      "         ...,\n",
      "         [ -5621.9785, -13548.9980,  -2965.9717,  ...,   4485.0117,\n",
      "            5066.0156,  -1306.0007],\n",
      "         [  6239.0352,   4512.0117,    917.9951,  ...,   1770.0146,\n",
      "          -27694.0098,  -5623.0229],\n",
      "         [  5237.0093,  10078.9971,  -3845.9722,  ...,   8645.0117,\n",
      "           -7054.0083,  -8885.9902]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[  1046.9990,   2403.9993, -14486.0049,  ..., -10922.0020,\n",
      "           -1419.0020,  -2983.9980],\n",
      "         [-11039.0195,  -4615.9844,  -9097.9941,  ...,  20989.0117,\n",
      "            1785.0371,  12549.0078],\n",
      "         [   495.9717,   9521.9854,   5449.0596,  ...,  -8905.0488,\n",
      "           -6685.0020,   5359.9692],\n",
      "         ...,\n",
      "         [ -5621.9785, -13548.9980,  -2965.9717,  ...,   4485.0117,\n",
      "            5066.0156,  -1306.0007],\n",
      "         [  6239.0352,   4512.0117,    917.9951,  ...,   1770.0146,\n",
      "          -27694.0098,  -5623.0229],\n",
      "         [  5237.0093,  10078.9971,  -3845.9722,  ...,   8645.0117,\n",
      "           -7054.0083,  -8885.9902]]]),) and output (tensor([[[-3.7670e+03,  5.3080e+03, -1.4221e+04,  ..., -1.5035e+04,\n",
      "          -3.6300e+03,  4.1680e+03],\n",
      "         [-1.1812e+04, -6.1970e+03, -1.4160e+04,  ...,  1.8999e+04,\n",
      "          -3.6570e+03,  1.1272e+04],\n",
      "         [-3.3540e+03,  9.5210e+03,  5.3121e+03,  ..., -1.0496e+04,\n",
      "          -5.5140e+03,  2.8720e+03],\n",
      "         ...,\n",
      "         [-8.0630e+03, -1.7494e+04,  2.3640e+03,  ...,  5.6980e+03,\n",
      "           1.8200e+03,  6.0100e+02],\n",
      "         [-1.9965e+01,  4.9770e+03, -3.0960e+03,  ...,  1.0280e+03,\n",
      "          -3.0874e+04, -6.1410e+03],\n",
      "         [ 3.1530e+03,  7.9430e+03, -5.6080e+03,  ...,  8.3630e+03,\n",
      "          -1.0181e+04, -4.3840e+03]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[-3.7670e+03,  5.3080e+03, -1.4221e+04,  ..., -1.5035e+04,\n",
      "          -3.6300e+03,  4.1680e+03],\n",
      "         [-1.1812e+04, -6.1970e+03, -1.4160e+04,  ...,  1.8999e+04,\n",
      "          -3.6570e+03,  1.1272e+04],\n",
      "         [-3.3540e+03,  9.5210e+03,  5.3121e+03,  ..., -1.0496e+04,\n",
      "          -5.5140e+03,  2.8720e+03],\n",
      "         ...,\n",
      "         [-8.0630e+03, -1.7494e+04,  2.3640e+03,  ...,  5.6980e+03,\n",
      "           1.8200e+03,  6.0100e+02],\n",
      "         [-1.9965e+01,  4.9770e+03, -3.0960e+03,  ...,  1.0280e+03,\n",
      "          -3.0874e+04, -6.1410e+03],\n",
      "         [ 3.1530e+03,  7.9430e+03, -5.6080e+03,  ...,  8.3630e+03,\n",
      "          -1.0181e+04, -4.3840e+03]]]),) and output (tensor([[[ -1326.0010,   1223.9990, -16643.0039,  ..., -13240.0020,\n",
      "           -5677.0020,   9639.0020],\n",
      "         [-13300.0195,   1067.0156, -11543.9941,  ...,  18783.0117,\n",
      "           -7601.9629,  17149.0078],\n",
      "         [ -3055.0283,   1688.9854,   7527.0596,  ...,  -9379.0488,\n",
      "           -6114.0020,   1385.9692],\n",
      "         ...,\n",
      "         [-10428.9785, -13926.9980,   -155.9717,  ...,   3542.0117,\n",
      "            1684.0156,   5433.9990],\n",
      "         [  -896.9648,   1868.0117,  -1617.0049,  ...,  -4093.9854,\n",
      "          -36708.0078,  -7870.0229],\n",
      "         [  2744.0093,  10508.9971,  -3098.9722,  ...,   7658.0117,\n",
      "           -5482.0078,  -3693.9902]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[ -1326.0010,   1223.9990, -16643.0039,  ..., -13240.0020,\n",
      "           -5677.0020,   9639.0020],\n",
      "         [-13300.0195,   1067.0156, -11543.9941,  ...,  18783.0117,\n",
      "           -7601.9629,  17149.0078],\n",
      "         [ -3055.0283,   1688.9854,   7527.0596,  ...,  -9379.0488,\n",
      "           -6114.0020,   1385.9692],\n",
      "         ...,\n",
      "         [-10428.9785, -13926.9980,   -155.9717,  ...,   3542.0117,\n",
      "            1684.0156,   5433.9990],\n",
      "         [  -896.9648,   1868.0117,  -1617.0049,  ...,  -4093.9854,\n",
      "          -36708.0078,  -7870.0229],\n",
      "         [  2744.0093,  10508.9971,  -3098.9722,  ...,   7658.0117,\n",
      "           -5482.0078,  -3693.9902]]]),) and output (tensor([[[ -5120.0010,   2246.9990, -15993.0039,  ..., -19818.0020,\n",
      "           -9385.0020,  11924.0020],\n",
      "         [-15750.0195,   1626.0156, -12394.9941,  ...,  25716.0117,\n",
      "           -4788.9629,  11092.0078],\n",
      "         [ -2706.0283,   2851.9854,   6977.0596,  ..., -12344.0488,\n",
      "           -6629.0020,   1195.9692],\n",
      "         ...,\n",
      "         [-11910.9785, -10307.9980,    265.0283,  ...,   5623.0117,\n",
      "           -2704.9844,   1873.9990],\n",
      "         [ -6041.9648,   3506.0117,   -380.0049,  ...,  -1434.9854,\n",
      "          -39693.0078,  -5798.0229],\n",
      "         [   311.0093,  14323.9971,  -2759.9722,  ...,  10045.0117,\n",
      "           -5345.0078,  -4997.9902]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[ -5120.0010,   2246.9990, -15993.0039,  ..., -19818.0020,\n",
      "           -9385.0020,  11924.0020],\n",
      "         [-15750.0195,   1626.0156, -12394.9941,  ...,  25716.0117,\n",
      "           -4788.9629,  11092.0078],\n",
      "         [ -2706.0283,   2851.9854,   6977.0596,  ..., -12344.0488,\n",
      "           -6629.0020,   1195.9692],\n",
      "         ...,\n",
      "         [-11910.9785, -10307.9980,    265.0283,  ...,   5623.0117,\n",
      "           -2704.9844,   1873.9990],\n",
      "         [ -6041.9648,   3506.0117,   -380.0049,  ...,  -1434.9854,\n",
      "          -39693.0078,  -5798.0229],\n",
      "         [   311.0093,  14323.9971,  -2759.9722,  ...,  10045.0117,\n",
      "           -5345.0078,  -4997.9902]]]),) and output (tensor([[[-6.6850e+03, -1.2780e+03, -1.5553e+04,  ..., -1.9846e+04,\n",
      "          -1.0418e+04,  1.3376e+04],\n",
      "         [-1.5219e+04,  6.5502e+02, -1.3632e+04,  ...,  2.5960e+04,\n",
      "          -6.2900e+03,  1.6620e+04],\n",
      "         [-3.1270e+03,  3.1850e+03,  7.0791e+03,  ..., -7.2440e+03,\n",
      "          -6.8270e+03,  1.7740e+03],\n",
      "         ...,\n",
      "         [-1.2980e+04, -8.4200e+03, -2.5972e+01,  ...,  3.0480e+03,\n",
      "          -5.2700e+03,  4.3090e+03],\n",
      "         [-4.7850e+03, -8.7699e+02,  4.2150e+03,  ...,  1.6201e+02,\n",
      "          -4.1894e+04, -4.6670e+03],\n",
      "         [-4.1499e+02,  1.9176e+04, -4.5490e+03,  ...,  1.3926e+04,\n",
      "          -2.6710e+03, -7.7240e+03]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[-6.6850e+03, -1.2780e+03, -1.5553e+04,  ..., -1.9846e+04,\n",
      "          -1.0418e+04,  1.3376e+04],\n",
      "         [-1.5219e+04,  6.5502e+02, -1.3632e+04,  ...,  2.5960e+04,\n",
      "          -6.2900e+03,  1.6620e+04],\n",
      "         [-3.1270e+03,  3.1850e+03,  7.0791e+03,  ..., -7.2440e+03,\n",
      "          -6.8270e+03,  1.7740e+03],\n",
      "         ...,\n",
      "         [-1.2980e+04, -8.4200e+03, -2.5972e+01,  ...,  3.0480e+03,\n",
      "          -5.2700e+03,  4.3090e+03],\n",
      "         [-4.7850e+03, -8.7699e+02,  4.2150e+03,  ...,  1.6201e+02,\n",
      "          -4.1894e+04, -4.6670e+03],\n",
      "         [-4.1499e+02,  1.9176e+04, -4.5490e+03,  ...,  1.3926e+04,\n",
      "          -2.6710e+03, -7.7240e+03]]]),) and output (tensor([[[ -8422.0010,    339.9990, -15289.0039,  ..., -20884.0020,\n",
      "           -6908.0020,  11014.0020],\n",
      "         [-18186.0195,   3799.0156, -12341.9941,  ...,  26233.0117,\n",
      "           -6512.9629,  18736.0078],\n",
      "         [ -5919.0283,  -2140.0146,   7923.0596,  ..., -10479.0488,\n",
      "           -8597.0020,  -6249.0308],\n",
      "         ...,\n",
      "         [-12671.9785,  -7762.9980,  -3622.9717,  ...,   3162.0117,\n",
      "           -9190.9844,   4888.9990],\n",
      "         [ -4956.9648,  -5443.9883,    770.9951,  ...,   4275.0146,\n",
      "          -45735.0078,   1214.9771],\n",
      "         [  1945.0093,  18087.9961,    547.0278,  ...,  16480.0117,\n",
      "            1457.9922,  -7851.9902]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[ -8422.0010,    339.9990, -15289.0039,  ..., -20884.0020,\n",
      "           -6908.0020,  11014.0020],\n",
      "         [-18186.0195,   3799.0156, -12341.9941,  ...,  26233.0117,\n",
      "           -6512.9629,  18736.0078],\n",
      "         [ -5919.0283,  -2140.0146,   7923.0596,  ..., -10479.0488,\n",
      "           -8597.0020,  -6249.0308],\n",
      "         ...,\n",
      "         [-12671.9785,  -7762.9980,  -3622.9717,  ...,   3162.0117,\n",
      "           -9190.9844,   4888.9990],\n",
      "         [ -4956.9648,  -5443.9883,    770.9951,  ...,   4275.0146,\n",
      "          -45735.0078,   1214.9771],\n",
      "         [  1945.0093,  18087.9961,    547.0278,  ...,  16480.0117,\n",
      "            1457.9922,  -7851.9902]]]),) and output (tensor([[[-10989.0010,   7034.9990, -13964.0039,  ..., -23968.0020,\n",
      "           -3749.0020,  11730.0020],\n",
      "         [-25211.0195,   2004.0156,  -7888.9941,  ...,  32907.0117,\n",
      "           -1640.9629,  18832.0078],\n",
      "         [ -1777.0283,  -5597.0146,   7201.0596,  ...,  -7716.0488,\n",
      "          -14449.0020,  -1915.0308],\n",
      "         ...,\n",
      "         [-14744.9785,  -9041.9980,  -3658.9717,  ...,   6782.0117,\n",
      "           -8503.9844,  -1092.0010],\n",
      "         [ -3218.9648,  -7292.9883,   1075.9951,  ...,   7017.0146,\n",
      "          -49232.0078,   1302.9771],\n",
      "         [ -2129.9907,  18986.9961,  -6746.9722,  ...,  15430.0117,\n",
      "            3626.9922,  -4542.9902]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[-10989.0010,   7034.9990, -13964.0039,  ..., -23968.0020,\n",
      "           -3749.0020,  11730.0020],\n",
      "         [-25211.0195,   2004.0156,  -7888.9941,  ...,  32907.0117,\n",
      "           -1640.9629,  18832.0078],\n",
      "         [ -1777.0283,  -5597.0146,   7201.0596,  ...,  -7716.0488,\n",
      "          -14449.0020,  -1915.0308],\n",
      "         ...,\n",
      "         [-14744.9785,  -9041.9980,  -3658.9717,  ...,   6782.0117,\n",
      "           -8503.9844,  -1092.0010],\n",
      "         [ -3218.9648,  -7292.9883,   1075.9951,  ...,   7017.0146,\n",
      "          -49232.0078,   1302.9771],\n",
      "         [ -2129.9907,  18986.9961,  -6746.9722,  ...,  15430.0117,\n",
      "            3626.9922,  -4542.9902]]]),) and output (tensor([[[ -7159.0010,   6621.9990, -10967.0039,  ..., -17477.0020,\n",
      "           -2510.0020,  11888.0020],\n",
      "         [-36873.0195,   2701.0156,  -2889.9941,  ...,  36957.0117,\n",
      "            4065.0371,  22275.0078],\n",
      "         [  2049.9717,  -1272.0146,   5566.0596,  ...,  -8314.0488,\n",
      "          -10559.0020,   1991.9692],\n",
      "         ...,\n",
      "         [-14125.9785,  -5346.9980,   -904.9717,  ...,   1685.0117,\n",
      "           -2388.9844,  -1312.0010],\n",
      "         [  -362.9648, -10048.9883,   4023.9951,  ...,   4202.0146,\n",
      "          -54222.0078,   3908.9771],\n",
      "         [  3220.0093,  15428.9961,  -6862.9722,  ...,  11981.0117,\n",
      "            1538.9922,  -9587.9902]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[ -7159.0010,   6621.9990, -10967.0039,  ..., -17477.0020,\n",
      "           -2510.0020,  11888.0020],\n",
      "         [-36873.0195,   2701.0156,  -2889.9941,  ...,  36957.0117,\n",
      "            4065.0371,  22275.0078],\n",
      "         [  2049.9717,  -1272.0146,   5566.0596,  ...,  -8314.0488,\n",
      "          -10559.0020,   1991.9692],\n",
      "         ...,\n",
      "         [-14125.9785,  -5346.9980,   -904.9717,  ...,   1685.0117,\n",
      "           -2388.9844,  -1312.0010],\n",
      "         [  -362.9648, -10048.9883,   4023.9951,  ...,   4202.0146,\n",
      "          -54222.0078,   3908.9771],\n",
      "         [  3220.0093,  15428.9961,  -6862.9722,  ...,  11981.0117,\n",
      "            1538.9922,  -9587.9902]]]),) and output (tensor([[[ -4822.0010,   3124.9990,  -3885.0039,  ..., -19007.0020,\n",
      "            -551.0020,  16285.0020],\n",
      "         [-34153.0195,   3441.0156,  -3132.9941,  ...,  43765.0117,\n",
      "            4980.0371,  25625.0078],\n",
      "         [  5397.9717,   3807.9854,   5788.0596,  ...,  -7821.0488,\n",
      "           -5396.0020,   5873.9692],\n",
      "         ...,\n",
      "         [-12061.9785, -11562.9980,   6955.0283,  ...,  -3421.9883,\n",
      "            2229.0156,   -733.0010],\n",
      "         [  7550.0352, -13702.9883,   -751.0049,  ...,    180.0146,\n",
      "          -68206.0078,   5784.9771],\n",
      "         [  6026.0093,  18233.9961,  -5939.9722,  ...,  10550.0117,\n",
      "           10788.9922,  -8642.9902]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[ -4822.0010,   3124.9990,  -3885.0039,  ..., -19007.0020,\n",
      "            -551.0020,  16285.0020],\n",
      "         [-34153.0195,   3441.0156,  -3132.9941,  ...,  43765.0117,\n",
      "            4980.0371,  25625.0078],\n",
      "         [  5397.9717,   3807.9854,   5788.0596,  ...,  -7821.0488,\n",
      "           -5396.0020,   5873.9692],\n",
      "         ...,\n",
      "         [-12061.9785, -11562.9980,   6955.0283,  ...,  -3421.9883,\n",
      "            2229.0156,   -733.0010],\n",
      "         [  7550.0352, -13702.9883,   -751.0049,  ...,    180.0146,\n",
      "          -68206.0078,   5784.9771],\n",
      "         [  6026.0093,  18233.9961,  -5939.9722,  ...,  10550.0117,\n",
      "           10788.9922,  -8642.9902]]]),) and output (tensor([[[ -7235.0010,   6583.9990,  -4212.0039,  ..., -19104.0020,\n",
      "            3087.9980,  17084.0020],\n",
      "         [-42014.0195,   2064.0156,   1619.0059,  ...,  45089.0117,\n",
      "           15408.0371,  19235.0078],\n",
      "         [  9537.9717,   8489.9854,  11363.0596,  ...,  -6227.0488,\n",
      "           -7506.0020,   9205.9688],\n",
      "         ...,\n",
      "         [-11863.9785,  -6914.9980,   6446.0283,  ...,   1040.0117,\n",
      "           10335.0156,    635.9990],\n",
      "         [ 10550.0352, -16559.9883,  -1814.0049,  ...,  -7577.9854,\n",
      "          -81589.0078,  11284.9766],\n",
      "         [ 13201.0098,  16838.9961,  -8262.9727,  ...,   4570.0117,\n",
      "           14749.9922,  -6654.9902]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[ -7235.0010,   6583.9990,  -4212.0039,  ..., -19104.0020,\n",
      "            3087.9980,  17084.0020],\n",
      "         [-42014.0195,   2064.0156,   1619.0059,  ...,  45089.0117,\n",
      "           15408.0371,  19235.0078],\n",
      "         [  9537.9717,   8489.9854,  11363.0596,  ...,  -6227.0488,\n",
      "           -7506.0020,   9205.9688],\n",
      "         ...,\n",
      "         [-11863.9785,  -6914.9980,   6446.0283,  ...,   1040.0117,\n",
      "           10335.0156,    635.9990],\n",
      "         [ 10550.0352, -16559.9883,  -1814.0049,  ...,  -7577.9854,\n",
      "          -81589.0078,  11284.9766],\n",
      "         [ 13201.0098,  16838.9961,  -8262.9727,  ...,   4570.0117,\n",
      "           14749.9922,  -6654.9902]]]),) and output (tensor([[[-5.5860e+03,  7.6460e+03, -5.0500e+02,  ..., -1.5346e+04,\n",
      "          -6.2300e+02,  1.2986e+04],\n",
      "         [-4.0811e+04,  3.8930e+03,  1.6590e+03,  ...,  4.5045e+04,\n",
      "           8.2170e+03,  1.4146e+04],\n",
      "         [ 9.0300e+03,  6.8840e+03,  1.1419e+04,  ..., -8.5710e+03,\n",
      "          -1.2877e+04,  3.7969e+01],\n",
      "         ...,\n",
      "         [-1.1514e+04, -1.3802e+04,  4.1930e+03,  ...,  3.3140e+03,\n",
      "           4.2970e+03, -1.0855e+04],\n",
      "         [ 1.0920e+04, -1.3067e+04,  6.5300e+02,  ..., -2.2720e+03,\n",
      "          -8.9850e+04,  1.1268e+04],\n",
      "         [ 8.4320e+03,  1.0468e+04, -9.9830e+03,  ...,  1.9780e+03,\n",
      "           8.7280e+03, -1.1715e+04]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[-5.5860e+03,  7.6460e+03, -5.0500e+02,  ..., -1.5346e+04,\n",
      "          -6.2300e+02,  1.2986e+04],\n",
      "         [-4.0811e+04,  3.8930e+03,  1.6590e+03,  ...,  4.5045e+04,\n",
      "           8.2170e+03,  1.4146e+04],\n",
      "         [ 9.0300e+03,  6.8840e+03,  1.1419e+04,  ..., -8.5710e+03,\n",
      "          -1.2877e+04,  3.7969e+01],\n",
      "         ...,\n",
      "         [-1.1514e+04, -1.3802e+04,  4.1930e+03,  ...,  3.3140e+03,\n",
      "           4.2970e+03, -1.0855e+04],\n",
      "         [ 1.0920e+04, -1.3067e+04,  6.5300e+02,  ..., -2.2720e+03,\n",
      "          -8.9850e+04,  1.1268e+04],\n",
      "         [ 8.4320e+03,  1.0468e+04, -9.9830e+03,  ...,  1.9780e+03,\n",
      "           8.7280e+03, -1.1715e+04]]]),) and output (tensor([[[ -8916.0010,   2362.9990,  -4833.0039,  ..., -12075.0020,\n",
      "           -1124.0020,  12438.0020],\n",
      "         [-40303.0195,   7217.0156,   1278.0059,  ...,  46189.0117,\n",
      "            -494.9629,  11190.0078],\n",
      "         [ 16127.9717,   5095.9854,  10415.0596,  ...,  -7443.0488,\n",
      "           -7194.0020,   2950.9688],\n",
      "         ...,\n",
      "         [-14254.9785, -12592.9980,   8659.0283,  ...,  -1365.9883,\n",
      "            7144.0156, -13514.0010],\n",
      "         [ 15752.0352, -19819.9883,   3652.9951,  ...,  -7210.9854,\n",
      "          -93470.0078,  11686.9766],\n",
      "         [  7949.0098,  15432.9961,  -7933.9727,  ...,  14897.0117,\n",
      "            4289.9922,  -3846.9902]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[ -8916.0010,   2362.9990,  -4833.0039,  ..., -12075.0020,\n",
      "           -1124.0020,  12438.0020],\n",
      "         [-40303.0195,   7217.0156,   1278.0059,  ...,  46189.0117,\n",
      "            -494.9629,  11190.0078],\n",
      "         [ 16127.9717,   5095.9854,  10415.0596,  ...,  -7443.0488,\n",
      "           -7194.0020,   2950.9688],\n",
      "         ...,\n",
      "         [-14254.9785, -12592.9980,   8659.0283,  ...,  -1365.9883,\n",
      "            7144.0156, -13514.0010],\n",
      "         [ 15752.0352, -19819.9883,   3652.9951,  ...,  -7210.9854,\n",
      "          -93470.0078,  11686.9766],\n",
      "         [  7949.0098,  15432.9961,  -7933.9727,  ...,  14897.0117,\n",
      "            4289.9922,  -3846.9902]]]),) and output (tensor([[[-20230.0000,   7877.9990,    161.9961,  ..., -10250.0020,\n",
      "             676.9980,   7095.0020],\n",
      "         [-44647.0195,   3520.0156,   4988.0059,  ...,  39470.0117,\n",
      "           -1128.9629,  19176.0078],\n",
      "         [ 14057.9717,   9069.9854,  11226.0596,  ...,  -9868.0488,\n",
      "           -4801.0020,   3261.9688],\n",
      "         ...,\n",
      "         [-20899.9785, -13161.9980,   5320.0283,  ...,  -7470.9883,\n",
      "            3469.0156,  -6640.0010],\n",
      "         [ 21102.0352, -24908.9883,   4597.9951,  ...,  -4101.9854,\n",
      "          -90213.0078,   5113.9766],\n",
      "         [  2820.0098,  11875.9961,  -7313.9727,  ...,  16112.0117,\n",
      "            2343.9922,  -8999.9902]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[-20230.0000,   7877.9990,    161.9961,  ..., -10250.0020,\n",
      "             676.9980,   7095.0020],\n",
      "         [-44647.0195,   3520.0156,   4988.0059,  ...,  39470.0117,\n",
      "           -1128.9629,  19176.0078],\n",
      "         [ 14057.9717,   9069.9854,  11226.0596,  ...,  -9868.0488,\n",
      "           -4801.0020,   3261.9688],\n",
      "         ...,\n",
      "         [-20899.9785, -13161.9980,   5320.0283,  ...,  -7470.9883,\n",
      "            3469.0156,  -6640.0010],\n",
      "         [ 21102.0352, -24908.9883,   4597.9951,  ...,  -4101.9854,\n",
      "          -90213.0078,   5113.9766],\n",
      "         [  2820.0098,  11875.9961,  -7313.9727,  ...,  16112.0117,\n",
      "            2343.9922,  -8999.9902]]]),) and output (tensor([[[-25670.0000,  13293.9990,   9099.9961,  ...,  -6547.0020,\n",
      "           -5658.0020,   4192.0020],\n",
      "         [-42573.0195,   6837.0156,   3885.0059,  ...,  42483.0117,\n",
      "            2748.0371,  26794.0078],\n",
      "         [ 16185.9717,   3852.9854,   9431.0596,  ...,  -6563.0488,\n",
      "            4170.9980,   2853.9688],\n",
      "         ...,\n",
      "         [-20052.9785, -14999.9980,   7984.0283,  ..., -10768.9883,\n",
      "             157.0156,  -8991.0010],\n",
      "         [ 25361.0352, -23986.9883,  -1108.0049,  ...,   -877.9854,\n",
      "          -88578.0078,   2811.9766],\n",
      "         [ -8217.9902,  13009.9961, -11098.9727,  ...,  16347.0117,\n",
      "            5086.9922,  -8759.9902]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[-25670.0000,  13293.9990,   9099.9961,  ...,  -6547.0020,\n",
      "           -5658.0020,   4192.0020],\n",
      "         [-42573.0195,   6837.0156,   3885.0059,  ...,  42483.0117,\n",
      "            2748.0371,  26794.0078],\n",
      "         [ 16185.9717,   3852.9854,   9431.0596,  ...,  -6563.0488,\n",
      "            4170.9980,   2853.9688],\n",
      "         ...,\n",
      "         [-20052.9785, -14999.9980,   7984.0283,  ..., -10768.9883,\n",
      "             157.0156,  -8991.0010],\n",
      "         [ 25361.0352, -23986.9883,  -1108.0049,  ...,   -877.9854,\n",
      "          -88578.0078,   2811.9766],\n",
      "         [ -8217.9902,  13009.9961, -11098.9727,  ...,  16347.0117,\n",
      "            5086.9922,  -8759.9902]]]),) and output (tensor([[[-25652.0000,  13180.9990,  15728.9961,  ...,  -2831.0020,\n",
      "             158.9980,   2204.0020],\n",
      "         [-39215.0195,   9450.0156,   2896.0059,  ...,  37873.0117,\n",
      "            3521.0371,  30351.0078],\n",
      "         [ 13348.9717,   6343.9854,  10112.0596,  ...,  -3108.0488,\n",
      "             420.9980,   6087.9688],\n",
      "         ...,\n",
      "         [-16912.9785, -13264.9980,  11472.0283,  ..., -14987.9883,\n",
      "           -4601.9844,  -2824.0010],\n",
      "         [ 18635.0352, -25469.9883,   7103.9951,  ...,  -2425.9854,\n",
      "          -87668.0078,   1782.9766],\n",
      "         [-10459.9902,  20290.9961,   5008.0273,  ...,  16622.0117,\n",
      "            3435.9922, -10528.9902]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[-25652.0000,  13180.9990,  15728.9961,  ...,  -2831.0020,\n",
      "             158.9980,   2204.0020],\n",
      "         [-39215.0195,   9450.0156,   2896.0059,  ...,  37873.0117,\n",
      "            3521.0371,  30351.0078],\n",
      "         [ 13348.9717,   6343.9854,  10112.0596,  ...,  -3108.0488,\n",
      "             420.9980,   6087.9688],\n",
      "         ...,\n",
      "         [-16912.9785, -13264.9980,  11472.0283,  ..., -14987.9883,\n",
      "           -4601.9844,  -2824.0010],\n",
      "         [ 18635.0352, -25469.9883,   7103.9951,  ...,  -2425.9854,\n",
      "          -87668.0078,   1782.9766],\n",
      "         [-10459.9902,  20290.9961,   5008.0273,  ...,  16622.0117,\n",
      "            3435.9922, -10528.9902]]]),) and output (tensor([[[-14791.0000,  15299.9990,  16129.9961,  ...,   3187.9980,\n",
      "           -2552.0020,   5626.0020],\n",
      "         [-31834.0195,  11634.0156,   2549.0059,  ...,  42224.0117,\n",
      "            -230.9629,  31789.0078],\n",
      "         [ 14272.9717,  13726.9854,  10845.0596,  ...,  -7004.0488,\n",
      "           -1864.0020,  -5837.0312],\n",
      "         ...,\n",
      "         [-20310.9785, -12290.9980,  13008.0283,  ..., -13490.9883,\n",
      "           -1607.9844,  -5633.0010],\n",
      "         [ 16383.0352, -26028.9883,   6223.9951,  ...,  -1885.9854,\n",
      "          -87611.0078,   6941.9766],\n",
      "         [ -5087.9902,  19017.9961,   9375.0273,  ...,  25044.0117,\n",
      "            1079.9922, -14496.9902]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[-14791.0000,  15299.9990,  16129.9961,  ...,   3187.9980,\n",
      "           -2552.0020,   5626.0020],\n",
      "         [-31834.0195,  11634.0156,   2549.0059,  ...,  42224.0117,\n",
      "            -230.9629,  31789.0078],\n",
      "         [ 14272.9717,  13726.9854,  10845.0596,  ...,  -7004.0488,\n",
      "           -1864.0020,  -5837.0312],\n",
      "         ...,\n",
      "         [-20310.9785, -12290.9980,  13008.0283,  ..., -13490.9883,\n",
      "           -1607.9844,  -5633.0010],\n",
      "         [ 16383.0352, -26028.9883,   6223.9951,  ...,  -1885.9854,\n",
      "          -87611.0078,   6941.9766],\n",
      "         [ -5087.9902,  19017.9961,   9375.0273,  ...,  25044.0117,\n",
      "            1079.9922, -14496.9902]]]),) and output (tensor([[[-13284.0000,  26639.0000,   7467.9961,  ...,   4113.9980,\n",
      "           -6608.0020,   5336.0020],\n",
      "         [-27603.0195,  17111.0156,  -4505.9941,  ...,  36456.0117,\n",
      "            2125.0371,  26287.0078],\n",
      "         [  5048.9717,  10664.9854,   9145.0596,  ...,  -9608.0488,\n",
      "           -9245.0020,  -3616.0312],\n",
      "         ...,\n",
      "         [-20187.9785, -19747.9980,  15955.0283,  ..., -15764.9883,\n",
      "            -136.9844, -10050.0010],\n",
      "         [ 14780.0352, -27139.9883,   9019.9951,  ...,   5988.0146,\n",
      "          -89699.0078,   6415.9766],\n",
      "         [ -5941.9902,  12471.9961,  11223.0273,  ...,  29240.0117,\n",
      "           -1934.0078, -20842.9902]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[-13284.0000,  26639.0000,   7467.9961,  ...,   4113.9980,\n",
      "           -6608.0020,   5336.0020],\n",
      "         [-27603.0195,  17111.0156,  -4505.9941,  ...,  36456.0117,\n",
      "            2125.0371,  26287.0078],\n",
      "         [  5048.9717,  10664.9854,   9145.0596,  ...,  -9608.0488,\n",
      "           -9245.0020,  -3616.0312],\n",
      "         ...,\n",
      "         [-20187.9785, -19747.9980,  15955.0283,  ..., -15764.9883,\n",
      "            -136.9844, -10050.0010],\n",
      "         [ 14780.0352, -27139.9883,   9019.9951,  ...,   5988.0146,\n",
      "          -89699.0078,   6415.9766],\n",
      "         [ -5941.9902,  12471.9961,  11223.0273,  ...,  29240.0117,\n",
      "           -1934.0078, -20842.9902]]]),) and output (tensor([[[-21532.0000,  32796.0000,  16281.9961,  ...,  18208.9980,\n",
      "           -4282.0020,   5756.0020],\n",
      "         [-33438.0195,  21369.0156, -12050.9941,  ...,  35320.0117,\n",
      "            2727.0371,  31318.0078],\n",
      "         [ -2217.0283,   3028.9854,  10062.0596,  ...,  -8654.0488,\n",
      "           -6519.0020,   9871.9688],\n",
      "         ...,\n",
      "         [-29170.9785, -21467.9980,  23206.0273,  ..., -14487.9883,\n",
      "            4783.0156, -12745.0010],\n",
      "         [ 11067.0352, -17873.9883,  14000.9951,  ...,  12259.0146,\n",
      "          -89103.0078,  11099.9766],\n",
      "         [ -8937.9902,  14736.9961,    781.0273,  ...,  26802.0117,\n",
      "           -1425.0078, -23141.9902]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[-21532.0000,  32796.0000,  16281.9961,  ...,  18208.9980,\n",
      "           -4282.0020,   5756.0020],\n",
      "         [-33438.0195,  21369.0156, -12050.9941,  ...,  35320.0117,\n",
      "            2727.0371,  31318.0078],\n",
      "         [ -2217.0283,   3028.9854,  10062.0596,  ...,  -8654.0488,\n",
      "           -6519.0020,   9871.9688],\n",
      "         ...,\n",
      "         [-29170.9785, -21467.9980,  23206.0273,  ..., -14487.9883,\n",
      "            4783.0156, -12745.0010],\n",
      "         [ 11067.0352, -17873.9883,  14000.9951,  ...,  12259.0146,\n",
      "          -89103.0078,  11099.9766],\n",
      "         [ -8937.9902,  14736.9961,    781.0273,  ...,  26802.0117,\n",
      "           -1425.0078, -23141.9902]]]),) and output (tensor([[[-24082.0000,  33387.0000,  15374.9961,  ...,  18359.9980,\n",
      "           -7500.0020,  11519.0020],\n",
      "         [-23756.0195,  24607.0156, -17376.9941,  ...,  28381.0117,\n",
      "            -239.9629,  32381.0078],\n",
      "         [  -258.0283,   2329.9854,  18151.0586,  ...,  -1466.0488,\n",
      "           -4882.0020,   4815.9688],\n",
      "         ...,\n",
      "         [-21603.9785, -18588.9980,  20807.0273,  ..., -10378.9883,\n",
      "            7738.0156,  -8901.0010],\n",
      "         [  8911.0352, -15918.9883,   9747.9951,  ...,  10155.0146,\n",
      "          -84695.0078,   8829.9766],\n",
      "         [ -4376.9902,   7178.9961,   1392.0273,  ...,  41803.0117,\n",
      "            4344.9922, -24498.9902]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[-24082.0000,  33387.0000,  15374.9961,  ...,  18359.9980,\n",
      "           -7500.0020,  11519.0020],\n",
      "         [-23756.0195,  24607.0156, -17376.9941,  ...,  28381.0117,\n",
      "            -239.9629,  32381.0078],\n",
      "         [  -258.0283,   2329.9854,  18151.0586,  ...,  -1466.0488,\n",
      "           -4882.0020,   4815.9688],\n",
      "         ...,\n",
      "         [-21603.9785, -18588.9980,  20807.0273,  ..., -10378.9883,\n",
      "            7738.0156,  -8901.0010],\n",
      "         [  8911.0352, -15918.9883,   9747.9951,  ...,  10155.0146,\n",
      "          -84695.0078,   8829.9766],\n",
      "         [ -4376.9902,   7178.9961,   1392.0273,  ...,  41803.0117,\n",
      "            4344.9922, -24498.9902]]]),) and output (tensor([[[-1.4869e+04,  3.2284e+04,  3.7430e+03,  ...,  1.2459e+04,\n",
      "          -5.4520e+03,  7.5900e+03],\n",
      "         [-2.7703e+04,  2.5083e+04, -1.2788e+04,  ...,  2.8352e+04,\n",
      "           2.6540e+03,  2.1791e+04],\n",
      "         [-3.8403e+02,  1.1870e+04,  2.0896e+04,  ..., -1.0144e+04,\n",
      "          -1.1874e+04, -1.3530e+03],\n",
      "         ...,\n",
      "         [-1.7532e+04, -1.8046e+04,  2.9551e+04,  ..., -7.6720e+03,\n",
      "           3.0156e+00, -1.1296e+04],\n",
      "         [ 1.3239e+04, -1.5855e+04,  8.8830e+03,  ...,  1.0480e+04,\n",
      "          -8.4245e+04,  4.8780e+03],\n",
      "         [ 1.8930e+03,  1.2412e+04,  6.0620e+03,  ...,  4.2399e+04,\n",
      "           1.1960e+04, -2.6842e+04]]]),)\n",
      "[Debug] Layer names being passed: ['model.layers.0', 'model.layers.1', 'model.layers.2', 'model.layers.3', 'model.layers.4', 'model.layers.5', 'model.layers.6', 'model.layers.7', 'model.layers.8', 'model.layers.9', 'model.layers.10', 'model.layers.11', 'model.layers.12', 'model.layers.13', 'model.layers.14', 'model.layers.15', 'model.layers.16', 'model.layers.17', 'model.layers.18', 'model.layers.19', 'model.layers.20', 'model.layers.21', 'model.layers.22', 'model.layers.23', 'model.layers.24', 'model.layers.25', 'model.layers.26', 'model.layers.27', 'model.embed_tokens']\n",
      "[Debug] Trying to access layer: model.layers.0\n",
      "[Debug] Successfully found layer: model.layers.0\n",
      "[Debug] Trying to access layer: model.layers.1\n",
      "[Debug] Successfully found layer: model.layers.1\n",
      "[Debug] Trying to access layer: model.layers.2\n",
      "[Debug] Successfully found layer: model.layers.2\n",
      "[Debug] Trying to access layer: model.layers.3\n",
      "[Debug] Successfully found layer: model.layers.3\n",
      "[Debug] Trying to access layer: model.layers.4\n",
      "[Debug] Successfully found layer: model.layers.4\n",
      "[Debug] Trying to access layer: model.layers.5\n",
      "[Debug] Successfully found layer: model.layers.5\n",
      "[Debug] Trying to access layer: model.layers.6\n",
      "[Debug] Successfully found layer: model.layers.6\n",
      "[Debug] Trying to access layer: model.layers.7\n",
      "[Debug] Successfully found layer: model.layers.7\n",
      "[Debug] Trying to access layer: model.layers.8\n",
      "[Debug] Successfully found layer: model.layers.8\n",
      "[Debug] Trying to access layer: model.layers.9\n",
      "[Debug] Successfully found layer: model.layers.9\n",
      "[Debug] Trying to access layer: model.layers.10\n",
      "[Debug] Successfully found layer: model.layers.10\n",
      "[Debug] Trying to access layer: model.layers.11\n",
      "[Debug] Successfully found layer: model.layers.11\n",
      "[Debug] Trying to access layer: model.layers.12\n",
      "[Debug] Successfully found layer: model.layers.12\n",
      "[Debug] Trying to access layer: model.layers.13\n",
      "[Debug] Successfully found layer: model.layers.13\n",
      "[Debug] Trying to access layer: model.layers.14\n",
      "[Debug] Successfully found layer: model.layers.14\n",
      "[Debug] Trying to access layer: model.layers.15\n",
      "[Debug] Successfully found layer: model.layers.15\n",
      "[Debug] Trying to access layer: model.layers.16\n",
      "[Debug] Successfully found layer: model.layers.16\n",
      "[Debug] Trying to access layer: model.layers.17\n",
      "[Debug] Successfully found layer: model.layers.17\n",
      "[Debug] Trying to access layer: model.layers.18\n",
      "[Debug] Successfully found layer: model.layers.18\n",
      "[Debug] Trying to access layer: model.layers.19\n",
      "[Debug] Successfully found layer: model.layers.19\n",
      "[Debug] Trying to access layer: model.layers.20\n",
      "[Debug] Successfully found layer: model.layers.20\n",
      "[Debug] Trying to access layer: model.layers.21\n",
      "[Debug] Successfully found layer: model.layers.21\n",
      "[Debug] Trying to access layer: model.layers.22\n",
      "[Debug] Successfully found layer: model.layers.22\n",
      "[Debug] Trying to access layer: model.layers.23\n",
      "[Debug] Successfully found layer: model.layers.23\n",
      "[Debug] Trying to access layer: model.layers.24\n",
      "[Debug] Successfully found layer: model.layers.24\n",
      "[Debug] Trying to access layer: model.layers.25\n",
      "[Debug] Successfully found layer: model.layers.25\n",
      "[Debug] Trying to access layer: model.layers.26\n",
      "[Debug] Successfully found layer: model.layers.26\n",
      "[Debug] Trying to access layer: model.layers.27\n",
      "[Debug] Successfully found layer: model.layers.27\n",
      "[Debug] Trying to access layer: model.embed_tokens\n",
      "[Debug] Successfully found layer: model.embed_tokens\n",
      "[Hook] Layer Embedding(128256, 3072, padding_idx=128004) received input (tensor([[128000,    845,    339,  35769,    315,    279,  11842,  65473,   1571,\n",
      "          29098,    578,  30048,  44660,    315,   1403,  38846,   4455,     25,\n",
      "            264,    330,  48195,  64239,  30155,      1,    389,    220,   1627,\n",
      "            323,    220,   1544,   6287,    220,    679,     17,     11,    323,\n",
      "            264,    330,   6349,   1601,    532,  30155,      1,    389,    220,\n",
      "           1591,    323,    220,   1682,   6287,    220,    679,     17,     13,\n",
      "            578,   6164,  30048,   3952,   2035,    389,    220,    966,    323,\n",
      "            220,   2148,   6287,   8032,     20,   1483,     22,     60,  33589,\n",
      "           4900,  65552,    386,  74187,  19073,  23415,    279,  32858,    315,\n",
      "            279,  11842,  65473,   1571,  29098,    320,     45,   1428,      8,\n",
      "            311,  28501,   4900,  94286,  55417,    483,  63186,     11,   2391,\n",
      "            279,  54559,  22260,    315,  28986,      6,  30155,   8032,     23,\n",
      "             60,  10471,    690,   3412,    279,    452,   1428,  32858,    369,\n",
      "           3116,   1667,   3156,    279,    220,   1114,    339,  30048,    304,\n",
      "          37003,    304,    220,    679,     21,     13]]),) and output tensor([[[-0.0010, -0.0007, -0.0046,  ..., -0.0014, -0.0020,  0.0020],\n",
      "         [ 0.0110, -0.0002,  0.0243,  ..., -0.0016,  0.0010, -0.0126],\n",
      "         [-0.0081,  0.0239,  0.0041,  ...,  0.0311, -0.0126,  0.0238],\n",
      "         ...,\n",
      "         [ 0.0240, -0.0150, -0.0019,  ..., -0.0222, -0.0195,  0.0127],\n",
      "         [ 0.0139,  0.0073,  0.0117,  ..., -0.0192,  0.0009, -0.0121],\n",
      "         [ 0.0093, -0.0031,  0.0278,  ...,  0.0117, -0.0084,  0.0096]]])\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[-0.0010, -0.0007, -0.0046,  ..., -0.0014, -0.0020,  0.0020],\n",
      "         [ 0.0110, -0.0002,  0.0243,  ..., -0.0016,  0.0010, -0.0126],\n",
      "         [-0.0081,  0.0239,  0.0041,  ...,  0.0311, -0.0126,  0.0238],\n",
      "         ...,\n",
      "         [ 0.0240, -0.0150, -0.0019,  ..., -0.0222, -0.0195,  0.0127],\n",
      "         [ 0.0139,  0.0073,  0.0117,  ..., -0.0192,  0.0009, -0.0121],\n",
      "         [ 0.0093, -0.0031,  0.0278,  ...,  0.0117, -0.0084,  0.0096]]]),) and output (tensor([[[ 442.9990,  239.9993,  302.9954,  ..., -154.0014,  -15.0020,\n",
      "           185.0020],\n",
      "         [  31.0110,  184.9998,  125.0243,  ...,  -54.0016,  -38.9990,\n",
      "           -20.0126],\n",
      "         [  54.9919,    9.0239, -101.9959,  ...,   59.0311,  -97.0126,\n",
      "           -89.9762],\n",
      "         ...,\n",
      "         [ 167.0240, -394.0150, -290.0019,  ..., -214.0222, -158.0195,\n",
      "          -164.9873],\n",
      "         [ 184.0139,   46.0073, -482.9883,  ..., -315.0192,   97.0009,\n",
      "            37.9879],\n",
      "         [-218.9907, -202.0031,   21.0278,  ...,  -66.9883,  396.9916,\n",
      "          -462.9904]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[ 442.9990,  239.9993,  302.9954,  ..., -154.0014,  -15.0020,\n",
      "           185.0020],\n",
      "         [  31.0110,  184.9998,  125.0243,  ...,  -54.0016,  -38.9990,\n",
      "           -20.0126],\n",
      "         [  54.9919,    9.0239, -101.9959,  ...,   59.0311,  -97.0126,\n",
      "           -89.9762],\n",
      "         ...,\n",
      "         [ 167.0240, -394.0150, -290.0019,  ..., -214.0222, -158.0195,\n",
      "          -164.9873],\n",
      "         [ 184.0139,   46.0073, -482.9883,  ..., -315.0192,   97.0009,\n",
      "            37.9879],\n",
      "         [-218.9907, -202.0031,   21.0278,  ...,  -66.9883,  396.9916,\n",
      "          -462.9904]]]),) and output (tensor([[[  439.9990,  -144.0007,  -410.0046,  ...,  -765.0015,\n",
      "           1193.9979, -1027.9980],\n",
      "         [ -587.9890,   521.9998,   571.0243,  ...,  -937.0016,\n",
      "            379.0010,  -585.0126],\n",
      "         [ -557.0081,  1322.0239,   -83.9959,  ...,  -509.9689,\n",
      "           -277.0126,  -949.9762],\n",
      "         ...,\n",
      "         [ -563.9760,  -346.0150,  -352.0019,  ...,  -627.0222,\n",
      "           -585.0195,  -350.9873],\n",
      "         [-1146.9861,   696.0073,  -320.9883,  ...,  -737.0192,\n",
      "           -807.9991,  -853.0121],\n",
      "         [-1242.9907, -1137.0031,   202.0278,  ...,  -956.9883,\n",
      "          -3037.0083,  -735.9904]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[  439.9990,  -144.0007,  -410.0046,  ...,  -765.0015,\n",
      "           1193.9979, -1027.9980],\n",
      "         [ -587.9890,   521.9998,   571.0243,  ...,  -937.0016,\n",
      "            379.0010,  -585.0126],\n",
      "         [ -557.0081,  1322.0239,   -83.9959,  ...,  -509.9689,\n",
      "           -277.0126,  -949.9762],\n",
      "         ...,\n",
      "         [ -563.9760,  -346.0150,  -352.0019,  ...,  -627.0222,\n",
      "           -585.0195,  -350.9873],\n",
      "         [-1146.9861,   696.0073,  -320.9883,  ...,  -737.0192,\n",
      "           -807.9991,  -853.0121],\n",
      "         [-1242.9907, -1137.0031,   202.0278,  ...,  -956.9883,\n",
      "          -3037.0083,  -735.9904]]]),) and output (tensor([[[  551.9990,   993.9993, -1790.0046,  ...,   445.9985,\n",
      "          -1386.0021, -1245.9980],\n",
      "         [ -396.9890,  -119.0002,  1020.0243,  ..., -2189.0015,\n",
      "          -3462.9990, -4315.0127],\n",
      "         [ -411.0081,  1300.0239, -1611.9958,  ..., -1651.9689,\n",
      "           -881.0126, -2124.9761],\n",
      "         ...,\n",
      "         [-2487.9761,  1841.9850,  -740.0018,  ...,  4643.9775,\n",
      "          -1137.0195,  3659.0127],\n",
      "         [  -98.9861, -3206.9927,  1290.0117,  ..., -3519.0190,\n",
      "          -2132.9990,  -182.0121],\n",
      "         [ -316.9907, -1532.0031,  1948.0278,  ..., -2525.9883,\n",
      "          -2876.0083,   233.0096]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[  551.9990,   993.9993, -1790.0046,  ...,   445.9985,\n",
      "          -1386.0021, -1245.9980],\n",
      "         [ -396.9890,  -119.0002,  1020.0243,  ..., -2189.0015,\n",
      "          -3462.9990, -4315.0127],\n",
      "         [ -411.0081,  1300.0239, -1611.9958,  ..., -1651.9689,\n",
      "           -881.0126, -2124.9761],\n",
      "         ...,\n",
      "         [-2487.9761,  1841.9850,  -740.0018,  ...,  4643.9775,\n",
      "          -1137.0195,  3659.0127],\n",
      "         [  -98.9861, -3206.9927,  1290.0117,  ..., -3519.0190,\n",
      "          -2132.9990,  -182.0121],\n",
      "         [ -316.9907, -1532.0031,  1948.0278,  ..., -2525.9883,\n",
      "          -2876.0083,   233.0096]]]),) and output (tensor([[[ 2131.9990,  -737.0007, -1071.0046,  ..., -2781.0015,\n",
      "           -885.0021,    86.0020],\n",
      "         [  -90.9890,  -981.0002, -1176.9757,  ...,  -203.0015,\n",
      "           -959.9990, -1818.0127],\n",
      "         [-1720.0081,  1550.0239,  3740.0042,  ..., -7476.9688,\n",
      "          -2089.0127, -2544.9761],\n",
      "         ...,\n",
      "         [-7050.9761,   -59.0150,  1505.9982,  ...,  4016.9775,\n",
      "          -3029.0195,   712.0127],\n",
      "         [  557.0139,  2666.0073,  1228.0117,  ..., -9432.0195,\n",
      "            907.0010, -2917.0122],\n",
      "         [-4318.9907, -3882.0029,  2812.0278,  ...,   727.0117,\n",
      "          -6746.0083, -6120.9902]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[ 2131.9990,  -737.0007, -1071.0046,  ..., -2781.0015,\n",
      "           -885.0021,    86.0020],\n",
      "         [  -90.9890,  -981.0002, -1176.9757,  ...,  -203.0015,\n",
      "           -959.9990, -1818.0127],\n",
      "         [-1720.0081,  1550.0239,  3740.0042,  ..., -7476.9688,\n",
      "          -2089.0127, -2544.9761],\n",
      "         ...,\n",
      "         [-7050.9761,   -59.0150,  1505.9982,  ...,  4016.9775,\n",
      "          -3029.0195,   712.0127],\n",
      "         [  557.0139,  2666.0073,  1228.0117,  ..., -9432.0195,\n",
      "            907.0010, -2917.0122],\n",
      "         [-4318.9907, -3882.0029,  2812.0278,  ...,   727.0117,\n",
      "          -6746.0083, -6120.9902]]]),) and output (tensor([[[  3420.9990,    760.9993,  -5376.0049,  ...,  -4193.0015,\n",
      "            2860.9980,  -4080.9980],\n",
      "         [ -3533.9890,   1988.9998,   -638.9757,  ...,   7602.9985,\n",
      "           -3241.9990,   -722.0127],\n",
      "         [  1228.9919,   2114.0239,   3516.0042,  ...,  -9547.9688,\n",
      "           -4615.0127,   -602.9761],\n",
      "         ...,\n",
      "         [ -6212.9761,  -1466.0150,   1446.9982,  ...,   6242.9775,\n",
      "           -4428.0195,    -74.9873],\n",
      "         [ -2077.9861,   2670.0073,    872.0117,  ...,  -8219.0195,\n",
      "           -1751.9990,  -6451.0122],\n",
      "         [ -4512.9907,  -4984.0029,   2967.0278,  ...,  -1586.9883,\n",
      "          -10005.0078,  -8402.9902]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[  3420.9990,    760.9993,  -5376.0049,  ...,  -4193.0015,\n",
      "            2860.9980,  -4080.9980],\n",
      "         [ -3533.9890,   1988.9998,   -638.9757,  ...,   7602.9985,\n",
      "           -3241.9990,   -722.0127],\n",
      "         [  1228.9919,   2114.0239,   3516.0042,  ...,  -9547.9688,\n",
      "           -4615.0127,   -602.9761],\n",
      "         ...,\n",
      "         [ -6212.9761,  -1466.0150,   1446.9982,  ...,   6242.9775,\n",
      "           -4428.0195,    -74.9873],\n",
      "         [ -2077.9861,   2670.0073,    872.0117,  ...,  -8219.0195,\n",
      "           -1751.9990,  -6451.0122],\n",
      "         [ -4512.9907,  -4984.0029,   2967.0278,  ...,  -1586.9883,\n",
      "          -10005.0078,  -8402.9902]]]),) and output (tensor([[[ 2.7430e+03, -1.5120e+03, -1.0493e+04,  ..., -9.9450e+03,\n",
      "          -2.0170e+03, -1.8030e+03],\n",
      "         [-1.8510e+03, -1.3910e+03,  5.1302e+02,  ...,  3.1840e+03,\n",
      "          -2.2880e+03,  1.0590e+03],\n",
      "         [ 4.4920e+03,  6.2020e+03,  4.5560e+03,  ..., -1.0965e+04,\n",
      "          -4.4380e+03,  1.5580e+03],\n",
      "         ...,\n",
      "         [-4.2620e+03, -3.6700e+03, -1.0002e+01,  ...,  6.6160e+03,\n",
      "          -5.7200e+03,  2.6130e+03],\n",
      "         [-4.6199e+02,  1.3470e+03,  1.7040e+03,  ..., -1.2719e+04,\n",
      "          -4.7650e+03, -3.9960e+03],\n",
      "         [-1.0423e+04, -4.3500e+03,  1.6400e+03,  ...,  6.3840e+03,\n",
      "          -1.3587e+04, -8.3870e+03]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[ 2.7430e+03, -1.5120e+03, -1.0493e+04,  ..., -9.9450e+03,\n",
      "          -2.0170e+03, -1.8030e+03],\n",
      "         [-1.8510e+03, -1.3910e+03,  5.1302e+02,  ...,  3.1840e+03,\n",
      "          -2.2880e+03,  1.0590e+03],\n",
      "         [ 4.4920e+03,  6.2020e+03,  4.5560e+03,  ..., -1.0965e+04,\n",
      "          -4.4380e+03,  1.5580e+03],\n",
      "         ...,\n",
      "         [-4.2620e+03, -3.6700e+03, -1.0002e+01,  ...,  6.6160e+03,\n",
      "          -5.7200e+03,  2.6130e+03],\n",
      "         [-4.6199e+02,  1.3470e+03,  1.7040e+03,  ..., -1.2719e+04,\n",
      "          -4.7650e+03, -3.9960e+03],\n",
      "         [-1.0423e+04, -4.3500e+03,  1.6400e+03,  ...,  6.3840e+03,\n",
      "          -1.3587e+04, -8.3870e+03]]]),) and output (tensor([[[  3032.9990,  -3420.0007, -10291.0049,  ...,  -6511.0020,\n",
      "           -1953.0020,   2001.0020],\n",
      "         [ -2837.9890,  -2553.0002,    896.0243,  ...,  -3178.0015,\n",
      "           -1669.9990,  -3033.0127],\n",
      "         [   685.9922,  12979.0234,   7630.0039,  ..., -11201.9688,\n",
      "             -72.0127,   -475.9761],\n",
      "         ...,\n",
      "         [ -4537.9761,   -662.0151,  -1191.0018,  ...,   6318.9775,\n",
      "           -3559.0195,   2008.0127],\n",
      "         [ -2043.9861,   4124.0073,   1371.0117,  ..., -22395.0195,\n",
      "           -4864.9990, -11456.0117],\n",
      "         [ -5985.9902,   -260.0029,    635.0278,  ...,   8643.0117,\n",
      "          -23175.0078, -14612.9902]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[  3032.9990,  -3420.0007, -10291.0049,  ...,  -6511.0020,\n",
      "           -1953.0020,   2001.0020],\n",
      "         [ -2837.9890,  -2553.0002,    896.0243,  ...,  -3178.0015,\n",
      "           -1669.9990,  -3033.0127],\n",
      "         [   685.9922,  12979.0234,   7630.0039,  ..., -11201.9688,\n",
      "             -72.0127,   -475.9761],\n",
      "         ...,\n",
      "         [ -4537.9761,   -662.0151,  -1191.0018,  ...,   6318.9775,\n",
      "           -3559.0195,   2008.0127],\n",
      "         [ -2043.9861,   4124.0073,   1371.0117,  ..., -22395.0195,\n",
      "           -4864.9990, -11456.0117],\n",
      "         [ -5985.9902,   -260.0029,    635.0278,  ...,   8643.0117,\n",
      "          -23175.0078, -14612.9902]]]),) and output (tensor([[[  3132.9990,  -1970.0007, -11544.0049,  ...,  -5780.0020,\n",
      "           -2486.0020,  -1921.9980],\n",
      "         [ -4562.9893,  -5110.0000,  -3704.9756,  ...,  -2191.0015,\n",
      "           -2333.9990,  -6294.0127],\n",
      "         [  1346.9922,  12088.0234,   -191.9961,  ..., -13964.9688,\n",
      "            1136.9873,  -8403.9766],\n",
      "         ...,\n",
      "         [ -2774.9761,  -4194.0151,  -2967.0020,  ...,   2904.9775,\n",
      "           -2348.0195,   2399.0127],\n",
      "         [ -8764.9863,   9893.0078,  -1771.9883,  ..., -32087.0195,\n",
      "           -3637.9990, -11700.0117],\n",
      "         [ -5484.9902,  -3833.0029,   2078.0278,  ...,   4012.0117,\n",
      "          -28968.0078,  -8839.9902]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[  3132.9990,  -1970.0007, -11544.0049,  ...,  -5780.0020,\n",
      "           -2486.0020,  -1921.9980],\n",
      "         [ -4562.9893,  -5110.0000,  -3704.9756,  ...,  -2191.0015,\n",
      "           -2333.9990,  -6294.0127],\n",
      "         [  1346.9922,  12088.0234,   -191.9961,  ..., -13964.9688,\n",
      "            1136.9873,  -8403.9766],\n",
      "         ...,\n",
      "         [ -2774.9761,  -4194.0151,  -2967.0020,  ...,   2904.9775,\n",
      "           -2348.0195,   2399.0127],\n",
      "         [ -8764.9863,   9893.0078,  -1771.9883,  ..., -32087.0195,\n",
      "           -3637.9990, -11700.0117],\n",
      "         [ -5484.9902,  -3833.0029,   2078.0278,  ...,   4012.0117,\n",
      "          -28968.0078,  -8839.9902]]]),) and output (tensor([[[  1046.9990,   2403.9993, -14486.0049,  ..., -10922.0020,\n",
      "           -1419.0020,  -2983.9980],\n",
      "         [-10818.9893, -12700.0000,  -5302.9756,  ...,   2214.9985,\n",
      "           -6466.9990, -19106.0117],\n",
      "         [  3529.9922,   8927.0234,   -368.9961,  ..., -12917.9688,\n",
      "            4996.9873, -13604.9766],\n",
      "         ...,\n",
      "         [  -453.9761,  -5023.0151,  -3309.0020,  ...,   4316.9775,\n",
      "           -6306.0195,  -1213.9873],\n",
      "         [-11015.9863,  12920.0078,  -3260.9883,  ..., -33676.0195,\n",
      "           -7528.9990, -18710.0117],\n",
      "         [ -9840.9902,  -1637.0029,   1339.0278,  ...,   5883.0117,\n",
      "          -36529.0078, -15741.9902]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[  1046.9990,   2403.9993, -14486.0049,  ..., -10922.0020,\n",
      "           -1419.0020,  -2983.9980],\n",
      "         [-10818.9893, -12700.0000,  -5302.9756,  ...,   2214.9985,\n",
      "           -6466.9990, -19106.0117],\n",
      "         [  3529.9922,   8927.0234,   -368.9961,  ..., -12917.9688,\n",
      "            4996.9873, -13604.9766],\n",
      "         ...,\n",
      "         [  -453.9761,  -5023.0151,  -3309.0020,  ...,   4316.9775,\n",
      "           -6306.0195,  -1213.9873],\n",
      "         [-11015.9863,  12920.0078,  -3260.9883,  ..., -33676.0195,\n",
      "           -7528.9990, -18710.0117],\n",
      "         [ -9840.9902,  -1637.0029,   1339.0278,  ...,   5883.0117,\n",
      "          -36529.0078, -15741.9902]]]),) and output (tensor([[[ -3767.0010,   5307.9990, -14221.0049,  ..., -15035.0020,\n",
      "           -3630.0020,   4168.0020],\n",
      "         [-18706.9883, -11766.0000,  -5452.9756,  ...,   6497.9985,\n",
      "           -6660.9990, -17014.0117],\n",
      "         [   752.9922,  12710.0234,  -2541.9961,  ..., -14829.9688,\n",
      "            8041.9873, -17751.9766],\n",
      "         ...,\n",
      "         [  1511.0239,  -7111.0151,  -4286.0020,  ...,   -506.0225,\n",
      "           -5919.0195,    224.0127],\n",
      "         [-11780.9863,   9228.0078,  -7569.9883,  ..., -39204.0195,\n",
      "           -4963.9990, -18313.0117],\n",
      "         [ -8712.9902,   5776.9971,   1743.0278,  ...,   1941.0117,\n",
      "          -36595.0078, -15074.9902]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[ -3767.0010,   5307.9990, -14221.0049,  ..., -15035.0020,\n",
      "           -3630.0020,   4168.0020],\n",
      "         [-18706.9883, -11766.0000,  -5452.9756,  ...,   6497.9985,\n",
      "           -6660.9990, -17014.0117],\n",
      "         [   752.9922,  12710.0234,  -2541.9961,  ..., -14829.9688,\n",
      "            8041.9873, -17751.9766],\n",
      "         ...,\n",
      "         [  1511.0239,  -7111.0151,  -4286.0020,  ...,   -506.0225,\n",
      "           -5919.0195,    224.0127],\n",
      "         [-11780.9863,   9228.0078,  -7569.9883,  ..., -39204.0195,\n",
      "           -4963.9990, -18313.0117],\n",
      "         [ -8712.9902,   5776.9971,   1743.0278,  ...,   1941.0117,\n",
      "          -36595.0078, -15074.9902]]]),) and output (tensor([[[ -1326.0010,   1223.9990, -16643.0039,  ..., -13240.0020,\n",
      "           -5677.0020,   9639.0020],\n",
      "         [-18987.9883, -10551.0000,  -4043.9756,  ...,  10456.9980,\n",
      "           -9956.9990, -12827.0117],\n",
      "         [  2055.9922,  10921.0234,  -2935.9961,  ..., -14644.9688,\n",
      "            9703.9873, -16362.9766],\n",
      "         ...,\n",
      "         [  7630.0239,  -5478.0151,  -4010.0020,  ...,   4877.9775,\n",
      "           -4832.0195,   2315.0127],\n",
      "         [ -9467.9863,   9076.0078, -13462.9883,  ..., -40773.0195,\n",
      "           -1657.9990, -13184.0117],\n",
      "         [-12394.9902,   7116.9971,   3959.0278,  ...,   2096.0117,\n",
      "          -41151.0078, -15673.9902]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[ -1326.0010,   1223.9990, -16643.0039,  ..., -13240.0020,\n",
      "           -5677.0020,   9639.0020],\n",
      "         [-18987.9883, -10551.0000,  -4043.9756,  ...,  10456.9980,\n",
      "           -9956.9990, -12827.0117],\n",
      "         [  2055.9922,  10921.0234,  -2935.9961,  ..., -14644.9688,\n",
      "            9703.9873, -16362.9766],\n",
      "         ...,\n",
      "         [  7630.0239,  -5478.0151,  -4010.0020,  ...,   4877.9775,\n",
      "           -4832.0195,   2315.0127],\n",
      "         [ -9467.9863,   9076.0078, -13462.9883,  ..., -40773.0195,\n",
      "           -1657.9990, -13184.0117],\n",
      "         [-12394.9902,   7116.9971,   3959.0278,  ...,   2096.0117,\n",
      "          -41151.0078, -15673.9902]]]),) and output (tensor([[[ -5120.0010,   2246.9990, -15993.0039,  ..., -19818.0020,\n",
      "           -9385.0020,  11924.0020],\n",
      "         [-20213.9883,  -7357.0000,  -1321.9756,  ...,   7460.9980,\n",
      "          -10925.9990, -10021.0117],\n",
      "         [  1515.9922,  12230.0234,  -1709.9961,  ..., -19689.9688,\n",
      "           13956.9873, -11909.9766],\n",
      "         ...,\n",
      "         [  5301.0234,  -4457.0151,   -574.0020,  ...,   4088.9775,\n",
      "           -2724.0195,   5865.0127],\n",
      "         [-10094.9863,  11009.0078, -10298.9883,  ..., -46599.0195,\n",
      "            -426.9990, -13374.0117],\n",
      "         [ -6141.9902,  10070.9971,   1661.0278,  ...,   -944.9883,\n",
      "          -45433.0078, -10019.9902]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[ -5120.0010,   2246.9990, -15993.0039,  ..., -19818.0020,\n",
      "           -9385.0020,  11924.0020],\n",
      "         [-20213.9883,  -7357.0000,  -1321.9756,  ...,   7460.9980,\n",
      "          -10925.9990, -10021.0117],\n",
      "         [  1515.9922,  12230.0234,  -1709.9961,  ..., -19689.9688,\n",
      "           13956.9873, -11909.9766],\n",
      "         ...,\n",
      "         [  5301.0234,  -4457.0151,   -574.0020,  ...,   4088.9775,\n",
      "           -2724.0195,   5865.0127],\n",
      "         [-10094.9863,  11009.0078, -10298.9883,  ..., -46599.0195,\n",
      "            -426.9990, -13374.0117],\n",
      "         [ -6141.9902,  10070.9971,   1661.0278,  ...,   -944.9883,\n",
      "          -45433.0078, -10019.9902]]]),) and output (tensor([[[ -6685.0010,  -1278.0010, -15553.0039,  ..., -19846.0020,\n",
      "          -10418.0020,  13376.0020],\n",
      "         [-21502.9883, -11218.0000,  -5690.9756,  ...,  17418.9980,\n",
      "          -13630.9990,  -8718.0117],\n",
      "         [ 10471.9922,   9108.0234,  -4798.9961,  ..., -31568.9688,\n",
      "            9424.9873,  -9316.9766],\n",
      "         ...,\n",
      "         [ 11465.0234,  -3669.0151,  -5702.0020,  ...,   2799.9775,\n",
      "           -1900.0195,  17470.0117],\n",
      "         [ -8729.9863,   4288.0078,  -8624.9883,  ..., -49942.0195,\n",
      "           -6386.9990, -10857.0117],\n",
      "         [ -5313.9902,  13398.9971,   1899.0278,  ...,  -1708.9883,\n",
      "          -43367.0078,  -2687.9902]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[ -6685.0010,  -1278.0010, -15553.0039,  ..., -19846.0020,\n",
      "          -10418.0020,  13376.0020],\n",
      "         [-21502.9883, -11218.0000,  -5690.9756,  ...,  17418.9980,\n",
      "          -13630.9990,  -8718.0117],\n",
      "         [ 10471.9922,   9108.0234,  -4798.9961,  ..., -31568.9688,\n",
      "            9424.9873,  -9316.9766],\n",
      "         ...,\n",
      "         [ 11465.0234,  -3669.0151,  -5702.0020,  ...,   2799.9775,\n",
      "           -1900.0195,  17470.0117],\n",
      "         [ -8729.9863,   4288.0078,  -8624.9883,  ..., -49942.0195,\n",
      "           -6386.9990, -10857.0117],\n",
      "         [ -5313.9902,  13398.9971,   1899.0278,  ...,  -1708.9883,\n",
      "          -43367.0078,  -2687.9902]]]),) and output (tensor([[[ -8422.0010,    339.9990, -15289.0039,  ..., -20884.0020,\n",
      "           -6908.0020,  11014.0020],\n",
      "         [-26931.9883,  -5803.0000,  -7039.9756,  ...,  13508.9980,\n",
      "          -21742.0000, -14080.0117],\n",
      "         [  9304.9922,   7125.0234,  -2995.9961,  ..., -44344.9688,\n",
      "           21772.9883, -14166.9766],\n",
      "         ...,\n",
      "         [ 18019.0234,   -674.0151,  -8538.0020,  ...,  -3629.0225,\n",
      "           -1632.0195,  14556.0117],\n",
      "         [ -4688.9863,   8378.0078,  -8445.9883,  ..., -56700.0195,\n",
      "          -16878.0000, -13804.0117],\n",
      "         [ -7121.9902,  13515.9971,  -1498.9722,  ...,    896.0117,\n",
      "          -49245.0078,  -4221.9902]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[ -8422.0010,    339.9990, -15289.0039,  ..., -20884.0020,\n",
      "           -6908.0020,  11014.0020],\n",
      "         [-26931.9883,  -5803.0000,  -7039.9756,  ...,  13508.9980,\n",
      "          -21742.0000, -14080.0117],\n",
      "         [  9304.9922,   7125.0234,  -2995.9961,  ..., -44344.9688,\n",
      "           21772.9883, -14166.9766],\n",
      "         ...,\n",
      "         [ 18019.0234,   -674.0151,  -8538.0020,  ...,  -3629.0225,\n",
      "           -1632.0195,  14556.0117],\n",
      "         [ -4688.9863,   8378.0078,  -8445.9883,  ..., -56700.0195,\n",
      "          -16878.0000, -13804.0117],\n",
      "         [ -7121.9902,  13515.9971,  -1498.9722,  ...,    896.0117,\n",
      "          -49245.0078,  -4221.9902]]]),) and output (tensor([[[-10989.0010,   7034.9990, -13964.0039,  ..., -23968.0020,\n",
      "           -3749.0020,  11730.0020],\n",
      "         [-25490.9883,  -2964.0000,  -4563.9756,  ...,  11039.9980,\n",
      "          -23437.0000, -20555.0117],\n",
      "         [ 13312.9922,   1721.0234,  -2766.9961,  ..., -49461.9688,\n",
      "           21877.9883, -13779.9766],\n",
      "         ...,\n",
      "         [ 23350.0234,  -1657.0151,  -2120.0020,  ...,  -3489.0225,\n",
      "          -13139.0195,   8181.0117],\n",
      "         [  -990.9863,   8682.0078,  -7245.9883,  ..., -57539.0195,\n",
      "          -20538.0000, -10444.0117],\n",
      "         [ -9163.9902,  14522.9971,  -2572.9722,  ...,  -1625.9883,\n",
      "          -56111.0078,  -7873.9902]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[-10989.0010,   7034.9990, -13964.0039,  ..., -23968.0020,\n",
      "           -3749.0020,  11730.0020],\n",
      "         [-25490.9883,  -2964.0000,  -4563.9756,  ...,  11039.9980,\n",
      "          -23437.0000, -20555.0117],\n",
      "         [ 13312.9922,   1721.0234,  -2766.9961,  ..., -49461.9688,\n",
      "           21877.9883, -13779.9766],\n",
      "         ...,\n",
      "         [ 23350.0234,  -1657.0151,  -2120.0020,  ...,  -3489.0225,\n",
      "          -13139.0195,   8181.0117],\n",
      "         [  -990.9863,   8682.0078,  -7245.9883,  ..., -57539.0195,\n",
      "          -20538.0000, -10444.0117],\n",
      "         [ -9163.9902,  14522.9971,  -2572.9722,  ...,  -1625.9883,\n",
      "          -56111.0078,  -7873.9902]]]),) and output (tensor([[[ -7159.0010,   6621.9990, -10967.0039,  ..., -17477.0020,\n",
      "           -2510.0020,  11888.0020],\n",
      "         [-30486.9883,  -6344.0000,  -8255.9756,  ...,  10838.9980,\n",
      "          -17116.0000, -20241.0117],\n",
      "         [ 16284.9922,  -2621.9766,  -1981.9961,  ..., -62608.9688,\n",
      "           21147.9883, -16594.9766],\n",
      "         ...,\n",
      "         [ 31949.0234,  -9533.0156,   4135.9980,  ...,  -1407.0225,\n",
      "           -6925.0195,  11084.0117],\n",
      "         [   868.0137,   8924.0078,  -5733.9883,  ..., -59377.0195,\n",
      "          -17842.0000, -15761.0117],\n",
      "         [ -8680.9902,   9812.9971,  -2044.9722,  ...,   2135.0117,\n",
      "          -65979.0078,  -5860.9902]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[ -7159.0010,   6621.9990, -10967.0039,  ..., -17477.0020,\n",
      "           -2510.0020,  11888.0020],\n",
      "         [-30486.9883,  -6344.0000,  -8255.9756,  ...,  10838.9980,\n",
      "          -17116.0000, -20241.0117],\n",
      "         [ 16284.9922,  -2621.9766,  -1981.9961,  ..., -62608.9688,\n",
      "           21147.9883, -16594.9766],\n",
      "         ...,\n",
      "         [ 31949.0234,  -9533.0156,   4135.9980,  ...,  -1407.0225,\n",
      "           -6925.0195,  11084.0117],\n",
      "         [   868.0137,   8924.0078,  -5733.9883,  ..., -59377.0195,\n",
      "          -17842.0000, -15761.0117],\n",
      "         [ -8680.9902,   9812.9971,  -2044.9722,  ...,   2135.0117,\n",
      "          -65979.0078,  -5860.9902]]]),) and output (tensor([[[ -4822.0010,   3124.9990,  -3885.0039,  ..., -19007.0020,\n",
      "            -551.0020,  16285.0020],\n",
      "         [-29454.9883,  -5218.0000,  -4989.9756,  ...,  15437.9980,\n",
      "          -15448.0000, -14306.0117],\n",
      "         [ 12292.9922,  -2688.9766,  -4261.9961,  ..., -66856.9688,\n",
      "           17356.9883, -23758.9766],\n",
      "         ...,\n",
      "         [ 32897.0234, -13634.0156,   2850.9980,  ...,  -2390.0225,\n",
      "           -4727.0195,   6525.0117],\n",
      "         [ -1795.9863,   7016.0078,  -9412.9883,  ..., -67516.0156,\n",
      "          -17275.0000, -11342.0117],\n",
      "         [ -3534.9902,   9186.9971,  -4267.9722,  ...,   7367.0117,\n",
      "          -72828.0078,  -4216.9902]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[ -4822.0010,   3124.9990,  -3885.0039,  ..., -19007.0020,\n",
      "            -551.0020,  16285.0020],\n",
      "         [-29454.9883,  -5218.0000,  -4989.9756,  ...,  15437.9980,\n",
      "          -15448.0000, -14306.0117],\n",
      "         [ 12292.9922,  -2688.9766,  -4261.9961,  ..., -66856.9688,\n",
      "           17356.9883, -23758.9766],\n",
      "         ...,\n",
      "         [ 32897.0234, -13634.0156,   2850.9980,  ...,  -2390.0225,\n",
      "           -4727.0195,   6525.0117],\n",
      "         [ -1795.9863,   7016.0078,  -9412.9883,  ..., -67516.0156,\n",
      "          -17275.0000, -11342.0117],\n",
      "         [ -3534.9902,   9186.9971,  -4267.9722,  ...,   7367.0117,\n",
      "          -72828.0078,  -4216.9902]]]),) and output (tensor([[[ -7235.0010,   6583.9990,  -4212.0039,  ..., -19104.0020,\n",
      "            3087.9980,  17084.0020],\n",
      "         [-24012.9883,  -3556.0000,  -9963.9756,  ...,  17893.9980,\n",
      "          -17265.0000, -17084.0117],\n",
      "         [ 14071.9922,  -4034.9766,  -6673.9961,  ..., -66022.9688,\n",
      "           15013.9883, -23119.9766],\n",
      "         ...,\n",
      "         [ 34265.0234, -14087.0156,   3481.9980,  ...,  -3294.0225,\n",
      "           -2346.0195,   6153.0117],\n",
      "         [ -2128.9863,   2682.0078, -13214.9883,  ..., -75108.0156,\n",
      "          -16025.0000, -14566.0117],\n",
      "         [  2467.0098,   8678.9971,  -7229.9722,  ...,   3675.0117,\n",
      "          -78105.0078, -11013.9902]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[ -7235.0010,   6583.9990,  -4212.0039,  ..., -19104.0020,\n",
      "            3087.9980,  17084.0020],\n",
      "         [-24012.9883,  -3556.0000,  -9963.9756,  ...,  17893.9980,\n",
      "          -17265.0000, -17084.0117],\n",
      "         [ 14071.9922,  -4034.9766,  -6673.9961,  ..., -66022.9688,\n",
      "           15013.9883, -23119.9766],\n",
      "         ...,\n",
      "         [ 34265.0234, -14087.0156,   3481.9980,  ...,  -3294.0225,\n",
      "           -2346.0195,   6153.0117],\n",
      "         [ -2128.9863,   2682.0078, -13214.9883,  ..., -75108.0156,\n",
      "          -16025.0000, -14566.0117],\n",
      "         [  2467.0098,   8678.9971,  -7229.9722,  ...,   3675.0117,\n",
      "          -78105.0078, -11013.9902]]]),) and output (tensor([[[ -5586.0010,   7645.9990,   -505.0039,  ..., -15346.0020,\n",
      "            -623.0020,  12986.0020],\n",
      "         [-12073.9883,  -6895.0000,  -6670.9756,  ...,  22198.9980,\n",
      "          -10329.0000, -23765.0117],\n",
      "         [ 15287.9922,   3616.0234,  -1108.9961,  ..., -77653.9688,\n",
      "           16411.9883, -22528.9766],\n",
      "         ...,\n",
      "         [ 44416.0234, -15976.0156,   1267.9980,  ...,  -4700.0225,\n",
      "           -3194.0195,    819.0117],\n",
      "         [  2127.0137,   4559.0078, -15578.9883,  ..., -79725.0156,\n",
      "          -13958.0000, -11686.0117],\n",
      "         [  1068.0098,   7592.9971,  -6928.9727,  ...,  10630.0117,\n",
      "          -79816.0078,  -7639.9902]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[ -5586.0010,   7645.9990,   -505.0039,  ..., -15346.0020,\n",
      "            -623.0020,  12986.0020],\n",
      "         [-12073.9883,  -6895.0000,  -6670.9756,  ...,  22198.9980,\n",
      "          -10329.0000, -23765.0117],\n",
      "         [ 15287.9922,   3616.0234,  -1108.9961,  ..., -77653.9688,\n",
      "           16411.9883, -22528.9766],\n",
      "         ...,\n",
      "         [ 44416.0234, -15976.0156,   1267.9980,  ...,  -4700.0225,\n",
      "           -3194.0195,    819.0117],\n",
      "         [  2127.0137,   4559.0078, -15578.9883,  ..., -79725.0156,\n",
      "          -13958.0000, -11686.0117],\n",
      "         [  1068.0098,   7592.9971,  -6928.9727,  ...,  10630.0117,\n",
      "          -79816.0078,  -7639.9902]]]),) and output (tensor([[[ -8916.0010,   2362.9990,  -4833.0039,  ..., -12075.0020,\n",
      "           -1124.0020,  12438.0020],\n",
      "         [-10814.9883,  -2583.0000,  -5462.9756,  ...,  23245.9980,\n",
      "          -10858.0000, -17843.0117],\n",
      "         [ 15413.9922,  -2438.9766,   8211.0039,  ..., -83642.9688,\n",
      "           15269.9883, -25189.9766],\n",
      "         ...,\n",
      "         [ 52521.0234, -15811.0156,   7151.9980,  ...,  -6935.0225,\n",
      "           -1471.0195,  12130.0117],\n",
      "         [ -4363.9863,   1925.0078, -22230.9883,  ..., -80041.0156,\n",
      "          -15368.0000,  -9273.0117],\n",
      "         [  1262.0098,   9785.9971,  -3188.9727,  ...,   1365.0117,\n",
      "          -89614.0078,  -8366.9902]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[ -8916.0010,   2362.9990,  -4833.0039,  ..., -12075.0020,\n",
      "           -1124.0020,  12438.0020],\n",
      "         [-10814.9883,  -2583.0000,  -5462.9756,  ...,  23245.9980,\n",
      "          -10858.0000, -17843.0117],\n",
      "         [ 15413.9922,  -2438.9766,   8211.0039,  ..., -83642.9688,\n",
      "           15269.9883, -25189.9766],\n",
      "         ...,\n",
      "         [ 52521.0234, -15811.0156,   7151.9980,  ...,  -6935.0225,\n",
      "           -1471.0195,  12130.0117],\n",
      "         [ -4363.9863,   1925.0078, -22230.9883,  ..., -80041.0156,\n",
      "          -15368.0000,  -9273.0117],\n",
      "         [  1262.0098,   9785.9971,  -3188.9727,  ...,   1365.0117,\n",
      "          -89614.0078,  -8366.9902]]]),) and output (tensor([[[-20230.0000,   7877.9990,    161.9961,  ..., -10250.0020,\n",
      "             676.9980,   7095.0020],\n",
      "         [-14023.9883,  -2175.0000,  -6519.9756,  ...,  23073.9980,\n",
      "          -10622.0000, -19953.0117],\n",
      "         [ 18991.9922,   1819.0234,  10826.0039,  ..., -83423.9688,\n",
      "            6869.9883, -28621.9766],\n",
      "         ...,\n",
      "         [ 57353.0234, -15109.0156,   6453.9980,  ...,  -1867.0225,\n",
      "             671.9805,  20364.0117],\n",
      "         [ -5746.9863,    647.0078, -28631.9883,  ..., -82319.0156,\n",
      "          -15290.0000, -12023.0117],\n",
      "         [  3573.0098,  13390.9971,  -1968.9727,  ...,  -5433.9883,\n",
      "          -86509.0078, -11864.9902]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[-20230.0000,   7877.9990,    161.9961,  ..., -10250.0020,\n",
      "             676.9980,   7095.0020],\n",
      "         [-14023.9883,  -2175.0000,  -6519.9756,  ...,  23073.9980,\n",
      "          -10622.0000, -19953.0117],\n",
      "         [ 18991.9922,   1819.0234,  10826.0039,  ..., -83423.9688,\n",
      "            6869.9883, -28621.9766],\n",
      "         ...,\n",
      "         [ 57353.0234, -15109.0156,   6453.9980,  ...,  -1867.0225,\n",
      "             671.9805,  20364.0117],\n",
      "         [ -5746.9863,    647.0078, -28631.9883,  ..., -82319.0156,\n",
      "          -15290.0000, -12023.0117],\n",
      "         [  3573.0098,  13390.9971,  -1968.9727,  ...,  -5433.9883,\n",
      "          -86509.0078, -11864.9902]]]),) and output (tensor([[[-25670.0000,  13293.9990,   9099.9961,  ...,  -6547.0020,\n",
      "           -5658.0020,   4192.0020],\n",
      "         [-11518.9883,   7973.0000,  -1785.9756,  ...,  27967.9980,\n",
      "          -13543.0000, -15506.0117],\n",
      "         [ 21656.9922,   -924.9766,   8240.0039,  ..., -87598.9688,\n",
      "           -1705.0117, -19459.9766],\n",
      "         ...,\n",
      "         [ 60709.0234, -12299.0156,   5419.9980,  ...,  -4448.0225,\n",
      "            2238.9805,  27089.0117],\n",
      "         [ -5947.9863,   5061.0078, -29391.9883,  ..., -88622.0156,\n",
      "          -17120.0000, -12504.0117],\n",
      "         [  1665.0098,  11453.9971,   1865.0273,  ...,  -5893.9883,\n",
      "          -94973.0078, -11733.9902]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[-25670.0000,  13293.9990,   9099.9961,  ...,  -6547.0020,\n",
      "           -5658.0020,   4192.0020],\n",
      "         [-11518.9883,   7973.0000,  -1785.9756,  ...,  27967.9980,\n",
      "          -13543.0000, -15506.0117],\n",
      "         [ 21656.9922,   -924.9766,   8240.0039,  ..., -87598.9688,\n",
      "           -1705.0117, -19459.9766],\n",
      "         ...,\n",
      "         [ 60709.0234, -12299.0156,   5419.9980,  ...,  -4448.0225,\n",
      "            2238.9805,  27089.0117],\n",
      "         [ -5947.9863,   5061.0078, -29391.9883,  ..., -88622.0156,\n",
      "          -17120.0000, -12504.0117],\n",
      "         [  1665.0098,  11453.9971,   1865.0273,  ...,  -5893.9883,\n",
      "          -94973.0078, -11733.9902]]]),) and output (tensor([[[-25652.0000,  13180.9990,  15728.9961,  ...,  -2831.0020,\n",
      "             158.9980,   2204.0020],\n",
      "         [ -7453.9883,  -1649.0000,  -8985.9756,  ...,  29517.9980,\n",
      "          -11824.0000, -17393.0117],\n",
      "         [ 14345.9922,   4131.0234,   8523.0039,  ..., -88530.9688,\n",
      "            1449.9883, -17542.9766],\n",
      "         ...,\n",
      "         [ 62053.0234,  -8790.0156,   5104.9980,  ...,  -4147.0225,\n",
      "           -5485.0195,  25949.0117],\n",
      "         [ -8627.9863,   7077.0078, -31817.9883,  ..., -93775.0156,\n",
      "          -10209.0000, -14840.0117],\n",
      "         [  7703.0098,  13651.9971,    888.0273,  ...,  -4956.9883,\n",
      "          -96264.0078, -11448.9902]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[-25652.0000,  13180.9990,  15728.9961,  ...,  -2831.0020,\n",
      "             158.9980,   2204.0020],\n",
      "         [ -7453.9883,  -1649.0000,  -8985.9756,  ...,  29517.9980,\n",
      "          -11824.0000, -17393.0117],\n",
      "         [ 14345.9922,   4131.0234,   8523.0039,  ..., -88530.9688,\n",
      "            1449.9883, -17542.9766],\n",
      "         ...,\n",
      "         [ 62053.0234,  -8790.0156,   5104.9980,  ...,  -4147.0225,\n",
      "           -5485.0195,  25949.0117],\n",
      "         [ -8627.9863,   7077.0078, -31817.9883,  ..., -93775.0156,\n",
      "          -10209.0000, -14840.0117],\n",
      "         [  7703.0098,  13651.9971,    888.0273,  ...,  -4956.9883,\n",
      "          -96264.0078, -11448.9902]]]),) and output (tensor([[[ -14791.0000,   15299.9990,   16129.9961,  ...,    3187.9980,\n",
      "            -2552.0020,    5626.0020],\n",
      "         [ -11844.9883,  -11814.0000,  -12356.9756,  ...,   27207.9980,\n",
      "            -4301.0000,  -20988.0117],\n",
      "         [  14480.9922,     494.0234,    1073.0039,  ...,  -92323.9688,\n",
      "            -5827.0117,  -20735.9766],\n",
      "         ...,\n",
      "         [  65936.0234,  -13473.0156,    6786.9980,  ...,   -8428.0225,\n",
      "            -3398.0195,   26800.0117],\n",
      "         [  -9074.9863,   10981.0078,  -32414.9883,  ..., -101219.0156,\n",
      "            -3774.0000,  -15309.0117],\n",
      "         [   2653.0098,   10171.9971,   -8583.9727,  ...,   -9865.9883,\n",
      "           -99931.0078,   -7179.9902]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[ -14791.0000,   15299.9990,   16129.9961,  ...,    3187.9980,\n",
      "            -2552.0020,    5626.0020],\n",
      "         [ -11844.9883,  -11814.0000,  -12356.9756,  ...,   27207.9980,\n",
      "            -4301.0000,  -20988.0117],\n",
      "         [  14480.9922,     494.0234,    1073.0039,  ...,  -92323.9688,\n",
      "            -5827.0117,  -20735.9766],\n",
      "         ...,\n",
      "         [  65936.0234,  -13473.0156,    6786.9980,  ...,   -8428.0225,\n",
      "            -3398.0195,   26800.0117],\n",
      "         [  -9074.9863,   10981.0078,  -32414.9883,  ..., -101219.0156,\n",
      "            -3774.0000,  -15309.0117],\n",
      "         [   2653.0098,   10171.9971,   -8583.9727,  ...,   -9865.9883,\n",
      "           -99931.0078,   -7179.9902]]]),) and output (tensor([[[ -13284.0000,   26639.0000,    7467.9961,  ...,    4113.9980,\n",
      "            -6608.0020,    5336.0020],\n",
      "         [ -21734.9883,   -9553.0000,  -13787.9756,  ...,   28007.9980,\n",
      "             1742.0000,  -24201.0117],\n",
      "         [  16646.9922,    6301.0234,   -4448.9961,  ...,  -88193.9688,\n",
      "           -15263.0117,  -19205.9766],\n",
      "         ...,\n",
      "         [  64037.0234,  -16160.0156,     211.9980,  ...,   -8619.0225,\n",
      "            -2324.0195,   22787.0117],\n",
      "         [ -10472.9863,    8647.0078,  -35245.9883,  ...,  -99582.0156,\n",
      "            -4409.0000,  -14209.0117],\n",
      "         [  -4484.9902,    9021.9971,  -10127.9727,  ...,  -13551.9883,\n",
      "          -105732.0078,   -1000.9902]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[ -13284.0000,   26639.0000,    7467.9961,  ...,    4113.9980,\n",
      "            -6608.0020,    5336.0020],\n",
      "         [ -21734.9883,   -9553.0000,  -13787.9756,  ...,   28007.9980,\n",
      "             1742.0000,  -24201.0117],\n",
      "         [  16646.9922,    6301.0234,   -4448.9961,  ...,  -88193.9688,\n",
      "           -15263.0117,  -19205.9766],\n",
      "         ...,\n",
      "         [  64037.0234,  -16160.0156,     211.9980,  ...,   -8619.0225,\n",
      "            -2324.0195,   22787.0117],\n",
      "         [ -10472.9863,    8647.0078,  -35245.9883,  ...,  -99582.0156,\n",
      "            -4409.0000,  -14209.0117],\n",
      "         [  -4484.9902,    9021.9971,  -10127.9727,  ...,  -13551.9883,\n",
      "          -105732.0078,   -1000.9902]]]),) and output (tensor([[[ -21532.0000,   32796.0000,   16281.9961,  ...,   18208.9980,\n",
      "            -4282.0020,    5756.0020],\n",
      "         [ -20714.9883,  -12527.0000,  -10678.9756,  ...,   25906.9980,\n",
      "              634.0000,  -25072.0117],\n",
      "         [  17091.9922,    8254.0234,   -5648.9961,  ...,  -77438.9688,\n",
      "           -15429.0117,   -8284.9766],\n",
      "         ...,\n",
      "         [  61840.0234,  -22941.0156,     649.9980,  ...,   -9327.0225,\n",
      "            -8335.0195,   26589.0117],\n",
      "         [ -12571.9863,   13296.0078,  -36566.9883,  ..., -101356.0156,\n",
      "            -1667.0000,  -19238.0117],\n",
      "         [   3367.0098,    5125.9971,   -7304.9727,  ...,   -5552.9883,\n",
      "           -97334.0078,   -4375.9902]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[ -21532.0000,   32796.0000,   16281.9961,  ...,   18208.9980,\n",
      "            -4282.0020,    5756.0020],\n",
      "         [ -20714.9883,  -12527.0000,  -10678.9756,  ...,   25906.9980,\n",
      "              634.0000,  -25072.0117],\n",
      "         [  17091.9922,    8254.0234,   -5648.9961,  ...,  -77438.9688,\n",
      "           -15429.0117,   -8284.9766],\n",
      "         ...,\n",
      "         [  61840.0234,  -22941.0156,     649.9980,  ...,   -9327.0225,\n",
      "            -8335.0195,   26589.0117],\n",
      "         [ -12571.9863,   13296.0078,  -36566.9883,  ..., -101356.0156,\n",
      "            -1667.0000,  -19238.0117],\n",
      "         [   3367.0098,    5125.9971,   -7304.9727,  ...,   -5552.9883,\n",
      "           -97334.0078,   -4375.9902]]]),) and output (tensor([[[ -24082.0000,   33387.0000,   15374.9961,  ...,   18359.9980,\n",
      "            -7500.0020,   11519.0020],\n",
      "         [ -14620.9883,   -8845.0000,    3928.0244,  ...,   26756.9980,\n",
      "              151.0000,  -27101.0117],\n",
      "         [  19550.9922,    6704.0234,   -5775.9961,  ...,  -75064.9688,\n",
      "           -23272.0117,   -6779.9766],\n",
      "         ...,\n",
      "         [  66098.0234,  -16872.0156,     795.9980,  ...,  -14605.0225,\n",
      "            -8632.0195,   28473.0117],\n",
      "         [ -11965.9863,    3025.0078,  -42731.9883,  ..., -100002.0156,\n",
      "              690.0000,  -13990.0117],\n",
      "         [   6842.0098,    3115.9971,  -16100.9727,  ...,   -2712.9883,\n",
      "           -87932.0078,   -1091.9902]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[ -24082.0000,   33387.0000,   15374.9961,  ...,   18359.9980,\n",
      "            -7500.0020,   11519.0020],\n",
      "         [ -14620.9883,   -8845.0000,    3928.0244,  ...,   26756.9980,\n",
      "              151.0000,  -27101.0117],\n",
      "         [  19550.9922,    6704.0234,   -5775.9961,  ...,  -75064.9688,\n",
      "           -23272.0117,   -6779.9766],\n",
      "         ...,\n",
      "         [  66098.0234,  -16872.0156,     795.9980,  ...,  -14605.0225,\n",
      "            -8632.0195,   28473.0117],\n",
      "         [ -11965.9863,    3025.0078,  -42731.9883,  ..., -100002.0156,\n",
      "              690.0000,  -13990.0117],\n",
      "         [   6842.0098,    3115.9971,  -16100.9727,  ...,   -2712.9883,\n",
      "           -87932.0078,   -1091.9902]]]),) and output (tensor([[[-14869.0000,  32284.0000,   3742.9961,  ...,  12458.9980,\n",
      "           -5452.0020,   7590.0020],\n",
      "         [-15067.9883, -12039.0000,   2045.0244,  ...,  22389.9980,\n",
      "           -5174.0000, -20159.0117],\n",
      "         [ 18069.9922,   6830.0234,  -5752.9961,  ..., -71610.9688,\n",
      "          -15064.0117, -13657.9766],\n",
      "         ...,\n",
      "         [ 63119.0234, -20845.0156,   3296.9980,  ..., -13576.0225,\n",
      "           -9074.0195,  34079.0117],\n",
      "         [-10535.9863,   1590.0078, -40009.9883,  ..., -84282.0156,\n",
      "           -1715.0000,  -3247.0117],\n",
      "         [ 14888.0098,   4488.9971, -14736.9727,  ...,  -5939.9883,\n",
      "          -93274.0078,   3156.0098]]]),)\n",
      "[Debug] Layer names being passed: ['model.layers.0', 'model.layers.1', 'model.layers.2', 'model.layers.3', 'model.layers.4', 'model.layers.5', 'model.layers.6', 'model.layers.7', 'model.layers.8', 'model.layers.9', 'model.layers.10', 'model.layers.11', 'model.layers.12', 'model.layers.13', 'model.layers.14', 'model.layers.15', 'model.layers.16', 'model.layers.17', 'model.layers.18', 'model.layers.19', 'model.layers.20', 'model.layers.21', 'model.layers.22', 'model.layers.23', 'model.layers.24', 'model.layers.25', 'model.layers.26', 'model.layers.27', 'model.embed_tokens']\n",
      "[Debug] Trying to access layer: model.layers.0\n",
      "[Debug] Successfully found layer: model.layers.0\n",
      "[Debug] Trying to access layer: model.layers.1\n",
      "[Debug] Successfully found layer: model.layers.1\n",
      "[Debug] Trying to access layer: model.layers.2\n",
      "[Debug] Successfully found layer: model.layers.2\n",
      "[Debug] Trying to access layer: model.layers.3\n",
      "[Debug] Successfully found layer: model.layers.3\n",
      "[Debug] Trying to access layer: model.layers.4\n",
      "[Debug] Successfully found layer: model.layers.4\n",
      "[Debug] Trying to access layer: model.layers.5\n",
      "[Debug] Successfully found layer: model.layers.5\n",
      "[Debug] Trying to access layer: model.layers.6\n",
      "[Debug] Successfully found layer: model.layers.6\n",
      "[Debug] Trying to access layer: model.layers.7\n",
      "[Debug] Successfully found layer: model.layers.7\n",
      "[Debug] Trying to access layer: model.layers.8\n",
      "[Debug] Successfully found layer: model.layers.8\n",
      "[Debug] Trying to access layer: model.layers.9\n",
      "[Debug] Successfully found layer: model.layers.9\n",
      "[Debug] Trying to access layer: model.layers.10\n",
      "[Debug] Successfully found layer: model.layers.10\n",
      "[Debug] Trying to access layer: model.layers.11\n",
      "[Debug] Successfully found layer: model.layers.11\n",
      "[Debug] Trying to access layer: model.layers.12\n",
      "[Debug] Successfully found layer: model.layers.12\n",
      "[Debug] Trying to access layer: model.layers.13\n",
      "[Debug] Successfully found layer: model.layers.13\n",
      "[Debug] Trying to access layer: model.layers.14\n",
      "[Debug] Successfully found layer: model.layers.14\n",
      "[Debug] Trying to access layer: model.layers.15\n",
      "[Debug] Successfully found layer: model.layers.15\n",
      "[Debug] Trying to access layer: model.layers.16\n",
      "[Debug] Successfully found layer: model.layers.16\n",
      "[Debug] Trying to access layer: model.layers.17\n",
      "[Debug] Successfully found layer: model.layers.17\n",
      "[Debug] Trying to access layer: model.layers.18\n",
      "[Debug] Successfully found layer: model.layers.18\n",
      "[Debug] Trying to access layer: model.layers.19\n",
      "[Debug] Successfully found layer: model.layers.19\n",
      "[Debug] Trying to access layer: model.layers.20\n",
      "[Debug] Successfully found layer: model.layers.20\n",
      "[Debug] Trying to access layer: model.layers.21\n",
      "[Debug] Successfully found layer: model.layers.21\n",
      "[Debug] Trying to access layer: model.layers.22\n",
      "[Debug] Successfully found layer: model.layers.22\n",
      "[Debug] Trying to access layer: model.layers.23\n",
      "[Debug] Successfully found layer: model.layers.23\n",
      "[Debug] Trying to access layer: model.layers.24\n",
      "[Debug] Successfully found layer: model.layers.24\n",
      "[Debug] Trying to access layer: model.layers.25\n",
      "[Debug] Successfully found layer: model.layers.25\n",
      "[Debug] Trying to access layer: model.layers.26\n",
      "[Debug] Successfully found layer: model.layers.26\n",
      "[Debug] Trying to access layer: model.layers.27\n",
      "[Debug] Successfully found layer: model.layers.27\n",
      "[Debug] Trying to access layer: model.embed_tokens\n",
      "[Debug] Successfully found layer: model.embed_tokens\n",
      "[Hook] Layer Embedding(128256, 3072, padding_idx=128004) received input (tensor([[128000,  33731,    323,    279,  34823,  90849,  10771,    311,  12074,\n",
      "            520,    279,  23978,    304,  58814,    323,  81801,     11,    279,\n",
      "           3446,  44853,    810,   1109,    220,     20,     11,    931,   1667,\n",
      "           4227,     11,   3196,    389,    264,  24716,   5438,  62488,   3446,\n",
      "           1376,    902,    374,   1457,  21771,    555,  29036,   9761,   1705,\n",
      "            439,   7520,     52,    220,  16884,    578,  16576,  10699,    800,\n",
      "           1286,  64922,    596,  58248,   8032,     22,     60]]),) and output tensor([[[-0.0010, -0.0007, -0.0046,  ..., -0.0014, -0.0020,  0.0020],\n",
      "         [-0.0056, -0.0038,  0.0167,  ...,  0.0139, -0.0273,  0.0048],\n",
      "         [ 0.0141,  0.0072, -0.0022,  ..., -0.0171,  0.0049,  0.0076],\n",
      "         ...,\n",
      "         [ 0.0145, -0.0280, -0.0145,  ...,  0.0136, -0.0054, -0.0027],\n",
      "         [ 0.0079,  0.0114,  0.0211,  ...,  0.0079, -0.0139, -0.0129],\n",
      "         [-0.0045, -0.0086,  0.0126,  ...,  0.0082,  0.0127,  0.0007]]])\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[-0.0010, -0.0007, -0.0046,  ..., -0.0014, -0.0020,  0.0020],\n",
      "         [-0.0056, -0.0038,  0.0167,  ...,  0.0139, -0.0273,  0.0048],\n",
      "         [ 0.0141,  0.0072, -0.0022,  ..., -0.0171,  0.0049,  0.0076],\n",
      "         ...,\n",
      "         [ 0.0145, -0.0280, -0.0145,  ...,  0.0136, -0.0054, -0.0027],\n",
      "         [ 0.0079,  0.0114,  0.0211,  ...,  0.0079, -0.0139, -0.0129],\n",
      "         [-0.0045, -0.0086,  0.0126,  ...,  0.0082,  0.0127,  0.0007]]]),) and output (tensor([[[  442.9990,   239.9993,   302.9954,  ...,  -154.0014,\n",
      "            -15.0020,   185.0020],\n",
      "         [  449.9944,  -109.0038,   971.0167,  ..., -1481.9861,\n",
      "            276.9727,   179.0048],\n",
      "         [   54.0141,    90.0072,  -166.0022,  ...,  -114.0171,\n",
      "            -30.9951,    47.0076],\n",
      "         ...,\n",
      "         [  115.0145,   184.9720,  -152.0145,  ...,  -417.9864,\n",
      "            179.9946,   -34.0027],\n",
      "         [  -56.9921,  -154.9886,  -513.9789,  ...,   -38.9921,\n",
      "             56.9861,  -157.0129],\n",
      "         [  249.9955,    57.9914,   250.0126,  ...,  -518.9918,\n",
      "            343.0127,  -324.9993]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[  442.9990,   239.9993,   302.9954,  ...,  -154.0014,\n",
      "            -15.0020,   185.0020],\n",
      "         [  449.9944,  -109.0038,   971.0167,  ..., -1481.9861,\n",
      "            276.9727,   179.0048],\n",
      "         [   54.0141,    90.0072,  -166.0022,  ...,  -114.0171,\n",
      "            -30.9951,    47.0076],\n",
      "         ...,\n",
      "         [  115.0145,   184.9720,  -152.0145,  ...,  -417.9864,\n",
      "            179.9946,   -34.0027],\n",
      "         [  -56.9921,  -154.9886,  -513.9789,  ...,   -38.9921,\n",
      "             56.9861,  -157.0129],\n",
      "         [  249.9955,    57.9914,   250.0126,  ...,  -518.9918,\n",
      "            343.0127,  -324.9993]]]),) and output (tensor([[[  439.9990,  -144.0007,  -410.0046,  ...,  -765.0015,\n",
      "           1193.9979, -1027.9980],\n",
      "         [  548.9944,   310.9962,   555.0167,  ..., -2931.9861,\n",
      "            541.9727,    29.0048],\n",
      "         [ -109.9859,    47.0072,  -144.0022,  ...,  -591.0171,\n",
      "            528.0049,   250.0076],\n",
      "         ...,\n",
      "         [  251.0145,   197.9720,  -670.0145,  ...,   394.0136,\n",
      "           -855.0054,   133.9973],\n",
      "         [ -630.9921, -1018.9886,  -820.9789,  ...,  -299.9921,\n",
      "           -739.0139, -1115.0129],\n",
      "         [ -837.0045,  -367.0086,  -798.9874,  ...,  -369.9918,\n",
      "           1241.0127,  -703.9993]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[  439.9990,  -144.0007,  -410.0046,  ...,  -765.0015,\n",
      "           1193.9979, -1027.9980],\n",
      "         [  548.9944,   310.9962,   555.0167,  ..., -2931.9861,\n",
      "            541.9727,    29.0048],\n",
      "         [ -109.9859,    47.0072,  -144.0022,  ...,  -591.0171,\n",
      "            528.0049,   250.0076],\n",
      "         ...,\n",
      "         [  251.0145,   197.9720,  -670.0145,  ...,   394.0136,\n",
      "           -855.0054,   133.9973],\n",
      "         [ -630.9921, -1018.9886,  -820.9789,  ...,  -299.9921,\n",
      "           -739.0139, -1115.0129],\n",
      "         [ -837.0045,  -367.0086,  -798.9874,  ...,  -369.9918,\n",
      "           1241.0127,  -703.9993]]]),) and output (tensor([[[  551.9990,   993.9993, -1790.0046,  ...,   445.9985,\n",
      "          -1386.0021, -1245.9980],\n",
      "         [ 2753.9944, -1672.0038,   428.0167,  ..., -1707.9861,\n",
      "            -54.0273,  -162.9952],\n",
      "         [   89.0142,  -521.9928,  1898.9978,  ..., -2247.0171,\n",
      "          -3378.9951,   783.0076],\n",
      "         ...,\n",
      "         [ 4129.0146,  -917.0280, -5220.0146,  ...,  1441.0137,\n",
      "           1001.9946, -2825.0027],\n",
      "         [ 1092.0079, -2579.9885, -3814.9790,  ...,  2164.0078,\n",
      "          -1429.0139, -3191.0129],\n",
      "         [ -401.0045,   601.9914, -2282.9873,  ..., -2836.9917,\n",
      "           1988.0127,  1323.0007]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[  551.9990,   993.9993, -1790.0046,  ...,   445.9985,\n",
      "          -1386.0021, -1245.9980],\n",
      "         [ 2753.9944, -1672.0038,   428.0167,  ..., -1707.9861,\n",
      "            -54.0273,  -162.9952],\n",
      "         [   89.0142,  -521.9928,  1898.9978,  ..., -2247.0171,\n",
      "          -3378.9951,   783.0076],\n",
      "         ...,\n",
      "         [ 4129.0146,  -917.0280, -5220.0146,  ...,  1441.0137,\n",
      "           1001.9946, -2825.0027],\n",
      "         [ 1092.0079, -2579.9885, -3814.9790,  ...,  2164.0078,\n",
      "          -1429.0139, -3191.0129],\n",
      "         [ -401.0045,   601.9914, -2282.9873,  ..., -2836.9917,\n",
      "           1988.0127,  1323.0007]]]),) and output (tensor([[[ 2131.9990,  -737.0007, -1071.0046,  ..., -2781.0015,\n",
      "           -885.0021,    86.0020],\n",
      "         [ 4087.9944,  1207.9961,  1181.0167,  ..., -3062.9861,\n",
      "           3029.9727,  2967.0049],\n",
      "         [  -81.9858, -2725.9927,  1562.9978,  ..., -4800.0171,\n",
      "          -3352.9951,  1428.0076],\n",
      "         ...,\n",
      "         [ 5928.0146, -2057.0278, -5360.0146,  ...,  3072.0137,\n",
      "           3744.9946, -4345.0029],\n",
      "         [ 4337.0078,  3073.0115, -2020.9790,  ...,  3139.0078,\n",
      "          -1920.0139, -5318.0127],\n",
      "         [ -332.0045, -2637.0085, -1561.9873,  ..., -4024.9917,\n",
      "           7277.0127, -4119.9990]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[ 2131.9990,  -737.0007, -1071.0046,  ..., -2781.0015,\n",
      "           -885.0021,    86.0020],\n",
      "         [ 4087.9944,  1207.9961,  1181.0167,  ..., -3062.9861,\n",
      "           3029.9727,  2967.0049],\n",
      "         [  -81.9858, -2725.9927,  1562.9978,  ..., -4800.0171,\n",
      "          -3352.9951,  1428.0076],\n",
      "         ...,\n",
      "         [ 5928.0146, -2057.0278, -5360.0146,  ...,  3072.0137,\n",
      "           3744.9946, -4345.0029],\n",
      "         [ 4337.0078,  3073.0115, -2020.9790,  ...,  3139.0078,\n",
      "          -1920.0139, -5318.0127],\n",
      "         [ -332.0045, -2637.0085, -1561.9873,  ..., -4024.9917,\n",
      "           7277.0127, -4119.9990]]]),) and output (tensor([[[ 3420.9990,   760.9993, -5376.0049,  ..., -4193.0015,\n",
      "           2860.9980, -4080.9980],\n",
      "         [ 3820.9944,  1502.9961,  5584.0166,  ...,   486.0139,\n",
      "            434.9727,  6336.0049],\n",
      "         [-5059.9858, -4264.9927,  9712.9980,  ..., -6350.0171,\n",
      "          -6893.9951, -6094.9922],\n",
      "         ...,\n",
      "         [  387.0146, -2595.0278, -2555.0146,  ...,  4761.0137,\n",
      "            664.9946, -7779.0029],\n",
      "         [ 7584.0078,  5824.0117,  -241.9790,  ...,  6808.0078,\n",
      "           2558.9861, -7241.0127],\n",
      "         [ -526.0045, -4199.0088, -1290.9873,  ..., -7367.9917,\n",
      "           6769.0127, -5465.9990]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[ 3420.9990,   760.9993, -5376.0049,  ..., -4193.0015,\n",
      "           2860.9980, -4080.9980],\n",
      "         [ 3820.9944,  1502.9961,  5584.0166,  ...,   486.0139,\n",
      "            434.9727,  6336.0049],\n",
      "         [-5059.9858, -4264.9927,  9712.9980,  ..., -6350.0171,\n",
      "          -6893.9951, -6094.9922],\n",
      "         ...,\n",
      "         [  387.0146, -2595.0278, -2555.0146,  ...,  4761.0137,\n",
      "            664.9946, -7779.0029],\n",
      "         [ 7584.0078,  5824.0117,  -241.9790,  ...,  6808.0078,\n",
      "           2558.9861, -7241.0127],\n",
      "         [ -526.0045, -4199.0088, -1290.9873,  ..., -7367.9917,\n",
      "           6769.0127, -5465.9990]]]),) and output (tensor([[[ 2.7430e+03, -1.5120e+03, -1.0493e+04,  ..., -9.9450e+03,\n",
      "          -2.0170e+03, -1.8030e+03],\n",
      "         [ 8.3400e+03, -2.0039e+00,  5.4630e+03,  ...,  6.7701e+02,\n",
      "           1.2820e+03,  1.2838e+04],\n",
      "         [-4.4190e+03, -3.6950e+03,  8.3560e+03,  ..., -8.7300e+03,\n",
      "          -7.0920e+03, -7.3150e+03],\n",
      "         ...,\n",
      "         [ 3.5080e+03, -7.1250e+03, -1.8015e+01,  ...,  1.4980e+03,\n",
      "          -4.3710e+03, -6.3010e+03],\n",
      "         [ 5.4480e+03,  8.2340e+03,  2.8000e+03,  ...,  3.3840e+03,\n",
      "           5.2470e+03, -5.1090e+03],\n",
      "         [ 5.2180e+03,  2.6270e+03, -1.6860e+03,  ..., -8.9110e+03,\n",
      "           5.3960e+03, -2.7390e+03]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[ 2.7430e+03, -1.5120e+03, -1.0493e+04,  ..., -9.9450e+03,\n",
      "          -2.0170e+03, -1.8030e+03],\n",
      "         [ 8.3400e+03, -2.0039e+00,  5.4630e+03,  ...,  6.7701e+02,\n",
      "           1.2820e+03,  1.2838e+04],\n",
      "         [-4.4190e+03, -3.6950e+03,  8.3560e+03,  ..., -8.7300e+03,\n",
      "          -7.0920e+03, -7.3150e+03],\n",
      "         ...,\n",
      "         [ 3.5080e+03, -7.1250e+03, -1.8015e+01,  ...,  1.4980e+03,\n",
      "          -4.3710e+03, -6.3010e+03],\n",
      "         [ 5.4480e+03,  8.2340e+03,  2.8000e+03,  ...,  3.3840e+03,\n",
      "           5.2470e+03, -5.1090e+03],\n",
      "         [ 5.2180e+03,  2.6270e+03, -1.6860e+03,  ..., -8.9110e+03,\n",
      "           5.3960e+03, -2.7390e+03]]]),) and output (tensor([[[  3032.9990,  -3420.0007, -10291.0049,  ...,  -6511.0020,\n",
      "           -1953.0020,   2001.0020],\n",
      "         [  8320.9941,   -983.0039,   3881.0166,  ...,    533.0139,\n",
      "            1518.9727,  10744.0049],\n",
      "         [ -2036.9858,   7382.0073,   9378.9980,  ..., -10874.0176,\n",
      "          -10372.9951, -13931.9922],\n",
      "         ...,\n",
      "         [  3951.0146, -12238.0273,    207.9854,  ...,   5241.0137,\n",
      "           -9062.0059,    -30.0029],\n",
      "         [ 13906.0078,  11927.0117,   1441.0210,  ...,   2231.0078,\n",
      "            4232.9863,  -6678.0127],\n",
      "         [  4652.9956,    436.9912,    137.0127,  ..., -10349.9922,\n",
      "            1126.0127,  -3728.9990]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[  3032.9990,  -3420.0007, -10291.0049,  ...,  -6511.0020,\n",
      "           -1953.0020,   2001.0020],\n",
      "         [  8320.9941,   -983.0039,   3881.0166,  ...,    533.0139,\n",
      "            1518.9727,  10744.0049],\n",
      "         [ -2036.9858,   7382.0073,   9378.9980,  ..., -10874.0176,\n",
      "          -10372.9951, -13931.9922],\n",
      "         ...,\n",
      "         [  3951.0146, -12238.0273,    207.9854,  ...,   5241.0137,\n",
      "           -9062.0059,    -30.0029],\n",
      "         [ 13906.0078,  11927.0117,   1441.0210,  ...,   2231.0078,\n",
      "            4232.9863,  -6678.0127],\n",
      "         [  4652.9956,    436.9912,    137.0127,  ..., -10349.9922,\n",
      "            1126.0127,  -3728.9990]]]),) and output (tensor([[[  3132.9990,  -1970.0007, -11544.0049,  ...,  -5780.0020,\n",
      "           -2486.0020,  -1921.9980],\n",
      "         [ 12409.9941,  -2963.0039,   1742.0166,  ...,   2759.0139,\n",
      "             903.9727,  11909.0049],\n",
      "         [  2560.0142,   7024.0073,   5754.9980,  ..., -12557.0176,\n",
      "          -15632.9951, -11581.9922],\n",
      "         ...,\n",
      "         [  3026.0146,  -5740.0273,  -3173.0146,  ...,   6509.0137,\n",
      "          -12719.0059,   1895.9971],\n",
      "         [ 13600.0078,   9101.0117,   -562.9790,  ...,   3495.0078,\n",
      "            4329.9863,  -1223.0127],\n",
      "         [ 13879.9961,   6838.9912,  -4925.9873,  ..., -11186.9922,\n",
      "           -3702.9873,   1265.0010]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[  3132.9990,  -1970.0007, -11544.0049,  ...,  -5780.0020,\n",
      "           -2486.0020,  -1921.9980],\n",
      "         [ 12409.9941,  -2963.0039,   1742.0166,  ...,   2759.0139,\n",
      "             903.9727,  11909.0049],\n",
      "         [  2560.0142,   7024.0073,   5754.9980,  ..., -12557.0176,\n",
      "          -15632.9951, -11581.9922],\n",
      "         ...,\n",
      "         [  3026.0146,  -5740.0273,  -3173.0146,  ...,   6509.0137,\n",
      "          -12719.0059,   1895.9971],\n",
      "         [ 13600.0078,   9101.0117,   -562.9790,  ...,   3495.0078,\n",
      "            4329.9863,  -1223.0127],\n",
      "         [ 13879.9961,   6838.9912,  -4925.9873,  ..., -11186.9922,\n",
      "           -3702.9873,   1265.0010]]]),) and output (tensor([[[  1046.9990,   2403.9993, -14486.0049,  ..., -10922.0020,\n",
      "           -1419.0020,  -2983.9980],\n",
      "         [  6948.9941,   1890.9961,  -1621.9834,  ...,   6775.0137,\n",
      "           -1108.0273,  14427.0049],\n",
      "         [ -4482.9858,   6466.0073,   7817.9980,  ..., -12373.0176,\n",
      "          -15782.9951,  -8118.9922],\n",
      "         ...,\n",
      "         [  4745.0146,  -5120.0273,  -3580.0146,  ...,   7020.0137,\n",
      "           -8025.0059,   1125.9971],\n",
      "         [  9813.0078,   8299.0117,  -7000.9790,  ...,   -328.9922,\n",
      "           11922.9863,  -3969.0127],\n",
      "         [ 21568.9961,  10182.9912,  -4118.9873,  ..., -18606.9922,\n",
      "           -4536.9873,   2084.0010]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[  1046.9990,   2403.9993, -14486.0049,  ..., -10922.0020,\n",
      "           -1419.0020,  -2983.9980],\n",
      "         [  6948.9941,   1890.9961,  -1621.9834,  ...,   6775.0137,\n",
      "           -1108.0273,  14427.0049],\n",
      "         [ -4482.9858,   6466.0073,   7817.9980,  ..., -12373.0176,\n",
      "          -15782.9951,  -8118.9922],\n",
      "         ...,\n",
      "         [  4745.0146,  -5120.0273,  -3580.0146,  ...,   7020.0137,\n",
      "           -8025.0059,   1125.9971],\n",
      "         [  9813.0078,   8299.0117,  -7000.9790,  ...,   -328.9922,\n",
      "           11922.9863,  -3969.0127],\n",
      "         [ 21568.9961,  10182.9912,  -4118.9873,  ..., -18606.9922,\n",
      "           -4536.9873,   2084.0010]]]),) and output (tensor([[[ -3767.0010,   5307.9990, -14221.0049,  ..., -15035.0020,\n",
      "           -3630.0020,   4168.0020],\n",
      "         [  1694.9941,  -6606.0039,  -3039.9834,  ...,   3912.0137,\n",
      "           -2889.0273,  10027.0049],\n",
      "         [-11794.9863,   8254.0078,   4957.9980,  ..., -15582.0176,\n",
      "          -19801.9961, -13751.9922],\n",
      "         ...,\n",
      "         [  2024.0146,  -7663.0273,  -6695.0146,  ...,   2612.0137,\n",
      "          -12360.0059,  -7747.0029],\n",
      "         [  8493.0078,   2017.0117,  -5891.9790,  ...,  -3925.9922,\n",
      "           16576.9863,   -275.0127],\n",
      "         [ 21981.9961,   3490.9912,  -9304.9873,  ..., -22113.9922,\n",
      "            1255.0127,   6365.0010]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[ -3767.0010,   5307.9990, -14221.0049,  ..., -15035.0020,\n",
      "           -3630.0020,   4168.0020],\n",
      "         [  1694.9941,  -6606.0039,  -3039.9834,  ...,   3912.0137,\n",
      "           -2889.0273,  10027.0049],\n",
      "         [-11794.9863,   8254.0078,   4957.9980,  ..., -15582.0176,\n",
      "          -19801.9961, -13751.9922],\n",
      "         ...,\n",
      "         [  2024.0146,  -7663.0273,  -6695.0146,  ...,   2612.0137,\n",
      "          -12360.0059,  -7747.0029],\n",
      "         [  8493.0078,   2017.0117,  -5891.9790,  ...,  -3925.9922,\n",
      "           16576.9863,   -275.0127],\n",
      "         [ 21981.9961,   3490.9912,  -9304.9873,  ..., -22113.9922,\n",
      "            1255.0127,   6365.0010]]]),) and output (tensor([[[ -1326.0010,   1223.9990, -16643.0039,  ..., -13240.0020,\n",
      "           -5677.0020,   9639.0020],\n",
      "         [  4129.9941,  -8818.0039,  -1451.9834,  ...,    949.0137,\n",
      "           -3353.0273,  15646.0049],\n",
      "         [-14186.9863,   1451.0078,   7757.9980,  ..., -17801.0176,\n",
      "          -19675.9961, -12765.9922],\n",
      "         ...,\n",
      "         [  4961.0146,  -8853.0273, -10787.0146,  ...,   4195.0137,\n",
      "          -13358.0059,  -6329.0029],\n",
      "         [ 10494.0078,  -1188.9883,  -8385.9785,  ...,  -4150.9922,\n",
      "           21222.9863,  -3151.0127],\n",
      "         [ 21382.9961,   5961.9912,  -6717.9873,  ..., -24310.9922,\n",
      "            2479.0127,   5129.0010]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[ -1326.0010,   1223.9990, -16643.0039,  ..., -13240.0020,\n",
      "           -5677.0020,   9639.0020],\n",
      "         [  4129.9941,  -8818.0039,  -1451.9834,  ...,    949.0137,\n",
      "           -3353.0273,  15646.0049],\n",
      "         [-14186.9863,   1451.0078,   7757.9980,  ..., -17801.0176,\n",
      "          -19675.9961, -12765.9922],\n",
      "         ...,\n",
      "         [  4961.0146,  -8853.0273, -10787.0146,  ...,   4195.0137,\n",
      "          -13358.0059,  -6329.0029],\n",
      "         [ 10494.0078,  -1188.9883,  -8385.9785,  ...,  -4150.9922,\n",
      "           21222.9863,  -3151.0127],\n",
      "         [ 21382.9961,   5961.9912,  -6717.9873,  ..., -24310.9922,\n",
      "            2479.0127,   5129.0010]]]),) and output (tensor([[[ -5120.0010,   2246.9990, -15993.0039,  ..., -19818.0020,\n",
      "           -9385.0020,  11924.0020],\n",
      "         [  3574.9941,  -9539.0039,   1250.0166,  ...,   1397.0137,\n",
      "           -6763.0273,  18077.0039],\n",
      "         [-17212.9863,   4166.0078,  10735.9980,  ..., -21182.0176,\n",
      "          -18582.9961,  -6457.9922],\n",
      "         ...,\n",
      "         [  6632.0146,  -5881.0273,  -9730.0146,  ...,   7795.0137,\n",
      "          -19111.0059,  -6188.0029],\n",
      "         [  8533.0078,  -6135.9883,  -7176.9785,  ...,  -9663.9922,\n",
      "           23638.9863,  -3314.0127],\n",
      "         [ 20996.9961,   1018.9912,  -7047.9873,  ..., -29026.9922,\n",
      "            2218.0127,   8615.0010]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[ -5120.0010,   2246.9990, -15993.0039,  ..., -19818.0020,\n",
      "           -9385.0020,  11924.0020],\n",
      "         [  3574.9941,  -9539.0039,   1250.0166,  ...,   1397.0137,\n",
      "           -6763.0273,  18077.0039],\n",
      "         [-17212.9863,   4166.0078,  10735.9980,  ..., -21182.0176,\n",
      "          -18582.9961,  -6457.9922],\n",
      "         ...,\n",
      "         [  6632.0146,  -5881.0273,  -9730.0146,  ...,   7795.0137,\n",
      "          -19111.0059,  -6188.0029],\n",
      "         [  8533.0078,  -6135.9883,  -7176.9785,  ...,  -9663.9922,\n",
      "           23638.9863,  -3314.0127],\n",
      "         [ 20996.9961,   1018.9912,  -7047.9873,  ..., -29026.9922,\n",
      "            2218.0127,   8615.0010]]]),) and output (tensor([[[ -6685.0010,  -1278.0010, -15553.0039,  ..., -19846.0020,\n",
      "          -10418.0020,  13376.0020],\n",
      "         [  1071.9941, -11492.0039,   4457.0166,  ...,    315.0137,\n",
      "          -13150.0273,  16900.0039],\n",
      "         [-19735.9863,   6246.0078,   8483.9980,  ..., -23143.0176,\n",
      "          -25445.9961,  -2991.9922],\n",
      "         ...,\n",
      "         [  4605.0146,  -7251.0273, -13292.0146,  ...,   5948.0137,\n",
      "          -21500.0059,  -1208.0029],\n",
      "         [ 14298.0078, -13830.9883,  -9182.9785,  ...,  -8180.9922,\n",
      "           21698.9863,  -5766.0127],\n",
      "         [ 22383.9961,   3722.9912,  -4614.9873,  ..., -28984.9922,\n",
      "            4903.0127,   4824.0010]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[ -6685.0010,  -1278.0010, -15553.0039,  ..., -19846.0020,\n",
      "          -10418.0020,  13376.0020],\n",
      "         [  1071.9941, -11492.0039,   4457.0166,  ...,    315.0137,\n",
      "          -13150.0273,  16900.0039],\n",
      "         [-19735.9863,   6246.0078,   8483.9980,  ..., -23143.0176,\n",
      "          -25445.9961,  -2991.9922],\n",
      "         ...,\n",
      "         [  4605.0146,  -7251.0273, -13292.0146,  ...,   5948.0137,\n",
      "          -21500.0059,  -1208.0029],\n",
      "         [ 14298.0078, -13830.9883,  -9182.9785,  ...,  -8180.9922,\n",
      "           21698.9863,  -5766.0127],\n",
      "         [ 22383.9961,   3722.9912,  -4614.9873,  ..., -28984.9922,\n",
      "            4903.0127,   4824.0010]]]),) and output (tensor([[[-8.4220e+03,  3.4000e+02, -1.5289e+04,  ..., -2.0884e+04,\n",
      "          -6.9080e+03,  1.1014e+04],\n",
      "         [-1.9330e+03, -9.8010e+03,  2.9017e+01,  ..., -2.0300e+03,\n",
      "          -1.4126e+04,  1.7281e+04],\n",
      "         [-2.3211e+04, -3.3620e+03,  8.3140e+03,  ..., -2.4103e+04,\n",
      "          -3.5838e+04, -6.7760e+03],\n",
      "         ...,\n",
      "         [ 3.6530e+03, -8.0560e+03, -1.6143e+04,  ...,  4.3801e+02,\n",
      "          -1.6071e+04, -3.6550e+03],\n",
      "         [ 1.2519e+04, -1.2282e+04, -8.3560e+03,  ..., -1.0278e+04,\n",
      "           2.5012e+04, -2.7040e+03],\n",
      "         [ 2.2993e+04,  3.6270e+03, -6.7390e+03,  ..., -2.8919e+04,\n",
      "          -5.5270e+03,  8.2390e+03]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[-8.4220e+03,  3.4000e+02, -1.5289e+04,  ..., -2.0884e+04,\n",
      "          -6.9080e+03,  1.1014e+04],\n",
      "         [-1.9330e+03, -9.8010e+03,  2.9017e+01,  ..., -2.0300e+03,\n",
      "          -1.4126e+04,  1.7281e+04],\n",
      "         [-2.3211e+04, -3.3620e+03,  8.3140e+03,  ..., -2.4103e+04,\n",
      "          -3.5838e+04, -6.7760e+03],\n",
      "         ...,\n",
      "         [ 3.6530e+03, -8.0560e+03, -1.6143e+04,  ...,  4.3801e+02,\n",
      "          -1.6071e+04, -3.6550e+03],\n",
      "         [ 1.2519e+04, -1.2282e+04, -8.3560e+03,  ..., -1.0278e+04,\n",
      "           2.5012e+04, -2.7040e+03],\n",
      "         [ 2.2993e+04,  3.6270e+03, -6.7390e+03,  ..., -2.8919e+04,\n",
      "          -5.5270e+03,  8.2390e+03]]]),) and output (tensor([[[-10989.0010,   7034.9990, -13964.0039,  ..., -23968.0020,\n",
      "           -3749.0020,  11730.0020],\n",
      "         [ -5585.0059,  -8325.0039,  -3813.9834,  ...,  -1694.9863,\n",
      "          -15263.0273,  17500.0039],\n",
      "         [-20970.9863,  -1486.9922,   8568.9980,  ..., -21602.0176,\n",
      "          -37684.9961,  -4678.9922],\n",
      "         ...,\n",
      "         [  4910.0146,  -6248.0273, -22519.0156,  ...,    165.0137,\n",
      "          -16944.0059,    543.9971],\n",
      "         [ 12018.0078, -10853.9883, -11938.9785,  ..., -11390.9922,\n",
      "           19460.9863,  -6352.0127],\n",
      "         [ 27377.9961,   8085.9912, -11455.9873,  ..., -24693.9922,\n",
      "           -7323.9873,  12245.0010]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[-10989.0010,   7034.9990, -13964.0039,  ..., -23968.0020,\n",
      "           -3749.0020,  11730.0020],\n",
      "         [ -5585.0059,  -8325.0039,  -3813.9834,  ...,  -1694.9863,\n",
      "          -15263.0273,  17500.0039],\n",
      "         [-20970.9863,  -1486.9922,   8568.9980,  ..., -21602.0176,\n",
      "          -37684.9961,  -4678.9922],\n",
      "         ...,\n",
      "         [  4910.0146,  -6248.0273, -22519.0156,  ...,    165.0137,\n",
      "          -16944.0059,    543.9971],\n",
      "         [ 12018.0078, -10853.9883, -11938.9785,  ..., -11390.9922,\n",
      "           19460.9863,  -6352.0127],\n",
      "         [ 27377.9961,   8085.9912, -11455.9873,  ..., -24693.9922,\n",
      "           -7323.9873,  12245.0010]]]),) and output (tensor([[[ -7159.0010,   6621.9990, -10967.0039,  ..., -17477.0020,\n",
      "           -2510.0020,  11888.0020],\n",
      "         [ -7573.0059, -10783.0039,  -8931.9834,  ...,   -541.9863,\n",
      "          -13406.0273,  11849.0039],\n",
      "         [-23425.9863,   1955.0078,   8766.9980,  ..., -20477.0176,\n",
      "          -44088.9961,  -7541.9922],\n",
      "         ...,\n",
      "         [   596.0146,  -5141.0273, -31658.0156,  ...,   2362.0137,\n",
      "          -17095.0059,   5249.9971],\n",
      "         [ 11765.0078, -11571.9883, -10206.9785,  ..., -18667.9922,\n",
      "           21415.9863,  -9213.0127],\n",
      "         [ 19155.9961,  -1804.0088, -15723.9873,  ..., -26485.9922,\n",
      "          -11312.9873,   3780.0010]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[ -7159.0010,   6621.9990, -10967.0039,  ..., -17477.0020,\n",
      "           -2510.0020,  11888.0020],\n",
      "         [ -7573.0059, -10783.0039,  -8931.9834,  ...,   -541.9863,\n",
      "          -13406.0273,  11849.0039],\n",
      "         [-23425.9863,   1955.0078,   8766.9980,  ..., -20477.0176,\n",
      "          -44088.9961,  -7541.9922],\n",
      "         ...,\n",
      "         [   596.0146,  -5141.0273, -31658.0156,  ...,   2362.0137,\n",
      "          -17095.0059,   5249.9971],\n",
      "         [ 11765.0078, -11571.9883, -10206.9785,  ..., -18667.9922,\n",
      "           21415.9863,  -9213.0127],\n",
      "         [ 19155.9961,  -1804.0088, -15723.9873,  ..., -26485.9922,\n",
      "          -11312.9873,   3780.0010]]]),) and output (tensor([[[ -4822.0010,   3124.9990,  -3885.0039,  ..., -19007.0020,\n",
      "            -551.0020,  16285.0020],\n",
      "         [ -6715.0059, -15579.0039, -11940.9834,  ...,    -55.9863,\n",
      "          -10753.0273,  15825.0039],\n",
      "         [-28051.9863,   7485.0078,   5408.9980,  ..., -21973.0176,\n",
      "          -47847.9961,  -9716.9922],\n",
      "         ...,\n",
      "         [  1663.0146,  -2825.0273, -25625.0156,  ...,   2864.0137,\n",
      "           -9862.0059,  10244.9971],\n",
      "         [ 11913.0078, -16141.9883, -11485.9785,  ..., -11505.9922,\n",
      "           18418.9863, -11460.0127],\n",
      "         [ 24802.9961,   2390.9912, -12074.9873,  ..., -32148.9922,\n",
      "           -9065.9873,   2175.0010]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[ -4822.0010,   3124.9990,  -3885.0039,  ..., -19007.0020,\n",
      "            -551.0020,  16285.0020],\n",
      "         [ -6715.0059, -15579.0039, -11940.9834,  ...,    -55.9863,\n",
      "          -10753.0273,  15825.0039],\n",
      "         [-28051.9863,   7485.0078,   5408.9980,  ..., -21973.0176,\n",
      "          -47847.9961,  -9716.9922],\n",
      "         ...,\n",
      "         [  1663.0146,  -2825.0273, -25625.0156,  ...,   2864.0137,\n",
      "           -9862.0059,  10244.9971],\n",
      "         [ 11913.0078, -16141.9883, -11485.9785,  ..., -11505.9922,\n",
      "           18418.9863, -11460.0127],\n",
      "         [ 24802.9961,   2390.9912, -12074.9873,  ..., -32148.9922,\n",
      "           -9065.9873,   2175.0010]]]),) and output (tensor([[[ -7235.0010,   6583.9990,  -4212.0039,  ..., -19104.0020,\n",
      "            3087.9980,  17084.0020],\n",
      "         [ -7348.0059, -27522.0039,  -4979.9834,  ...,   1119.0137,\n",
      "          -12682.0273,   8694.0039],\n",
      "         [-26168.9863,  12877.0078,   7449.9980,  ..., -18997.0176,\n",
      "          -50529.9961,  -9102.9922],\n",
      "         ...,\n",
      "         [  6589.0146,   1586.9727, -26538.0156,  ...,  -5300.9863,\n",
      "           -9352.0059,  12208.9971],\n",
      "         [ 12667.0078, -16925.9883, -14948.9785,  ..., -19222.9922,\n",
      "           19609.9863, -10045.0127],\n",
      "         [ 26398.9961,   -157.0088, -18412.9883,  ..., -39880.9922,\n",
      "           -9475.9873,   1348.0010]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[ -7235.0010,   6583.9990,  -4212.0039,  ..., -19104.0020,\n",
      "            3087.9980,  17084.0020],\n",
      "         [ -7348.0059, -27522.0039,  -4979.9834,  ...,   1119.0137,\n",
      "          -12682.0273,   8694.0039],\n",
      "         [-26168.9863,  12877.0078,   7449.9980,  ..., -18997.0176,\n",
      "          -50529.9961,  -9102.9922],\n",
      "         ...,\n",
      "         [  6589.0146,   1586.9727, -26538.0156,  ...,  -5300.9863,\n",
      "           -9352.0059,  12208.9971],\n",
      "         [ 12667.0078, -16925.9883, -14948.9785,  ..., -19222.9922,\n",
      "           19609.9863, -10045.0127],\n",
      "         [ 26398.9961,   -157.0088, -18412.9883,  ..., -39880.9922,\n",
      "           -9475.9873,   1348.0010]]]),) and output (tensor([[[ -5586.0010,   7645.9990,   -505.0039,  ..., -15346.0020,\n",
      "            -623.0020,  12986.0020],\n",
      "         [ -5817.0059, -27785.0039,  -7031.9834,  ...,   5029.0137,\n",
      "          -13385.0273,   6362.0039],\n",
      "         [-30901.9863,   7405.0078,   8762.9980,  ..., -11408.0176,\n",
      "          -54729.9961,  -9112.9922],\n",
      "         ...,\n",
      "         [ 10692.0146,  11909.9727, -34773.0156,  ...,   1169.0137,\n",
      "          -13981.0059,   8472.9971],\n",
      "         [  8954.0078, -24252.9883, -16908.9785,  ..., -17042.9922,\n",
      "            7204.9863, -14951.0127],\n",
      "         [ 24497.9961,   9498.9912, -17257.9883,  ..., -51036.9922,\n",
      "           -8925.9873,   -620.9990]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[ -5586.0010,   7645.9990,   -505.0039,  ..., -15346.0020,\n",
      "            -623.0020,  12986.0020],\n",
      "         [ -5817.0059, -27785.0039,  -7031.9834,  ...,   5029.0137,\n",
      "          -13385.0273,   6362.0039],\n",
      "         [-30901.9863,   7405.0078,   8762.9980,  ..., -11408.0176,\n",
      "          -54729.9961,  -9112.9922],\n",
      "         ...,\n",
      "         [ 10692.0146,  11909.9727, -34773.0156,  ...,   1169.0137,\n",
      "          -13981.0059,   8472.9971],\n",
      "         [  8954.0078, -24252.9883, -16908.9785,  ..., -17042.9922,\n",
      "            7204.9863, -14951.0127],\n",
      "         [ 24497.9961,   9498.9912, -17257.9883,  ..., -51036.9922,\n",
      "           -8925.9873,   -620.9990]]]),) and output (tensor([[[ -8916.0010,   2362.9990,  -4833.0039,  ..., -12075.0020,\n",
      "           -1124.0020,  12438.0020],\n",
      "         [ -1092.0059, -24725.0039, -12086.9834,  ...,   4584.0137,\n",
      "          -15454.0273,   3992.0039],\n",
      "         [-37295.9844,   4583.0078,  11002.9980,  ...,  -6834.0176,\n",
      "          -54983.9961, -12372.9922],\n",
      "         ...,\n",
      "         [ 11945.0146,   8589.9727, -35937.0156,  ...,  -3173.9863,\n",
      "           -1840.0059,  14077.9971],\n",
      "         [   740.0078, -28815.9883, -18292.9785,  ..., -11839.9922,\n",
      "           11015.9863, -17742.0117],\n",
      "         [ 22159.9961,  11926.9912, -16632.9883,  ..., -55071.9922,\n",
      "          -16807.9883,  -4343.9990]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[ -8916.0010,   2362.9990,  -4833.0039,  ..., -12075.0020,\n",
      "           -1124.0020,  12438.0020],\n",
      "         [ -1092.0059, -24725.0039, -12086.9834,  ...,   4584.0137,\n",
      "          -15454.0273,   3992.0039],\n",
      "         [-37295.9844,   4583.0078,  11002.9980,  ...,  -6834.0176,\n",
      "          -54983.9961, -12372.9922],\n",
      "         ...,\n",
      "         [ 11945.0146,   8589.9727, -35937.0156,  ...,  -3173.9863,\n",
      "           -1840.0059,  14077.9971],\n",
      "         [   740.0078, -28815.9883, -18292.9785,  ..., -11839.9922,\n",
      "           11015.9863, -17742.0117],\n",
      "         [ 22159.9961,  11926.9912, -16632.9883,  ..., -55071.9922,\n",
      "          -16807.9883,  -4343.9990]]]),) and output (tensor([[[-20230.0000,   7877.9990,    161.9961,  ..., -10250.0020,\n",
      "             676.9980,   7095.0020],\n",
      "         [-11867.0059, -25134.0039, -12004.9834,  ...,   8960.0137,\n",
      "          -18222.0273,  -2076.9961],\n",
      "         [-43211.9844,   3321.0078,  11013.9980,  ...,  -4145.0176,\n",
      "          -53175.9961, -18837.9922],\n",
      "         ...,\n",
      "         [ 18614.0156,   6166.9727, -30270.0156,  ...,   5969.0137,\n",
      "           -3265.0059,  12773.9971],\n",
      "         [  -628.9922, -31149.9883, -16224.9785,  ..., -20378.9922,\n",
      "           10503.9863, -16925.0117],\n",
      "         [ 18900.9961,  21032.9922, -15273.9883,  ..., -65424.9922,\n",
      "           -8210.9883, -10457.9990]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[-20230.0000,   7877.9990,    161.9961,  ..., -10250.0020,\n",
      "             676.9980,   7095.0020],\n",
      "         [-11867.0059, -25134.0039, -12004.9834,  ...,   8960.0137,\n",
      "          -18222.0273,  -2076.9961],\n",
      "         [-43211.9844,   3321.0078,  11013.9980,  ...,  -4145.0176,\n",
      "          -53175.9961, -18837.9922],\n",
      "         ...,\n",
      "         [ 18614.0156,   6166.9727, -30270.0156,  ...,   5969.0137,\n",
      "           -3265.0059,  12773.9971],\n",
      "         [  -628.9922, -31149.9883, -16224.9785,  ..., -20378.9922,\n",
      "           10503.9863, -16925.0117],\n",
      "         [ 18900.9961,  21032.9922, -15273.9883,  ..., -65424.9922,\n",
      "           -8210.9883, -10457.9990]]]),) and output (tensor([[[-25670.0000,  13293.9990,   9099.9961,  ...,  -6547.0020,\n",
      "           -5658.0020,   4192.0020],\n",
      "         [ -2464.0059, -22378.0039,  -5376.9834,  ...,   8032.0137,\n",
      "          -26512.0273,   -495.9961],\n",
      "         [-49014.9844,   3179.0078,  13256.9980,  ...,  -5341.0176,\n",
      "          -56543.9961, -21281.9922],\n",
      "         ...,\n",
      "         [ 24845.0156,  12812.9727, -23908.0156,  ...,   5034.0137,\n",
      "           -1715.0059,   5808.9971],\n",
      "         [ -4724.9922, -41961.9883, -13047.9785,  ..., -25365.9922,\n",
      "           21302.9863, -16022.0117],\n",
      "         [ 15279.9961,  11291.9922, -13238.9883,  ..., -65768.9922,\n",
      "          -13019.9883, -15543.9990]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[-25670.0000,  13293.9990,   9099.9961,  ...,  -6547.0020,\n",
      "           -5658.0020,   4192.0020],\n",
      "         [ -2464.0059, -22378.0039,  -5376.9834,  ...,   8032.0137,\n",
      "          -26512.0273,   -495.9961],\n",
      "         [-49014.9844,   3179.0078,  13256.9980,  ...,  -5341.0176,\n",
      "          -56543.9961, -21281.9922],\n",
      "         ...,\n",
      "         [ 24845.0156,  12812.9727, -23908.0156,  ...,   5034.0137,\n",
      "           -1715.0059,   5808.9971],\n",
      "         [ -4724.9922, -41961.9883, -13047.9785,  ..., -25365.9922,\n",
      "           21302.9863, -16022.0117],\n",
      "         [ 15279.9961,  11291.9922, -13238.9883,  ..., -65768.9922,\n",
      "          -13019.9883, -15543.9990]]]),) and output (tensor([[[-2.5652e+04,  1.3181e+04,  1.5729e+04,  ..., -2.8310e+03,\n",
      "           1.5900e+02,  2.2040e+03],\n",
      "         [ 3.6340e+03, -2.9672e+04, -2.7590e+03,  ...,  2.3210e+03,\n",
      "          -3.0669e+04, -1.5560e+03],\n",
      "         [-4.4588e+04,  8.0120e+03,  6.9200e+03,  ..., -1.7578e-02,\n",
      "          -6.0791e+04, -2.0832e+04],\n",
      "         ...,\n",
      "         [ 2.6450e+04,  1.7526e+04, -2.5327e+04,  ...,  3.2680e+03,\n",
      "          -1.4860e+03, -9.8700e+02],\n",
      "         [-3.8390e+03, -3.0373e+04, -1.1996e+04,  ..., -3.3973e+04,\n",
      "           2.1818e+04, -1.5153e+04],\n",
      "         [ 3.0213e+04,  5.6990e+03, -1.0669e+04,  ..., -5.8385e+04,\n",
      "          -8.6780e+03, -1.9053e+04]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[-2.5652e+04,  1.3181e+04,  1.5729e+04,  ..., -2.8310e+03,\n",
      "           1.5900e+02,  2.2040e+03],\n",
      "         [ 3.6340e+03, -2.9672e+04, -2.7590e+03,  ...,  2.3210e+03,\n",
      "          -3.0669e+04, -1.5560e+03],\n",
      "         [-4.4588e+04,  8.0120e+03,  6.9200e+03,  ..., -1.7578e-02,\n",
      "          -6.0791e+04, -2.0832e+04],\n",
      "         ...,\n",
      "         [ 2.6450e+04,  1.7526e+04, -2.5327e+04,  ...,  3.2680e+03,\n",
      "          -1.4860e+03, -9.8700e+02],\n",
      "         [-3.8390e+03, -3.0373e+04, -1.1996e+04,  ..., -3.3973e+04,\n",
      "           2.1818e+04, -1.5153e+04],\n",
      "         [ 3.0213e+04,  5.6990e+03, -1.0669e+04,  ..., -5.8385e+04,\n",
      "          -8.6780e+03, -1.9053e+04]]]),) and output (tensor([[[-14791.0000,  15299.9990,  16129.9961,  ...,   3187.9980,\n",
      "           -2552.0020,   5626.0020],\n",
      "         [  3507.9941, -32274.0039,   2193.0166,  ...,  -1059.9863,\n",
      "          -33461.0273,  -4446.9961],\n",
      "         [-40715.9844,   5946.0078,  -6965.0020,  ...,   2129.9824,\n",
      "          -62398.9961, -27527.9922],\n",
      "         ...,\n",
      "         [ 12431.0156,  10027.9727, -24741.0156,  ...,  -3219.9863,\n",
      "            -292.0059,   5203.9971],\n",
      "         [ -2714.9922, -29307.9883, -15573.9785,  ..., -34278.9922,\n",
      "           26578.9863, -15494.0117],\n",
      "         [ 32149.9961,   7654.9922, -13096.9883,  ..., -57348.9922,\n",
      "           -7483.9883, -19594.0000]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[-14791.0000,  15299.9990,  16129.9961,  ...,   3187.9980,\n",
      "           -2552.0020,   5626.0020],\n",
      "         [  3507.9941, -32274.0039,   2193.0166,  ...,  -1059.9863,\n",
      "          -33461.0273,  -4446.9961],\n",
      "         [-40715.9844,   5946.0078,  -6965.0020,  ...,   2129.9824,\n",
      "          -62398.9961, -27527.9922],\n",
      "         ...,\n",
      "         [ 12431.0156,  10027.9727, -24741.0156,  ...,  -3219.9863,\n",
      "            -292.0059,   5203.9971],\n",
      "         [ -2714.9922, -29307.9883, -15573.9785,  ..., -34278.9922,\n",
      "           26578.9863, -15494.0117],\n",
      "         [ 32149.9961,   7654.9922, -13096.9883,  ..., -57348.9922,\n",
      "           -7483.9883, -19594.0000]]]),) and output (tensor([[[-13284.0000,  26639.0000,   7467.9961,  ...,   4113.9980,\n",
      "           -6608.0020,   5336.0020],\n",
      "         [  3338.9941, -26444.0039,  -2472.9834,  ...,   5251.0137,\n",
      "          -32285.0273,  -3049.9961],\n",
      "         [-37334.9844,  11885.0078,  -1006.0020,  ...,   7425.9824,\n",
      "          -61271.9961, -22606.9922],\n",
      "         ...,\n",
      "         [  7479.0156,  10460.9727, -27175.0156,  ...,  -1088.9863,\n",
      "            7213.9941,   1128.9971],\n",
      "         [ -9953.9922, -35993.9883, -18488.9785,  ..., -29791.9922,\n",
      "           31030.9863, -10534.0117],\n",
      "         [ 25656.9961,  10443.9922, -11052.9883,  ..., -65722.9922,\n",
      "           -1656.9883, -17003.0000]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[-13284.0000,  26639.0000,   7467.9961,  ...,   4113.9980,\n",
      "           -6608.0020,   5336.0020],\n",
      "         [  3338.9941, -26444.0039,  -2472.9834,  ...,   5251.0137,\n",
      "          -32285.0273,  -3049.9961],\n",
      "         [-37334.9844,  11885.0078,  -1006.0020,  ...,   7425.9824,\n",
      "          -61271.9961, -22606.9922],\n",
      "         ...,\n",
      "         [  7479.0156,  10460.9727, -27175.0156,  ...,  -1088.9863,\n",
      "            7213.9941,   1128.9971],\n",
      "         [ -9953.9922, -35993.9883, -18488.9785,  ..., -29791.9922,\n",
      "           31030.9863, -10534.0117],\n",
      "         [ 25656.9961,  10443.9922, -11052.9883,  ..., -65722.9922,\n",
      "           -1656.9883, -17003.0000]]]),) and output (tensor([[[-21532.0000,  32796.0000,  16281.9961,  ...,  18208.9980,\n",
      "           -4282.0020,   5756.0020],\n",
      "         [  9326.9941, -29230.0039,  -8927.9834,  ...,   4327.0137,\n",
      "          -25868.0273,  10131.0039],\n",
      "         [-39003.9844,   8460.0078,  -2798.0020,  ...,   3376.9824,\n",
      "          -53948.9961, -21360.9922],\n",
      "         ...,\n",
      "         [  2454.0156,   7673.9727, -23607.0156,  ...,  -5916.9863,\n",
      "            9606.9941,   1923.9971],\n",
      "         [ -8950.9922, -38171.9883, -11900.9785,  ..., -31276.9922,\n",
      "           21442.9863,  -6178.0117],\n",
      "         [ 13062.9961,   2467.9922,  -3690.9883,  ..., -66941.9922,\n",
      "            4239.0117, -16669.0000]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[-21532.0000,  32796.0000,  16281.9961,  ...,  18208.9980,\n",
      "           -4282.0020,   5756.0020],\n",
      "         [  9326.9941, -29230.0039,  -8927.9834,  ...,   4327.0137,\n",
      "          -25868.0273,  10131.0039],\n",
      "         [-39003.9844,   8460.0078,  -2798.0020,  ...,   3376.9824,\n",
      "          -53948.9961, -21360.9922],\n",
      "         ...,\n",
      "         [  2454.0156,   7673.9727, -23607.0156,  ...,  -5916.9863,\n",
      "            9606.9941,   1923.9971],\n",
      "         [ -8950.9922, -38171.9883, -11900.9785,  ..., -31276.9922,\n",
      "           21442.9863,  -6178.0117],\n",
      "         [ 13062.9961,   2467.9922,  -3690.9883,  ..., -66941.9922,\n",
      "            4239.0117, -16669.0000]]]),) and output (tensor([[[-24082.0000,  33387.0000,  15374.9961,  ...,  18359.9980,\n",
      "           -7500.0020,  11519.0020],\n",
      "         [ 14125.9941, -28382.0039,  -8519.9834,  ...,   3765.0137,\n",
      "          -29188.0273,   5159.0039],\n",
      "         [-40678.9844,   9376.0078,    425.9980,  ...,   4210.9824,\n",
      "          -56667.9961, -20249.9922],\n",
      "         ...,\n",
      "         [  4808.0156,  10851.9727, -26194.0156,  ...,  -1556.9863,\n",
      "            8494.9941,   8621.9971],\n",
      "         [-13814.9922, -40086.9883, -14052.9785,  ..., -34800.9922,\n",
      "           24013.9863,  -6358.0117],\n",
      "         [ 27856.9961,  -1053.0078,  -2279.9883,  ..., -63129.9922,\n",
      "            4927.0117, -16494.0000]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[-24082.0000,  33387.0000,  15374.9961,  ...,  18359.9980,\n",
      "           -7500.0020,  11519.0020],\n",
      "         [ 14125.9941, -28382.0039,  -8519.9834,  ...,   3765.0137,\n",
      "          -29188.0273,   5159.0039],\n",
      "         [-40678.9844,   9376.0078,    425.9980,  ...,   4210.9824,\n",
      "          -56667.9961, -20249.9922],\n",
      "         ...,\n",
      "         [  4808.0156,  10851.9727, -26194.0156,  ...,  -1556.9863,\n",
      "            8494.9941,   8621.9971],\n",
      "         [-13814.9922, -40086.9883, -14052.9785,  ..., -34800.9922,\n",
      "           24013.9863,  -6358.0117],\n",
      "         [ 27856.9961,  -1053.0078,  -2279.9883,  ..., -63129.9922,\n",
      "            4927.0117, -16494.0000]]]),) and output (tensor([[[-14869.0000,  32284.0000,   3742.9961,  ...,  12458.9980,\n",
      "           -5452.0020,   7590.0020],\n",
      "         [ 12419.9941, -17346.0039,   1989.0166,  ...,   6397.0137,\n",
      "          -28220.0273,   4817.0039],\n",
      "         [-33134.9844,  18669.0078,   5729.9980,  ...,  -5883.0176,\n",
      "          -60457.9961, -19318.9922],\n",
      "         ...,\n",
      "         [ 12182.0156,  18767.9727, -24354.0156,  ...,   2118.0137,\n",
      "            4710.9941,   5964.9971],\n",
      "         [-25094.9922, -31831.9883, -10637.9785,  ..., -35245.9922,\n",
      "           35618.9844,   6512.9883],\n",
      "         [ 23757.9961,   9994.9922,    761.0117,  ..., -57851.9922,\n",
      "            5725.0117, -12053.0000]]]),)\n",
      "[Debug] Layer names being passed: ['model.layers.0', 'model.layers.1', 'model.layers.2', 'model.layers.3', 'model.layers.4', 'model.layers.5', 'model.layers.6', 'model.layers.7', 'model.layers.8', 'model.layers.9', 'model.layers.10', 'model.layers.11', 'model.layers.12', 'model.layers.13', 'model.layers.14', 'model.layers.15', 'model.layers.16', 'model.layers.17', 'model.layers.18', 'model.layers.19', 'model.layers.20', 'model.layers.21', 'model.layers.22', 'model.layers.23', 'model.layers.24', 'model.layers.25', 'model.layers.26', 'model.layers.27', 'model.embed_tokens']\n",
      "[Debug] Trying to access layer: model.layers.0\n",
      "[Debug] Successfully found layer: model.layers.0\n",
      "[Debug] Trying to access layer: model.layers.1\n",
      "[Debug] Successfully found layer: model.layers.1\n",
      "[Debug] Trying to access layer: model.layers.2\n",
      "[Debug] Successfully found layer: model.layers.2\n",
      "[Debug] Trying to access layer: model.layers.3\n",
      "[Debug] Successfully found layer: model.layers.3\n",
      "[Debug] Trying to access layer: model.layers.4\n",
      "[Debug] Successfully found layer: model.layers.4\n",
      "[Debug] Trying to access layer: model.layers.5\n",
      "[Debug] Successfully found layer: model.layers.5\n",
      "[Debug] Trying to access layer: model.layers.6\n",
      "[Debug] Successfully found layer: model.layers.6\n",
      "[Debug] Trying to access layer: model.layers.7\n",
      "[Debug] Successfully found layer: model.layers.7\n",
      "[Debug] Trying to access layer: model.layers.8\n",
      "[Debug] Successfully found layer: model.layers.8\n",
      "[Debug] Trying to access layer: model.layers.9\n",
      "[Debug] Successfully found layer: model.layers.9\n",
      "[Debug] Trying to access layer: model.layers.10\n",
      "[Debug] Successfully found layer: model.layers.10\n",
      "[Debug] Trying to access layer: model.layers.11\n",
      "[Debug] Successfully found layer: model.layers.11\n",
      "[Debug] Trying to access layer: model.layers.12\n",
      "[Debug] Successfully found layer: model.layers.12\n",
      "[Debug] Trying to access layer: model.layers.13\n",
      "[Debug] Successfully found layer: model.layers.13\n",
      "[Debug] Trying to access layer: model.layers.14\n",
      "[Debug] Successfully found layer: model.layers.14\n",
      "[Debug] Trying to access layer: model.layers.15\n",
      "[Debug] Successfully found layer: model.layers.15\n",
      "[Debug] Trying to access layer: model.layers.16\n",
      "[Debug] Successfully found layer: model.layers.16\n",
      "[Debug] Trying to access layer: model.layers.17\n",
      "[Debug] Successfully found layer: model.layers.17\n",
      "[Debug] Trying to access layer: model.layers.18\n",
      "[Debug] Successfully found layer: model.layers.18\n",
      "[Debug] Trying to access layer: model.layers.19\n",
      "[Debug] Successfully found layer: model.layers.19\n",
      "[Debug] Trying to access layer: model.layers.20\n",
      "[Debug] Successfully found layer: model.layers.20\n",
      "[Debug] Trying to access layer: model.layers.21\n",
      "[Debug] Successfully found layer: model.layers.21\n",
      "[Debug] Trying to access layer: model.layers.22\n",
      "[Debug] Successfully found layer: model.layers.22\n",
      "[Debug] Trying to access layer: model.layers.23\n",
      "[Debug] Successfully found layer: model.layers.23\n",
      "[Debug] Trying to access layer: model.layers.24\n",
      "[Debug] Successfully found layer: model.layers.24\n",
      "[Debug] Trying to access layer: model.layers.25\n",
      "[Debug] Successfully found layer: model.layers.25\n",
      "[Debug] Trying to access layer: model.layers.26\n",
      "[Debug] Successfully found layer: model.layers.26\n",
      "[Debug] Trying to access layer: model.layers.27\n",
      "[Debug] Successfully found layer: model.layers.27\n",
      "[Debug] Trying to access layer: model.embed_tokens\n",
      "[Debug] Successfully found layer: model.embed_tokens\n",
      "[Hook] Layer Embedding(128256, 3072, padding_idx=128004) received input (tensor([[128000,  43824,    315,  19152,  22425,   2740,     11,  13673,   6593,\n",
      "           8853,    617,   8272,    279,   8013,  14135,    555,   2389,  14782,\n",
      "            279,  12628,    315,  42090,    315,  19152,    323,  42090,    315,\n",
      "          48190,    320,   8578,   7497,      8,    311,   1202,  38581,  24797,\n",
      "            594,  20073,    279,   2316,    315,  19150,    315,  19152,    320,\n",
      "           6204,      8,    369,    872,   3495,   4967,   8547,     11,  79283,\n",
      "            311,    279,  30661,     11,    477,    369,    872,  99119,  10896,\n",
      "            988,     13,  10541,    279,   8857,    315,  13673,  13642,   7497,\n",
      "          12628,    617,   1027,  19560,   7620,   2533,    279,    220,   2550,\n",
      "             15,     82,     11,   1234,    279,   3766,  13673,  20143,   7174,\n",
      "          24686,    320,  93173,     37,      8,    814,  14958,  71974,    439,\n",
      "           9580,    220,     22,  42090,    596,  12628,   3871,    449,   1023,\n",
      "          41534,   7620,     13]]),) and output tensor([[[-0.0010, -0.0007, -0.0046,  ..., -0.0014, -0.0020,  0.0020],\n",
      "         [ 0.0120,  0.0171,  0.0082,  ...,  0.0005,  0.0010,  0.0096],\n",
      "         [ 0.0045, -0.0109,  0.0005,  ...,  0.0231,  0.0219,  0.0065],\n",
      "         ...,\n",
      "         [ 0.0053, -0.0142, -0.0045,  ...,  0.0156, -0.0315,  0.0063],\n",
      "         [-0.0021,  0.0260,  0.0193,  ..., -0.0076, -0.0006,  0.0043],\n",
      "         [ 0.0093, -0.0031,  0.0278,  ...,  0.0117, -0.0084,  0.0096]]])\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[-0.0010, -0.0007, -0.0046,  ..., -0.0014, -0.0020,  0.0020],\n",
      "         [ 0.0120,  0.0171,  0.0082,  ...,  0.0005,  0.0010,  0.0096],\n",
      "         [ 0.0045, -0.0109,  0.0005,  ...,  0.0231,  0.0219,  0.0065],\n",
      "         ...,\n",
      "         [ 0.0053, -0.0142, -0.0045,  ...,  0.0156, -0.0315,  0.0063],\n",
      "         [-0.0021,  0.0260,  0.0193,  ..., -0.0076, -0.0006,  0.0043],\n",
      "         [ 0.0093, -0.0031,  0.0278,  ...,  0.0117, -0.0084,  0.0096]]]),) and output (tensor([[[ 442.9990,  239.9993,  302.9954,  ..., -154.0014,  -15.0020,\n",
      "           185.0020],\n",
      "         [ -19.9880,  -90.9829,  352.0082,  ..., -253.9995,  157.0010,\n",
      "           124.0096],\n",
      "         [  28.0045,  -32.0109,  -77.9995,  ...,  -39.9769,  -62.9781,\n",
      "           247.0065],\n",
      "         ...,\n",
      "         [ 237.0053,  170.9858,  -55.0045,  ...,  -21.9844,  -86.0315,\n",
      "            42.0063],\n",
      "         [  36.9979, -172.9740, -315.9807,  ..., -320.0076, -177.0006,\n",
      "             4.0043],\n",
      "         [  35.0093,  144.9969,   -3.9722,  ...,  -44.9883,  -19.0084,\n",
      "           -25.9904]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[ 442.9990,  239.9993,  302.9954,  ..., -154.0014,  -15.0020,\n",
      "           185.0020],\n",
      "         [ -19.9880,  -90.9829,  352.0082,  ..., -253.9995,  157.0010,\n",
      "           124.0096],\n",
      "         [  28.0045,  -32.0109,  -77.9995,  ...,  -39.9769,  -62.9781,\n",
      "           247.0065],\n",
      "         ...,\n",
      "         [ 237.0053,  170.9858,  -55.0045,  ...,  -21.9844,  -86.0315,\n",
      "            42.0063],\n",
      "         [  36.9979, -172.9740, -315.9807,  ..., -320.0076, -177.0006,\n",
      "             4.0043],\n",
      "         [  35.0093,  144.9969,   -3.9722,  ...,  -44.9883,  -19.0084,\n",
      "           -25.9904]]]),) and output (tensor([[[  439.9990,  -144.0007,  -410.0046,  ...,  -765.0015,\n",
      "           1193.9979, -1027.9980],\n",
      "         [   18.0120,    76.0171,    52.0082,  ...,  -319.9995,\n",
      "            642.0010,    91.0096],\n",
      "         [ -117.9955,   433.9891,   162.0005,  ...,  -284.9769,\n",
      "            704.0219,   922.0065],\n",
      "         ...,\n",
      "         [   53.0053, -1042.0142,  -224.0045,  ...,   -31.9844,\n",
      "           -190.0315,    21.0063],\n",
      "         [ -463.0021,  -916.9740, -1980.9807,  ...,   459.9924,\n",
      "            308.9994,   439.0043],\n",
      "         [ -549.9907,   416.9969,   426.0278,  ...,   139.0117,\n",
      "            464.9916,  -336.9904]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[  439.9990,  -144.0007,  -410.0046,  ...,  -765.0015,\n",
      "           1193.9979, -1027.9980],\n",
      "         [   18.0120,    76.0171,    52.0082,  ...,  -319.9995,\n",
      "            642.0010,    91.0096],\n",
      "         [ -117.9955,   433.9891,   162.0005,  ...,  -284.9769,\n",
      "            704.0219,   922.0065],\n",
      "         ...,\n",
      "         [   53.0053, -1042.0142,  -224.0045,  ...,   -31.9844,\n",
      "           -190.0315,    21.0063],\n",
      "         [ -463.0021,  -916.9740, -1980.9807,  ...,   459.9924,\n",
      "            308.9994,   439.0043],\n",
      "         [ -549.9907,   416.9969,   426.0278,  ...,   139.0117,\n",
      "            464.9916,  -336.9904]]]),) and output (tensor([[[  551.9990,   993.9993, -1790.0046,  ...,   445.9985,\n",
      "          -1386.0021, -1245.9980],\n",
      "         [-1301.9880, -5753.9829, -1432.9918,  ..., -2787.9995,\n",
      "            125.0010,  1791.0096],\n",
      "         [-1426.9956, -2170.0110,  2072.0005,  ..., -3722.9771,\n",
      "           -306.9781,   156.0065],\n",
      "         ...,\n",
      "         [ -303.9947, -2465.0142,   816.9956,  ...,  -942.9844,\n",
      "             57.9685,  5698.0063],\n",
      "         [ 1681.9979,  1674.0260, -2823.9807,  ...,  4582.9922,\n",
      "            824.9994,  4494.0044],\n",
      "         [ 1441.0093,  1063.9969,  2619.0278,  ...,  3399.0117,\n",
      "           1074.9917,  -216.9904]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[  551.9990,   993.9993, -1790.0046,  ...,   445.9985,\n",
      "          -1386.0021, -1245.9980],\n",
      "         [-1301.9880, -5753.9829, -1432.9918,  ..., -2787.9995,\n",
      "            125.0010,  1791.0096],\n",
      "         [-1426.9956, -2170.0110,  2072.0005,  ..., -3722.9771,\n",
      "           -306.9781,   156.0065],\n",
      "         ...,\n",
      "         [ -303.9947, -2465.0142,   816.9956,  ...,  -942.9844,\n",
      "             57.9685,  5698.0063],\n",
      "         [ 1681.9979,  1674.0260, -2823.9807,  ...,  4582.9922,\n",
      "            824.9994,  4494.0044],\n",
      "         [ 1441.0093,  1063.9969,  2619.0278,  ...,  3399.0117,\n",
      "           1074.9917,  -216.9904]]]),) and output (tensor([[[ 2131.9990,  -737.0007, -1071.0046,  ..., -2781.0015,\n",
      "           -885.0021,    86.0020],\n",
      "         [ 1011.0120, -8839.9824, -3167.9917,  ..., -6076.9995,\n",
      "           2679.0010,  5603.0098],\n",
      "         [-3802.9956,  -975.0110,  2805.0005,  ..., -6718.9771,\n",
      "            820.0219,  6138.0063],\n",
      "         ...,\n",
      "         [-2338.9946,  1183.9858,  1826.9956,  ..., -3076.9844,\n",
      "          -3466.0315,  2214.0063],\n",
      "         [ 4755.9980,  2059.0259, -1133.9807,  ...,  9820.9922,\n",
      "          -2330.0005,  2782.0044],\n",
      "         [ 2237.0093, -3404.0029, -2061.9722,  ...,  5634.0117,\n",
      "           6468.9917,   109.0096]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[ 2131.9990,  -737.0007, -1071.0046,  ..., -2781.0015,\n",
      "           -885.0021,    86.0020],\n",
      "         [ 1011.0120, -8839.9824, -3167.9917,  ..., -6076.9995,\n",
      "           2679.0010,  5603.0098],\n",
      "         [-3802.9956,  -975.0110,  2805.0005,  ..., -6718.9771,\n",
      "            820.0219,  6138.0063],\n",
      "         ...,\n",
      "         [-2338.9946,  1183.9858,  1826.9956,  ..., -3076.9844,\n",
      "          -3466.0315,  2214.0063],\n",
      "         [ 4755.9980,  2059.0259, -1133.9807,  ...,  9820.9922,\n",
      "          -2330.0005,  2782.0044],\n",
      "         [ 2237.0093, -3404.0029, -2061.9722,  ...,  5634.0117,\n",
      "           6468.9917,   109.0096]]]),) and output (tensor([[[  3420.9990,    760.9993,  -5376.0049,  ...,  -4193.0015,\n",
      "            2860.9980,  -4080.9980],\n",
      "         [  2160.0120, -18106.9824,  -2300.9917,  ...,  -5832.9995,\n",
      "            5421.0010,   3348.0098],\n",
      "         [ -5834.9956,  -3547.0110,   4762.0005,  ...,  -6160.9771,\n",
      "           -6348.9780,   6755.0063],\n",
      "         ...,\n",
      "         [ -3073.9946,   3189.9858,   5321.9956,  ...,  -5133.9844,\n",
      "           -6939.0312,   5150.0063],\n",
      "         [  2555.9980,   2126.0259,  -2502.9807,  ...,  10195.9922,\n",
      "           -6600.0005,   4641.0044],\n",
      "         [  -720.9907, -10282.0029,  -7699.9722,  ...,  10218.0117,\n",
      "            6631.9917,   5342.0098]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[  3420.9990,    760.9993,  -5376.0049,  ...,  -4193.0015,\n",
      "            2860.9980,  -4080.9980],\n",
      "         [  2160.0120, -18106.9824,  -2300.9917,  ...,  -5832.9995,\n",
      "            5421.0010,   3348.0098],\n",
      "         [ -5834.9956,  -3547.0110,   4762.0005,  ...,  -6160.9771,\n",
      "           -6348.9780,   6755.0063],\n",
      "         ...,\n",
      "         [ -3073.9946,   3189.9858,   5321.9956,  ...,  -5133.9844,\n",
      "           -6939.0312,   5150.0063],\n",
      "         [  2555.9980,   2126.0259,  -2502.9807,  ...,  10195.9922,\n",
      "           -6600.0005,   4641.0044],\n",
      "         [  -720.9907, -10282.0029,  -7699.9722,  ...,  10218.0117,\n",
      "            6631.9917,   5342.0098]]]),) and output (tensor([[[  2742.9990,  -1512.0007, -10493.0049,  ...,  -9945.0020,\n",
      "           -2017.0020,  -1802.9980],\n",
      "         [  4848.0117, -20133.9824,  -4631.9917,  ..., -11004.0000,\n",
      "            7197.0010,   5744.0098],\n",
      "         [ -9161.9961,  -2834.0110,   2280.0005,  ...,  -5978.9771,\n",
      "           -1020.9780,   9721.0059],\n",
      "         ...,\n",
      "         [ -8864.9941,   9219.9863,   4078.9956,  ...,  -2597.9844,\n",
      "           -9034.0312,  12268.0059],\n",
      "         [  4518.9980,   -266.9741,    -27.9807,  ...,   8976.9922,\n",
      "           -4588.0005,   2580.0044],\n",
      "         [ -9109.9902,  -6211.0029,  -9682.9727,  ...,   7501.0117,\n",
      "            9292.9922,   7573.0098]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[  2742.9990,  -1512.0007, -10493.0049,  ...,  -9945.0020,\n",
      "           -2017.0020,  -1802.9980],\n",
      "         [  4848.0117, -20133.9824,  -4631.9917,  ..., -11004.0000,\n",
      "            7197.0010,   5744.0098],\n",
      "         [ -9161.9961,  -2834.0110,   2280.0005,  ...,  -5978.9771,\n",
      "           -1020.9780,   9721.0059],\n",
      "         ...,\n",
      "         [ -8864.9941,   9219.9863,   4078.9956,  ...,  -2597.9844,\n",
      "           -9034.0312,  12268.0059],\n",
      "         [  4518.9980,   -266.9741,    -27.9807,  ...,   8976.9922,\n",
      "           -4588.0005,   2580.0044],\n",
      "         [ -9109.9902,  -6211.0029,  -9682.9727,  ...,   7501.0117,\n",
      "            9292.9922,   7573.0098]]]),) and output (tensor([[[  3032.9990,  -3420.0007, -10291.0049,  ...,  -6511.0020,\n",
      "           -1953.0020,   2001.0020],\n",
      "         [  6141.0117, -26766.9824,  -3063.9917,  ...,  -8216.0000,\n",
      "            7533.0010,   6743.0098],\n",
      "         [-11377.9961,  -4773.0107,   2988.0005,  ...,  -5670.9771,\n",
      "           -3169.9780,   6942.0059],\n",
      "         ...,\n",
      "         [ -3783.9941,   7691.9863,   2385.9956,  ...,  -3268.9844,\n",
      "           -8805.0312,   8966.0059],\n",
      "         [  4078.9980,  -1659.9741,  -1652.9807,  ...,  12982.9922,\n",
      "          -10202.0000,  -2735.9956],\n",
      "         [ -9621.9902,  -6528.0029,  -3521.9727,  ...,   1832.0117,\n",
      "           11640.9922,   6578.0098]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[  3032.9990,  -3420.0007, -10291.0049,  ...,  -6511.0020,\n",
      "           -1953.0020,   2001.0020],\n",
      "         [  6141.0117, -26766.9824,  -3063.9917,  ...,  -8216.0000,\n",
      "            7533.0010,   6743.0098],\n",
      "         [-11377.9961,  -4773.0107,   2988.0005,  ...,  -5670.9771,\n",
      "           -3169.9780,   6942.0059],\n",
      "         ...,\n",
      "         [ -3783.9941,   7691.9863,   2385.9956,  ...,  -3268.9844,\n",
      "           -8805.0312,   8966.0059],\n",
      "         [  4078.9980,  -1659.9741,  -1652.9807,  ...,  12982.9922,\n",
      "          -10202.0000,  -2735.9956],\n",
      "         [ -9621.9902,  -6528.0029,  -3521.9727,  ...,   1832.0117,\n",
      "           11640.9922,   6578.0098]]]),) and output (tensor([[[  3132.9990,  -1970.0007, -11544.0049,  ...,  -5780.0020,\n",
      "           -2486.0020,  -1921.9980],\n",
      "         [  7218.0117, -34364.9844,  -9234.9922,  ...,  -6830.0000,\n",
      "           10698.0010,   9332.0098],\n",
      "         [-13128.9961,  -8524.0107,   3756.0005,  ...,  -6286.9771,\n",
      "            3686.0220,   8312.0059],\n",
      "         ...,\n",
      "         [ -2866.9941,   4680.9863,     83.9956,  ...,  -5199.9844,\n",
      "          -11171.0312,   6612.0059],\n",
      "         [  5323.9980,  -5340.9741,  -3164.9807,  ...,   7454.9922,\n",
      "          -14353.0000,   -543.9956],\n",
      "         [ -5857.9902,  -3504.0029,  -1218.9727,  ...,   5390.0117,\n",
      "           11593.9922,   7156.0098]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[  3132.9990,  -1970.0007, -11544.0049,  ...,  -5780.0020,\n",
      "           -2486.0020,  -1921.9980],\n",
      "         [  7218.0117, -34364.9844,  -9234.9922,  ...,  -6830.0000,\n",
      "           10698.0010,   9332.0098],\n",
      "         [-13128.9961,  -8524.0107,   3756.0005,  ...,  -6286.9771,\n",
      "            3686.0220,   8312.0059],\n",
      "         ...,\n",
      "         [ -2866.9941,   4680.9863,     83.9956,  ...,  -5199.9844,\n",
      "          -11171.0312,   6612.0059],\n",
      "         [  5323.9980,  -5340.9741,  -3164.9807,  ...,   7454.9922,\n",
      "          -14353.0000,   -543.9956],\n",
      "         [ -5857.9902,  -3504.0029,  -1218.9727,  ...,   5390.0117,\n",
      "           11593.9922,   7156.0098]]]),) and output (tensor([[[  1046.9990,   2403.9993, -14486.0049,  ..., -10922.0020,\n",
      "           -1419.0020,  -2983.9980],\n",
      "         [  2312.0117, -34311.9844,  -8349.9922,  ...,  -8408.0000,\n",
      "            9972.0010,  11352.0098],\n",
      "         [-11968.9961,  -3797.0107,   1707.0005,  ...,  -5408.9771,\n",
      "           -2319.9780,   1959.0059],\n",
      "         ...,\n",
      "         [ -9154.9941,   6856.9863,    927.9956,  ...,  -5789.9844,\n",
      "           -3003.0312,   3400.0059],\n",
      "         [  2197.9980,  -6837.9741,  -1078.9807,  ...,  12911.9922,\n",
      "          -20562.0000,   3886.0044],\n",
      "         [ -6959.9902,  -6770.0029,  -5227.9727,  ...,   5890.0117,\n",
      "           11051.9922,    844.0098]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[  1046.9990,   2403.9993, -14486.0049,  ..., -10922.0020,\n",
      "           -1419.0020,  -2983.9980],\n",
      "         [  2312.0117, -34311.9844,  -8349.9922,  ...,  -8408.0000,\n",
      "            9972.0010,  11352.0098],\n",
      "         [-11968.9961,  -3797.0107,   1707.0005,  ...,  -5408.9771,\n",
      "           -2319.9780,   1959.0059],\n",
      "         ...,\n",
      "         [ -9154.9941,   6856.9863,    927.9956,  ...,  -5789.9844,\n",
      "           -3003.0312,   3400.0059],\n",
      "         [  2197.9980,  -6837.9741,  -1078.9807,  ...,  12911.9922,\n",
      "          -20562.0000,   3886.0044],\n",
      "         [ -6959.9902,  -6770.0029,  -5227.9727,  ...,   5890.0117,\n",
      "           11051.9922,    844.0098]]]),) and output (tensor([[[ -3767.0010,   5307.9990, -14221.0049,  ..., -15035.0020,\n",
      "           -3630.0020,   4168.0020],\n",
      "         [  3121.0117, -41050.9844, -12118.9922,  ..., -10691.0000,\n",
      "           17399.0000,  11362.0098],\n",
      "         [-19679.9961,   2421.9893,  -2288.9995,  ..., -10458.9766,\n",
      "            4208.0220,   -768.9941],\n",
      "         ...,\n",
      "         [ -8949.9941,   3976.9863,  -2397.0044,  ...,  -9708.9844,\n",
      "           -4579.0312,   6412.0059],\n",
      "         [  3174.9980,  -7504.9741,  -2997.9807,  ...,  17335.9922,\n",
      "          -21489.0000,   8767.0039],\n",
      "         [ -9335.9902,  -6939.0029,  -6287.9727,  ...,  11614.0117,\n",
      "           11275.9922,   1571.0098]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[ -3767.0010,   5307.9990, -14221.0049,  ..., -15035.0020,\n",
      "           -3630.0020,   4168.0020],\n",
      "         [  3121.0117, -41050.9844, -12118.9922,  ..., -10691.0000,\n",
      "           17399.0000,  11362.0098],\n",
      "         [-19679.9961,   2421.9893,  -2288.9995,  ..., -10458.9766,\n",
      "            4208.0220,   -768.9941],\n",
      "         ...,\n",
      "         [ -8949.9941,   3976.9863,  -2397.0044,  ...,  -9708.9844,\n",
      "           -4579.0312,   6412.0059],\n",
      "         [  3174.9980,  -7504.9741,  -2997.9807,  ...,  17335.9922,\n",
      "          -21489.0000,   8767.0039],\n",
      "         [ -9335.9902,  -6939.0029,  -6287.9727,  ...,  11614.0117,\n",
      "           11275.9922,   1571.0098]]]),) and output (tensor([[[ -1326.0010,   1223.9990, -16643.0039,  ..., -13240.0020,\n",
      "           -5677.0020,   9639.0020],\n",
      "         [  7715.0117, -43564.9844,  -9276.9922,  ..., -15619.0000,\n",
      "           17919.0000,  11011.0098],\n",
      "         [-23428.9961,  -1209.0107,  -2310.9995,  ..., -11779.9766,\n",
      "            -300.9780,  -1675.9941],\n",
      "         ...,\n",
      "         [-10973.9941,   7327.9863,  -2892.0044,  ..., -13292.9844,\n",
      "           -2067.0312,   6834.0059],\n",
      "         [  1711.9980,  -8792.9746,  -4510.9805,  ...,  15328.9922,\n",
      "          -21233.0000,   9263.0039],\n",
      "         [-11458.9902,  -4384.0029,  -7386.9727,  ...,   8187.0117,\n",
      "            9144.9922,   1611.0098]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[ -1326.0010,   1223.9990, -16643.0039,  ..., -13240.0020,\n",
      "           -5677.0020,   9639.0020],\n",
      "         [  7715.0117, -43564.9844,  -9276.9922,  ..., -15619.0000,\n",
      "           17919.0000,  11011.0098],\n",
      "         [-23428.9961,  -1209.0107,  -2310.9995,  ..., -11779.9766,\n",
      "            -300.9780,  -1675.9941],\n",
      "         ...,\n",
      "         [-10973.9941,   7327.9863,  -2892.0044,  ..., -13292.9844,\n",
      "           -2067.0312,   6834.0059],\n",
      "         [  1711.9980,  -8792.9746,  -4510.9805,  ...,  15328.9922,\n",
      "          -21233.0000,   9263.0039],\n",
      "         [-11458.9902,  -4384.0029,  -7386.9727,  ...,   8187.0117,\n",
      "            9144.9922,   1611.0098]]]),) and output (tensor([[[ -5120.0010,   2246.9990, -15993.0039,  ..., -19818.0020,\n",
      "           -9385.0020,  11924.0020],\n",
      "         [  2356.0117, -46409.9844, -11548.9922,  ..., -20100.0000,\n",
      "           16576.0000,  13327.0098],\n",
      "         [-22120.9961,  -3920.0107,  -4199.9995,  ...,  -8381.9766,\n",
      "            2772.0220,  -1032.9941],\n",
      "         ...,\n",
      "         [-13458.9941,   6276.9863,   1459.9956,  ..., -13343.9844,\n",
      "           -8578.0312,   4028.0059],\n",
      "         [ -4161.0020,  -8142.9746,  -5839.9805,  ...,  17312.9922,\n",
      "          -25085.0000,   7521.0039],\n",
      "         [-13285.9902,  -7463.0029,  -8617.9727,  ...,  12310.0117,\n",
      "           10492.9922,   2421.0098]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[ -5120.0010,   2246.9990, -15993.0039,  ..., -19818.0020,\n",
      "           -9385.0020,  11924.0020],\n",
      "         [  2356.0117, -46409.9844, -11548.9922,  ..., -20100.0000,\n",
      "           16576.0000,  13327.0098],\n",
      "         [-22120.9961,  -3920.0107,  -4199.9995,  ...,  -8381.9766,\n",
      "            2772.0220,  -1032.9941],\n",
      "         ...,\n",
      "         [-13458.9941,   6276.9863,   1459.9956,  ..., -13343.9844,\n",
      "           -8578.0312,   4028.0059],\n",
      "         [ -4161.0020,  -8142.9746,  -5839.9805,  ...,  17312.9922,\n",
      "          -25085.0000,   7521.0039],\n",
      "         [-13285.9902,  -7463.0029,  -8617.9727,  ...,  12310.0117,\n",
      "           10492.9922,   2421.0098]]]),) and output (tensor([[[ -6685.0010,  -1278.0010, -15553.0039,  ..., -19846.0020,\n",
      "          -10418.0020,  13376.0020],\n",
      "         [ 11349.0117, -57648.9844, -15309.9922,  ..., -15408.0000,\n",
      "           11545.0000,  11805.0098],\n",
      "         [-21555.9961,  -8000.0107,  -2714.9995,  ...,  -7780.9766,\n",
      "           -6026.9780,  -2937.9941],\n",
      "         ...,\n",
      "         [-18891.9941,    609.9863,    690.9956,  ..., -13253.9844,\n",
      "          -10271.0312,   5936.0059],\n",
      "         [  -386.0020, -14442.9746,  -4323.9805,  ...,  18096.9922,\n",
      "          -29483.0000,  12387.0039],\n",
      "         [-12144.9902, -13284.0029, -10352.9727,  ...,  13535.0117,\n",
      "            2828.9922,   2802.0098]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[ -6685.0010,  -1278.0010, -15553.0039,  ..., -19846.0020,\n",
      "          -10418.0020,  13376.0020],\n",
      "         [ 11349.0117, -57648.9844, -15309.9922,  ..., -15408.0000,\n",
      "           11545.0000,  11805.0098],\n",
      "         [-21555.9961,  -8000.0107,  -2714.9995,  ...,  -7780.9766,\n",
      "           -6026.9780,  -2937.9941],\n",
      "         ...,\n",
      "         [-18891.9941,    609.9863,    690.9956,  ..., -13253.9844,\n",
      "          -10271.0312,   5936.0059],\n",
      "         [  -386.0020, -14442.9746,  -4323.9805,  ...,  18096.9922,\n",
      "          -29483.0000,  12387.0039],\n",
      "         [-12144.9902, -13284.0029, -10352.9727,  ...,  13535.0117,\n",
      "            2828.9922,   2802.0098]]]),) and output (tensor([[[ -8422.0010,    339.9990, -15289.0039,  ..., -20884.0020,\n",
      "           -6908.0020,  11014.0020],\n",
      "         [  8939.0117, -59674.9844, -14094.9922,  ..., -20928.0000,\n",
      "           14719.0000,  11142.0098],\n",
      "         [-21302.9961,  -8243.0107,    539.0005,  ...,   1461.0234,\n",
      "           -8501.9785,  -1229.9941],\n",
      "         ...,\n",
      "         [-21298.9941,   2361.9863,    895.9956,  ..., -11313.9844,\n",
      "           -8254.0312,   7338.0059],\n",
      "         [  -804.0020, -16716.9746,  -8134.9805,  ...,  17932.9922,\n",
      "          -32983.0000,   7869.0039],\n",
      "         [-12071.9902, -13533.0029, -10117.9727,  ...,  11151.0117,\n",
      "            2303.9922,   -887.9902]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[ -8422.0010,    339.9990, -15289.0039,  ..., -20884.0020,\n",
      "           -6908.0020,  11014.0020],\n",
      "         [  8939.0117, -59674.9844, -14094.9922,  ..., -20928.0000,\n",
      "           14719.0000,  11142.0098],\n",
      "         [-21302.9961,  -8243.0107,    539.0005,  ...,   1461.0234,\n",
      "           -8501.9785,  -1229.9941],\n",
      "         ...,\n",
      "         [-21298.9941,   2361.9863,    895.9956,  ..., -11313.9844,\n",
      "           -8254.0312,   7338.0059],\n",
      "         [  -804.0020, -16716.9746,  -8134.9805,  ...,  17932.9922,\n",
      "          -32983.0000,   7869.0039],\n",
      "         [-12071.9902, -13533.0029, -10117.9727,  ...,  11151.0117,\n",
      "            2303.9922,   -887.9902]]]),) and output (tensor([[[-10989.0010,   7034.9990, -13964.0039,  ..., -23968.0020,\n",
      "           -3749.0020,  11730.0020],\n",
      "         [ 13361.0117, -65262.9844,  -5564.9922,  ..., -23184.0000,\n",
      "           19652.0000,   9879.0098],\n",
      "         [-13971.9961, -13232.0107,  -9159.0000,  ...,   2656.0234,\n",
      "           -3285.9785, -12785.9941],\n",
      "         ...,\n",
      "         [-25841.9941,  -3086.0137,   1574.9956,  ..., -12994.9844,\n",
      "          -10901.0312,   7059.0059],\n",
      "         [ -4241.0020, -19799.9746, -10748.9805,  ...,  17818.9922,\n",
      "          -38289.0000,  10269.0039],\n",
      "         [-11628.9902, -16275.0029,  -7957.9727,  ...,  12347.0117,\n",
      "            5494.9922,  -5152.9902]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[-10989.0010,   7034.9990, -13964.0039,  ..., -23968.0020,\n",
      "           -3749.0020,  11730.0020],\n",
      "         [ 13361.0117, -65262.9844,  -5564.9922,  ..., -23184.0000,\n",
      "           19652.0000,   9879.0098],\n",
      "         [-13971.9961, -13232.0107,  -9159.0000,  ...,   2656.0234,\n",
      "           -3285.9785, -12785.9941],\n",
      "         ...,\n",
      "         [-25841.9941,  -3086.0137,   1574.9956,  ..., -12994.9844,\n",
      "          -10901.0312,   7059.0059],\n",
      "         [ -4241.0020, -19799.9746, -10748.9805,  ...,  17818.9922,\n",
      "          -38289.0000,  10269.0039],\n",
      "         [-11628.9902, -16275.0029,  -7957.9727,  ...,  12347.0117,\n",
      "            5494.9922,  -5152.9902]]]),) and output (tensor([[[ -7159.0010,   6621.9990, -10967.0039,  ..., -17477.0020,\n",
      "           -2510.0020,  11888.0020],\n",
      "         [ 16106.0117, -70560.9844,  -3355.9922,  ..., -28617.0000,\n",
      "           18292.0000,  13082.0098],\n",
      "         [-13556.9961, -13057.0107, -13148.0000,  ...,   4357.0234,\n",
      "            4711.0215,  -6495.9941],\n",
      "         ...,\n",
      "         [-25746.9941,  -6072.0137,   6088.9956,  ..., -12964.9844,\n",
      "          -14932.0312,   8101.0059],\n",
      "         [  3726.9980, -21485.9746,  -8645.9805,  ...,  20549.9922,\n",
      "          -35470.0000,  10002.0039],\n",
      "         [ -2144.9902, -16339.0029, -13599.9727,  ...,  13719.0117,\n",
      "           13086.9922,   4360.0098]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[ -7159.0010,   6621.9990, -10967.0039,  ..., -17477.0020,\n",
      "           -2510.0020,  11888.0020],\n",
      "         [ 16106.0117, -70560.9844,  -3355.9922,  ..., -28617.0000,\n",
      "           18292.0000,  13082.0098],\n",
      "         [-13556.9961, -13057.0107, -13148.0000,  ...,   4357.0234,\n",
      "            4711.0215,  -6495.9941],\n",
      "         ...,\n",
      "         [-25746.9941,  -6072.0137,   6088.9956,  ..., -12964.9844,\n",
      "          -14932.0312,   8101.0059],\n",
      "         [  3726.9980, -21485.9746,  -8645.9805,  ...,  20549.9922,\n",
      "          -35470.0000,  10002.0039],\n",
      "         [ -2144.9902, -16339.0029, -13599.9727,  ...,  13719.0117,\n",
      "           13086.9922,   4360.0098]]]),) and output (tensor([[[ -4822.0010,   3124.9990,  -3885.0039,  ..., -19007.0020,\n",
      "            -551.0020,  16285.0020],\n",
      "         [ 13439.0117, -72951.9844,  -4918.9922,  ..., -32805.0000,\n",
      "           27405.0000,  17595.0098],\n",
      "         [-11427.9961, -18690.0117, -22095.0000,  ...,   7326.0234,\n",
      "            8524.0215,  -7215.9941],\n",
      "         ...,\n",
      "         [-32853.9922,   2406.9863,   3462.9956,  ..., -10925.9844,\n",
      "          -10816.0312,   6044.0059],\n",
      "         [  6158.9980, -24074.9746,  -4051.9805,  ...,  19411.9922,\n",
      "          -41972.0000,  12472.0039],\n",
      "         [  -189.9902, -16429.0039, -15537.9727,  ...,  16725.0117,\n",
      "           14168.9922,   4524.0098]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[ -4822.0010,   3124.9990,  -3885.0039,  ..., -19007.0020,\n",
      "            -551.0020,  16285.0020],\n",
      "         [ 13439.0117, -72951.9844,  -4918.9922,  ..., -32805.0000,\n",
      "           27405.0000,  17595.0098],\n",
      "         [-11427.9961, -18690.0117, -22095.0000,  ...,   7326.0234,\n",
      "            8524.0215,  -7215.9941],\n",
      "         ...,\n",
      "         [-32853.9922,   2406.9863,   3462.9956,  ..., -10925.9844,\n",
      "          -10816.0312,   6044.0059],\n",
      "         [  6158.9980, -24074.9746,  -4051.9805,  ...,  19411.9922,\n",
      "          -41972.0000,  12472.0039],\n",
      "         [  -189.9902, -16429.0039, -15537.9727,  ...,  16725.0117,\n",
      "           14168.9922,   4524.0098]]]),) and output (tensor([[[-7.2350e+03,  6.5840e+03, -4.2120e+03,  ..., -1.9104e+04,\n",
      "           3.0880e+03,  1.7084e+04],\n",
      "         [ 8.9460e+03, -8.1111e+04, -3.2310e+03,  ..., -4.0387e+04,\n",
      "           1.9234e+04,  1.5353e+04],\n",
      "         [-1.4221e+04, -1.5010e+04, -2.6119e+04,  ...,  1.2569e+04,\n",
      "           1.2456e+04, -5.2220e+03],\n",
      "         ...,\n",
      "         [-3.4710e+04,  4.8040e+03,  3.8810e+03,  ..., -9.2770e+03,\n",
      "          -1.0559e+04,  5.4710e+03],\n",
      "         [ 6.5120e+03, -2.4193e+04, -7.9330e+03,  ...,  1.7094e+04,\n",
      "          -4.2789e+04,  9.9560e+03],\n",
      "         [ 5.3010e+01, -9.3480e+03, -1.6067e+04,  ...,  2.3938e+04,\n",
      "           1.9775e+04,  5.4900e+03]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[-7.2350e+03,  6.5840e+03, -4.2120e+03,  ..., -1.9104e+04,\n",
      "           3.0880e+03,  1.7084e+04],\n",
      "         [ 8.9460e+03, -8.1111e+04, -3.2310e+03,  ..., -4.0387e+04,\n",
      "           1.9234e+04,  1.5353e+04],\n",
      "         [-1.4221e+04, -1.5010e+04, -2.6119e+04,  ...,  1.2569e+04,\n",
      "           1.2456e+04, -5.2220e+03],\n",
      "         ...,\n",
      "         [-3.4710e+04,  4.8040e+03,  3.8810e+03,  ..., -9.2770e+03,\n",
      "          -1.0559e+04,  5.4710e+03],\n",
      "         [ 6.5120e+03, -2.4193e+04, -7.9330e+03,  ...,  1.7094e+04,\n",
      "          -4.2789e+04,  9.9560e+03],\n",
      "         [ 5.3010e+01, -9.3480e+03, -1.6067e+04,  ...,  2.3938e+04,\n",
      "           1.9775e+04,  5.4900e+03]]]),) and output (tensor([[[-5.5860e+03,  7.6460e+03, -5.0500e+02,  ..., -1.5346e+04,\n",
      "          -6.2300e+02,  1.2986e+04],\n",
      "         [ 3.0390e+03, -8.7933e+04, -1.3736e+04,  ..., -4.4732e+04,\n",
      "           2.3480e+04,  1.7284e+04],\n",
      "         [-1.8761e+04, -1.5986e+04, -2.0909e+04,  ...,  1.1592e+04,\n",
      "           3.5600e+03, -3.4600e+03],\n",
      "         ...,\n",
      "         [-3.6992e+04,  2.1240e+03,  2.5070e+03,  ..., -7.9930e+03,\n",
      "          -1.1966e+04,  3.8850e+03],\n",
      "         [ 1.0328e+04, -2.5054e+04,  6.9102e+02,  ...,  1.8311e+04,\n",
      "          -5.5696e+04,  2.3490e+03],\n",
      "         [-7.9902e+00,  4.4000e+02, -1.7026e+04,  ...,  1.6201e+04,\n",
      "           1.9987e+04,  5.0750e+03]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[-5.5860e+03,  7.6460e+03, -5.0500e+02,  ..., -1.5346e+04,\n",
      "          -6.2300e+02,  1.2986e+04],\n",
      "         [ 3.0390e+03, -8.7933e+04, -1.3736e+04,  ..., -4.4732e+04,\n",
      "           2.3480e+04,  1.7284e+04],\n",
      "         [-1.8761e+04, -1.5986e+04, -2.0909e+04,  ...,  1.1592e+04,\n",
      "           3.5600e+03, -3.4600e+03],\n",
      "         ...,\n",
      "         [-3.6992e+04,  2.1240e+03,  2.5070e+03,  ..., -7.9930e+03,\n",
      "          -1.1966e+04,  3.8850e+03],\n",
      "         [ 1.0328e+04, -2.5054e+04,  6.9102e+02,  ...,  1.8311e+04,\n",
      "          -5.5696e+04,  2.3490e+03],\n",
      "         [-7.9902e+00,  4.4000e+02, -1.7026e+04,  ...,  1.6201e+04,\n",
      "           1.9987e+04,  5.0750e+03]]]),) and output (tensor([[[ -8916.0010,   2362.9990,  -4833.0039,  ..., -12075.0020,\n",
      "           -1124.0020,  12438.0020],\n",
      "         [ -1180.9883, -92720.9844, -18792.9922,  ..., -40133.0000,\n",
      "           23837.0000,  19261.0098],\n",
      "         [-14232.9961, -22321.0117, -16603.0000,  ...,   4098.0234,\n",
      "            2675.0215,  -4025.9941],\n",
      "         ...,\n",
      "         [-36142.9922,   4027.9863,   -201.0044,  ...,  -4727.9844,\n",
      "          -10097.0312,   9995.0059],\n",
      "         [  5727.9980, -27693.9746,   -998.9805,  ...,  17704.9922,\n",
      "          -55196.0000,   6117.0039],\n",
      "         [ -5337.9902,   2166.9961, -20867.9727,  ...,  20139.0117,\n",
      "           21373.9922, -10681.9902]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[ -8916.0010,   2362.9990,  -4833.0039,  ..., -12075.0020,\n",
      "           -1124.0020,  12438.0020],\n",
      "         [ -1180.9883, -92720.9844, -18792.9922,  ..., -40133.0000,\n",
      "           23837.0000,  19261.0098],\n",
      "         [-14232.9961, -22321.0117, -16603.0000,  ...,   4098.0234,\n",
      "            2675.0215,  -4025.9941],\n",
      "         ...,\n",
      "         [-36142.9922,   4027.9863,   -201.0044,  ...,  -4727.9844,\n",
      "          -10097.0312,   9995.0059],\n",
      "         [  5727.9980, -27693.9746,   -998.9805,  ...,  17704.9922,\n",
      "          -55196.0000,   6117.0039],\n",
      "         [ -5337.9902,   2166.9961, -20867.9727,  ...,  20139.0117,\n",
      "           21373.9922, -10681.9902]]]),) and output (tensor([[[ -20230.0000,    7877.9990,     161.9961,  ...,  -10250.0020,\n",
      "              676.9980,    7095.0020],\n",
      "         [  -2459.9883, -101916.9844,  -12201.9922,  ...,  -44284.0000,\n",
      "            27919.0000,   19043.0098],\n",
      "         [ -10439.9961,  -25092.0117,  -12145.0000,  ...,     568.0234,\n",
      "             4142.0215,    3902.0059],\n",
      "         ...,\n",
      "         [ -38903.9922,    5428.9863,   -7137.0044,  ...,    1192.0156,\n",
      "            -9787.0312,    5147.0059],\n",
      "         [   3458.9980,  -29596.9746,   -2334.9805,  ...,   19033.9922,\n",
      "           -56948.0000,     705.0039],\n",
      "         [  -3112.9902,    1476.9961,  -16584.9727,  ...,   23186.0117,\n",
      "            21814.9922,   -9721.9902]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[ -20230.0000,    7877.9990,     161.9961,  ...,  -10250.0020,\n",
      "              676.9980,    7095.0020],\n",
      "         [  -2459.9883, -101916.9844,  -12201.9922,  ...,  -44284.0000,\n",
      "            27919.0000,   19043.0098],\n",
      "         [ -10439.9961,  -25092.0117,  -12145.0000,  ...,     568.0234,\n",
      "             4142.0215,    3902.0059],\n",
      "         ...,\n",
      "         [ -38903.9922,    5428.9863,   -7137.0044,  ...,    1192.0156,\n",
      "            -9787.0312,    5147.0059],\n",
      "         [   3458.9980,  -29596.9746,   -2334.9805,  ...,   19033.9922,\n",
      "           -56948.0000,     705.0039],\n",
      "         [  -3112.9902,    1476.9961,  -16584.9727,  ...,   23186.0117,\n",
      "            21814.9922,   -9721.9902]]]),) and output (tensor([[[ -25670.0000,   13293.9990,    9099.9961,  ...,   -6547.0020,\n",
      "            -5658.0020,    4192.0020],\n",
      "         [  -5437.9883, -100503.9844,  -10832.9922,  ...,  -39047.0000,\n",
      "            29383.0000,   25368.0098],\n",
      "         [  -8787.9961,  -22748.0117,  -10807.0000,  ...,    8196.0234,\n",
      "             1937.0215,     367.0059],\n",
      "         ...,\n",
      "         [ -41284.9922,    2995.9863,   -7459.0044,  ...,    6158.0156,\n",
      "           -14683.0312,    3174.0059],\n",
      "         [   7325.9980,  -25597.9746,   -3236.9805,  ...,   27398.9922,\n",
      "           -63939.0000,   -5565.9961],\n",
      "         [  -3073.9902,   -2329.0039,  -15798.9727,  ...,   23594.0117,\n",
      "            22887.9922,    -888.9902]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[ -25670.0000,   13293.9990,    9099.9961,  ...,   -6547.0020,\n",
      "            -5658.0020,    4192.0020],\n",
      "         [  -5437.9883, -100503.9844,  -10832.9922,  ...,  -39047.0000,\n",
      "            29383.0000,   25368.0098],\n",
      "         [  -8787.9961,  -22748.0117,  -10807.0000,  ...,    8196.0234,\n",
      "             1937.0215,     367.0059],\n",
      "         ...,\n",
      "         [ -41284.9922,    2995.9863,   -7459.0044,  ...,    6158.0156,\n",
      "           -14683.0312,    3174.0059],\n",
      "         [   7325.9980,  -25597.9746,   -3236.9805,  ...,   27398.9922,\n",
      "           -63939.0000,   -5565.9961],\n",
      "         [  -3073.9902,   -2329.0039,  -15798.9727,  ...,   23594.0117,\n",
      "            22887.9922,    -888.9902]]]),) and output (tensor([[[-25652.0000,  13180.9990,  15728.9961,  ...,  -2831.0020,\n",
      "             158.9980,   2204.0020],\n",
      "         [  1596.0117, -95111.9844, -11281.9922,  ..., -52705.0000,\n",
      "           31905.0000,  33072.0078],\n",
      "         [-11684.9961, -17272.0117,  -9144.0000,  ...,  13807.0234,\n",
      "            9129.0215,   5412.0059],\n",
      "         ...,\n",
      "         [-42349.9922,   3113.9863, -12070.0039,  ...,   1258.0156,\n",
      "          -13347.0312,  -5895.9941],\n",
      "         [ 14212.9980, -25580.9746,  -3645.9805,  ...,  31234.9922,\n",
      "          -56832.0000,  -3802.9961],\n",
      "         [ -5106.9902,   2144.9961, -11190.9727,  ...,  14785.0117,\n",
      "           19695.9922,   7182.0098]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[-25652.0000,  13180.9990,  15728.9961,  ...,  -2831.0020,\n",
      "             158.9980,   2204.0020],\n",
      "         [  1596.0117, -95111.9844, -11281.9922,  ..., -52705.0000,\n",
      "           31905.0000,  33072.0078],\n",
      "         [-11684.9961, -17272.0117,  -9144.0000,  ...,  13807.0234,\n",
      "            9129.0215,   5412.0059],\n",
      "         ...,\n",
      "         [-42349.9922,   3113.9863, -12070.0039,  ...,   1258.0156,\n",
      "          -13347.0312,  -5895.9941],\n",
      "         [ 14212.9980, -25580.9746,  -3645.9805,  ...,  31234.9922,\n",
      "          -56832.0000,  -3802.9961],\n",
      "         [ -5106.9902,   2144.9961, -11190.9727,  ...,  14785.0117,\n",
      "           19695.9922,   7182.0098]]]),) and output (tensor([[[-14791.0000,  15299.9990,  16129.9961,  ...,   3187.9980,\n",
      "           -2552.0020,   5626.0020],\n",
      "         [  6318.0117, -92541.9844, -12711.9922,  ..., -53411.0000,\n",
      "           27367.0000,  26552.0078],\n",
      "         [ -8426.9961, -24594.0117, -10673.0000,  ...,   8605.0234,\n",
      "           12218.0215,   1235.0059],\n",
      "         ...,\n",
      "         [-43659.9922,   8755.9863, -15533.0039,  ...,   6185.0156,\n",
      "          -17592.0312, -13589.9941],\n",
      "         [ 12329.9980, -23230.9746,  -2472.9805,  ...,  30829.9922,\n",
      "          -51375.0000,  -6291.9961],\n",
      "         [-10612.9902,   5011.9961, -13522.9727,  ...,  10001.0117,\n",
      "           22616.9922,  12503.0098]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[-14791.0000,  15299.9990,  16129.9961,  ...,   3187.9980,\n",
      "           -2552.0020,   5626.0020],\n",
      "         [  6318.0117, -92541.9844, -12711.9922,  ..., -53411.0000,\n",
      "           27367.0000,  26552.0078],\n",
      "         [ -8426.9961, -24594.0117, -10673.0000,  ...,   8605.0234,\n",
      "           12218.0215,   1235.0059],\n",
      "         ...,\n",
      "         [-43659.9922,   8755.9863, -15533.0039,  ...,   6185.0156,\n",
      "          -17592.0312, -13589.9941],\n",
      "         [ 12329.9980, -23230.9746,  -2472.9805,  ...,  30829.9922,\n",
      "          -51375.0000,  -6291.9961],\n",
      "         [-10612.9902,   5011.9961, -13522.9727,  ...,  10001.0117,\n",
      "           22616.9922,  12503.0098]]]),) and output (tensor([[[-13284.0000,  26639.0000,   7467.9961,  ...,   4113.9980,\n",
      "           -6608.0020,   5336.0020],\n",
      "         [ -3586.9883, -91784.9844, -16373.9922,  ..., -48086.0000,\n",
      "           22361.0000,  25320.0078],\n",
      "         [ -7195.9961, -26846.0117, -13990.0000,  ...,  20829.0234,\n",
      "           17848.0215,   2299.0059],\n",
      "         ...,\n",
      "         [-38363.9922,   -213.0137, -11282.0039,  ...,   7941.0156,\n",
      "          -11444.0312, -19829.9941],\n",
      "         [  7152.9980, -22518.9746,  -2310.9805,  ...,  25703.9922,\n",
      "          -57386.0000,  -5287.9961],\n",
      "         [-21101.9902,   9079.9961,  -9787.9727,  ...,  17865.0117,\n",
      "           28799.9922,  15583.0098]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[-13284.0000,  26639.0000,   7467.9961,  ...,   4113.9980,\n",
      "           -6608.0020,   5336.0020],\n",
      "         [ -3586.9883, -91784.9844, -16373.9922,  ..., -48086.0000,\n",
      "           22361.0000,  25320.0078],\n",
      "         [ -7195.9961, -26846.0117, -13990.0000,  ...,  20829.0234,\n",
      "           17848.0215,   2299.0059],\n",
      "         ...,\n",
      "         [-38363.9922,   -213.0137, -11282.0039,  ...,   7941.0156,\n",
      "          -11444.0312, -19829.9941],\n",
      "         [  7152.9980, -22518.9746,  -2310.9805,  ...,  25703.9922,\n",
      "          -57386.0000,  -5287.9961],\n",
      "         [-21101.9902,   9079.9961,  -9787.9727,  ...,  17865.0117,\n",
      "           28799.9922,  15583.0098]]]),) and output (tensor([[[-21532.0000,  32796.0000,  16281.9961,  ...,  18208.9980,\n",
      "           -4282.0020,   5756.0020],\n",
      "         [ -8061.9883, -88414.9844,  -8774.9922,  ..., -42212.0000,\n",
      "           15742.0000,  17174.0078],\n",
      "         [-11142.9961, -28376.0117, -11392.0000,  ...,  23368.0234,\n",
      "           11979.0215,  -1953.9941],\n",
      "         ...,\n",
      "         [-28976.9922,  -2487.0137, -16835.0039,  ...,  13449.0156,\n",
      "           -7535.0312, -19270.9941],\n",
      "         [  1788.9980, -10662.9746,   3122.0195,  ...,  30419.9922,\n",
      "          -57503.0000,  -3950.9961],\n",
      "         [-21357.9902,   9923.9961, -15247.9727,  ...,   5438.0117,\n",
      "           17716.9922,  12700.0098]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[-21532.0000,  32796.0000,  16281.9961,  ...,  18208.9980,\n",
      "           -4282.0020,   5756.0020],\n",
      "         [ -8061.9883, -88414.9844,  -8774.9922,  ..., -42212.0000,\n",
      "           15742.0000,  17174.0078],\n",
      "         [-11142.9961, -28376.0117, -11392.0000,  ...,  23368.0234,\n",
      "           11979.0215,  -1953.9941],\n",
      "         ...,\n",
      "         [-28976.9922,  -2487.0137, -16835.0039,  ...,  13449.0156,\n",
      "           -7535.0312, -19270.9941],\n",
      "         [  1788.9980, -10662.9746,   3122.0195,  ...,  30419.9922,\n",
      "          -57503.0000,  -3950.9961],\n",
      "         [-21357.9902,   9923.9961, -15247.9727,  ...,   5438.0117,\n",
      "           17716.9922,  12700.0098]]]),) and output (tensor([[[-24082.0000,  33387.0000,  15374.9961,  ...,  18359.9980,\n",
      "           -7500.0020,  11519.0020],\n",
      "         [-11687.9883, -80858.9844,  -8911.9922,  ..., -40026.0000,\n",
      "           14308.0000,  11738.0078],\n",
      "         [ -5428.9961, -25614.0117, -20856.0000,  ...,  27123.0234,\n",
      "           19570.0215,  -9206.9941],\n",
      "         ...,\n",
      "         [-35214.9922,  -2071.0137,  -6981.0039,  ...,  17021.0156,\n",
      "           -3987.0312,  -5340.9941],\n",
      "         [  3932.9980, -13769.9746,    596.0195,  ...,  34478.9922,\n",
      "          -60926.0000,   1389.0039],\n",
      "         [-11103.9902,  11600.9961, -17656.9727,  ...,   1047.0117,\n",
      "           19250.9922,  13373.0098]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[-24082.0000,  33387.0000,  15374.9961,  ...,  18359.9980,\n",
      "           -7500.0020,  11519.0020],\n",
      "         [-11687.9883, -80858.9844,  -8911.9922,  ..., -40026.0000,\n",
      "           14308.0000,  11738.0078],\n",
      "         [ -5428.9961, -25614.0117, -20856.0000,  ...,  27123.0234,\n",
      "           19570.0215,  -9206.9941],\n",
      "         ...,\n",
      "         [-35214.9922,  -2071.0137,  -6981.0039,  ...,  17021.0156,\n",
      "           -3987.0312,  -5340.9941],\n",
      "         [  3932.9980, -13769.9746,    596.0195,  ...,  34478.9922,\n",
      "          -60926.0000,   1389.0039],\n",
      "         [-11103.9902,  11600.9961, -17656.9727,  ...,   1047.0117,\n",
      "           19250.9922,  13373.0098]]]),) and output (tensor([[[-14869.0000,  32284.0000,   3742.9961,  ...,  12458.9980,\n",
      "           -5452.0020,   7590.0020],\n",
      "         [ -9571.9883, -80811.9844, -15199.9922,  ..., -48687.0000,\n",
      "           14728.0000,   9620.0078],\n",
      "         [-13275.9961, -18485.0117, -18709.0000,  ...,  30815.0234,\n",
      "           19459.0215, -11320.9941],\n",
      "         ...,\n",
      "         [-30971.9922,   3773.9863,  -2924.0039,  ...,  16042.0156,\n",
      "           -7002.0312, -13957.9941],\n",
      "         [  3076.9980, -15571.9746,  -4669.9805,  ...,  40159.9922,\n",
      "          -55757.0000,   1729.0039],\n",
      "         [-16855.9902,   8870.9961, -14190.9727,  ...,   7278.0117,\n",
      "           21411.9922,  13430.0098]]]),)\n",
      "[Debug] Layer names being passed: ['model.layers.0', 'model.layers.1', 'model.layers.2', 'model.layers.3', 'model.layers.4', 'model.layers.5', 'model.layers.6', 'model.layers.7', 'model.layers.8', 'model.layers.9', 'model.layers.10', 'model.layers.11', 'model.layers.12', 'model.layers.13', 'model.layers.14', 'model.layers.15', 'model.layers.16', 'model.layers.17', 'model.layers.18', 'model.layers.19', 'model.layers.20', 'model.layers.21', 'model.layers.22', 'model.layers.23', 'model.layers.24', 'model.layers.25', 'model.layers.26', 'model.layers.27', 'model.embed_tokens']\n",
      "[Debug] Trying to access layer: model.layers.0\n",
      "[Debug] Successfully found layer: model.layers.0\n",
      "[Debug] Trying to access layer: model.layers.1\n",
      "[Debug] Successfully found layer: model.layers.1\n",
      "[Debug] Trying to access layer: model.layers.2\n",
      "[Debug] Successfully found layer: model.layers.2\n",
      "[Debug] Trying to access layer: model.layers.3\n",
      "[Debug] Successfully found layer: model.layers.3\n",
      "[Debug] Trying to access layer: model.layers.4\n",
      "[Debug] Successfully found layer: model.layers.4\n",
      "[Debug] Trying to access layer: model.layers.5\n",
      "[Debug] Successfully found layer: model.layers.5\n",
      "[Debug] Trying to access layer: model.layers.6\n",
      "[Debug] Successfully found layer: model.layers.6\n",
      "[Debug] Trying to access layer: model.layers.7\n",
      "[Debug] Successfully found layer: model.layers.7\n",
      "[Debug] Trying to access layer: model.layers.8\n",
      "[Debug] Successfully found layer: model.layers.8\n",
      "[Debug] Trying to access layer: model.layers.9\n",
      "[Debug] Successfully found layer: model.layers.9\n",
      "[Debug] Trying to access layer: model.layers.10\n",
      "[Debug] Successfully found layer: model.layers.10\n",
      "[Debug] Trying to access layer: model.layers.11\n",
      "[Debug] Successfully found layer: model.layers.11\n",
      "[Debug] Trying to access layer: model.layers.12\n",
      "[Debug] Successfully found layer: model.layers.12\n",
      "[Debug] Trying to access layer: model.layers.13\n",
      "[Debug] Successfully found layer: model.layers.13\n",
      "[Debug] Trying to access layer: model.layers.14\n",
      "[Debug] Successfully found layer: model.layers.14\n",
      "[Debug] Trying to access layer: model.layers.15\n",
      "[Debug] Successfully found layer: model.layers.15\n",
      "[Debug] Trying to access layer: model.layers.16\n",
      "[Debug] Successfully found layer: model.layers.16\n",
      "[Debug] Trying to access layer: model.layers.17\n",
      "[Debug] Successfully found layer: model.layers.17\n",
      "[Debug] Trying to access layer: model.layers.18\n",
      "[Debug] Successfully found layer: model.layers.18\n",
      "[Debug] Trying to access layer: model.layers.19\n",
      "[Debug] Successfully found layer: model.layers.19\n",
      "[Debug] Trying to access layer: model.layers.20\n",
      "[Debug] Successfully found layer: model.layers.20\n",
      "[Debug] Trying to access layer: model.layers.21\n",
      "[Debug] Successfully found layer: model.layers.21\n",
      "[Debug] Trying to access layer: model.layers.22\n",
      "[Debug] Successfully found layer: model.layers.22\n",
      "[Debug] Trying to access layer: model.layers.23\n",
      "[Debug] Successfully found layer: model.layers.23\n",
      "[Debug] Trying to access layer: model.layers.24\n",
      "[Debug] Successfully found layer: model.layers.24\n",
      "[Debug] Trying to access layer: model.layers.25\n",
      "[Debug] Successfully found layer: model.layers.25\n",
      "[Debug] Trying to access layer: model.layers.26\n",
      "[Debug] Successfully found layer: model.layers.26\n",
      "[Debug] Trying to access layer: model.layers.27\n",
      "[Debug] Successfully found layer: model.layers.27\n",
      "[Debug] Trying to access layer: model.embed_tokens\n",
      "[Debug] Successfully found layer: model.embed_tokens\n",
      "[Hook] Layer Embedding(128256, 3072, padding_idx=128004) received input (tensor([[128000,  69032,   9777,   1169,  19912,   9777,   1169,    374,    279,\n",
      "           2316,   3752,    323,  46684,    315,  12656,  42482,    596,  31926,\n",
      "           9777,   1169,     13,   1283,    374,    279,  19912,    315,  35440,\n",
      "             11,  63904,    311,    279,  96188,  10194,  62412,   9334,     11,\n",
      "            323,   4538,    315,   6342,   9777,   1169,     11,    279,   3766,\n",
      "           6342,    315,  35440,     13,   2468,    279,   7314,    315,    279,\n",
      "           1514,     11,    568,  28970,    449,   3508,     11,    323,   1268,\n",
      "             11,    311,    264,  53305,    279,  10102,    315,    813,   7126,\n",
      "             11,    323,  28970,    449,    813,   1866,  47942,   3235,    279,\n",
      "           1648,     13,   3296,    279,    842,    315,    279,  31926,     11,\n",
      "           9777,   1169,    706,   9057,    279,  16779,    315,   3735,    263,\n",
      "           9334,     11,   5034,    531,    288,     11,  62412,   9334,     11,\n",
      "            323,   1403,  54627,   3095,    315,    813,    505,    279,   3907,\n",
      "            315,    468,  23257,   7881,  16870,    967,  35534,     89,    323,\n",
      "          33592,    268,    267,    944,     13,   1283,    374,   1101,  46345,\n",
      "           6532,    304,    279,  16779,    315,    813,   3021,    507,    764,\n",
      "          37029,    320,     67,  51520,      8,    323,    315,    813,   6691,\n",
      "          20524,    376,    799,    320,   5481,   3416,    291,    555,  62412,\n",
      "           9334,    555,  16930,    570]]),) and output tensor([[[-0.0010, -0.0007, -0.0046,  ..., -0.0014, -0.0020,  0.0020],\n",
      "         [-0.0110,  0.0175,  0.0168,  ...,  0.0289, -0.0045, -0.0334],\n",
      "         [-0.0183, -0.0447, -0.0112,  ..., -0.0261, -0.0447, -0.0322],\n",
      "         ...,\n",
      "         [ 0.0081, -0.0060,  0.0062,  ..., -0.0149, -0.0048,  0.0339],\n",
      "         [ 0.0131, -0.0229,  0.0177,  ...,  0.0004, -0.0012, -0.0118],\n",
      "         [-0.0198, -0.0332,  0.0024,  ..., -0.0077,  0.0262,  0.0134]]])\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[-0.0010, -0.0007, -0.0046,  ..., -0.0014, -0.0020,  0.0020],\n",
      "         [-0.0110,  0.0175,  0.0168,  ...,  0.0289, -0.0045, -0.0334],\n",
      "         [-0.0183, -0.0447, -0.0112,  ..., -0.0261, -0.0447, -0.0322],\n",
      "         ...,\n",
      "         [ 0.0081, -0.0060,  0.0062,  ..., -0.0149, -0.0048,  0.0339],\n",
      "         [ 0.0131, -0.0229,  0.0177,  ...,  0.0004, -0.0012, -0.0118],\n",
      "         [-0.0198, -0.0332,  0.0024,  ..., -0.0077,  0.0262,  0.0134]]]),) and output (tensor([[[ 442.9990,  239.9993,  302.9954,  ..., -154.0014,  -15.0020,\n",
      "           185.0020],\n",
      "         [ 484.9890,  384.0175,  350.0168,  ..., -236.9711,  177.9955,\n",
      "          -179.0334],\n",
      "         [ 235.9817,  548.9553,  -62.0112,  ..., -438.0261,  -35.0447,\n",
      "          -278.0322],\n",
      "         ...,\n",
      "         [ 199.0081, -189.0060,  168.0062,  ..., -255.0149,  105.9952,\n",
      "             5.0339],\n",
      "         [-104.9869,  155.9771,  155.0177,  ..., -156.9996,  126.9988,\n",
      "           -44.0118],\n",
      "         [ 272.9802,  389.9668,  -56.9976,  ...,  -73.0077,  138.0262,\n",
      "          -193.9866]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[ 442.9990,  239.9993,  302.9954,  ..., -154.0014,  -15.0020,\n",
      "           185.0020],\n",
      "         [ 484.9890,  384.0175,  350.0168,  ..., -236.9711,  177.9955,\n",
      "          -179.0334],\n",
      "         [ 235.9817,  548.9553,  -62.0112,  ..., -438.0261,  -35.0447,\n",
      "          -278.0322],\n",
      "         ...,\n",
      "         [ 199.0081, -189.0060,  168.0062,  ..., -255.0149,  105.9952,\n",
      "             5.0339],\n",
      "         [-104.9869,  155.9771,  155.0177,  ..., -156.9996,  126.9988,\n",
      "           -44.0118],\n",
      "         [ 272.9802,  389.9668,  -56.9976,  ...,  -73.0077,  138.0262,\n",
      "          -193.9866]]]),) and output (tensor([[[ 4.4000e+02, -1.4400e+02, -4.1000e+02,  ..., -7.6500e+02,\n",
      "           1.1940e+03, -1.0280e+03],\n",
      "         [ 1.6500e+03,  4.0175e+00,  2.7402e+02,  ..., -8.9297e+02,\n",
      "           3.0500e+02, -7.3003e+02],\n",
      "         [ 4.1498e+02, -1.0704e+02, -6.8501e+02,  ..., -1.5400e+03,\n",
      "          -4.4678e-02, -2.9303e+02],\n",
      "         ...,\n",
      "         [-5.2599e+02, -1.1301e+02,  1.6050e+03,  ...,  3.3899e+02,\n",
      "           1.0720e+03,  5.3303e+02],\n",
      "         [ 5.3301e+02,  1.3350e+03, -7.4982e+01,  ..., -9.8800e+02,\n",
      "          -8.2700e+02, -6.3901e+02],\n",
      "         [ 7.4598e+02,  2.7997e+02, -5.6600e+02,  ..., -8.7701e+02,\n",
      "          -6.7597e+02, -5.2599e+02]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[ 4.4000e+02, -1.4400e+02, -4.1000e+02,  ..., -7.6500e+02,\n",
      "           1.1940e+03, -1.0280e+03],\n",
      "         [ 1.6500e+03,  4.0175e+00,  2.7402e+02,  ..., -8.9297e+02,\n",
      "           3.0500e+02, -7.3003e+02],\n",
      "         [ 4.1498e+02, -1.0704e+02, -6.8501e+02,  ..., -1.5400e+03,\n",
      "          -4.4678e-02, -2.9303e+02],\n",
      "         ...,\n",
      "         [-5.2599e+02, -1.1301e+02,  1.6050e+03,  ...,  3.3899e+02,\n",
      "           1.0720e+03,  5.3303e+02],\n",
      "         [ 5.3301e+02,  1.3350e+03, -7.4982e+01,  ..., -9.8800e+02,\n",
      "          -8.2700e+02, -6.3901e+02],\n",
      "         [ 7.4598e+02,  2.7997e+02, -5.6600e+02,  ..., -8.7701e+02,\n",
      "          -6.7597e+02, -5.2599e+02]]]),) and output (tensor([[[  551.9990,   993.9993, -1790.0046,  ...,   445.9985,\n",
      "          -1386.0021, -1245.9980],\n",
      "         [-2134.0110,  4313.0176,  2431.0168,  ..., -2522.9712,\n",
      "          -3077.0044,  -504.0334],\n",
      "         [ 4156.9814,  -314.0447,  -653.0112,  ..., -2981.0261,\n",
      "          -1085.0447, -4762.0322],\n",
      "         ...,\n",
      "         [ -991.9919, -2987.0061,  2612.0061,  ...,  1693.9851,\n",
      "            662.9951,  1453.0339],\n",
      "         [  775.0131,    23.9771, -1297.9823,  ..., -4006.9995,\n",
      "          -1265.0012, -4241.0117],\n",
      "         [ 1178.9802,  -578.0332, -3161.9976,  ...,  -146.0077,\n",
      "           -762.9738,   599.0134]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[  551.9990,   993.9993, -1790.0046,  ...,   445.9985,\n",
      "          -1386.0021, -1245.9980],\n",
      "         [-2134.0110,  4313.0176,  2431.0168,  ..., -2522.9712,\n",
      "          -3077.0044,  -504.0334],\n",
      "         [ 4156.9814,  -314.0447,  -653.0112,  ..., -2981.0261,\n",
      "          -1085.0447, -4762.0322],\n",
      "         ...,\n",
      "         [ -991.9919, -2987.0061,  2612.0061,  ...,  1693.9851,\n",
      "            662.9951,  1453.0339],\n",
      "         [  775.0131,    23.9771, -1297.9823,  ..., -4006.9995,\n",
      "          -1265.0012, -4241.0117],\n",
      "         [ 1178.9802,  -578.0332, -3161.9976,  ...,  -146.0077,\n",
      "           -762.9738,   599.0134]]]),) and output (tensor([[[ 2131.9990,  -737.0007, -1071.0046,  ..., -2781.0015,\n",
      "           -885.0021,    86.0020],\n",
      "         [-1254.0110,  -558.9824,  3030.0168,  ..., -5291.9712,\n",
      "            912.9956,  -618.0334],\n",
      "         [10474.9814, -2257.0447,  -377.0112,  ..., -2943.0261,\n",
      "            276.9553, -3842.0322],\n",
      "         ...,\n",
      "         [ 1910.0081, -2196.0061,  5052.0059,  ..., -1690.0149,\n",
      "           -455.0049,   112.0339],\n",
      "         [  491.0132,  -407.0229,   254.0177,  ..., -2985.9995,\n",
      "          -2219.0012, -3706.0117],\n",
      "         [ 4865.9805, -1082.0332, -2066.9976,  ..., -7343.0078,\n",
      "            129.0262,   694.0134]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[ 2131.9990,  -737.0007, -1071.0046,  ..., -2781.0015,\n",
      "           -885.0021,    86.0020],\n",
      "         [-1254.0110,  -558.9824,  3030.0168,  ..., -5291.9712,\n",
      "            912.9956,  -618.0334],\n",
      "         [10474.9814, -2257.0447,  -377.0112,  ..., -2943.0261,\n",
      "            276.9553, -3842.0322],\n",
      "         ...,\n",
      "         [ 1910.0081, -2196.0061,  5052.0059,  ..., -1690.0149,\n",
      "           -455.0049,   112.0339],\n",
      "         [  491.0132,  -407.0229,   254.0177,  ..., -2985.9995,\n",
      "          -2219.0012, -3706.0117],\n",
      "         [ 4865.9805, -1082.0332, -2066.9976,  ..., -7343.0078,\n",
      "            129.0262,   694.0134]]]),) and output (tensor([[[ 3420.9990,   760.9993, -5376.0049,  ..., -4193.0015,\n",
      "           2860.9980, -4080.9980],\n",
      "         [-4453.0107, -3075.9824,  1754.0168,  ..., -4489.9712,\n",
      "          -4189.0044,  6886.9668],\n",
      "         [16649.9805,    75.9553,  1635.9888,  ...,  3850.9739,\n",
      "          -4136.0449, -5049.0322],\n",
      "         ...,\n",
      "         [ 5956.0078,  3749.9939,   809.0059,  ...,  3542.9851,\n",
      "           2451.9951,  2878.0339],\n",
      "         [ 3468.0132,  5019.9771, -1991.9823,  ..., -4067.9995,\n",
      "          -1645.0012, -5063.0117],\n",
      "         [ 4215.9805, -7755.0332, -3135.9976,  ..., -9251.0078,\n",
      "          -6340.9736,   179.0134]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[ 3420.9990,   760.9993, -5376.0049,  ..., -4193.0015,\n",
      "           2860.9980, -4080.9980],\n",
      "         [-4453.0107, -3075.9824,  1754.0168,  ..., -4489.9712,\n",
      "          -4189.0044,  6886.9668],\n",
      "         [16649.9805,    75.9553,  1635.9888,  ...,  3850.9739,\n",
      "          -4136.0449, -5049.0322],\n",
      "         ...,\n",
      "         [ 5956.0078,  3749.9939,   809.0059,  ...,  3542.9851,\n",
      "           2451.9951,  2878.0339],\n",
      "         [ 3468.0132,  5019.9771, -1991.9823,  ..., -4067.9995,\n",
      "          -1645.0012, -5063.0117],\n",
      "         [ 4215.9805, -7755.0332, -3135.9976,  ..., -9251.0078,\n",
      "          -6340.9736,   179.0134]]]),) and output (tensor([[[  2742.9990,  -1512.0007, -10493.0049,  ...,  -9945.0020,\n",
      "           -2017.0020,  -1802.9980],\n",
      "         [  -458.0107, -10661.9824,  -1766.9832,  ...,  -1950.9712,\n",
      "           -2099.0044,   3070.9668],\n",
      "         [ 22617.9805,  -1028.0447,    417.9888,  ...,   6231.9736,\n",
      "           -1069.0449,  -1873.0322],\n",
      "         ...,\n",
      "         [  4886.0078,    -65.0061,   1172.0059,  ...,  -1838.0149,\n",
      "            4281.9951,   5606.0342],\n",
      "         [  7579.0132,   6585.9771,   -992.9824,  ...,  -7778.9995,\n",
      "           -4596.0010,   2888.9883],\n",
      "         [  1885.9805,  -5656.0332,  -3040.9976,  ..., -15812.0078,\n",
      "           -3412.9736,   6177.0137]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[  2742.9990,  -1512.0007, -10493.0049,  ...,  -9945.0020,\n",
      "           -2017.0020,  -1802.9980],\n",
      "         [  -458.0107, -10661.9824,  -1766.9832,  ...,  -1950.9712,\n",
      "           -2099.0044,   3070.9668],\n",
      "         [ 22617.9805,  -1028.0447,    417.9888,  ...,   6231.9736,\n",
      "           -1069.0449,  -1873.0322],\n",
      "         ...,\n",
      "         [  4886.0078,    -65.0061,   1172.0059,  ...,  -1838.0149,\n",
      "            4281.9951,   5606.0342],\n",
      "         [  7579.0132,   6585.9771,   -992.9824,  ...,  -7778.9995,\n",
      "           -4596.0010,   2888.9883],\n",
      "         [  1885.9805,  -5656.0332,  -3040.9976,  ..., -15812.0078,\n",
      "           -3412.9736,   6177.0137]]]),) and output (tensor([[[  3032.9990,  -3420.0007, -10291.0049,  ...,  -6511.0020,\n",
      "           -1953.0020,   2001.0020],\n",
      "         [  4397.9893,  -9255.9824,  -2445.9832,  ...,    876.0288,\n",
      "             634.9956,   2160.9668],\n",
      "         [ 26885.9805,  -4392.0449,   5306.9888,  ...,   8995.9736,\n",
      "           -1150.0449,  -5617.0322],\n",
      "         ...,\n",
      "         [  4970.0078,   3366.9939,   -362.9941,  ...,  -5229.0146,\n",
      "             241.9951,   3319.0342],\n",
      "         [  6482.0137,  15080.9766,  -2928.9824,  ...,  -5934.9995,\n",
      "           -2080.0010,   -931.0117],\n",
      "         [  5675.9805,  -3516.0332,  -3273.9976,  ..., -20885.0078,\n",
      "           -5332.9736,   8171.0137]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[  3032.9990,  -3420.0007, -10291.0049,  ...,  -6511.0020,\n",
      "           -1953.0020,   2001.0020],\n",
      "         [  4397.9893,  -9255.9824,  -2445.9832,  ...,    876.0288,\n",
      "             634.9956,   2160.9668],\n",
      "         [ 26885.9805,  -4392.0449,   5306.9888,  ...,   8995.9736,\n",
      "           -1150.0449,  -5617.0322],\n",
      "         ...,\n",
      "         [  4970.0078,   3366.9939,   -362.9941,  ...,  -5229.0146,\n",
      "             241.9951,   3319.0342],\n",
      "         [  6482.0137,  15080.9766,  -2928.9824,  ...,  -5934.9995,\n",
      "           -2080.0010,   -931.0117],\n",
      "         [  5675.9805,  -3516.0332,  -3273.9976,  ..., -20885.0078,\n",
      "           -5332.9736,   8171.0137]]]),) and output (tensor([[[  3132.9990,  -1970.0007, -11544.0049,  ...,  -5780.0020,\n",
      "           -2486.0020,  -1921.9980],\n",
      "         [  1964.9893, -14201.9824,  -3686.9832,  ...,  -1986.9712,\n",
      "            5522.9956,   3574.9668],\n",
      "         [ 36101.9805,  -9709.0449,   1322.9888,  ...,  12064.9736,\n",
      "           -6194.0449,  -5233.0322],\n",
      "         ...,\n",
      "         [  5670.0078,    561.9939,  -4038.9941,  ...,  -9977.0146,\n",
      "            6117.9951,  11340.0342],\n",
      "         [  4694.0137,  21430.9766,  -4463.9824,  ...,  -8853.0000,\n",
      "           -1797.0010,    164.9883],\n",
      "         [  6691.9805,  -4216.0332,  -3445.9976,  ..., -17789.0078,\n",
      "           -5454.9736,  11718.0137]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[  3132.9990,  -1970.0007, -11544.0049,  ...,  -5780.0020,\n",
      "           -2486.0020,  -1921.9980],\n",
      "         [  1964.9893, -14201.9824,  -3686.9832,  ...,  -1986.9712,\n",
      "            5522.9956,   3574.9668],\n",
      "         [ 36101.9805,  -9709.0449,   1322.9888,  ...,  12064.9736,\n",
      "           -6194.0449,  -5233.0322],\n",
      "         ...,\n",
      "         [  5670.0078,    561.9939,  -4038.9941,  ...,  -9977.0146,\n",
      "            6117.9951,  11340.0342],\n",
      "         [  4694.0137,  21430.9766,  -4463.9824,  ...,  -8853.0000,\n",
      "           -1797.0010,    164.9883],\n",
      "         [  6691.9805,  -4216.0332,  -3445.9976,  ..., -17789.0078,\n",
      "           -5454.9736,  11718.0137]]]),) and output (tensor([[[  1046.9990,   2403.9993, -14486.0049,  ..., -10922.0020,\n",
      "           -1419.0020,  -2983.9980],\n",
      "         [  2231.9893, -13039.9824,  -6900.9834,  ...,  -6445.9712,\n",
      "           12091.9961,   8810.9668],\n",
      "         [ 41722.9805, -10540.0449,   1355.9888,  ...,  15597.9736,\n",
      "          -11977.0449,  -9214.0322],\n",
      "         ...,\n",
      "         [  4742.0078,   4059.9939,  -5788.9941,  ...,  -6371.0146,\n",
      "            4707.9951,  12819.0342],\n",
      "         [  1757.0137,  24821.9766,  -5031.9824,  ...,  -7785.0000,\n",
      "           -6095.0010,  -5281.0117],\n",
      "         [ 12350.9805,  -7257.0332,  -2191.9976,  ..., -18584.0078,\n",
      "           -4992.9736,  13230.0137]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[  1046.9990,   2403.9993, -14486.0049,  ..., -10922.0020,\n",
      "           -1419.0020,  -2983.9980],\n",
      "         [  2231.9893, -13039.9824,  -6900.9834,  ...,  -6445.9712,\n",
      "           12091.9961,   8810.9668],\n",
      "         [ 41722.9805, -10540.0449,   1355.9888,  ...,  15597.9736,\n",
      "          -11977.0449,  -9214.0322],\n",
      "         ...,\n",
      "         [  4742.0078,   4059.9939,  -5788.9941,  ...,  -6371.0146,\n",
      "            4707.9951,  12819.0342],\n",
      "         [  1757.0137,  24821.9766,  -5031.9824,  ...,  -7785.0000,\n",
      "           -6095.0010,  -5281.0117],\n",
      "         [ 12350.9805,  -7257.0332,  -2191.9976,  ..., -18584.0078,\n",
      "           -4992.9736,  13230.0137]]]),) and output (tensor([[[ -3767.0010,   5307.9990, -14221.0049,  ..., -15035.0020,\n",
      "           -3630.0020,   4168.0020],\n",
      "         [  1938.9893, -17356.9824,  -7791.9834,  ..., -14905.9707,\n",
      "           12327.9961,  15477.9668],\n",
      "         [ 43834.9805,  -6860.0449,    781.9888,  ...,  12716.9736,\n",
      "          -16373.0449,  -6355.0322],\n",
      "         ...,\n",
      "         [  3766.0078,   -358.0061,  -7261.9941,  ..., -14023.0146,\n",
      "            1651.9951,  17387.0352],\n",
      "         [  2306.0137,  16473.9766,  -5997.9824,  ...,  -7483.0000,\n",
      "           -6159.0010,  -6590.0117],\n",
      "         [  3713.9805,  -8698.0332,   -859.9976,  ..., -20509.0078,\n",
      "            2779.0264,  17044.0137]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[ -3767.0010,   5307.9990, -14221.0049,  ..., -15035.0020,\n",
      "           -3630.0020,   4168.0020],\n",
      "         [  1938.9893, -17356.9824,  -7791.9834,  ..., -14905.9707,\n",
      "           12327.9961,  15477.9668],\n",
      "         [ 43834.9805,  -6860.0449,    781.9888,  ...,  12716.9736,\n",
      "          -16373.0449,  -6355.0322],\n",
      "         ...,\n",
      "         [  3766.0078,   -358.0061,  -7261.9941,  ..., -14023.0146,\n",
      "            1651.9951,  17387.0352],\n",
      "         [  2306.0137,  16473.9766,  -5997.9824,  ...,  -7483.0000,\n",
      "           -6159.0010,  -6590.0117],\n",
      "         [  3713.9805,  -8698.0332,   -859.9976,  ..., -20509.0078,\n",
      "            2779.0264,  17044.0137]]]),) and output (tensor([[[ -1326.0010,   1223.9990, -16643.0039,  ..., -13240.0020,\n",
      "           -5677.0020,   9639.0020],\n",
      "         [  3194.9893, -24248.9824,  -7331.9834,  ..., -11372.9707,\n",
      "            7724.9961,  16342.9668],\n",
      "         [ 51138.9805,  -8293.0449,   -447.0112,  ...,  11137.9736,\n",
      "          -15055.0449,  -2097.0322],\n",
      "         ...,\n",
      "         [  3084.0078,  -2629.0061,  -9541.9941,  ..., -19025.0156,\n",
      "            6767.9951,  26051.0352],\n",
      "         [  5727.0137,  12705.9766,  -8108.9824,  ...,  -6807.0000,\n",
      "           -8352.0010,  -6374.0117],\n",
      "         [  2531.9805,  -7650.0332,  -1409.9976,  ..., -13519.0078,\n",
      "            6193.0264,  24653.0137]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[ -1326.0010,   1223.9990, -16643.0039,  ..., -13240.0020,\n",
      "           -5677.0020,   9639.0020],\n",
      "         [  3194.9893, -24248.9824,  -7331.9834,  ..., -11372.9707,\n",
      "            7724.9961,  16342.9668],\n",
      "         [ 51138.9805,  -8293.0449,   -447.0112,  ...,  11137.9736,\n",
      "          -15055.0449,  -2097.0322],\n",
      "         ...,\n",
      "         [  3084.0078,  -2629.0061,  -9541.9941,  ..., -19025.0156,\n",
      "            6767.9951,  26051.0352],\n",
      "         [  5727.0137,  12705.9766,  -8108.9824,  ...,  -6807.0000,\n",
      "           -8352.0010,  -6374.0117],\n",
      "         [  2531.9805,  -7650.0332,  -1409.9976,  ..., -13519.0078,\n",
      "            6193.0264,  24653.0137]]]),) and output (tensor([[[ -5120.0010,   2246.9990, -15993.0039,  ..., -19818.0020,\n",
      "           -9385.0020,  11924.0020],\n",
      "         [  3525.9893, -23440.9824,  -8542.9834,  ..., -14417.9707,\n",
      "            3952.9961,  16623.9668],\n",
      "         [ 48042.9805,  -7943.0449,  -1641.0112,  ...,  13880.9736,\n",
      "          -13017.0449,   -966.0322],\n",
      "         ...,\n",
      "         [ -4000.9922,   4074.9939, -10164.9941,  ..., -22309.0156,\n",
      "            2092.9951,  24542.0352],\n",
      "         [  5968.0137,  13609.9766,  -7259.9824,  ...,  -4209.0000,\n",
      "          -10062.0010,   1618.9883],\n",
      "         [ -2038.0195,   1299.9668,   5667.0024,  ...,  -8088.0078,\n",
      "            5421.0264,  17116.0137]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[ -5120.0010,   2246.9990, -15993.0039,  ..., -19818.0020,\n",
      "           -9385.0020,  11924.0020],\n",
      "         [  3525.9893, -23440.9824,  -8542.9834,  ..., -14417.9707,\n",
      "            3952.9961,  16623.9668],\n",
      "         [ 48042.9805,  -7943.0449,  -1641.0112,  ...,  13880.9736,\n",
      "          -13017.0449,   -966.0322],\n",
      "         ...,\n",
      "         [ -4000.9922,   4074.9939, -10164.9941,  ..., -22309.0156,\n",
      "            2092.9951,  24542.0352],\n",
      "         [  5968.0137,  13609.9766,  -7259.9824,  ...,  -4209.0000,\n",
      "          -10062.0010,   1618.9883],\n",
      "         [ -2038.0195,   1299.9668,   5667.0024,  ...,  -8088.0078,\n",
      "            5421.0264,  17116.0137]]]),) and output (tensor([[[ -6685.0010,  -1278.0010, -15553.0039,  ..., -19846.0020,\n",
      "          -10418.0020,  13376.0020],\n",
      "         [  2475.9893, -26643.9824, -10267.9834,  ..., -15797.9707,\n",
      "            2346.9961,  18874.9668],\n",
      "         [ 52676.9805, -10746.0449,  -4151.0112,  ...,  13982.9736,\n",
      "           -7510.0449,    608.9678],\n",
      "         ...,\n",
      "         [  -608.9922,   2693.9941, -13239.9941,  ..., -20184.0156,\n",
      "           -2456.0049,  26356.0352],\n",
      "         [  7150.0137,  15985.9766,  -8615.9824,  ...,  -3157.0000,\n",
      "          -12389.0010,   4312.9883],\n",
      "         [ -4407.0195,   4660.9668,   4677.0024,  ...,  -5100.0078,\n",
      "           10785.0264,  21141.0137]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[ -6685.0010,  -1278.0010, -15553.0039,  ..., -19846.0020,\n",
      "          -10418.0020,  13376.0020],\n",
      "         [  2475.9893, -26643.9824, -10267.9834,  ..., -15797.9707,\n",
      "            2346.9961,  18874.9668],\n",
      "         [ 52676.9805, -10746.0449,  -4151.0112,  ...,  13982.9736,\n",
      "           -7510.0449,    608.9678],\n",
      "         ...,\n",
      "         [  -608.9922,   2693.9941, -13239.9941,  ..., -20184.0156,\n",
      "           -2456.0049,  26356.0352],\n",
      "         [  7150.0137,  15985.9766,  -8615.9824,  ...,  -3157.0000,\n",
      "          -12389.0010,   4312.9883],\n",
      "         [ -4407.0195,   4660.9668,   4677.0024,  ...,  -5100.0078,\n",
      "           10785.0264,  21141.0137]]]),) and output (tensor([[[ -8422.0010,    339.9990, -15289.0039,  ..., -20884.0020,\n",
      "           -6908.0020,  11014.0020],\n",
      "         [  4471.9893, -24478.9824,  -2682.9834,  ..., -16668.9707,\n",
      "            2464.9961,  13831.9668],\n",
      "         [ 55717.9805, -11755.0449,   1091.9888,  ...,  15120.9736,\n",
      "           -7538.0449,    729.9678],\n",
      "         ...,\n",
      "         [  3649.0078,   6011.9941, -17065.9941,  ..., -13677.0156,\n",
      "            3030.9951,  31569.0352],\n",
      "         [  6715.0137,  16270.9766, -11386.9824,  ...,  -1810.0000,\n",
      "           -8051.0010,   2740.9883],\n",
      "         [-10994.0195,  11842.9668,   4382.0024,  ...,  -5556.0078,\n",
      "           14681.0264,  20142.0137]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[ -8422.0010,    339.9990, -15289.0039,  ..., -20884.0020,\n",
      "           -6908.0020,  11014.0020],\n",
      "         [  4471.9893, -24478.9824,  -2682.9834,  ..., -16668.9707,\n",
      "            2464.9961,  13831.9668],\n",
      "         [ 55717.9805, -11755.0449,   1091.9888,  ...,  15120.9736,\n",
      "           -7538.0449,    729.9678],\n",
      "         ...,\n",
      "         [  3649.0078,   6011.9941, -17065.9941,  ..., -13677.0156,\n",
      "            3030.9951,  31569.0352],\n",
      "         [  6715.0137,  16270.9766, -11386.9824,  ...,  -1810.0000,\n",
      "           -8051.0010,   2740.9883],\n",
      "         [-10994.0195,  11842.9668,   4382.0024,  ...,  -5556.0078,\n",
      "           14681.0264,  20142.0137]]]),) and output (tensor([[[-10989.0010,   7034.9990, -13964.0039,  ..., -23968.0020,\n",
      "           -3749.0020,  11730.0020],\n",
      "         [ -2517.0107, -22890.9824, -10356.9834,  ..., -18372.9707,\n",
      "            4480.9961,  13518.9668],\n",
      "         [ 57788.9805, -12240.0449,   -171.0112,  ...,  13871.9736,\n",
      "          -11064.0449,   5149.9678],\n",
      "         ...,\n",
      "         [  8717.0078,  13347.9941, -17979.9941,  ..., -13229.0156,\n",
      "            3218.9951,  30614.0352],\n",
      "         [ 16218.0137,   6495.9766, -10590.9824,  ...,  -4791.0000,\n",
      "          -11263.0010,    740.9883],\n",
      "         [-19907.0195,   6545.9668,   2612.0024,  ..., -11633.0078,\n",
      "           17051.0273,  19091.0137]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[-10989.0010,   7034.9990, -13964.0039,  ..., -23968.0020,\n",
      "           -3749.0020,  11730.0020],\n",
      "         [ -2517.0107, -22890.9824, -10356.9834,  ..., -18372.9707,\n",
      "            4480.9961,  13518.9668],\n",
      "         [ 57788.9805, -12240.0449,   -171.0112,  ...,  13871.9736,\n",
      "          -11064.0449,   5149.9678],\n",
      "         ...,\n",
      "         [  8717.0078,  13347.9941, -17979.9941,  ..., -13229.0156,\n",
      "            3218.9951,  30614.0352],\n",
      "         [ 16218.0137,   6495.9766, -10590.9824,  ...,  -4791.0000,\n",
      "          -11263.0010,    740.9883],\n",
      "         [-19907.0195,   6545.9668,   2612.0024,  ..., -11633.0078,\n",
      "           17051.0273,  19091.0137]]]),) and output (tensor([[[ -7159.0010,   6621.9990, -10967.0039,  ..., -17477.0020,\n",
      "           -2510.0020,  11888.0020],\n",
      "         [ -8689.0107, -31389.9824,  -1388.9834,  ..., -20372.9707,\n",
      "            2341.9961,  15073.9668],\n",
      "         [ 66849.9844,  -9559.0449,  -4334.0112,  ...,  16430.9727,\n",
      "           -7341.0449,   3290.9678],\n",
      "         ...,\n",
      "         [  6735.0078,   9923.9941, -15069.9941,  ..., -13043.0156,\n",
      "            3412.9951,  34219.0352],\n",
      "         [ 18127.0137,   7519.9766, -11377.9824,  ..., -12123.0000,\n",
      "          -15239.0010,   -971.0117],\n",
      "         [-24525.0195,   7043.9668,   2079.0024,  ..., -18585.0078,\n",
      "           14143.0273,  22821.0137]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[ -7159.0010,   6621.9990, -10967.0039,  ..., -17477.0020,\n",
      "           -2510.0020,  11888.0020],\n",
      "         [ -8689.0107, -31389.9824,  -1388.9834,  ..., -20372.9707,\n",
      "            2341.9961,  15073.9668],\n",
      "         [ 66849.9844,  -9559.0449,  -4334.0112,  ...,  16430.9727,\n",
      "           -7341.0449,   3290.9678],\n",
      "         ...,\n",
      "         [  6735.0078,   9923.9941, -15069.9941,  ..., -13043.0156,\n",
      "            3412.9951,  34219.0352],\n",
      "         [ 18127.0137,   7519.9766, -11377.9824,  ..., -12123.0000,\n",
      "          -15239.0010,   -971.0117],\n",
      "         [-24525.0195,   7043.9668,   2079.0024,  ..., -18585.0078,\n",
      "           14143.0273,  22821.0137]]]),) and output (tensor([[[-4.8220e+03,  3.1250e+03, -3.8850e+03,  ..., -1.9007e+04,\n",
      "          -5.5100e+02,  1.6285e+04],\n",
      "         [-9.0740e+03, -3.0693e+04, -2.1380e+03,  ..., -1.3568e+04,\n",
      "           8.8800e+02,  1.5236e+04],\n",
      "         [ 7.5970e+04, -1.0906e+04, -7.0700e+03,  ...,  1.3380e+04,\n",
      "          -1.4410e+03, -2.3032e+01],\n",
      "         ...,\n",
      "         [ 9.5760e+03,  1.8780e+03, -1.7099e+04,  ..., -1.7604e+04,\n",
      "           6.4140e+03,  3.9263e+04],\n",
      "         [ 1.8032e+04,  7.5198e+02, -1.6091e+04,  ..., -2.1586e+04,\n",
      "          -1.1724e+04,  2.9883e+00],\n",
      "         [-1.6209e+04,  4.2760e+03,  4.5650e+03,  ..., -1.6233e+04,\n",
      "           1.4662e+04,  1.6336e+04]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[-4.8220e+03,  3.1250e+03, -3.8850e+03,  ..., -1.9007e+04,\n",
      "          -5.5100e+02,  1.6285e+04],\n",
      "         [-9.0740e+03, -3.0693e+04, -2.1380e+03,  ..., -1.3568e+04,\n",
      "           8.8800e+02,  1.5236e+04],\n",
      "         [ 7.5970e+04, -1.0906e+04, -7.0700e+03,  ...,  1.3380e+04,\n",
      "          -1.4410e+03, -2.3032e+01],\n",
      "         ...,\n",
      "         [ 9.5760e+03,  1.8780e+03, -1.7099e+04,  ..., -1.7604e+04,\n",
      "           6.4140e+03,  3.9263e+04],\n",
      "         [ 1.8032e+04,  7.5198e+02, -1.6091e+04,  ..., -2.1586e+04,\n",
      "          -1.1724e+04,  2.9883e+00],\n",
      "         [-1.6209e+04,  4.2760e+03,  4.5650e+03,  ..., -1.6233e+04,\n",
      "           1.4662e+04,  1.6336e+04]]]),) and output (tensor([[[ -7235.0010,   6583.9990,  -4212.0039,  ..., -19104.0020,\n",
      "            3087.9980,  17084.0020],\n",
      "         [ -5231.0107, -32010.9824,  -3421.9834,  ..., -14755.9707,\n",
      "            8047.9961,  16471.9668],\n",
      "         [ 79909.9844,  -8648.0449,  -9484.0117,  ...,  14489.9727,\n",
      "           -1654.0449,   1562.9678],\n",
      "         ...,\n",
      "         [  7629.0078,  -2251.0059, -16739.9941,  ..., -21934.0156,\n",
      "            1747.9951,  53868.0352],\n",
      "         [ 10494.0137,   5316.9766, -17274.9824,  ..., -25833.0000,\n",
      "          -17228.0000,   2156.9883],\n",
      "         [-13356.0195,  -2543.0332,    321.0024,  ..., -30603.0078,\n",
      "           10914.0273,  12354.0137]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[ -7235.0010,   6583.9990,  -4212.0039,  ..., -19104.0020,\n",
      "            3087.9980,  17084.0020],\n",
      "         [ -5231.0107, -32010.9824,  -3421.9834,  ..., -14755.9707,\n",
      "            8047.9961,  16471.9668],\n",
      "         [ 79909.9844,  -8648.0449,  -9484.0117,  ...,  14489.9727,\n",
      "           -1654.0449,   1562.9678],\n",
      "         ...,\n",
      "         [  7629.0078,  -2251.0059, -16739.9941,  ..., -21934.0156,\n",
      "            1747.9951,  53868.0352],\n",
      "         [ 10494.0137,   5316.9766, -17274.9824,  ..., -25833.0000,\n",
      "          -17228.0000,   2156.9883],\n",
      "         [-13356.0195,  -2543.0332,    321.0024,  ..., -30603.0078,\n",
      "           10914.0273,  12354.0137]]]),) and output (tensor([[[ -5586.0010,   7645.9990,   -505.0039,  ..., -15346.0020,\n",
      "            -623.0020,  12986.0020],\n",
      "         [  1866.9893, -26734.9824,   1158.0166,  ..., -14473.9707,\n",
      "            5318.9961,  22215.9668],\n",
      "         [ 80238.9844,   -880.0449,  -4812.0117,  ...,  21373.9727,\n",
      "            2609.9551,   6448.9678],\n",
      "         ...,\n",
      "         [  5288.0078,   1075.9941, -14376.9941,  ..., -25650.0156,\n",
      "           -7577.0049,  56206.0352],\n",
      "         [ 15182.0137,   7829.9766, -14279.9824,  ..., -31612.0000,\n",
      "          -15868.0000,  -1563.0117],\n",
      "         [-18938.0195,  -8453.0332,  -7350.9976,  ..., -26544.0078,\n",
      "            7122.0273,   8137.0137]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[ -5586.0010,   7645.9990,   -505.0039,  ..., -15346.0020,\n",
      "            -623.0020,  12986.0020],\n",
      "         [  1866.9893, -26734.9824,   1158.0166,  ..., -14473.9707,\n",
      "            5318.9961,  22215.9668],\n",
      "         [ 80238.9844,   -880.0449,  -4812.0117,  ...,  21373.9727,\n",
      "            2609.9551,   6448.9678],\n",
      "         ...,\n",
      "         [  5288.0078,   1075.9941, -14376.9941,  ..., -25650.0156,\n",
      "           -7577.0049,  56206.0352],\n",
      "         [ 15182.0137,   7829.9766, -14279.9824,  ..., -31612.0000,\n",
      "          -15868.0000,  -1563.0117],\n",
      "         [-18938.0195,  -8453.0332,  -7350.9976,  ..., -26544.0078,\n",
      "            7122.0273,   8137.0137]]]),) and output (tensor([[[ -8916.0010,   2362.9990,  -4833.0039,  ..., -12075.0020,\n",
      "           -1124.0020,  12438.0020],\n",
      "         [ -5037.0107, -18532.9824,   6770.0166,  ..., -15537.9707,\n",
      "            2981.9961,  23963.9668],\n",
      "         [ 79649.9844,   4455.9551,  -9628.0117,  ...,  21072.9727,\n",
      "             238.9551,   7672.9678],\n",
      "         ...,\n",
      "         [  1378.0078,  -4435.0059,  -9212.9941,  ..., -27387.0156,\n",
      "           -7898.0049,  59547.0352],\n",
      "         [ 12839.0137,   7199.9766,  -9324.9824,  ..., -25532.0000,\n",
      "          -15199.0000,  -2659.0117],\n",
      "         [-25697.0195,  -5322.0332,  -4047.9976,  ..., -21922.0078,\n",
      "           -1938.9727,   2137.0137]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[ -8916.0010,   2362.9990,  -4833.0039,  ..., -12075.0020,\n",
      "           -1124.0020,  12438.0020],\n",
      "         [ -5037.0107, -18532.9824,   6770.0166,  ..., -15537.9707,\n",
      "            2981.9961,  23963.9668],\n",
      "         [ 79649.9844,   4455.9551,  -9628.0117,  ...,  21072.9727,\n",
      "             238.9551,   7672.9678],\n",
      "         ...,\n",
      "         [  1378.0078,  -4435.0059,  -9212.9941,  ..., -27387.0156,\n",
      "           -7898.0049,  59547.0352],\n",
      "         [ 12839.0137,   7199.9766,  -9324.9824,  ..., -25532.0000,\n",
      "          -15199.0000,  -2659.0117],\n",
      "         [-25697.0195,  -5322.0332,  -4047.9976,  ..., -21922.0078,\n",
      "           -1938.9727,   2137.0137]]]),) and output (tensor([[[-2.0230e+04,  7.8780e+03,  1.6200e+02,  ..., -1.0250e+04,\n",
      "           6.7700e+02,  7.0950e+03],\n",
      "         [-1.5730e+03, -1.9591e+04,  6.7380e+03,  ..., -2.3530e+04,\n",
      "           9.7850e+03,  1.9134e+04],\n",
      "         [ 8.3108e+04,  3.8396e+02, -1.5678e+04,  ...,  1.5352e+04,\n",
      "           9.7200e+03,  1.1705e+04],\n",
      "         ...,\n",
      "         [-2.0100e+03, -6.5780e+03, -7.2320e+03,  ..., -2.6747e+04,\n",
      "          -1.1355e+04,  5.6548e+04],\n",
      "         [ 9.6870e+03,  1.2911e+04, -1.2695e+04,  ..., -2.0300e+04,\n",
      "          -2.0780e+04,  9.6760e+03],\n",
      "         [-2.1665e+04, -4.4940e+03, -2.2320e+03,  ..., -1.6676e+04,\n",
      "           3.7550e+03, -6.4986e+01]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[-2.0230e+04,  7.8780e+03,  1.6200e+02,  ..., -1.0250e+04,\n",
      "           6.7700e+02,  7.0950e+03],\n",
      "         [-1.5730e+03, -1.9591e+04,  6.7380e+03,  ..., -2.3530e+04,\n",
      "           9.7850e+03,  1.9134e+04],\n",
      "         [ 8.3108e+04,  3.8396e+02, -1.5678e+04,  ...,  1.5352e+04,\n",
      "           9.7200e+03,  1.1705e+04],\n",
      "         ...,\n",
      "         [-2.0100e+03, -6.5780e+03, -7.2320e+03,  ..., -2.6747e+04,\n",
      "          -1.1355e+04,  5.6548e+04],\n",
      "         [ 9.6870e+03,  1.2911e+04, -1.2695e+04,  ..., -2.0300e+04,\n",
      "          -2.0780e+04,  9.6760e+03],\n",
      "         [-2.1665e+04, -4.4940e+03, -2.2320e+03,  ..., -1.6676e+04,\n",
      "           3.7550e+03, -6.4986e+01]]]),) and output (tensor([[[-25670.0000,  13293.9990,   9099.9961,  ...,  -6547.0020,\n",
      "           -5658.0020,   4192.0020],\n",
      "         [ -4513.0107, -20669.9824,  12150.0166,  ..., -12071.9707,\n",
      "           10181.9961,  15704.9668],\n",
      "         [ 83111.9844,   3613.9551, -12575.0117,  ...,  21397.9727,\n",
      "            6558.9551,  12626.9678],\n",
      "         ...,\n",
      "         [-12463.9922, -12711.0059,  -3866.9941,  ..., -21180.0156,\n",
      "          -22206.0039,  55764.0352],\n",
      "         [  9440.0137,  14533.9766, -18989.9824,  ..., -31290.0000,\n",
      "          -22923.0000,   1819.9883],\n",
      "         [-26405.0195,  -5265.0332,  -1221.9976,  ..., -15129.0078,\n",
      "           -5637.9727,   4217.0137]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[-25670.0000,  13293.9990,   9099.9961,  ...,  -6547.0020,\n",
      "           -5658.0020,   4192.0020],\n",
      "         [ -4513.0107, -20669.9824,  12150.0166,  ..., -12071.9707,\n",
      "           10181.9961,  15704.9668],\n",
      "         [ 83111.9844,   3613.9551, -12575.0117,  ...,  21397.9727,\n",
      "            6558.9551,  12626.9678],\n",
      "         ...,\n",
      "         [-12463.9922, -12711.0059,  -3866.9941,  ..., -21180.0156,\n",
      "          -22206.0039,  55764.0352],\n",
      "         [  9440.0137,  14533.9766, -18989.9824,  ..., -31290.0000,\n",
      "          -22923.0000,   1819.9883],\n",
      "         [-26405.0195,  -5265.0332,  -1221.9976,  ..., -15129.0078,\n",
      "           -5637.9727,   4217.0137]]]),) and output (tensor([[[-25652.0000,  13180.9990,  15728.9961,  ...,  -2831.0020,\n",
      "             158.9980,   2204.0020],\n",
      "         [ -6987.0107, -26559.9824,  11422.0166,  ..., -14882.9707,\n",
      "           12232.9961,  16176.9668],\n",
      "         [ 76912.9844,   9096.9551,  -6987.0117,  ...,  25117.9727,\n",
      "            3546.9551,  17098.9688],\n",
      "         ...,\n",
      "         [-18216.9922, -15213.0059,  -1414.9941,  ..., -26093.0156,\n",
      "          -22985.0039,  61148.0352],\n",
      "         [  4368.0137,   6290.9766, -14413.9824,  ..., -31169.0000,\n",
      "          -28438.0000,  -3305.0117],\n",
      "         [-25330.0195,  -2398.0332,    112.0024,  ..., -22569.0078,\n",
      "           -2960.9727,   3422.0137]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[-25652.0000,  13180.9990,  15728.9961,  ...,  -2831.0020,\n",
      "             158.9980,   2204.0020],\n",
      "         [ -6987.0107, -26559.9824,  11422.0166,  ..., -14882.9707,\n",
      "           12232.9961,  16176.9668],\n",
      "         [ 76912.9844,   9096.9551,  -6987.0117,  ...,  25117.9727,\n",
      "            3546.9551,  17098.9688],\n",
      "         ...,\n",
      "         [-18216.9922, -15213.0059,  -1414.9941,  ..., -26093.0156,\n",
      "          -22985.0039,  61148.0352],\n",
      "         [  4368.0137,   6290.9766, -14413.9824,  ..., -31169.0000,\n",
      "          -28438.0000,  -3305.0117],\n",
      "         [-25330.0195,  -2398.0332,    112.0024,  ..., -22569.0078,\n",
      "           -2960.9727,   3422.0137]]]),) and output (tensor([[[-14791.0000,  15299.9990,  16129.9961,  ...,   3187.9980,\n",
      "           -2552.0020,   5626.0020],\n",
      "         [ -2739.0107, -25212.9824,  11370.0166,  ..., -14675.9707,\n",
      "            9887.9961,  22174.9668],\n",
      "         [ 81943.9844,   7925.9551,  -3020.0117,  ...,  35289.9727,\n",
      "             876.9551,  21729.9688],\n",
      "         ...,\n",
      "         [-15966.9922, -10913.0059,  -1046.9941,  ..., -28419.0156,\n",
      "          -27258.0039,  54720.0352],\n",
      "         [   208.0137,   6446.9766, -15220.9824,  ..., -24224.0000,\n",
      "          -30463.0000,  -9775.0117],\n",
      "         [-22516.0195,   6721.9668,   8726.0020,  ..., -19414.0078,\n",
      "           -1713.9727,   4834.0137]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[-14791.0000,  15299.9990,  16129.9961,  ...,   3187.9980,\n",
      "           -2552.0020,   5626.0020],\n",
      "         [ -2739.0107, -25212.9824,  11370.0166,  ..., -14675.9707,\n",
      "            9887.9961,  22174.9668],\n",
      "         [ 81943.9844,   7925.9551,  -3020.0117,  ...,  35289.9727,\n",
      "             876.9551,  21729.9688],\n",
      "         ...,\n",
      "         [-15966.9922, -10913.0059,  -1046.9941,  ..., -28419.0156,\n",
      "          -27258.0039,  54720.0352],\n",
      "         [   208.0137,   6446.9766, -15220.9824,  ..., -24224.0000,\n",
      "          -30463.0000,  -9775.0117],\n",
      "         [-22516.0195,   6721.9668,   8726.0020,  ..., -19414.0078,\n",
      "           -1713.9727,   4834.0137]]]),) and output (tensor([[[-13284.0000,  26639.0000,   7467.9961,  ...,   4113.9980,\n",
      "           -6608.0020,   5336.0020],\n",
      "         [  1592.9893, -26302.9824,  11765.0166,  ...,  -9188.9707,\n",
      "           11998.9961,  25480.9668],\n",
      "         [ 84107.9844,   1485.9551,   1533.9883,  ...,  37145.9727,\n",
      "           -9063.0449,  14717.9688],\n",
      "         ...,\n",
      "         [-25758.9922,    995.9941, -13063.9941,  ..., -25671.0156,\n",
      "          -34233.0039,  55343.0352],\n",
      "         [  2686.0137,   7799.9766, -20549.9824,  ..., -27971.0000,\n",
      "          -37509.0000, -15561.0117],\n",
      "         [-25607.0195,  13731.9668,   3833.0020,  ..., -26884.0078,\n",
      "            1736.0273,   4143.0137]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[-13284.0000,  26639.0000,   7467.9961,  ...,   4113.9980,\n",
      "           -6608.0020,   5336.0020],\n",
      "         [  1592.9893, -26302.9824,  11765.0166,  ...,  -9188.9707,\n",
      "           11998.9961,  25480.9668],\n",
      "         [ 84107.9844,   1485.9551,   1533.9883,  ...,  37145.9727,\n",
      "           -9063.0449,  14717.9688],\n",
      "         ...,\n",
      "         [-25758.9922,    995.9941, -13063.9941,  ..., -25671.0156,\n",
      "          -34233.0039,  55343.0352],\n",
      "         [  2686.0137,   7799.9766, -20549.9824,  ..., -27971.0000,\n",
      "          -37509.0000, -15561.0117],\n",
      "         [-25607.0195,  13731.9668,   3833.0020,  ..., -26884.0078,\n",
      "            1736.0273,   4143.0137]]]),) and output (tensor([[[-21532.0000,  32796.0000,  16281.9961,  ...,  18208.9980,\n",
      "           -4282.0020,   5756.0020],\n",
      "         [-11031.0107, -21918.9824,  15878.0166,  ..., -19851.9707,\n",
      "           14524.9961,  23913.9668],\n",
      "         [ 77445.9844,   -253.0449,   3058.9883,  ...,  36679.9727,\n",
      "           -2078.0449,  20667.9688],\n",
      "         ...,\n",
      "         [-25064.9922,  -1500.0059, -12821.9941,  ..., -27706.0156,\n",
      "          -29500.0039,  54031.0352],\n",
      "         [  3284.0137,  -2163.0234, -23679.9824,  ..., -21160.0000,\n",
      "          -38473.0000, -12264.0117],\n",
      "         [-30914.0195,  20612.9668,   8487.0020,  ..., -24854.0078,\n",
      "            4906.0273,  11771.0137]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[-21532.0000,  32796.0000,  16281.9961,  ...,  18208.9980,\n",
      "           -4282.0020,   5756.0020],\n",
      "         [-11031.0107, -21918.9824,  15878.0166,  ..., -19851.9707,\n",
      "           14524.9961,  23913.9668],\n",
      "         [ 77445.9844,   -253.0449,   3058.9883,  ...,  36679.9727,\n",
      "           -2078.0449,  20667.9688],\n",
      "         ...,\n",
      "         [-25064.9922,  -1500.0059, -12821.9941,  ..., -27706.0156,\n",
      "          -29500.0039,  54031.0352],\n",
      "         [  3284.0137,  -2163.0234, -23679.9824,  ..., -21160.0000,\n",
      "          -38473.0000, -12264.0117],\n",
      "         [-30914.0195,  20612.9668,   8487.0020,  ..., -24854.0078,\n",
      "            4906.0273,  11771.0137]]]),) and output (tensor([[[-24082.0000,  33387.0000,  15374.9961,  ...,  18359.9980,\n",
      "           -7500.0020,  11519.0020],\n",
      "         [ -3198.0107, -19254.9824,  24628.0156,  ..., -19232.9707,\n",
      "           17535.9961,  32832.9688],\n",
      "         [ 84097.9844,  -1293.0449,   4818.9883,  ...,  32155.9727,\n",
      "            6722.9551,  26008.9688],\n",
      "         ...,\n",
      "         [-17649.9922,   3783.9941, -23829.9941,  ..., -32953.0156,\n",
      "          -35883.0039,  51780.0352],\n",
      "         [  4161.0137,  -1348.0234, -26909.9824,  ..., -15500.0000,\n",
      "          -29451.0000, -19581.0117],\n",
      "         [-31191.0195,  29878.9668,   5969.0020,  ..., -18478.0078,\n",
      "            3364.0273,  18795.0137]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[-24082.0000,  33387.0000,  15374.9961,  ...,  18359.9980,\n",
      "           -7500.0020,  11519.0020],\n",
      "         [ -3198.0107, -19254.9824,  24628.0156,  ..., -19232.9707,\n",
      "           17535.9961,  32832.9688],\n",
      "         [ 84097.9844,  -1293.0449,   4818.9883,  ...,  32155.9727,\n",
      "            6722.9551,  26008.9688],\n",
      "         ...,\n",
      "         [-17649.9922,   3783.9941, -23829.9941,  ..., -32953.0156,\n",
      "          -35883.0039,  51780.0352],\n",
      "         [  4161.0137,  -1348.0234, -26909.9824,  ..., -15500.0000,\n",
      "          -29451.0000, -19581.0117],\n",
      "         [-31191.0195,  29878.9668,   5969.0020,  ..., -18478.0078,\n",
      "            3364.0273,  18795.0137]]]),) and output (tensor([[[-14869.0000,  32284.0000,   3742.9961,  ...,  12458.9980,\n",
      "           -5452.0020,   7590.0020],\n",
      "         [ -8864.0107, -18694.9824,  33759.0156,  ..., -20035.9707,\n",
      "           15006.9961,  38917.9688],\n",
      "         [ 75652.9844,  -4784.0449,   8156.9883,  ...,  31148.9727,\n",
      "            2467.9551,  29050.9688],\n",
      "         ...,\n",
      "         [-20814.9922,   3035.9941, -30488.9941,  ..., -27838.0156,\n",
      "          -35345.0039,  53153.0352],\n",
      "         [  7989.0137,  -3799.0234, -19587.9824,  ..., -18045.0000,\n",
      "          -29964.0000, -24057.0117],\n",
      "         [-35120.0195,  33271.9688,   6451.0020,  ..., -13678.0078,\n",
      "            2764.0273,   9267.0137]]]),)\n",
      "[Debug] Layer names being passed: ['model.layers.0', 'model.layers.1', 'model.layers.2', 'model.layers.3', 'model.layers.4', 'model.layers.5', 'model.layers.6', 'model.layers.7', 'model.layers.8', 'model.layers.9', 'model.layers.10', 'model.layers.11', 'model.layers.12', 'model.layers.13', 'model.layers.14', 'model.layers.15', 'model.layers.16', 'model.layers.17', 'model.layers.18', 'model.layers.19', 'model.layers.20', 'model.layers.21', 'model.layers.22', 'model.layers.23', 'model.layers.24', 'model.layers.25', 'model.layers.26', 'model.layers.27', 'model.embed_tokens']\n",
      "[Debug] Trying to access layer: model.layers.0\n",
      "[Debug] Successfully found layer: model.layers.0\n",
      "[Debug] Trying to access layer: model.layers.1\n",
      "[Debug] Successfully found layer: model.layers.1\n",
      "[Debug] Trying to access layer: model.layers.2\n",
      "[Debug] Successfully found layer: model.layers.2\n",
      "[Debug] Trying to access layer: model.layers.3\n",
      "[Debug] Successfully found layer: model.layers.3\n",
      "[Debug] Trying to access layer: model.layers.4\n",
      "[Debug] Successfully found layer: model.layers.4\n",
      "[Debug] Trying to access layer: model.layers.5\n",
      "[Debug] Successfully found layer: model.layers.5\n",
      "[Debug] Trying to access layer: model.layers.6\n",
      "[Debug] Successfully found layer: model.layers.6\n",
      "[Debug] Trying to access layer: model.layers.7\n",
      "[Debug] Successfully found layer: model.layers.7\n",
      "[Debug] Trying to access layer: model.layers.8\n",
      "[Debug] Successfully found layer: model.layers.8\n",
      "[Debug] Trying to access layer: model.layers.9\n",
      "[Debug] Successfully found layer: model.layers.9\n",
      "[Debug] Trying to access layer: model.layers.10\n",
      "[Debug] Successfully found layer: model.layers.10\n",
      "[Debug] Trying to access layer: model.layers.11\n",
      "[Debug] Successfully found layer: model.layers.11\n",
      "[Debug] Trying to access layer: model.layers.12\n",
      "[Debug] Successfully found layer: model.layers.12\n",
      "[Debug] Trying to access layer: model.layers.13\n",
      "[Debug] Successfully found layer: model.layers.13\n",
      "[Debug] Trying to access layer: model.layers.14\n",
      "[Debug] Successfully found layer: model.layers.14\n",
      "[Debug] Trying to access layer: model.layers.15\n",
      "[Debug] Successfully found layer: model.layers.15\n",
      "[Debug] Trying to access layer: model.layers.16\n",
      "[Debug] Successfully found layer: model.layers.16\n",
      "[Debug] Trying to access layer: model.layers.17\n",
      "[Debug] Successfully found layer: model.layers.17\n",
      "[Debug] Trying to access layer: model.layers.18\n",
      "[Debug] Successfully found layer: model.layers.18\n",
      "[Debug] Trying to access layer: model.layers.19\n",
      "[Debug] Successfully found layer: model.layers.19\n",
      "[Debug] Trying to access layer: model.layers.20\n",
      "[Debug] Successfully found layer: model.layers.20\n",
      "[Debug] Trying to access layer: model.layers.21\n",
      "[Debug] Successfully found layer: model.layers.21\n",
      "[Debug] Trying to access layer: model.layers.22\n",
      "[Debug] Successfully found layer: model.layers.22\n",
      "[Debug] Trying to access layer: model.layers.23\n",
      "[Debug] Successfully found layer: model.layers.23\n",
      "[Debug] Trying to access layer: model.layers.24\n",
      "[Debug] Successfully found layer: model.layers.24\n",
      "[Debug] Trying to access layer: model.layers.25\n",
      "[Debug] Successfully found layer: model.layers.25\n",
      "[Debug] Trying to access layer: model.layers.26\n",
      "[Debug] Successfully found layer: model.layers.26\n",
      "[Debug] Trying to access layer: model.layers.27\n",
      "[Debug] Successfully found layer: model.layers.27\n",
      "[Debug] Trying to access layer: model.embed_tokens\n",
      "[Debug] Successfully found layer: model.embed_tokens\n",
      "[Hook] Layer Embedding(128256, 3072, padding_idx=128004) received input (tensor([[128000,     32,  59064,  15996,     88,  18449,    330,     32,  59064,\n",
      "          15996,     88,  18449,      1,    374,    264,   3224,   5609,   5439,\n",
      "            555,   4418,  43179,    350,    676,     13,  25842,  25891,    330,\n",
      "           8161,    956,  25672,   3092,  18449,      1,    323,  10887,    555,\n",
      "            578,   2947,  11377,  34179,    304,    220,   2550,     16,     11,\n",
      "           1202,    836,    574,   3010,   5614,    311,    330,     32,  59064,\n",
      "          15996,     88,  18449,      1,    323,  10887,    555,  33919,  13558,\n",
      "          71924,    389,    813,    220,   2550,     17,   8176,   4427,    480,\n",
      "            525,   2052,     13,    578,   5609,    374,  71924,      6,  17755,\n",
      "           3254,    323,  12223,   5609,     11,    433,   1903,   1461,  11495,\n",
      "            323,    706,   1027,    813,   1455,   6992,   5609,     13,   1102,\n",
      "           6244,    279,   1176,   3254,   3596,    311,  11322,  24657,  45092,\n",
      "           2704,    304,   8494,     58,     16,     60,    323,   1101,    220,\n",
      "           2550,     17,    596,   1888,  48724,   3254,    304,    279,   1890,\n",
      "           3224,   8032,     17,   1483,     18,     60,    763,    279,   3723,\n",
      "           4273,    433,   6244,    264,  49480,   4295,    389,   2477,    323,\n",
      "           3224,   9063,     11,   1069,   1802,    520,   1396,    220,     19,\n",
      "            389,    279,  67293,   8166,    220,   1041,    323,  61376,    279,\n",
      "           8166,  14438,  40200,   9676,     11,  10671,    279,   1176,   3224,\n",
      "           3254,    311,    387,  23759,  45092,   2533,  49419,  34467,    323,\n",
      "            423,   8788,   3744,    263,    596,    330,   3957,   8329,    304,\n",
      "            279,   9384,      1,    304,    220,   3753,     18,   8032,     19,\n",
      "             60,    578,   3254,  40901,    304,   3892,   5961,     11,    323,\n",
      "           1306,   1694,  15109,    389,   7054,    315,    279,    393,   3806,\n",
      "            304,    279,   3723,  15422,     11,  78292,    520,   1396,    220,\n",
      "             18,    389,    279,   6560,  47422,  21964,     13,   1102,   8625,\n",
      "          71924,    596,   8706,   4295,   3254,    304,    279,    549,    815,\n",
      "             13,    311,   2457,     11,    323,    813,   1193,    832,    311,\n",
      "           5662,    279,   1948,    220,    605,    315,    279,  67293,   8166,\n",
      "            220,   1041,     13,  11361,    311,    279,   2835,    315,    420,\n",
      "           4295,     11,   1070,    574,    279,  25176,    315,    279,   1584,\n",
      "          15612,   1139,    279,  21391,     11,  10671,    264,  46141,   3059,\n",
      "           8032,     20,   1483,     21,   1483,     22,   1483,     23,     60,\n",
      "            578,   5609,    374,   6646,    555,   1063,    439,    832,    315,\n",
      "            279,  12047,  11936,    315,    682,    892,     11,  16850,    520,\n",
      "           1396,   1403,    304,  99122,     16,    323,  88668,    596,   1160,\n",
      "            315,    279,    330,   1135,   7648,  18371,    288,    316,    989,\n",
      "          11717,  40200,  18374,  61046,     24,     60,  11458,    433,    374,\n",
      "          15324,    439,    264,  66743,   4261,    304,   3224,   4731,   1405,\n",
      "          71924,   7263,  36646,   2802,    304,    264,  23069,  28875,    315,\n",
      "           4731,  24059,  14992,  24475,     13]]),) and output tensor([[[-0.0010, -0.0007, -0.0046,  ..., -0.0014, -0.0020,  0.0020],\n",
      "         [-0.0121,  0.0047, -0.0172,  ..., -0.0012,  0.0217,  0.0070],\n",
      "         [ 0.0062,  0.0117, -0.0542,  ..., -0.0408,  0.0145, -0.0011],\n",
      "         ...,\n",
      "         [ 0.0060,  0.0025,  0.0032,  ...,  0.0007,  0.0065, -0.0295],\n",
      "         [ 0.0381, -0.0093,  0.0248,  ..., -0.0040, -0.0062, -0.0130],\n",
      "         [ 0.0093, -0.0031,  0.0278,  ...,  0.0117, -0.0084,  0.0096]]])\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[-0.0010, -0.0007, -0.0046,  ..., -0.0014, -0.0020,  0.0020],\n",
      "         [-0.0121,  0.0047, -0.0172,  ..., -0.0012,  0.0217,  0.0070],\n",
      "         [ 0.0062,  0.0117, -0.0542,  ..., -0.0408,  0.0145, -0.0011],\n",
      "         ...,\n",
      "         [ 0.0060,  0.0025,  0.0032,  ...,  0.0007,  0.0065, -0.0295],\n",
      "         [ 0.0381, -0.0093,  0.0248,  ..., -0.0040, -0.0062, -0.0130],\n",
      "         [ 0.0093, -0.0031,  0.0278,  ...,  0.0117, -0.0084,  0.0096]]]),) and output (tensor([[[ 442.9990,  239.9993,  302.9954,  ..., -154.0014,  -15.0020,\n",
      "           185.0020],\n",
      "         [-112.0121, -265.9953, -144.0172,  ...,   15.9988,  -92.9783,\n",
      "            -9.9930],\n",
      "         [ 138.0062,  202.0117,  187.9458,  ..., -420.0408,  454.0145,\n",
      "          -391.0011],\n",
      "         ...,\n",
      "         [  12.0060,  -42.9975,  -85.9968,  ...,   -1.9993, -121.9935,\n",
      "           -51.0295],\n",
      "         [ 318.0381, -108.0093, -480.9752,  ..., -678.0040, -223.0062,\n",
      "          -281.0130],\n",
      "         [  27.0093,   -1.0031,  119.0278,  ..., -172.9883,  -19.0084,\n",
      "          -161.9904]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[ 442.9990,  239.9993,  302.9954,  ..., -154.0014,  -15.0020,\n",
      "           185.0020],\n",
      "         [-112.0121, -265.9953, -144.0172,  ...,   15.9988,  -92.9783,\n",
      "            -9.9930],\n",
      "         [ 138.0062,  202.0117,  187.9458,  ..., -420.0408,  454.0145,\n",
      "          -391.0011],\n",
      "         ...,\n",
      "         [  12.0060,  -42.9975,  -85.9968,  ...,   -1.9993, -121.9935,\n",
      "           -51.0295],\n",
      "         [ 318.0381, -108.0093, -480.9752,  ..., -678.0040, -223.0062,\n",
      "          -281.0130],\n",
      "         [  27.0093,   -1.0031,  119.0278,  ..., -172.9883,  -19.0084,\n",
      "          -161.9904]]]),) and output (tensor([[[  439.9990,  -144.0007,  -410.0046,  ...,  -765.0015,\n",
      "           1193.9979, -1027.9980],\n",
      "         [ -324.0121, -1202.9954,   -64.0172,  ...,    25.9988,\n",
      "            465.0217,   118.0070],\n",
      "         [ -491.9938,  -507.9883,   133.9458,  ...,  -110.0408,\n",
      "            837.0145,   251.9989],\n",
      "         ...,\n",
      "         [  628.0060,  -157.9975,  -109.9968,  ...,   805.0007,\n",
      "            367.0065,   491.9705],\n",
      "         [  480.0381,  1102.9907,   -99.9752,  ...,  -451.0040,\n",
      "            217.9938,    23.9870],\n",
      "         [ -984.9907,   390.9969,  -391.9722,  ...,   771.0117,\n",
      "           1323.9917,   270.0096]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[  439.9990,  -144.0007,  -410.0046,  ...,  -765.0015,\n",
      "           1193.9979, -1027.9980],\n",
      "         [ -324.0121, -1202.9954,   -64.0172,  ...,    25.9988,\n",
      "            465.0217,   118.0070],\n",
      "         [ -491.9938,  -507.9883,   133.9458,  ...,  -110.0408,\n",
      "            837.0145,   251.9989],\n",
      "         ...,\n",
      "         [  628.0060,  -157.9975,  -109.9968,  ...,   805.0007,\n",
      "            367.0065,   491.9705],\n",
      "         [  480.0381,  1102.9907,   -99.9752,  ...,  -451.0040,\n",
      "            217.9938,    23.9870],\n",
      "         [ -984.9907,   390.9969,  -391.9722,  ...,   771.0117,\n",
      "           1323.9917,   270.0096]]]),) and output (tensor([[[  551.9990,   993.9993, -1790.0046,  ...,   445.9985,\n",
      "          -1386.0021, -1245.9980],\n",
      "         [  607.9879,   445.0046, -2709.0171,  ...,   343.9988,\n",
      "           2436.0217, -1768.9929],\n",
      "         [ 1469.0062,  -888.9883, -1324.0542,  ...,  2641.9592,\n",
      "          -2918.9856,  4090.9990],\n",
      "         ...,\n",
      "         [-3081.9941,   621.0024, -1264.9968,  ...,  3060.0007,\n",
      "           -991.9935,   124.9705],\n",
      "         [ 1980.0381,   361.9907, -1383.9752,  ...,  1199.9960,\n",
      "          -2598.0063,  1907.9871],\n",
      "         [ -672.9907, -2141.0029, -1587.9722,  ...,  1304.0117,\n",
      "           3268.9917,  2032.0098]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[  551.9990,   993.9993, -1790.0046,  ...,   445.9985,\n",
      "          -1386.0021, -1245.9980],\n",
      "         [  607.9879,   445.0046, -2709.0171,  ...,   343.9988,\n",
      "           2436.0217, -1768.9929],\n",
      "         [ 1469.0062,  -888.9883, -1324.0542,  ...,  2641.9592,\n",
      "          -2918.9856,  4090.9990],\n",
      "         ...,\n",
      "         [-3081.9941,   621.0024, -1264.9968,  ...,  3060.0007,\n",
      "           -991.9935,   124.9705],\n",
      "         [ 1980.0381,   361.9907, -1383.9752,  ...,  1199.9960,\n",
      "          -2598.0063,  1907.9871],\n",
      "         [ -672.9907, -2141.0029, -1587.9722,  ...,  1304.0117,\n",
      "           3268.9917,  2032.0098]]]),) and output (tensor([[[ 2131.9990,  -737.0007, -1071.0046,  ..., -2781.0015,\n",
      "           -885.0021,    86.0020],\n",
      "         [  981.9878,  2861.0046,   458.9829,  ..., -2465.0012,\n",
      "           -930.9783, -3326.9929],\n",
      "         [ 3193.0063, -3564.9883, -8994.0547,  ..., -2585.0408,\n",
      "           -384.9856,   884.9990],\n",
      "         ...,\n",
      "         [-1409.9941,  -118.9976, -2436.9968,  ...,  4358.0010,\n",
      "            366.0065, -2149.0295],\n",
      "         [ 4687.0381, -1590.0093, -3562.9751,  ...,  1378.9960,\n",
      "          -7624.0063,   276.9871],\n",
      "         [  -18.9907, -1785.0029, -3729.9722,  ...,  1387.0117,\n",
      "            917.9917,   -23.9902]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[ 2131.9990,  -737.0007, -1071.0046,  ..., -2781.0015,\n",
      "           -885.0021,    86.0020],\n",
      "         [  981.9878,  2861.0046,   458.9829,  ..., -2465.0012,\n",
      "           -930.9783, -3326.9929],\n",
      "         [ 3193.0063, -3564.9883, -8994.0547,  ..., -2585.0408,\n",
      "           -384.9856,   884.9990],\n",
      "         ...,\n",
      "         [-1409.9941,  -118.9976, -2436.9968,  ...,  4358.0010,\n",
      "            366.0065, -2149.0295],\n",
      "         [ 4687.0381, -1590.0093, -3562.9751,  ...,  1378.9960,\n",
      "          -7624.0063,   276.9871],\n",
      "         [  -18.9907, -1785.0029, -3729.9722,  ...,  1387.0117,\n",
      "            917.9917,   -23.9902]]]),) and output (tensor([[[ 3420.9990,   760.9993, -5376.0049,  ..., -4193.0015,\n",
      "           2860.9980, -4080.9980],\n",
      "         [ 1285.9878,  3693.0046, -1588.0171,  ...,   182.9988,\n",
      "          -3333.9783, -1768.9929],\n",
      "         [ 2138.0063, -4129.9883, -7615.0547,  ...,   937.9592,\n",
      "          -4017.9856, -2794.0010],\n",
      "         ...,\n",
      "         [ 2706.0059, -3300.9976,  3196.0032,  ...,  3594.0010,\n",
      "           1185.0065, -5924.0293],\n",
      "         [ 8389.0381, -1895.0093,  1032.0249,  ...,   262.9961,\n",
      "          -6560.0063,   815.9871],\n",
      "         [-4142.9907, -7529.0029, -2197.9722,  ...,  2939.0117,\n",
      "           6597.9917, -4141.9902]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[ 3420.9990,   760.9993, -5376.0049,  ..., -4193.0015,\n",
      "           2860.9980, -4080.9980],\n",
      "         [ 1285.9878,  3693.0046, -1588.0171,  ...,   182.9988,\n",
      "          -3333.9783, -1768.9929],\n",
      "         [ 2138.0063, -4129.9883, -7615.0547,  ...,   937.9592,\n",
      "          -4017.9856, -2794.0010],\n",
      "         ...,\n",
      "         [ 2706.0059, -3300.9976,  3196.0032,  ...,  3594.0010,\n",
      "           1185.0065, -5924.0293],\n",
      "         [ 8389.0381, -1895.0093,  1032.0249,  ...,   262.9961,\n",
      "          -6560.0063,   815.9871],\n",
      "         [-4142.9907, -7529.0029, -2197.9722,  ...,  2939.0117,\n",
      "           6597.9917, -4141.9902]]]),) and output (tensor([[[  2742.9990,  -1512.0007, -10493.0049,  ...,  -9945.0020,\n",
      "           -2017.0020,  -1802.9980],\n",
      "         [  -642.0122,   3154.0049,   -477.0171,  ...,   2992.9988,\n",
      "           -1538.9783,   1921.0071],\n",
      "         [  3693.0063,  -7202.9883,  -6339.0547,  ...,   1169.9592,\n",
      "            2487.0144,  -4075.0010],\n",
      "         ...,\n",
      "         [   107.0059,   1119.0024,   1791.0032,  ...,  -3251.9990,\n",
      "            -568.9935,  -8615.0293],\n",
      "         [  6790.0381,  -6653.0093,   1887.0249,  ...,   -849.0039,\n",
      "           -9389.0059,   1932.9871],\n",
      "         [ -2774.9907, -11996.0029,  -5507.9722,  ...,   1815.0117,\n",
      "           11537.9922,  -2125.9902]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[  2742.9990,  -1512.0007, -10493.0049,  ...,  -9945.0020,\n",
      "           -2017.0020,  -1802.9980],\n",
      "         [  -642.0122,   3154.0049,   -477.0171,  ...,   2992.9988,\n",
      "           -1538.9783,   1921.0071],\n",
      "         [  3693.0063,  -7202.9883,  -6339.0547,  ...,   1169.9592,\n",
      "            2487.0144,  -4075.0010],\n",
      "         ...,\n",
      "         [   107.0059,   1119.0024,   1791.0032,  ...,  -3251.9990,\n",
      "            -568.9935,  -8615.0293],\n",
      "         [  6790.0381,  -6653.0093,   1887.0249,  ...,   -849.0039,\n",
      "           -9389.0059,   1932.9871],\n",
      "         [ -2774.9907, -11996.0029,  -5507.9722,  ...,   1815.0117,\n",
      "           11537.9922,  -2125.9902]]]),) and output (tensor([[[  3032.9990,  -3420.0007, -10291.0049,  ...,  -6511.0020,\n",
      "           -1953.0020,   2001.0020],\n",
      "         [  1151.9878,    911.0049,  -1875.0171,  ...,   8884.9990,\n",
      "           -6370.9785,   5633.0068],\n",
      "         [  2237.0063,  -5513.9883,  -9814.0547,  ...,   2004.9592,\n",
      "           -6561.9854,  -8303.0010],\n",
      "         ...,\n",
      "         [   -13.9941,   1636.0024,   3634.0032,  ...,  -5352.9990,\n",
      "           -3799.9937,  -8674.0293],\n",
      "         [  6652.0381,  -7542.0093,  -1351.9751,  ...,  -1582.0039,\n",
      "           -7459.0059,   2815.9871],\n",
      "         [ -1260.9907, -13357.0029,  -6903.9722,  ...,  -1489.9883,\n",
      "            8459.9922,   1948.0098]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[  3032.9990,  -3420.0007, -10291.0049,  ...,  -6511.0020,\n",
      "           -1953.0020,   2001.0020],\n",
      "         [  1151.9878,    911.0049,  -1875.0171,  ...,   8884.9990,\n",
      "           -6370.9785,   5633.0068],\n",
      "         [  2237.0063,  -5513.9883,  -9814.0547,  ...,   2004.9592,\n",
      "           -6561.9854,  -8303.0010],\n",
      "         ...,\n",
      "         [   -13.9941,   1636.0024,   3634.0032,  ...,  -5352.9990,\n",
      "           -3799.9937,  -8674.0293],\n",
      "         [  6652.0381,  -7542.0093,  -1351.9751,  ...,  -1582.0039,\n",
      "           -7459.0059,   2815.9871],\n",
      "         [ -1260.9907, -13357.0029,  -6903.9722,  ...,  -1489.9883,\n",
      "            8459.9922,   1948.0098]]]),) and output (tensor([[[  3132.9990,  -1970.0007, -11544.0049,  ...,  -5780.0020,\n",
      "           -2486.0020,  -1921.9980],\n",
      "         [  -412.0122,   7278.0049,  -4753.0171,  ...,  12842.9990,\n",
      "           -8431.9785,   5631.0068],\n",
      "         [  6743.0063, -10106.9883,  -9737.0547,  ...,   4372.9590,\n",
      "           -7052.9854,  -5770.0010],\n",
      "         ...,\n",
      "         [  -602.9941,   -384.9976,    998.0032,  ...,  -5346.9990,\n",
      "           -1991.9937,  -4264.0293],\n",
      "         [  5282.0381,  -9760.0098,  -5375.9751,  ...,   -288.0039,\n",
      "           -2391.0059,   5867.9873],\n",
      "         [ -3166.9907,  -5964.0029,  -4710.9722,  ...,  -2438.9883,\n",
      "            8729.9922,   2771.0098]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[  3132.9990,  -1970.0007, -11544.0049,  ...,  -5780.0020,\n",
      "           -2486.0020,  -1921.9980],\n",
      "         [  -412.0122,   7278.0049,  -4753.0171,  ...,  12842.9990,\n",
      "           -8431.9785,   5631.0068],\n",
      "         [  6743.0063, -10106.9883,  -9737.0547,  ...,   4372.9590,\n",
      "           -7052.9854,  -5770.0010],\n",
      "         ...,\n",
      "         [  -602.9941,   -384.9976,    998.0032,  ...,  -5346.9990,\n",
      "           -1991.9937,  -4264.0293],\n",
      "         [  5282.0381,  -9760.0098,  -5375.9751,  ...,   -288.0039,\n",
      "           -2391.0059,   5867.9873],\n",
      "         [ -3166.9907,  -5964.0029,  -4710.9722,  ...,  -2438.9883,\n",
      "            8729.9922,   2771.0098]]]),) and output (tensor([[[  1046.9990,   2403.9993, -14486.0049,  ..., -10922.0020,\n",
      "           -1419.0020,  -2983.9980],\n",
      "         [ -1361.0122,  14215.0049,  -4703.0171,  ...,  18707.0000,\n",
      "          -10793.9785,   4546.0068],\n",
      "         [  4019.0063,  -9410.9883,  -8898.0547,  ...,   1611.9590,\n",
      "           -1041.9854,  -7427.0010],\n",
      "         ...,\n",
      "         [  3038.0059,  -1578.9976,  -2039.9968,  ...,  -9651.9990,\n",
      "             205.0063,  -3484.0293],\n",
      "         [  1610.0381, -11432.0098,  -5837.9751,  ...,    877.9961,\n",
      "          -10468.0059,   8343.9873],\n",
      "         [  -802.9907,  -2124.0029,  -5558.9722,  ...,  -3590.9883,\n",
      "            6979.9922,   8674.0098]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[  1046.9990,   2403.9993, -14486.0049,  ..., -10922.0020,\n",
      "           -1419.0020,  -2983.9980],\n",
      "         [ -1361.0122,  14215.0049,  -4703.0171,  ...,  18707.0000,\n",
      "          -10793.9785,   4546.0068],\n",
      "         [  4019.0063,  -9410.9883,  -8898.0547,  ...,   1611.9590,\n",
      "           -1041.9854,  -7427.0010],\n",
      "         ...,\n",
      "         [  3038.0059,  -1578.9976,  -2039.9968,  ...,  -9651.9990,\n",
      "             205.0063,  -3484.0293],\n",
      "         [  1610.0381, -11432.0098,  -5837.9751,  ...,    877.9961,\n",
      "          -10468.0059,   8343.9873],\n",
      "         [  -802.9907,  -2124.0029,  -5558.9722,  ...,  -3590.9883,\n",
      "            6979.9922,   8674.0098]]]),) and output (tensor([[[ -3767.0010,   5307.9990, -14221.0049,  ..., -15035.0020,\n",
      "           -3630.0020,   4168.0020],\n",
      "         [ -3938.0122,  13817.0049, -11330.0176,  ...,  19410.0000,\n",
      "          -15594.9785,   3816.0068],\n",
      "         [  -362.9937, -12856.9883,  -5907.0547,  ...,   2310.9590,\n",
      "           -3509.9854,  -8657.0010],\n",
      "         ...,\n",
      "         [ -2552.9941,   3638.0024,  -3223.9968,  ..., -12602.9990,\n",
      "             443.0063,  -4344.0293],\n",
      "         [ -2942.9619, -13732.0098,  -9722.9746,  ...,  -5266.0039,\n",
      "          -12744.0059,   3670.9873],\n",
      "         [ -4960.9907,    -62.0029,  -7939.9722,  ...,  -4130.9883,\n",
      "            5971.9922,   2046.0098]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[ -3767.0010,   5307.9990, -14221.0049,  ..., -15035.0020,\n",
      "           -3630.0020,   4168.0020],\n",
      "         [ -3938.0122,  13817.0049, -11330.0176,  ...,  19410.0000,\n",
      "          -15594.9785,   3816.0068],\n",
      "         [  -362.9937, -12856.9883,  -5907.0547,  ...,   2310.9590,\n",
      "           -3509.9854,  -8657.0010],\n",
      "         ...,\n",
      "         [ -2552.9941,   3638.0024,  -3223.9968,  ..., -12602.9990,\n",
      "             443.0063,  -4344.0293],\n",
      "         [ -2942.9619, -13732.0098,  -9722.9746,  ...,  -5266.0039,\n",
      "          -12744.0059,   3670.9873],\n",
      "         [ -4960.9907,    -62.0029,  -7939.9722,  ...,  -4130.9883,\n",
      "            5971.9922,   2046.0098]]]),) and output (tensor([[[ -1326.0010,   1223.9990, -16643.0039,  ..., -13240.0020,\n",
      "           -5677.0020,   9639.0020],\n",
      "         [ -6096.0122,  15028.0049, -10420.0176,  ...,  12119.0000,\n",
      "          -14210.9785,   8733.0068],\n",
      "         [  1780.0063,  -8018.9883,  -5540.0547,  ...,  -1347.0410,\n",
      "           -5973.9854,  -8678.0010],\n",
      "         ...,\n",
      "         [ -5167.9941,   3020.0024,  -2551.9968,  ..., -18748.0000,\n",
      "            5607.0063, -12762.0293],\n",
      "         [ -4338.9619,  -9383.0098,  -6556.9746,  ...,  -1450.0039,\n",
      "          -14327.0059,   6071.9873],\n",
      "         [ -1530.9907,   3987.9971,  -4344.9722,  ...,  -2267.9883,\n",
      "             995.9922,  -1417.9902]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[ -1326.0010,   1223.9990, -16643.0039,  ..., -13240.0020,\n",
      "           -5677.0020,   9639.0020],\n",
      "         [ -6096.0122,  15028.0049, -10420.0176,  ...,  12119.0000,\n",
      "          -14210.9785,   8733.0068],\n",
      "         [  1780.0063,  -8018.9883,  -5540.0547,  ...,  -1347.0410,\n",
      "           -5973.9854,  -8678.0010],\n",
      "         ...,\n",
      "         [ -5167.9941,   3020.0024,  -2551.9968,  ..., -18748.0000,\n",
      "            5607.0063, -12762.0293],\n",
      "         [ -4338.9619,  -9383.0098,  -6556.9746,  ...,  -1450.0039,\n",
      "          -14327.0059,   6071.9873],\n",
      "         [ -1530.9907,   3987.9971,  -4344.9722,  ...,  -2267.9883,\n",
      "             995.9922,  -1417.9902]]]),) and output (tensor([[[ -5120.0010,   2246.9990, -15993.0039,  ..., -19818.0020,\n",
      "           -9385.0020,  11924.0020],\n",
      "         [ -5203.0122,  15551.0049,  -8000.0176,  ...,  13995.0000,\n",
      "          -14502.9785,   5987.0068],\n",
      "         [   893.0063,  -5770.9883,  -4353.0547,  ...,   3522.9590,\n",
      "          -10431.9854,  -7588.0010],\n",
      "         ...,\n",
      "         [ -1924.9941,  -2266.9976,  -1882.9968,  ..., -14644.0000,\n",
      "            3586.0063, -17009.0293],\n",
      "         [ -8410.9619, -10212.0098,  -8174.9746,  ...,  -1277.0039,\n",
      "           -2739.0059,  -2846.0127],\n",
      "         [ -5262.9907,  -3318.0029,  -5049.9722,  ...,   -960.9883,\n",
      "           -4752.0078,  -5659.9902]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[ -5120.0010,   2246.9990, -15993.0039,  ..., -19818.0020,\n",
      "           -9385.0020,  11924.0020],\n",
      "         [ -5203.0122,  15551.0049,  -8000.0176,  ...,  13995.0000,\n",
      "          -14502.9785,   5987.0068],\n",
      "         [   893.0063,  -5770.9883,  -4353.0547,  ...,   3522.9590,\n",
      "          -10431.9854,  -7588.0010],\n",
      "         ...,\n",
      "         [ -1924.9941,  -2266.9976,  -1882.9968,  ..., -14644.0000,\n",
      "            3586.0063, -17009.0293],\n",
      "         [ -8410.9619, -10212.0098,  -8174.9746,  ...,  -1277.0039,\n",
      "           -2739.0059,  -2846.0127],\n",
      "         [ -5262.9907,  -3318.0029,  -5049.9722,  ...,   -960.9883,\n",
      "           -4752.0078,  -5659.9902]]]),) and output (tensor([[[ -6685.0010,  -1278.0010, -15553.0039,  ..., -19846.0020,\n",
      "          -10418.0020,  13376.0020],\n",
      "         [ -9661.0117,  15569.0049,  -7217.0176,  ...,   7485.0000,\n",
      "          -20229.9785,  14204.0068],\n",
      "         [  1437.0063,  -1010.9883,  -1159.0547,  ...,   8113.9590,\n",
      "          -12633.9854, -10359.0010],\n",
      "         ...,\n",
      "         [ -3925.9941,  -6078.9976,  -2769.9968,  ..., -14216.0000,\n",
      "            2094.0063, -15507.0293],\n",
      "         [ -8824.9619, -11743.0098,  -7839.9746,  ...,  -1215.0039,\n",
      "           -2881.0059,  -5903.0127],\n",
      "         [ -6451.9907,   3424.9971,  -3992.9722,  ...,  -3048.9883,\n",
      "          -13208.0078,  -3450.9902]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[ -6685.0010,  -1278.0010, -15553.0039,  ..., -19846.0020,\n",
      "          -10418.0020,  13376.0020],\n",
      "         [ -9661.0117,  15569.0049,  -7217.0176,  ...,   7485.0000,\n",
      "          -20229.9785,  14204.0068],\n",
      "         [  1437.0063,  -1010.9883,  -1159.0547,  ...,   8113.9590,\n",
      "          -12633.9854, -10359.0010],\n",
      "         ...,\n",
      "         [ -3925.9941,  -6078.9976,  -2769.9968,  ..., -14216.0000,\n",
      "            2094.0063, -15507.0293],\n",
      "         [ -8824.9619, -11743.0098,  -7839.9746,  ...,  -1215.0039,\n",
      "           -2881.0059,  -5903.0127],\n",
      "         [ -6451.9907,   3424.9971,  -3992.9722,  ...,  -3048.9883,\n",
      "          -13208.0078,  -3450.9902]]]),) and output (tensor([[[ -8422.0010,    339.9990, -15289.0039,  ..., -20884.0020,\n",
      "           -6908.0020,  11014.0020],\n",
      "         [ -7062.0117,  23751.0039, -10938.0176,  ...,   4292.0000,\n",
      "          -20474.9785,  11771.0068],\n",
      "         [  1587.0063,  -3508.9883,    774.9453,  ...,   8936.9590,\n",
      "          -18978.9844, -18078.0000],\n",
      "         ...,\n",
      "         [-11217.9941,  -2821.9976,  -4098.9971,  ..., -14661.0000,\n",
      "            1179.0063, -14389.0293],\n",
      "         [-15388.9619,  -8954.0098, -12942.9746,  ...,  -4904.0039,\n",
      "           -6609.0059,  -2824.0127],\n",
      "         [-11749.9902,   -172.0029,  -2081.9722,  ...,   -358.9883,\n",
      "          -15046.0078,  -3477.9902]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[ -8422.0010,    339.9990, -15289.0039,  ..., -20884.0020,\n",
      "           -6908.0020,  11014.0020],\n",
      "         [ -7062.0117,  23751.0039, -10938.0176,  ...,   4292.0000,\n",
      "          -20474.9785,  11771.0068],\n",
      "         [  1587.0063,  -3508.9883,    774.9453,  ...,   8936.9590,\n",
      "          -18978.9844, -18078.0000],\n",
      "         ...,\n",
      "         [-11217.9941,  -2821.9976,  -4098.9971,  ..., -14661.0000,\n",
      "            1179.0063, -14389.0293],\n",
      "         [-15388.9619,  -8954.0098, -12942.9746,  ...,  -4904.0039,\n",
      "           -6609.0059,  -2824.0127],\n",
      "         [-11749.9902,   -172.0029,  -2081.9722,  ...,   -358.9883,\n",
      "          -15046.0078,  -3477.9902]]]),) and output (tensor([[[-10989.0010,   7034.9990, -13964.0039,  ..., -23968.0020,\n",
      "           -3749.0020,  11730.0020],\n",
      "         [ -6384.0117,  27700.0039, -11593.0176,  ...,  -1661.0000,\n",
      "          -21851.9785,  11293.0068],\n",
      "         [   577.0063,   -427.9883,   4123.9453,  ...,  10342.9590,\n",
      "          -19003.9844, -13177.0000],\n",
      "         ...,\n",
      "         [ -9669.9941,   1210.0024,  -4324.9971,  ..., -19400.0000,\n",
      "            3767.0063, -17414.0293],\n",
      "         [-15478.9619, -11281.0098, -14321.9746,  ...,  -3758.0039,\n",
      "          -12942.0059,  -2683.0127],\n",
      "         [ -9549.9902,   1463.9971,    451.0278,  ...,   1967.0117,\n",
      "           -9112.0078,  -2120.9902]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[-10989.0010,   7034.9990, -13964.0039,  ..., -23968.0020,\n",
      "           -3749.0020,  11730.0020],\n",
      "         [ -6384.0117,  27700.0039, -11593.0176,  ...,  -1661.0000,\n",
      "          -21851.9785,  11293.0068],\n",
      "         [   577.0063,   -427.9883,   4123.9453,  ...,  10342.9590,\n",
      "          -19003.9844, -13177.0000],\n",
      "         ...,\n",
      "         [ -9669.9941,   1210.0024,  -4324.9971,  ..., -19400.0000,\n",
      "            3767.0063, -17414.0293],\n",
      "         [-15478.9619, -11281.0098, -14321.9746,  ...,  -3758.0039,\n",
      "          -12942.0059,  -2683.0127],\n",
      "         [ -9549.9902,   1463.9971,    451.0278,  ...,   1967.0117,\n",
      "           -9112.0078,  -2120.9902]]]),) and output (tensor([[[ -7159.0010,   6621.9990, -10967.0039,  ..., -17477.0020,\n",
      "           -2510.0020,  11888.0020],\n",
      "         [ -7178.0117,  28179.0039, -12116.0176,  ...,  -6400.0000,\n",
      "          -23346.9785,  13812.0068],\n",
      "         [   260.0063,   2131.0117,   3892.9453,  ...,  11731.9590,\n",
      "          -16032.9844, -14310.0000],\n",
      "         ...,\n",
      "         [ -6465.9941,  -4393.9976,  -4950.9971,  ..., -17175.0000,\n",
      "            7356.0063, -10342.0293],\n",
      "         [-16970.9609,  -6467.0098, -10987.9746,  ...,  -1090.0039,\n",
      "           -7223.0059,    326.9873],\n",
      "         [ -6689.9902,    916.9971,  -2592.9722,  ...,   4565.0117,\n",
      "           -5174.0078,   -832.9902]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[ -7159.0010,   6621.9990, -10967.0039,  ..., -17477.0020,\n",
      "           -2510.0020,  11888.0020],\n",
      "         [ -7178.0117,  28179.0039, -12116.0176,  ...,  -6400.0000,\n",
      "          -23346.9785,  13812.0068],\n",
      "         [   260.0063,   2131.0117,   3892.9453,  ...,  11731.9590,\n",
      "          -16032.9844, -14310.0000],\n",
      "         ...,\n",
      "         [ -6465.9941,  -4393.9976,  -4950.9971,  ..., -17175.0000,\n",
      "            7356.0063, -10342.0293],\n",
      "         [-16970.9609,  -6467.0098, -10987.9746,  ...,  -1090.0039,\n",
      "           -7223.0059,    326.9873],\n",
      "         [ -6689.9902,    916.9971,  -2592.9722,  ...,   4565.0117,\n",
      "           -5174.0078,   -832.9902]]]),) and output (tensor([[[ -4822.0010,   3124.9990,  -3885.0039,  ..., -19007.0020,\n",
      "            -551.0020,  16285.0020],\n",
      "         [ -3368.0117,  29422.0039,  -7631.0176,  ...,  -4357.0000,\n",
      "          -22257.9785,  13533.0068],\n",
      "         [  3495.0063,    665.0117,   3767.9453,  ...,  11752.9590,\n",
      "          -16832.9844, -18762.0000],\n",
      "         ...,\n",
      "         [ -5584.9941,   3275.0024,   -197.9971,  ..., -15497.0000,\n",
      "            3192.0063, -17044.0293],\n",
      "         [-10854.9609,  -6833.0098, -12398.9746,  ...,   -511.0039,\n",
      "           -5830.0059,   5226.9873],\n",
      "         [-13203.9902,  -1322.0029,  -1325.9722,  ...,  11722.0117,\n",
      "           -8119.0078,  -4060.9902]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[ -4822.0010,   3124.9990,  -3885.0039,  ..., -19007.0020,\n",
      "            -551.0020,  16285.0020],\n",
      "         [ -3368.0117,  29422.0039,  -7631.0176,  ...,  -4357.0000,\n",
      "          -22257.9785,  13533.0068],\n",
      "         [  3495.0063,    665.0117,   3767.9453,  ...,  11752.9590,\n",
      "          -16832.9844, -18762.0000],\n",
      "         ...,\n",
      "         [ -5584.9941,   3275.0024,   -197.9971,  ..., -15497.0000,\n",
      "            3192.0063, -17044.0293],\n",
      "         [-10854.9609,  -6833.0098, -12398.9746,  ...,   -511.0039,\n",
      "           -5830.0059,   5226.9873],\n",
      "         [-13203.9902,  -1322.0029,  -1325.9722,  ...,  11722.0117,\n",
      "           -8119.0078,  -4060.9902]]]),) and output (tensor([[[ -7235.0010,   6583.9990,  -4212.0039,  ..., -19104.0020,\n",
      "            3087.9980,  17084.0020],\n",
      "         [ -2404.0117,  31869.0039, -11400.0176,  ...,  -5137.0000,\n",
      "          -23399.9785,  17602.0078],\n",
      "         [  8197.0059,  -1896.9883,   7127.9453,  ...,  13701.9590,\n",
      "           -7991.9844, -16700.0000],\n",
      "         ...,\n",
      "         [ -6166.9941,   4214.0024,   4332.0029,  ..., -19050.0000,\n",
      "            5106.0063, -21115.0293],\n",
      "         [-19369.9609, -11911.0098, -15729.9746,  ...,   -771.0039,\n",
      "           -2690.0059,  10924.9873],\n",
      "         [-14832.9902,   5388.9971,   2695.0278,  ...,  13088.0117,\n",
      "           -4264.0078,  -3676.9902]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[ -7235.0010,   6583.9990,  -4212.0039,  ..., -19104.0020,\n",
      "            3087.9980,  17084.0020],\n",
      "         [ -2404.0117,  31869.0039, -11400.0176,  ...,  -5137.0000,\n",
      "          -23399.9785,  17602.0078],\n",
      "         [  8197.0059,  -1896.9883,   7127.9453,  ...,  13701.9590,\n",
      "           -7991.9844, -16700.0000],\n",
      "         ...,\n",
      "         [ -6166.9941,   4214.0024,   4332.0029,  ..., -19050.0000,\n",
      "            5106.0063, -21115.0293],\n",
      "         [-19369.9609, -11911.0098, -15729.9746,  ...,   -771.0039,\n",
      "           -2690.0059,  10924.9873],\n",
      "         [-14832.9902,   5388.9971,   2695.0278,  ...,  13088.0117,\n",
      "           -4264.0078,  -3676.9902]]]),) and output (tensor([[[ -5586.0010,   7645.9990,   -505.0039,  ..., -15346.0020,\n",
      "            -623.0020,  12986.0020],\n",
      "         [  2237.9883,  37127.0039,  -8913.0176,  ...,  -3254.0000,\n",
      "          -28232.9785,  17718.0078],\n",
      "         [ 13153.0059,   -699.9883,   9900.9453,  ...,  10892.9590,\n",
      "          -11018.9844, -16924.0000],\n",
      "         ...,\n",
      "         [ -6484.9941,   6824.0024,  13310.0029,  ..., -17336.0000,\n",
      "            5718.0063, -19409.0293],\n",
      "         [-15821.9609, -15200.0098, -13676.9746,  ...,  -6012.0039,\n",
      "           -1627.0059,  -2557.0127],\n",
      "         [-19723.9902,   1511.9971,   8804.0273,  ...,  26671.0117,\n",
      "           -7804.0078,  -8865.9902]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[ -5586.0010,   7645.9990,   -505.0039,  ..., -15346.0020,\n",
      "            -623.0020,  12986.0020],\n",
      "         [  2237.9883,  37127.0039,  -8913.0176,  ...,  -3254.0000,\n",
      "          -28232.9785,  17718.0078],\n",
      "         [ 13153.0059,   -699.9883,   9900.9453,  ...,  10892.9590,\n",
      "          -11018.9844, -16924.0000],\n",
      "         ...,\n",
      "         [ -6484.9941,   6824.0024,  13310.0029,  ..., -17336.0000,\n",
      "            5718.0063, -19409.0293],\n",
      "         [-15821.9609, -15200.0098, -13676.9746,  ...,  -6012.0039,\n",
      "           -1627.0059,  -2557.0127],\n",
      "         [-19723.9902,   1511.9971,   8804.0273,  ...,  26671.0117,\n",
      "           -7804.0078,  -8865.9902]]]),) and output (tensor([[[-8.9160e+03,  2.3630e+03, -4.8330e+03,  ..., -1.2075e+04,\n",
      "          -1.1240e+03,  1.2438e+04],\n",
      "         [ 7.9883e+00,  3.4662e+04, -6.1450e+03,  ..., -9.4200e+02,\n",
      "          -2.6744e+04,  2.1865e+04],\n",
      "         [ 7.8300e+03,  3.8700e+03,  1.7653e+04,  ...,  1.1620e+04,\n",
      "          -1.9126e+04, -1.1932e+04],\n",
      "         ...,\n",
      "         [-1.5410e+03,  7.2840e+03,  1.4338e+04,  ..., -1.8433e+04,\n",
      "           9.7710e+03, -1.0466e+04],\n",
      "         [-1.7226e+04, -1.1340e+04, -1.0648e+04,  ..., -1.2891e+04,\n",
      "           5.5160e+03,  8.6299e+02],\n",
      "         [-2.7091e+04,  1.0035e+04,  6.6740e+03,  ...,  2.6783e+04,\n",
      "          -1.2700e+04, -1.0090e+04]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[-8.9160e+03,  2.3630e+03, -4.8330e+03,  ..., -1.2075e+04,\n",
      "          -1.1240e+03,  1.2438e+04],\n",
      "         [ 7.9883e+00,  3.4662e+04, -6.1450e+03,  ..., -9.4200e+02,\n",
      "          -2.6744e+04,  2.1865e+04],\n",
      "         [ 7.8300e+03,  3.8700e+03,  1.7653e+04,  ...,  1.1620e+04,\n",
      "          -1.9126e+04, -1.1932e+04],\n",
      "         ...,\n",
      "         [-1.5410e+03,  7.2840e+03,  1.4338e+04,  ..., -1.8433e+04,\n",
      "           9.7710e+03, -1.0466e+04],\n",
      "         [-1.7226e+04, -1.1340e+04, -1.0648e+04,  ..., -1.2891e+04,\n",
      "           5.5160e+03,  8.6299e+02],\n",
      "         [-2.7091e+04,  1.0035e+04,  6.6740e+03,  ...,  2.6783e+04,\n",
      "          -1.2700e+04, -1.0090e+04]]]),) and output (tensor([[[-20230.0000,   7877.9990,    161.9961,  ..., -10250.0020,\n",
      "             676.9980,   7095.0020],\n",
      "         [ -2392.0117,  38946.0039,  -4252.0176,  ...,   1809.0000,\n",
      "          -33538.9766,  21034.0078],\n",
      "         [  8743.0059,   1100.0117,  24892.9453,  ...,  14179.9590,\n",
      "          -23597.9844, -23538.0000],\n",
      "         ...,\n",
      "         [  1190.0059,   8894.0020,   7199.0029,  ..., -13748.0000,\n",
      "           11792.0059, -17481.0293],\n",
      "         [-21285.9609, -12598.0098,  -8219.9746,  ..., -13923.0039,\n",
      "              95.9941,  -8217.0127],\n",
      "         [-27944.9902,  16816.9961,   7225.0273,  ...,  29880.0117,\n",
      "          -10307.0078,  -2752.9902]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[-20230.0000,   7877.9990,    161.9961,  ..., -10250.0020,\n",
      "             676.9980,   7095.0020],\n",
      "         [ -2392.0117,  38946.0039,  -4252.0176,  ...,   1809.0000,\n",
      "          -33538.9766,  21034.0078],\n",
      "         [  8743.0059,   1100.0117,  24892.9453,  ...,  14179.9590,\n",
      "          -23597.9844, -23538.0000],\n",
      "         ...,\n",
      "         [  1190.0059,   8894.0020,   7199.0029,  ..., -13748.0000,\n",
      "           11792.0059, -17481.0293],\n",
      "         [-21285.9609, -12598.0098,  -8219.9746,  ..., -13923.0039,\n",
      "              95.9941,  -8217.0127],\n",
      "         [-27944.9902,  16816.9961,   7225.0273,  ...,  29880.0117,\n",
      "          -10307.0078,  -2752.9902]]]),) and output (tensor([[[-25670.0000,  13293.9990,   9099.9961,  ...,  -6547.0020,\n",
      "           -5658.0020,   4192.0020],\n",
      "         [  -745.0117,  43661.0039, -10709.0176,  ...,  -1138.0000,\n",
      "          -36077.9766,  30980.0078],\n",
      "         [  5070.0059,   -932.9883,  38207.9453,  ...,  12994.9590,\n",
      "          -30616.9844, -36663.0000],\n",
      "         ...,\n",
      "         [   661.0059,   7430.0020,  10526.0029,  ..., -12493.0000,\n",
      "            5402.0059, -27086.0293],\n",
      "         [-14526.9609, -15239.0098,  -6831.9746,  ...,  -9602.0039,\n",
      "          -10042.0059,  -2763.0127],\n",
      "         [-33433.9922,  18720.9961,   4366.0273,  ...,  21073.0117,\n",
      "           -7654.0078,    840.0098]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[-25670.0000,  13293.9990,   9099.9961,  ...,  -6547.0020,\n",
      "           -5658.0020,   4192.0020],\n",
      "         [  -745.0117,  43661.0039, -10709.0176,  ...,  -1138.0000,\n",
      "          -36077.9766,  30980.0078],\n",
      "         [  5070.0059,   -932.9883,  38207.9453,  ...,  12994.9590,\n",
      "          -30616.9844, -36663.0000],\n",
      "         ...,\n",
      "         [   661.0059,   7430.0020,  10526.0029,  ..., -12493.0000,\n",
      "            5402.0059, -27086.0293],\n",
      "         [-14526.9609, -15239.0098,  -6831.9746,  ...,  -9602.0039,\n",
      "          -10042.0059,  -2763.0127],\n",
      "         [-33433.9922,  18720.9961,   4366.0273,  ...,  21073.0117,\n",
      "           -7654.0078,    840.0098]]]),) and output (tensor([[[-25652.0000,  13180.9990,  15728.9961,  ...,  -2831.0020,\n",
      "             158.9980,   2204.0020],\n",
      "         [ -1911.0117,  51467.0039,  -8510.0176,  ...,  -7280.0000,\n",
      "          -44896.9766,  22765.0078],\n",
      "         [  3815.0059,    867.0117,  43463.9453,  ...,   6133.9590,\n",
      "          -27054.9844, -40761.0000],\n",
      "         ...,\n",
      "         [   616.0059,   8933.0020,  15162.0029,  ..., -10570.0000,\n",
      "           -4483.9941, -26383.0293],\n",
      "         [-12628.9609,  -3793.0098, -10811.9746,  ...,  -4082.0039,\n",
      "           -9561.0059, -11001.0127],\n",
      "         [-31381.9922,  15210.9961,   4955.0273,  ...,  30256.0117,\n",
      "           -4518.0078,  -4886.9902]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[-25652.0000,  13180.9990,  15728.9961,  ...,  -2831.0020,\n",
      "             158.9980,   2204.0020],\n",
      "         [ -1911.0117,  51467.0039,  -8510.0176,  ...,  -7280.0000,\n",
      "          -44896.9766,  22765.0078],\n",
      "         [  3815.0059,    867.0117,  43463.9453,  ...,   6133.9590,\n",
      "          -27054.9844, -40761.0000],\n",
      "         ...,\n",
      "         [   616.0059,   8933.0020,  15162.0029,  ..., -10570.0000,\n",
      "           -4483.9941, -26383.0293],\n",
      "         [-12628.9609,  -3793.0098, -10811.9746,  ...,  -4082.0039,\n",
      "           -9561.0059, -11001.0127],\n",
      "         [-31381.9922,  15210.9961,   4955.0273,  ...,  30256.0117,\n",
      "           -4518.0078,  -4886.9902]]]),) and output (tensor([[[-14791.0000,  15299.9990,  16129.9961,  ...,   3187.9980,\n",
      "           -2552.0020,   5626.0020],\n",
      "         [ -3879.0117,  37861.0039, -14850.0176,  ...,  -7245.0000,\n",
      "          -47695.9766,  32740.0078],\n",
      "         [ 10986.0059,   6291.0117,  46571.9453,  ...,   3935.9590,\n",
      "          -25398.9844, -35042.0000],\n",
      "         ...,\n",
      "         [  9331.0059,   5933.0020,  20680.0039,  ...,  -3655.0000,\n",
      "           -5251.9941, -26472.0293],\n",
      "         [-21589.9609,   2415.9902,  -5358.9746,  ...,  -9053.0039,\n",
      "          -14937.0059, -13791.0127],\n",
      "         [-33248.9922,  16374.9961,   3647.0273,  ...,  28046.0117,\n",
      "          -12714.0078,  -2182.9902]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[-14791.0000,  15299.9990,  16129.9961,  ...,   3187.9980,\n",
      "           -2552.0020,   5626.0020],\n",
      "         [ -3879.0117,  37861.0039, -14850.0176,  ...,  -7245.0000,\n",
      "          -47695.9766,  32740.0078],\n",
      "         [ 10986.0059,   6291.0117,  46571.9453,  ...,   3935.9590,\n",
      "          -25398.9844, -35042.0000],\n",
      "         ...,\n",
      "         [  9331.0059,   5933.0020,  20680.0039,  ...,  -3655.0000,\n",
      "           -5251.9941, -26472.0293],\n",
      "         [-21589.9609,   2415.9902,  -5358.9746,  ...,  -9053.0039,\n",
      "          -14937.0059, -13791.0127],\n",
      "         [-33248.9922,  16374.9961,   3647.0273,  ...,  28046.0117,\n",
      "          -12714.0078,  -2182.9902]]]),) and output (tensor([[[-13284.0000,  26639.0000,   7467.9961,  ...,   4113.9980,\n",
      "           -6608.0020,   5336.0020],\n",
      "         [   -80.0117,  50155.0039, -17164.0176,  ...,   2598.0000,\n",
      "          -50510.9766,  41294.0078],\n",
      "         [ 15804.0059,   2016.0117,  50600.9453,  ...,    349.9590,\n",
      "          -27313.9844, -36747.0000],\n",
      "         ...,\n",
      "         [  8803.0059,    -71.9980,  28533.0039,  ...,  -2631.0000,\n",
      "           -9099.9941, -22858.0293],\n",
      "         [-28764.9609,   9230.9902,  -8177.9746,  ...,  -9450.0039,\n",
      "          -15130.0059,  -9617.0127],\n",
      "         [-40285.9922,  13948.9961,   1968.0273,  ...,  25769.0117,\n",
      "          -15283.0078,  -1826.9902]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[-13284.0000,  26639.0000,   7467.9961,  ...,   4113.9980,\n",
      "           -6608.0020,   5336.0020],\n",
      "         [   -80.0117,  50155.0039, -17164.0176,  ...,   2598.0000,\n",
      "          -50510.9766,  41294.0078],\n",
      "         [ 15804.0059,   2016.0117,  50600.9453,  ...,    349.9590,\n",
      "          -27313.9844, -36747.0000],\n",
      "         ...,\n",
      "         [  8803.0059,    -71.9980,  28533.0039,  ...,  -2631.0000,\n",
      "           -9099.9941, -22858.0293],\n",
      "         [-28764.9609,   9230.9902,  -8177.9746,  ...,  -9450.0039,\n",
      "          -15130.0059,  -9617.0127],\n",
      "         [-40285.9922,  13948.9961,   1968.0273,  ...,  25769.0117,\n",
      "          -15283.0078,  -1826.9902]]]),) and output (tensor([[[-21532.0000,  32796.0000,  16281.9961,  ...,  18208.9980,\n",
      "           -4282.0020,   5756.0020],\n",
      "         [ -3665.0117,  43034.0039, -21869.0176,  ...,   3061.0000,\n",
      "          -49784.9766,  51701.0078],\n",
      "         [  7703.0059,  -1517.9883,  53286.9453,  ...,  -1535.0410,\n",
      "          -19828.9844, -34562.0000],\n",
      "         ...,\n",
      "         [ 13817.0059,   8859.0020,  34926.0039,  ...,   1883.0000,\n",
      "          -12459.9941, -19093.0293],\n",
      "         [-26185.9609,   6800.9902,  -8806.9746,  ...,  -8779.0039,\n",
      "          -14031.0059, -16395.0117],\n",
      "         [-31303.9922,   6653.9961,   1703.0273,  ...,  23489.0117,\n",
      "          -10662.0078,  -9239.9902]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[-21532.0000,  32796.0000,  16281.9961,  ...,  18208.9980,\n",
      "           -4282.0020,   5756.0020],\n",
      "         [ -3665.0117,  43034.0039, -21869.0176,  ...,   3061.0000,\n",
      "          -49784.9766,  51701.0078],\n",
      "         [  7703.0059,  -1517.9883,  53286.9453,  ...,  -1535.0410,\n",
      "          -19828.9844, -34562.0000],\n",
      "         ...,\n",
      "         [ 13817.0059,   8859.0020,  34926.0039,  ...,   1883.0000,\n",
      "          -12459.9941, -19093.0293],\n",
      "         [-26185.9609,   6800.9902,  -8806.9746,  ...,  -8779.0039,\n",
      "          -14031.0059, -16395.0117],\n",
      "         [-31303.9922,   6653.9961,   1703.0273,  ...,  23489.0117,\n",
      "          -10662.0078,  -9239.9902]]]),) and output (tensor([[[-24082.0000,  33387.0000,  15374.9961,  ...,  18359.9980,\n",
      "           -7500.0020,  11519.0020],\n",
      "         [ -6755.0117,  39031.0039, -29248.0176,  ...,  11550.0000,\n",
      "          -58485.9766,  42699.0078],\n",
      "         [  9959.0059,    740.0117,  51274.9453,  ...,  -9807.0410,\n",
      "          -27950.9844, -32783.0000],\n",
      "         ...,\n",
      "         [  9620.0059,   9121.0020,  31753.0039,  ...,   3954.0000,\n",
      "           -6095.9941, -24287.0293],\n",
      "         [-15757.9609,   3211.9902, -13233.9746,  ...,  -7027.0039,\n",
      "          -17748.0059, -17604.0117],\n",
      "         [-27200.9922,   6079.9961,    687.0273,  ...,  19202.0117,\n",
      "           -9785.0078, -13380.9902]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[-24082.0000,  33387.0000,  15374.9961,  ...,  18359.9980,\n",
      "           -7500.0020,  11519.0020],\n",
      "         [ -6755.0117,  39031.0039, -29248.0176,  ...,  11550.0000,\n",
      "          -58485.9766,  42699.0078],\n",
      "         [  9959.0059,    740.0117,  51274.9453,  ...,  -9807.0410,\n",
      "          -27950.9844, -32783.0000],\n",
      "         ...,\n",
      "         [  9620.0059,   9121.0020,  31753.0039,  ...,   3954.0000,\n",
      "           -6095.9941, -24287.0293],\n",
      "         [-15757.9609,   3211.9902, -13233.9746,  ...,  -7027.0039,\n",
      "          -17748.0059, -17604.0117],\n",
      "         [-27200.9922,   6079.9961,    687.0273,  ...,  19202.0117,\n",
      "           -9785.0078, -13380.9902]]]),) and output (tensor([[[-14869.0000,  32284.0000,   3742.9961,  ...,  12458.9980,\n",
      "           -5452.0020,   7590.0020],\n",
      "         [ -5612.0117,  28596.0039, -27058.0176,  ...,  10303.0000,\n",
      "          -59088.9766,  33381.0078],\n",
      "         [  7420.0059,   3427.0117,  50144.9453,  ..., -10445.0410,\n",
      "          -32070.9844, -23628.0000],\n",
      "         ...,\n",
      "         [  5916.0059,   8214.0020,  37222.0039,  ...,   7170.0000,\n",
      "          -12331.9941, -25447.0293],\n",
      "         [-18965.9609,    -96.0098, -12670.9746,  ..., -13917.0039,\n",
      "          -18677.0059, -23550.0117],\n",
      "         [-23555.9922,   6309.9961,   1705.0273,  ...,  31055.0117,\n",
      "           -6825.0078, -13065.9902]]]),)\n",
      "[Debug] Layer names being passed: ['model.layers.0', 'model.layers.1', 'model.layers.2', 'model.layers.3', 'model.layers.4', 'model.layers.5', 'model.layers.6', 'model.layers.7', 'model.layers.8', 'model.layers.9', 'model.layers.10', 'model.layers.11', 'model.layers.12', 'model.layers.13', 'model.layers.14', 'model.layers.15', 'model.layers.16', 'model.layers.17', 'model.layers.18', 'model.layers.19', 'model.layers.20', 'model.layers.21', 'model.layers.22', 'model.layers.23', 'model.layers.24', 'model.layers.25', 'model.layers.26', 'model.layers.27', 'model.embed_tokens']\n",
      "[Debug] Trying to access layer: model.layers.0\n",
      "[Debug] Successfully found layer: model.layers.0\n",
      "[Debug] Trying to access layer: model.layers.1\n",
      "[Debug] Successfully found layer: model.layers.1\n",
      "[Debug] Trying to access layer: model.layers.2\n",
      "[Debug] Successfully found layer: model.layers.2\n",
      "[Debug] Trying to access layer: model.layers.3\n",
      "[Debug] Successfully found layer: model.layers.3\n",
      "[Debug] Trying to access layer: model.layers.4\n",
      "[Debug] Successfully found layer: model.layers.4\n",
      "[Debug] Trying to access layer: model.layers.5\n",
      "[Debug] Successfully found layer: model.layers.5\n",
      "[Debug] Trying to access layer: model.layers.6\n",
      "[Debug] Successfully found layer: model.layers.6\n",
      "[Debug] Trying to access layer: model.layers.7\n",
      "[Debug] Successfully found layer: model.layers.7\n",
      "[Debug] Trying to access layer: model.layers.8\n",
      "[Debug] Successfully found layer: model.layers.8\n",
      "[Debug] Trying to access layer: model.layers.9\n",
      "[Debug] Successfully found layer: model.layers.9\n",
      "[Debug] Trying to access layer: model.layers.10\n",
      "[Debug] Successfully found layer: model.layers.10\n",
      "[Debug] Trying to access layer: model.layers.11\n",
      "[Debug] Successfully found layer: model.layers.11\n",
      "[Debug] Trying to access layer: model.layers.12\n",
      "[Debug] Successfully found layer: model.layers.12\n",
      "[Debug] Trying to access layer: model.layers.13\n",
      "[Debug] Successfully found layer: model.layers.13\n",
      "[Debug] Trying to access layer: model.layers.14\n",
      "[Debug] Successfully found layer: model.layers.14\n",
      "[Debug] Trying to access layer: model.layers.15\n",
      "[Debug] Successfully found layer: model.layers.15\n",
      "[Debug] Trying to access layer: model.layers.16\n",
      "[Debug] Successfully found layer: model.layers.16\n",
      "[Debug] Trying to access layer: model.layers.17\n",
      "[Debug] Successfully found layer: model.layers.17\n",
      "[Debug] Trying to access layer: model.layers.18\n",
      "[Debug] Successfully found layer: model.layers.18\n",
      "[Debug] Trying to access layer: model.layers.19\n",
      "[Debug] Successfully found layer: model.layers.19\n",
      "[Debug] Trying to access layer: model.layers.20\n",
      "[Debug] Successfully found layer: model.layers.20\n",
      "[Debug] Trying to access layer: model.layers.21\n",
      "[Debug] Successfully found layer: model.layers.21\n",
      "[Debug] Trying to access layer: model.layers.22\n",
      "[Debug] Successfully found layer: model.layers.22\n",
      "[Debug] Trying to access layer: model.layers.23\n",
      "[Debug] Successfully found layer: model.layers.23\n",
      "[Debug] Trying to access layer: model.layers.24\n",
      "[Debug] Successfully found layer: model.layers.24\n",
      "[Debug] Trying to access layer: model.layers.25\n",
      "[Debug] Successfully found layer: model.layers.25\n",
      "[Debug] Trying to access layer: model.layers.26\n",
      "[Debug] Successfully found layer: model.layers.26\n",
      "[Debug] Trying to access layer: model.layers.27\n",
      "[Debug] Successfully found layer: model.layers.27\n",
      "[Debug] Trying to access layer: model.embed_tokens\n",
      "[Debug] Successfully found layer: model.embed_tokens\n",
      "[Hook] Layer Embedding(128256, 3072, padding_idx=128004) received input (tensor([[128000,    791,  99671,   2617,    315,  15754,  43761,  25187,    580,\n",
      "             25,   3778,  24845,  15457,    578,  99671,   2617,    315,  15754,\n",
      "          43761,  25187,    580,     25,   3778,  24845,  15457,    374,    279,\n",
      "           2132,   3280,    315,    279,  30001,    837,   9977,  84108,  12707,\n",
      "           4101,   3778,  24845,  15457,     13,    578,   3280,  85170,    389,\n",
      "           6186,    220,   1114,     11,    220,    679,     23,  17706,     16,\n",
      "           1483,     17,     60,    323,  20536,    389,   5587,    220,   1691,\n",
      "             11,    220,    679,     23,     13,   1102,  17610,    315,    264,\n",
      "           2860,    315,    220,     24,  18243,  17706,     18,     60,    323,\n",
      "          41424,    279,  10102,    315,  15034,  15754,  43761,  25187,    580,\n",
      "            555,   6275,  25534,  13929,    356,    359,  29718,     11,   3196,\n",
      "            389,  11583,  96672,  32210,    596,   2363,  38506,  12440,    435,\n",
      "          51585,     25,  13929,    356,    359,  29718,     11,  15754,  43761,\n",
      "          25187,    580,     11,    323,    279,  84419,  22092,   2418,  81175,\n",
      "            304,    549,    815,     13,  11346,   8032,     17,   1483,     19,\n",
      "             60]]),) and output tensor([[[-0.0010, -0.0007, -0.0046,  ..., -0.0014, -0.0020,  0.0020],\n",
      "         [ 0.0081, -0.0330, -0.0093,  ..., -0.0302,  0.0201, -0.0017],\n",
      "         [-0.0281, -0.0117, -0.0156,  ...,  0.0205, -0.0461, -0.0130],\n",
      "         ...,\n",
      "         [ 0.0142, -0.0265, -0.0103,  ..., -0.0017,  0.0217,  0.0054],\n",
      "         [ 0.0190, -0.0011,  0.0210,  ..., -0.0170, -0.0048, -0.0023],\n",
      "         [-0.0045, -0.0086,  0.0126,  ...,  0.0082,  0.0127,  0.0007]]])\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[-0.0010, -0.0007, -0.0046,  ..., -0.0014, -0.0020,  0.0020],\n",
      "         [ 0.0081, -0.0330, -0.0093,  ..., -0.0302,  0.0201, -0.0017],\n",
      "         [-0.0281, -0.0117, -0.0156,  ...,  0.0205, -0.0461, -0.0130],\n",
      "         ...,\n",
      "         [ 0.0142, -0.0265, -0.0103,  ..., -0.0017,  0.0217,  0.0054],\n",
      "         [ 0.0190, -0.0011,  0.0210,  ..., -0.0170, -0.0048, -0.0023],\n",
      "         [-0.0045, -0.0086,  0.0126,  ...,  0.0082,  0.0127,  0.0007]]]),) and output (tensor([[[ 442.9990,  239.9993,  302.9954,  ..., -154.0014,  -15.0020,\n",
      "           185.0020],\n",
      "         [  83.0081,   26.9670,  -30.0093,  ...,  109.9698,   39.0201,\n",
      "            19.9983],\n",
      "         [-159.0281,   55.9883,  -96.0156,  ..., -266.9795,   43.9539,\n",
      "           -45.0130],\n",
      "         ...,\n",
      "         [ -28.9858,  -83.0265, -288.0103,  ..., -219.0017,  -76.9783,\n",
      "           -53.9946],\n",
      "         [ -73.9810, -504.0011, -389.9790,  ..., -395.0170,  -13.0048,\n",
      "          -140.0023],\n",
      "         [ 149.9955,  -97.0086, -262.9874,  ..., -718.9918,    5.0127,\n",
      "          -313.9993]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[ 442.9990,  239.9993,  302.9954,  ..., -154.0014,  -15.0020,\n",
      "           185.0020],\n",
      "         [  83.0081,   26.9670,  -30.0093,  ...,  109.9698,   39.0201,\n",
      "            19.9983],\n",
      "         [-159.0281,   55.9883,  -96.0156,  ..., -266.9795,   43.9539,\n",
      "           -45.0130],\n",
      "         ...,\n",
      "         [ -28.9858,  -83.0265, -288.0103,  ..., -219.0017,  -76.9783,\n",
      "           -53.9946],\n",
      "         [ -73.9810, -504.0011, -389.9790,  ..., -395.0170,  -13.0048,\n",
      "          -140.0023],\n",
      "         [ 149.9955,  -97.0086, -262.9874,  ..., -718.9918,    5.0127,\n",
      "          -313.9993]]]),) and output (tensor([[[  439.9990,  -144.0007,  -410.0046,  ...,  -765.0015,\n",
      "           1193.9979, -1027.9980],\n",
      "         [    3.0081,  -148.0330,  -104.0093,  ...,   120.9698,\n",
      "            971.0201,   -87.0017],\n",
      "         [-1030.0281,  -189.0117,  -789.0156,  ...,  -797.9795,\n",
      "            645.9539,   963.9870],\n",
      "         ...,\n",
      "         [   33.0142,  -908.0265,   129.9897,  ...,    77.9983,\n",
      "            516.0217,   879.0054],\n",
      "         [  132.0190, -1698.0011,   581.0210,  ...,  -262.0170,\n",
      "           1073.9952,   528.9977],\n",
      "         [ -156.0045,   772.9914,   215.0126,  ...,   -79.9918,\n",
      "           -239.9873,  1583.0007]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[  439.9990,  -144.0007,  -410.0046,  ...,  -765.0015,\n",
      "           1193.9979, -1027.9980],\n",
      "         [    3.0081,  -148.0330,  -104.0093,  ...,   120.9698,\n",
      "            971.0201,   -87.0017],\n",
      "         [-1030.0281,  -189.0117,  -789.0156,  ...,  -797.9795,\n",
      "            645.9539,   963.9870],\n",
      "         ...,\n",
      "         [   33.0142,  -908.0265,   129.9897,  ...,    77.9983,\n",
      "            516.0217,   879.0054],\n",
      "         [  132.0190, -1698.0011,   581.0210,  ...,  -262.0170,\n",
      "           1073.9952,   528.9977],\n",
      "         [ -156.0045,   772.9914,   215.0126,  ...,   -79.9918,\n",
      "           -239.9873,  1583.0007]]]),) and output (tensor([[[  551.9990,   993.9993, -1790.0046,  ...,   445.9985,\n",
      "          -1386.0021, -1245.9980],\n",
      "         [ -795.9919,   -14.0330,   204.9907,  ..., -4269.0303,\n",
      "           2113.0200,  1758.9983],\n",
      "         [ -365.0281, -1681.0117,   552.9844,  ...,  2299.0205,\n",
      "           1521.9539,  4535.9868],\n",
      "         ...,\n",
      "         [  585.0142, -1049.0265, -1073.0103,  ...,  2767.9983,\n",
      "           1443.0217, -1308.9946],\n",
      "         [-1210.9810, -8134.0010,  4048.0210,  ..., -3003.0171,\n",
      "            564.9952, -1048.0023],\n",
      "         [-2169.0044,  4478.9912,   136.0126,  ...,  -368.9918,\n",
      "           1084.0127,  2623.0007]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[  551.9990,   993.9993, -1790.0046,  ...,   445.9985,\n",
      "          -1386.0021, -1245.9980],\n",
      "         [ -795.9919,   -14.0330,   204.9907,  ..., -4269.0303,\n",
      "           2113.0200,  1758.9983],\n",
      "         [ -365.0281, -1681.0117,   552.9844,  ...,  2299.0205,\n",
      "           1521.9539,  4535.9868],\n",
      "         ...,\n",
      "         [  585.0142, -1049.0265, -1073.0103,  ...,  2767.9983,\n",
      "           1443.0217, -1308.9946],\n",
      "         [-1210.9810, -8134.0010,  4048.0210,  ..., -3003.0171,\n",
      "            564.9952, -1048.0023],\n",
      "         [-2169.0044,  4478.9912,   136.0126,  ...,  -368.9918,\n",
      "           1084.0127,  2623.0007]]]),) and output (tensor([[[  2131.9990,   -737.0007,  -1071.0046,  ...,  -2781.0015,\n",
      "            -885.0021,     86.0020],\n",
      "         [  -901.9919,  -7235.0332,   3048.9907,  ...,  -8416.0303,\n",
      "            1019.0200,  -2568.0017],\n",
      "         [   207.9719,  -2159.0117,  -2769.0156,  ...,   1049.0205,\n",
      "            2209.9539,   6334.9868],\n",
      "         ...,\n",
      "         [  4991.0142,   1056.9735,   -388.0103,  ...,   6957.9980,\n",
      "            -720.9783,   1199.0054],\n",
      "         [ -2355.9810, -14401.0010,   2704.0210,  ...,  -3361.0171,\n",
      "            2586.9951,   1415.9977],\n",
      "         [ -5274.0044,   8583.9912,  -1963.9874,  ...,  -3590.9917,\n",
      "            6568.0127,   2356.0007]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[  2131.9990,   -737.0007,  -1071.0046,  ...,  -2781.0015,\n",
      "            -885.0021,     86.0020],\n",
      "         [  -901.9919,  -7235.0332,   3048.9907,  ...,  -8416.0303,\n",
      "            1019.0200,  -2568.0017],\n",
      "         [   207.9719,  -2159.0117,  -2769.0156,  ...,   1049.0205,\n",
      "            2209.9539,   6334.9868],\n",
      "         ...,\n",
      "         [  4991.0142,   1056.9735,   -388.0103,  ...,   6957.9980,\n",
      "            -720.9783,   1199.0054],\n",
      "         [ -2355.9810, -14401.0010,   2704.0210,  ...,  -3361.0171,\n",
      "            2586.9951,   1415.9977],\n",
      "         [ -5274.0044,   8583.9912,  -1963.9874,  ...,  -3590.9917,\n",
      "            6568.0127,   2356.0007]]]),) and output (tensor([[[  3420.9990,    760.9993,  -5376.0049,  ...,  -4193.0015,\n",
      "            2860.9980,  -4080.9980],\n",
      "         [   929.0081,  -9019.0332,  -1200.0093,  ...,  -9820.0303,\n",
      "            4790.0200,   6992.9980],\n",
      "         [  -407.0281,  -5978.0117,  -3132.0156,  ...,   5790.0205,\n",
      "           -6311.0459,   5145.9868],\n",
      "         ...,\n",
      "         [  4830.0142,    165.9735,   4296.9897,  ...,   7457.9980,\n",
      "           -4551.9785,   7069.0054],\n",
      "         [ -2938.9810, -23340.0000,     54.0210,  ...,  -1899.0171,\n",
      "            4730.9951,   6477.9976],\n",
      "         [ -3958.0044,  12180.9912,  -2489.9873,  ...,   -660.9917,\n",
      "            9792.0127,   -233.9993]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[  3420.9990,    760.9993,  -5376.0049,  ...,  -4193.0015,\n",
      "            2860.9980,  -4080.9980],\n",
      "         [   929.0081,  -9019.0332,  -1200.0093,  ...,  -9820.0303,\n",
      "            4790.0200,   6992.9980],\n",
      "         [  -407.0281,  -5978.0117,  -3132.0156,  ...,   5790.0205,\n",
      "           -6311.0459,   5145.9868],\n",
      "         ...,\n",
      "         [  4830.0142,    165.9735,   4296.9897,  ...,   7457.9980,\n",
      "           -4551.9785,   7069.0054],\n",
      "         [ -2938.9810, -23340.0000,     54.0210,  ...,  -1899.0171,\n",
      "            4730.9951,   6477.9976],\n",
      "         [ -3958.0044,  12180.9912,  -2489.9873,  ...,   -660.9917,\n",
      "            9792.0127,   -233.9993]]]),) and output (tensor([[[  2742.9990,  -1512.0007, -10493.0049,  ...,  -9945.0020,\n",
      "           -2017.0020,  -1802.9980],\n",
      "         [  7487.0078, -11050.0332,    598.9907,  ...,  -8657.0303,\n",
      "            7584.0200,  11643.9980],\n",
      "         [ -4883.0283,  -7778.0117,  -6749.0156,  ...,   4025.0205,\n",
      "           -9621.0459,   5216.9868],\n",
      "         ...,\n",
      "         [  5297.0142,   2406.9736,   1211.9897,  ...,   5178.9980,\n",
      "           -5184.9785,  14083.0059],\n",
      "         [ -4307.9810, -24677.0000,  -2522.9790,  ...,  -2283.0171,\n",
      "            5814.9951,   7468.9976],\n",
      "         [ -5884.0044,  15842.9912,  -5605.9873,  ...,     53.0083,\n",
      "            8361.0127,    685.0007]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[  2742.9990,  -1512.0007, -10493.0049,  ...,  -9945.0020,\n",
      "           -2017.0020,  -1802.9980],\n",
      "         [  7487.0078, -11050.0332,    598.9907,  ...,  -8657.0303,\n",
      "            7584.0200,  11643.9980],\n",
      "         [ -4883.0283,  -7778.0117,  -6749.0156,  ...,   4025.0205,\n",
      "           -9621.0459,   5216.9868],\n",
      "         ...,\n",
      "         [  5297.0142,   2406.9736,   1211.9897,  ...,   5178.9980,\n",
      "           -5184.9785,  14083.0059],\n",
      "         [ -4307.9810, -24677.0000,  -2522.9790,  ...,  -2283.0171,\n",
      "            5814.9951,   7468.9976],\n",
      "         [ -5884.0044,  15842.9912,  -5605.9873,  ...,     53.0083,\n",
      "            8361.0127,    685.0007]]]),) and output (tensor([[[  3032.9990,  -3420.0007, -10291.0049,  ...,  -6511.0020,\n",
      "           -1953.0020,   2001.0020],\n",
      "         [  4685.0078, -10750.0332,   1932.9907,  ...,  -7766.0303,\n",
      "            6003.0200,  12404.9980],\n",
      "         [ -8483.0283,  -9012.0117,  -5773.0156,  ...,   5532.0205,\n",
      "          -13686.0459,   4281.9868],\n",
      "         ...,\n",
      "         [  5077.0142,   7270.9736,   -552.0103,  ...,   4662.9980,\n",
      "           -4981.9785,  16035.0059],\n",
      "         [ -2088.9810, -25730.0000,  -1763.9790,  ...,  -4779.0171,\n",
      "           -1185.0049,   7389.9976],\n",
      "         [ -6193.0044,  23184.9922,  -7649.9873,  ...,   3146.0083,\n",
      "            6890.0127,   2146.0007]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[  3032.9990,  -3420.0007, -10291.0049,  ...,  -6511.0020,\n",
      "           -1953.0020,   2001.0020],\n",
      "         [  4685.0078, -10750.0332,   1932.9907,  ...,  -7766.0303,\n",
      "            6003.0200,  12404.9980],\n",
      "         [ -8483.0283,  -9012.0117,  -5773.0156,  ...,   5532.0205,\n",
      "          -13686.0459,   4281.9868],\n",
      "         ...,\n",
      "         [  5077.0142,   7270.9736,   -552.0103,  ...,   4662.9980,\n",
      "           -4981.9785,  16035.0059],\n",
      "         [ -2088.9810, -25730.0000,  -1763.9790,  ...,  -4779.0171,\n",
      "           -1185.0049,   7389.9976],\n",
      "         [ -6193.0044,  23184.9922,  -7649.9873,  ...,   3146.0083,\n",
      "            6890.0127,   2146.0007]]]),) and output (tensor([[[  3132.9990,  -1970.0007, -11544.0049,  ...,  -5780.0020,\n",
      "           -2486.0020,  -1921.9980],\n",
      "         [   321.0078, -12223.0332,   -381.0093,  ...,  -6874.0303,\n",
      "            3344.0200,  17915.9980],\n",
      "         [-11256.0283,  -6719.0117,  -3984.0156,  ...,   3629.0205,\n",
      "          -19760.0469,   7790.9868],\n",
      "         ...,\n",
      "         [  2482.0142,   4941.9736,   2671.9897,  ...,   2852.9980,\n",
      "           -5662.9785,  16839.0059],\n",
      "         [ -1434.9810, -22911.0000,   -343.9790,  ...,  -3545.0171,\n",
      "             901.9951,   -717.0024],\n",
      "         [ -5596.0044,  29390.9922,  -8900.9873,  ...,   7273.0083,\n",
      "            5719.0127,   6194.0010]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[  3132.9990,  -1970.0007, -11544.0049,  ...,  -5780.0020,\n",
      "           -2486.0020,  -1921.9980],\n",
      "         [   321.0078, -12223.0332,   -381.0093,  ...,  -6874.0303,\n",
      "            3344.0200,  17915.9980],\n",
      "         [-11256.0283,  -6719.0117,  -3984.0156,  ...,   3629.0205,\n",
      "          -19760.0469,   7790.9868],\n",
      "         ...,\n",
      "         [  2482.0142,   4941.9736,   2671.9897,  ...,   2852.9980,\n",
      "           -5662.9785,  16839.0059],\n",
      "         [ -1434.9810, -22911.0000,   -343.9790,  ...,  -3545.0171,\n",
      "             901.9951,   -717.0024],\n",
      "         [ -5596.0044,  29390.9922,  -8900.9873,  ...,   7273.0083,\n",
      "            5719.0127,   6194.0010]]]),) and output (tensor([[[  1046.9990,   2403.9993, -14486.0049,  ..., -10922.0020,\n",
      "           -1419.0020,  -2983.9980],\n",
      "         [ -4350.9922, -14949.0332,  -6962.0093,  ...,  -8433.0303,\n",
      "            6246.0200,  17415.9980],\n",
      "         [ -8674.0283,  -2975.0117, -10291.0156,  ...,   1659.0205,\n",
      "          -19781.0469,   6065.9868],\n",
      "         ...,\n",
      "         [  3319.0142,   1383.9736,   2050.9897,  ...,   6734.9980,\n",
      "           -8791.9785,  12738.0059],\n",
      "         [  -447.9810, -27023.0000,  -8996.9785,  ...,   1776.9829,\n",
      "            1608.9951,  -1674.0024],\n",
      "         [-11373.0039,  34699.9922,  -8874.9873,  ...,  12330.0078,\n",
      "            8420.0127,   6119.0010]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[  1046.9990,   2403.9993, -14486.0049,  ..., -10922.0020,\n",
      "           -1419.0020,  -2983.9980],\n",
      "         [ -4350.9922, -14949.0332,  -6962.0093,  ...,  -8433.0303,\n",
      "            6246.0200,  17415.9980],\n",
      "         [ -8674.0283,  -2975.0117, -10291.0156,  ...,   1659.0205,\n",
      "          -19781.0469,   6065.9868],\n",
      "         ...,\n",
      "         [  3319.0142,   1383.9736,   2050.9897,  ...,   6734.9980,\n",
      "           -8791.9785,  12738.0059],\n",
      "         [  -447.9810, -27023.0000,  -8996.9785,  ...,   1776.9829,\n",
      "            1608.9951,  -1674.0024],\n",
      "         [-11373.0039,  34699.9922,  -8874.9873,  ...,  12330.0078,\n",
      "            8420.0127,   6119.0010]]]),) and output (tensor([[[ -3767.0010,   5307.9990, -14221.0049,  ..., -15035.0020,\n",
      "           -3630.0020,   4168.0020],\n",
      "         [ -7974.9922, -24774.0332,  -9699.0098,  ...,  -5479.0303,\n",
      "            5091.0200,  14088.9980],\n",
      "         [-12968.0283,  -3636.0117, -11952.0156,  ...,  -1444.9795,\n",
      "          -24026.0469,   -652.0132],\n",
      "         ...,\n",
      "         [  6712.0142,   3265.9736,   6692.9897,  ...,  10448.9980,\n",
      "          -11988.9785,  16354.0059],\n",
      "         [  -971.9810, -31776.0000,  -4166.9785,  ...,  -1625.0171,\n",
      "            5258.9951,  -4643.0024],\n",
      "         [-12982.0039,  40329.9922,  -7432.9873,  ...,  10394.0078,\n",
      "            8193.0127,  11303.0010]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[ -3767.0010,   5307.9990, -14221.0049,  ..., -15035.0020,\n",
      "           -3630.0020,   4168.0020],\n",
      "         [ -7974.9922, -24774.0332,  -9699.0098,  ...,  -5479.0303,\n",
      "            5091.0200,  14088.9980],\n",
      "         [-12968.0283,  -3636.0117, -11952.0156,  ...,  -1444.9795,\n",
      "          -24026.0469,   -652.0132],\n",
      "         ...,\n",
      "         [  6712.0142,   3265.9736,   6692.9897,  ...,  10448.9980,\n",
      "          -11988.9785,  16354.0059],\n",
      "         [  -971.9810, -31776.0000,  -4166.9785,  ...,  -1625.0171,\n",
      "            5258.9951,  -4643.0024],\n",
      "         [-12982.0039,  40329.9922,  -7432.9873,  ...,  10394.0078,\n",
      "            8193.0127,  11303.0010]]]),) and output (tensor([[[ -1326.0010,   1223.9990, -16643.0039,  ..., -13240.0020,\n",
      "           -5677.0020,   9639.0020],\n",
      "         [ -7878.9922, -26135.0332,  -7095.0098,  ..., -10040.0303,\n",
      "            6748.0200,  13831.9980],\n",
      "         [-12541.0283,  -7774.0117, -14626.0156,  ..., -10345.9795,\n",
      "          -27345.0469,   1352.9868],\n",
      "         ...,\n",
      "         [  4365.0142,   -164.0264,   9495.9902,  ...,   7490.9980,\n",
      "          -10299.9785,   9618.0059],\n",
      "         [ -1063.9810, -33191.0000,   -389.9785,  ...,  -4224.0171,\n",
      "           -1723.0049,  -3638.0024],\n",
      "         [-13673.0039,  40516.9922, -11315.9873,  ...,  15327.0078,\n",
      "            3340.0127,  16339.0010]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[ -1326.0010,   1223.9990, -16643.0039,  ..., -13240.0020,\n",
      "           -5677.0020,   9639.0020],\n",
      "         [ -7878.9922, -26135.0332,  -7095.0098,  ..., -10040.0303,\n",
      "            6748.0200,  13831.9980],\n",
      "         [-12541.0283,  -7774.0117, -14626.0156,  ..., -10345.9795,\n",
      "          -27345.0469,   1352.9868],\n",
      "         ...,\n",
      "         [  4365.0142,   -164.0264,   9495.9902,  ...,   7490.9980,\n",
      "          -10299.9785,   9618.0059],\n",
      "         [ -1063.9810, -33191.0000,   -389.9785,  ...,  -4224.0171,\n",
      "           -1723.0049,  -3638.0024],\n",
      "         [-13673.0039,  40516.9922, -11315.9873,  ...,  15327.0078,\n",
      "            3340.0127,  16339.0010]]]),) and output (tensor([[[ -5120.0010,   2246.9990, -15993.0039,  ..., -19818.0020,\n",
      "           -9385.0020,  11924.0020],\n",
      "         [ -8612.9922, -20451.0332,  -6269.0098,  ...,  -8090.0303,\n",
      "           11776.0195,  13161.9980],\n",
      "         [ -7978.0283,  -1429.0117, -17510.0156,  ...,  -7652.9795,\n",
      "          -20098.0469,  -5756.0132],\n",
      "         ...,\n",
      "         [  5307.0142,  -1546.0264,   7412.9902,  ...,   4283.9980,\n",
      "           -7707.9785,  11141.0059],\n",
      "         [  -483.9810, -29549.0000,  -4337.9785,  ..., -13828.0176,\n",
      "           -6351.0049,  -3891.0024],\n",
      "         [-13148.0039,  38809.9922, -15083.9873,  ...,  16677.0078,\n",
      "            8580.0127,  21904.0000]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[ -5120.0010,   2246.9990, -15993.0039,  ..., -19818.0020,\n",
      "           -9385.0020,  11924.0020],\n",
      "         [ -8612.9922, -20451.0332,  -6269.0098,  ...,  -8090.0303,\n",
      "           11776.0195,  13161.9980],\n",
      "         [ -7978.0283,  -1429.0117, -17510.0156,  ...,  -7652.9795,\n",
      "          -20098.0469,  -5756.0132],\n",
      "         ...,\n",
      "         [  5307.0142,  -1546.0264,   7412.9902,  ...,   4283.9980,\n",
      "           -7707.9785,  11141.0059],\n",
      "         [  -483.9810, -29549.0000,  -4337.9785,  ..., -13828.0176,\n",
      "           -6351.0049,  -3891.0024],\n",
      "         [-13148.0039,  38809.9922, -15083.9873,  ...,  16677.0078,\n",
      "            8580.0127,  21904.0000]]]),) and output (tensor([[[ -6685.0010,  -1278.0010, -15553.0039,  ..., -19846.0020,\n",
      "          -10418.0020,  13376.0020],\n",
      "         [-12172.9922, -25925.0332,  -9775.0098,  ...,  -2311.0303,\n",
      "            4319.0195,  17070.9980],\n",
      "         [ -6756.0283,  -4851.0117, -15809.0156,  ..., -10514.9795,\n",
      "          -24830.0469,  -9986.0137],\n",
      "         ...,\n",
      "         [  2055.0142,  -4183.0264,   8201.9902,  ...,   3189.9980,\n",
      "           -7936.9785,  14950.0059],\n",
      "         [   119.0190, -26305.0000,  -6346.9785,  ..., -11383.0176,\n",
      "           -6380.0049,   1552.9976],\n",
      "         [-17130.0039,  47138.9922, -16794.9883,  ...,  19167.0078,\n",
      "             765.0127,  20281.0000]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[ -6685.0010,  -1278.0010, -15553.0039,  ..., -19846.0020,\n",
      "          -10418.0020,  13376.0020],\n",
      "         [-12172.9922, -25925.0332,  -9775.0098,  ...,  -2311.0303,\n",
      "            4319.0195,  17070.9980],\n",
      "         [ -6756.0283,  -4851.0117, -15809.0156,  ..., -10514.9795,\n",
      "          -24830.0469,  -9986.0137],\n",
      "         ...,\n",
      "         [  2055.0142,  -4183.0264,   8201.9902,  ...,   3189.9980,\n",
      "           -7936.9785,  14950.0059],\n",
      "         [   119.0190, -26305.0000,  -6346.9785,  ..., -11383.0176,\n",
      "           -6380.0049,   1552.9976],\n",
      "         [-17130.0039,  47138.9922, -16794.9883,  ...,  19167.0078,\n",
      "             765.0127,  20281.0000]]]),) and output (tensor([[[ -8422.0010,    339.9990, -15289.0039,  ..., -20884.0020,\n",
      "           -6908.0020,  11014.0020],\n",
      "         [-11803.9922, -15726.0332, -10763.0098,  ...,   -238.0303,\n",
      "            8956.0195,  16759.9980],\n",
      "         [ -5473.0283,    660.9883, -18418.0156,  ...,  -9072.9795,\n",
      "          -32208.0469,  -7718.0137],\n",
      "         ...,\n",
      "         [  2372.0142,  -7051.0264,   6362.9902,  ...,   5391.9980,\n",
      "           -7191.9785,   8686.0059],\n",
      "         [   598.0190, -29943.0000,  -2193.9785,  ..., -14694.0176,\n",
      "          -11051.0049,   2672.9976],\n",
      "         [-18702.0039,  52441.9922, -11179.9883,  ...,  21263.0078,\n",
      "             891.0127,  26131.0000]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[ -8422.0010,    339.9990, -15289.0039,  ..., -20884.0020,\n",
      "           -6908.0020,  11014.0020],\n",
      "         [-11803.9922, -15726.0332, -10763.0098,  ...,   -238.0303,\n",
      "            8956.0195,  16759.9980],\n",
      "         [ -5473.0283,    660.9883, -18418.0156,  ...,  -9072.9795,\n",
      "          -32208.0469,  -7718.0137],\n",
      "         ...,\n",
      "         [  2372.0142,  -7051.0264,   6362.9902,  ...,   5391.9980,\n",
      "           -7191.9785,   8686.0059],\n",
      "         [   598.0190, -29943.0000,  -2193.9785,  ..., -14694.0176,\n",
      "          -11051.0049,   2672.9976],\n",
      "         [-18702.0039,  52441.9922, -11179.9883,  ...,  21263.0078,\n",
      "             891.0127,  26131.0000]]]),) and output (tensor([[[-10989.0010,   7034.9990, -13964.0039,  ..., -23968.0020,\n",
      "           -3749.0020,  11730.0020],\n",
      "         [-11534.9922, -14364.0332,  -6326.0098,  ...,   2881.9697,\n",
      "            5131.0195,  16323.9980],\n",
      "         [ -1060.0283,   6529.9883, -18787.0156,  ...,  -8840.9795,\n",
      "          -30558.0469, -14085.0137],\n",
      "         ...,\n",
      "         [  1287.0142,  -3911.0264,   3126.9902,  ...,   4842.9980,\n",
      "           -3973.9785,   8203.0059],\n",
      "         [  2440.0190, -25830.0000,  -8310.9785,  ..., -10980.0176,\n",
      "          -13169.0049,   5252.9976],\n",
      "         [-18314.0039,  63151.9922,  -9366.9883,  ...,  29680.0078,\n",
      "             965.0127,  23422.0000]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[-10989.0010,   7034.9990, -13964.0039,  ..., -23968.0020,\n",
      "           -3749.0020,  11730.0020],\n",
      "         [-11534.9922, -14364.0332,  -6326.0098,  ...,   2881.9697,\n",
      "            5131.0195,  16323.9980],\n",
      "         [ -1060.0283,   6529.9883, -18787.0156,  ...,  -8840.9795,\n",
      "          -30558.0469, -14085.0137],\n",
      "         ...,\n",
      "         [  1287.0142,  -3911.0264,   3126.9902,  ...,   4842.9980,\n",
      "           -3973.9785,   8203.0059],\n",
      "         [  2440.0190, -25830.0000,  -8310.9785,  ..., -10980.0176,\n",
      "          -13169.0049,   5252.9976],\n",
      "         [-18314.0039,  63151.9922,  -9366.9883,  ...,  29680.0078,\n",
      "             965.0127,  23422.0000]]]),) and output (tensor([[[ -7159.0010,   6621.9990, -10967.0039,  ..., -17477.0020,\n",
      "           -2510.0020,  11888.0020],\n",
      "         [-10845.9922,  -8848.0332,  -9821.0098,  ...,   -182.0303,\n",
      "            8545.0195,  18742.9980],\n",
      "         [ -1584.0283,   9025.9883, -22161.0156,  ...,  -2461.9795,\n",
      "          -29510.0469, -13032.0137],\n",
      "         ...,\n",
      "         [ -1792.9858,   8120.9736,   3543.9902,  ...,   1008.9980,\n",
      "            1882.0215,    175.0059],\n",
      "         [ -2677.9810, -28888.0000,  -8744.9785,  ..., -11792.0176,\n",
      "          -11166.0049,   2632.9976],\n",
      "         [-20432.0039,  69287.9922,  -8117.9883,  ...,  36902.0078,\n",
      "           -3434.9873,  29126.0000]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[ -7159.0010,   6621.9990, -10967.0039,  ..., -17477.0020,\n",
      "           -2510.0020,  11888.0020],\n",
      "         [-10845.9922,  -8848.0332,  -9821.0098,  ...,   -182.0303,\n",
      "            8545.0195,  18742.9980],\n",
      "         [ -1584.0283,   9025.9883, -22161.0156,  ...,  -2461.9795,\n",
      "          -29510.0469, -13032.0137],\n",
      "         ...,\n",
      "         [ -1792.9858,   8120.9736,   3543.9902,  ...,   1008.9980,\n",
      "            1882.0215,    175.0059],\n",
      "         [ -2677.9810, -28888.0000,  -8744.9785,  ..., -11792.0176,\n",
      "          -11166.0049,   2632.9976],\n",
      "         [-20432.0039,  69287.9922,  -8117.9883,  ...,  36902.0078,\n",
      "           -3434.9873,  29126.0000]]]),) and output (tensor([[[ -4822.0010,   3124.9990,  -3885.0039,  ..., -19007.0020,\n",
      "            -551.0020,  16285.0020],\n",
      "         [ -9465.9922,  -6179.0332,  -6466.0098,  ...,   -194.0303,\n",
      "           20542.0195,  16041.9980],\n",
      "         [  2637.9717,  16521.9883, -28750.0156,  ...,    768.0205,\n",
      "          -37124.0469, -18400.0137],\n",
      "         ...,\n",
      "         [   485.0142,  11531.9736,   2821.9902,  ...,   2363.9980,\n",
      "            4356.0215,   6962.0059],\n",
      "         [ -3716.9810, -20314.0000,  -5148.9785,  ...,  -9057.0176,\n",
      "          -15764.0049,   3545.9976],\n",
      "         [-16518.0039,  81168.9922, -11544.9883,  ...,  43360.0078,\n",
      "           -7917.9873,  26854.0000]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[ -4822.0010,   3124.9990,  -3885.0039,  ..., -19007.0020,\n",
      "            -551.0020,  16285.0020],\n",
      "         [ -9465.9922,  -6179.0332,  -6466.0098,  ...,   -194.0303,\n",
      "           20542.0195,  16041.9980],\n",
      "         [  2637.9717,  16521.9883, -28750.0156,  ...,    768.0205,\n",
      "          -37124.0469, -18400.0137],\n",
      "         ...,\n",
      "         [   485.0142,  11531.9736,   2821.9902,  ...,   2363.9980,\n",
      "            4356.0215,   6962.0059],\n",
      "         [ -3716.9810, -20314.0000,  -5148.9785,  ...,  -9057.0176,\n",
      "          -15764.0049,   3545.9976],\n",
      "         [-16518.0039,  81168.9922, -11544.9883,  ...,  43360.0078,\n",
      "           -7917.9873,  26854.0000]]]),) and output (tensor([[[ -7235.0010,   6583.9990,  -4212.0039,  ..., -19104.0020,\n",
      "            3087.9980,  17084.0020],\n",
      "         [ -8326.9922,   2662.9668,  -3426.0098,  ...,  -5039.0303,\n",
      "           15379.0195,  24724.9980],\n",
      "         [  5848.9717,  22132.9883, -30878.0156,  ...,   -175.9795,\n",
      "          -35479.0469, -20815.0137],\n",
      "         ...,\n",
      "         [ -3101.9858,  11843.9736,   5382.9902,  ...,   5552.9980,\n",
      "            6338.0215,   5907.0059],\n",
      "         [  4764.0190, -26096.0000,  -5179.9785,  ..., -14855.0176,\n",
      "          -22678.0039,  13632.9980],\n",
      "         [-24662.0039,  81982.9922, -14443.9883,  ...,  39841.0078,\n",
      "           -5661.9873,  27247.0000]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[ -7235.0010,   6583.9990,  -4212.0039,  ..., -19104.0020,\n",
      "            3087.9980,  17084.0020],\n",
      "         [ -8326.9922,   2662.9668,  -3426.0098,  ...,  -5039.0303,\n",
      "           15379.0195,  24724.9980],\n",
      "         [  5848.9717,  22132.9883, -30878.0156,  ...,   -175.9795,\n",
      "          -35479.0469, -20815.0137],\n",
      "         ...,\n",
      "         [ -3101.9858,  11843.9736,   5382.9902,  ...,   5552.9980,\n",
      "            6338.0215,   5907.0059],\n",
      "         [  4764.0190, -26096.0000,  -5179.9785,  ..., -14855.0176,\n",
      "          -22678.0039,  13632.9980],\n",
      "         [-24662.0039,  81982.9922, -14443.9883,  ...,  39841.0078,\n",
      "           -5661.9873,  27247.0000]]]),) and output (tensor([[[ -5586.0010,   7645.9990,   -505.0039,  ..., -15346.0020,\n",
      "            -623.0020,  12986.0020],\n",
      "         [ -1623.9922,    151.9668,  -1238.0098,  ...,  -4377.0303,\n",
      "           19241.0195,  23137.9980],\n",
      "         [  8314.9717,  23479.9883, -24439.0156,  ...,   -134.9795,\n",
      "          -39573.0469, -19914.0137],\n",
      "         ...,\n",
      "         [  3036.0142,  13830.9736,   6352.9902,  ...,  10154.9980,\n",
      "           11267.0215,   9012.0059],\n",
      "         [  7762.0190, -26862.0000,   -462.9785,  ..., -15563.0176,\n",
      "          -25159.0039,   9044.9980],\n",
      "         [-24029.0039,  84393.9922,  -9728.9883,  ...,  46864.0078,\n",
      "           -3929.9873,  32988.0000]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[ -5586.0010,   7645.9990,   -505.0039,  ..., -15346.0020,\n",
      "            -623.0020,  12986.0020],\n",
      "         [ -1623.9922,    151.9668,  -1238.0098,  ...,  -4377.0303,\n",
      "           19241.0195,  23137.9980],\n",
      "         [  8314.9717,  23479.9883, -24439.0156,  ...,   -134.9795,\n",
      "          -39573.0469, -19914.0137],\n",
      "         ...,\n",
      "         [  3036.0142,  13830.9736,   6352.9902,  ...,  10154.9980,\n",
      "           11267.0215,   9012.0059],\n",
      "         [  7762.0190, -26862.0000,   -462.9785,  ..., -15563.0176,\n",
      "          -25159.0039,   9044.9980],\n",
      "         [-24029.0039,  84393.9922,  -9728.9883,  ...,  46864.0078,\n",
      "           -3929.9873,  32988.0000]]]),) and output (tensor([[[-8.9160e+03,  2.3630e+03, -4.8330e+03,  ..., -1.2075e+04,\n",
      "          -1.1240e+03,  1.2438e+04],\n",
      "         [-1.0090e+04,  2.6897e+02, -1.5480e+03,  ..., -2.3160e+03,\n",
      "           1.4229e+04,  2.6277e+04],\n",
      "         [ 1.1117e+04,  2.4285e+04, -2.4908e+04,  ...,  3.4502e+02,\n",
      "          -4.7719e+04, -1.3042e+04],\n",
      "         ...,\n",
      "         [ 8.0860e+03,  1.5209e+04,  9.5470e+03,  ...,  1.0250e+04,\n",
      "           1.0275e+04,  1.5635e+04],\n",
      "         [ 1.1437e+04, -2.6492e+04, -1.1710e+03,  ..., -4.9070e+03,\n",
      "          -2.6203e+04,  1.1791e+04],\n",
      "         [-2.6148e+04,  8.8474e+04, -1.5907e+04,  ...,  5.9830e+04,\n",
      "           2.0127e+00,  3.3356e+04]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[-8.9160e+03,  2.3630e+03, -4.8330e+03,  ..., -1.2075e+04,\n",
      "          -1.1240e+03,  1.2438e+04],\n",
      "         [-1.0090e+04,  2.6897e+02, -1.5480e+03,  ..., -2.3160e+03,\n",
      "           1.4229e+04,  2.6277e+04],\n",
      "         [ 1.1117e+04,  2.4285e+04, -2.4908e+04,  ...,  3.4502e+02,\n",
      "          -4.7719e+04, -1.3042e+04],\n",
      "         ...,\n",
      "         [ 8.0860e+03,  1.5209e+04,  9.5470e+03,  ...,  1.0250e+04,\n",
      "           1.0275e+04,  1.5635e+04],\n",
      "         [ 1.1437e+04, -2.6492e+04, -1.1710e+03,  ..., -4.9070e+03,\n",
      "          -2.6203e+04,  1.1791e+04],\n",
      "         [-2.6148e+04,  8.8474e+04, -1.5907e+04,  ...,  5.9830e+04,\n",
      "           2.0127e+00,  3.3356e+04]]]),) and output (tensor([[[-20230.0000,   7877.9990,    161.9961,  ..., -10250.0020,\n",
      "             676.9980,   7095.0020],\n",
      "         [-13658.9922,  -4313.0332,    514.9902,  ...,   1853.9697,\n",
      "            7896.0195,  24121.9980],\n",
      "         [  8453.9717,  29981.9883, -19761.0156,  ...,   -555.9795,\n",
      "          -55113.0469,  -9535.0137],\n",
      "         ...,\n",
      "         [  7122.0142,  17741.9727,   9225.9902,  ...,   5903.9980,\n",
      "           12611.0215,  11886.0059],\n",
      "         [ 11636.0195, -25577.0000,   2965.0215,  ..., -14861.0176,\n",
      "          -28484.0039,  17160.9980],\n",
      "         [-33094.0039,  90077.9922, -19373.9883,  ...,  65722.0078,\n",
      "           -1785.9873,  29431.0000]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[-20230.0000,   7877.9990,    161.9961,  ..., -10250.0020,\n",
      "             676.9980,   7095.0020],\n",
      "         [-13658.9922,  -4313.0332,    514.9902,  ...,   1853.9697,\n",
      "            7896.0195,  24121.9980],\n",
      "         [  8453.9717,  29981.9883, -19761.0156,  ...,   -555.9795,\n",
      "          -55113.0469,  -9535.0137],\n",
      "         ...,\n",
      "         [  7122.0142,  17741.9727,   9225.9902,  ...,   5903.9980,\n",
      "           12611.0215,  11886.0059],\n",
      "         [ 11636.0195, -25577.0000,   2965.0215,  ..., -14861.0176,\n",
      "          -28484.0039,  17160.9980],\n",
      "         [-33094.0039,  90077.9922, -19373.9883,  ...,  65722.0078,\n",
      "           -1785.9873,  29431.0000]]]),) and output (tensor([[[-25670.0000,  13293.9990,   9099.9961,  ...,  -6547.0020,\n",
      "           -5658.0020,   4192.0020],\n",
      "         [  1027.0078,   -994.0332,   1650.9902,  ...,   8167.9697,\n",
      "            -429.9805,  22968.9980],\n",
      "         [ 10215.9717,  34828.9883, -20752.0156,  ...,  -4526.9795,\n",
      "          -55120.0469,  -7535.0137],\n",
      "         ...,\n",
      "         [ 13995.0137,  13415.9727,   9702.9902,  ...,   3011.9980,\n",
      "            6998.0215,   8900.0059],\n",
      "         [  9334.0195, -29934.0000,   5119.0215,  ..., -10851.0176,\n",
      "          -34445.0039,  11030.9980],\n",
      "         [-30973.0039,  87963.9922, -17651.9883,  ...,  75795.0078,\n",
      "            2994.0127,  28875.0000]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[-25670.0000,  13293.9990,   9099.9961,  ...,  -6547.0020,\n",
      "           -5658.0020,   4192.0020],\n",
      "         [  1027.0078,   -994.0332,   1650.9902,  ...,   8167.9697,\n",
      "            -429.9805,  22968.9980],\n",
      "         [ 10215.9717,  34828.9883, -20752.0156,  ...,  -4526.9795,\n",
      "          -55120.0469,  -7535.0137],\n",
      "         ...,\n",
      "         [ 13995.0137,  13415.9727,   9702.9902,  ...,   3011.9980,\n",
      "            6998.0215,   8900.0059],\n",
      "         [  9334.0195, -29934.0000,   5119.0215,  ..., -10851.0176,\n",
      "          -34445.0039,  11030.9980],\n",
      "         [-30973.0039,  87963.9922, -17651.9883,  ...,  75795.0078,\n",
      "            2994.0127,  28875.0000]]]),) and output (tensor([[[-25652.0000,  13180.9990,  15728.9961,  ...,  -2831.0020,\n",
      "             158.9980,   2204.0020],\n",
      "         [  4529.0078,   1052.9668,   1580.9902,  ...,   3558.9697,\n",
      "           -2359.9805,  27563.9980],\n",
      "         [ 16263.9717,  32394.9883, -20975.0156,  ...,   -628.9795,\n",
      "          -48086.0469,   3759.9863],\n",
      "         ...,\n",
      "         [ 10939.0137,  20966.9727,  19286.9902,  ...,   9507.9980,\n",
      "           20992.0215,   5398.0059],\n",
      "         [ 15294.0195, -25739.0000,   1939.0215,  ..., -11361.0176,\n",
      "          -33571.0039,  14309.9980],\n",
      "         [-35874.0039,  94455.9922, -15143.9883,  ...,  81847.0078,\n",
      "           -2962.9873,  23997.0000]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[-25652.0000,  13180.9990,  15728.9961,  ...,  -2831.0020,\n",
      "             158.9980,   2204.0020],\n",
      "         [  4529.0078,   1052.9668,   1580.9902,  ...,   3558.9697,\n",
      "           -2359.9805,  27563.9980],\n",
      "         [ 16263.9717,  32394.9883, -20975.0156,  ...,   -628.9795,\n",
      "          -48086.0469,   3759.9863],\n",
      "         ...,\n",
      "         [ 10939.0137,  20966.9727,  19286.9902,  ...,   9507.9980,\n",
      "           20992.0215,   5398.0059],\n",
      "         [ 15294.0195, -25739.0000,   1939.0215,  ..., -11361.0176,\n",
      "          -33571.0039,  14309.9980],\n",
      "         [-35874.0039,  94455.9922, -15143.9883,  ...,  81847.0078,\n",
      "           -2962.9873,  23997.0000]]]),) and output (tensor([[[-14791.0000,  15299.9990,  16129.9961,  ...,   3187.9980,\n",
      "           -2552.0020,   5626.0020],\n",
      "         [ 12914.0078,  -1246.0332,    277.9902,  ...,    405.9697,\n",
      "           -7238.9805,  32707.9980],\n",
      "         [ 18355.9727,  31659.9883, -19397.0156,  ...,  -8450.9795,\n",
      "          -55905.0469,  12763.9863],\n",
      "         ...,\n",
      "         [  2544.0137,  19354.9727,  24638.9902,  ...,   1462.9980,\n",
      "           27163.0215,    721.0059],\n",
      "         [ 19686.0195, -18427.0000,   9543.0215,  ...,  -6403.0176,\n",
      "          -30085.0039,  19494.9980],\n",
      "         [-29783.0039, 108029.9922, -17115.9883,  ...,  81482.0078,\n",
      "           -3399.9873,  34204.0000]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[-14791.0000,  15299.9990,  16129.9961,  ...,   3187.9980,\n",
      "           -2552.0020,   5626.0020],\n",
      "         [ 12914.0078,  -1246.0332,    277.9902,  ...,    405.9697,\n",
      "           -7238.9805,  32707.9980],\n",
      "         [ 18355.9727,  31659.9883, -19397.0156,  ...,  -8450.9795,\n",
      "          -55905.0469,  12763.9863],\n",
      "         ...,\n",
      "         [  2544.0137,  19354.9727,  24638.9902,  ...,   1462.9980,\n",
      "           27163.0215,    721.0059],\n",
      "         [ 19686.0195, -18427.0000,   9543.0215,  ...,  -6403.0176,\n",
      "          -30085.0039,  19494.9980],\n",
      "         [-29783.0039, 108029.9922, -17115.9883,  ...,  81482.0078,\n",
      "           -3399.9873,  34204.0000]]]),) and output (tensor([[[-13284.0000,  26639.0000,   7467.9961,  ...,   4113.9980,\n",
      "           -6608.0020,   5336.0020],\n",
      "         [ 12877.0078,   3502.9668,  -4388.0098,  ...,   3269.9697,\n",
      "           -3907.9805,  36768.0000],\n",
      "         [ 25472.9727,  43033.9883, -18927.0156,  ...,  -6754.9795,\n",
      "          -50568.0469,  10743.9863],\n",
      "         ...,\n",
      "         [  4621.0137,  21010.9727,  21480.9902,  ...,  13198.9980,\n",
      "           27036.0215,  16401.0059],\n",
      "         [ 14153.0195, -14649.0000,   6734.0215,  ...,  -4643.0176,\n",
      "          -23259.0039,  15838.9980],\n",
      "         [-28065.0039, 105767.9922, -16437.9883,  ...,  86110.0078,\n",
      "           -8541.9873,  33084.0000]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[-13284.0000,  26639.0000,   7467.9961,  ...,   4113.9980,\n",
      "           -6608.0020,   5336.0020],\n",
      "         [ 12877.0078,   3502.9668,  -4388.0098,  ...,   3269.9697,\n",
      "           -3907.9805,  36768.0000],\n",
      "         [ 25472.9727,  43033.9883, -18927.0156,  ...,  -6754.9795,\n",
      "          -50568.0469,  10743.9863],\n",
      "         ...,\n",
      "         [  4621.0137,  21010.9727,  21480.9902,  ...,  13198.9980,\n",
      "           27036.0215,  16401.0059],\n",
      "         [ 14153.0195, -14649.0000,   6734.0215,  ...,  -4643.0176,\n",
      "          -23259.0039,  15838.9980],\n",
      "         [-28065.0039, 105767.9922, -16437.9883,  ...,  86110.0078,\n",
      "           -8541.9873,  33084.0000]]]),) and output (tensor([[[-21532.0000,  32796.0000,  16281.9961,  ...,  18208.9980,\n",
      "           -4282.0020,   5756.0020],\n",
      "         [ 12140.0078,  11371.9668,   -524.0098,  ...,  -1428.0303,\n",
      "           -6793.9805,  34724.0000],\n",
      "         [ 23032.9727,  42494.9883, -19654.0156,  ...,  -5564.9795,\n",
      "          -37788.0469,   9111.9863],\n",
      "         ...,\n",
      "         [  3588.0137,  20606.9727,  24434.9902,  ...,  11100.9980,\n",
      "           30093.0215,  24463.0059],\n",
      "         [ 14523.0195, -14210.0000,   5962.0215,  ...,  -4573.0176,\n",
      "          -30064.0039,  24204.9980],\n",
      "         [-24226.0039, 103030.9922, -16044.9883,  ...,  88486.0078,\n",
      "           -5843.9873,  34068.0000]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[-21532.0000,  32796.0000,  16281.9961,  ...,  18208.9980,\n",
      "           -4282.0020,   5756.0020],\n",
      "         [ 12140.0078,  11371.9668,   -524.0098,  ...,  -1428.0303,\n",
      "           -6793.9805,  34724.0000],\n",
      "         [ 23032.9727,  42494.9883, -19654.0156,  ...,  -5564.9795,\n",
      "          -37788.0469,   9111.9863],\n",
      "         ...,\n",
      "         [  3588.0137,  20606.9727,  24434.9902,  ...,  11100.9980,\n",
      "           30093.0215,  24463.0059],\n",
      "         [ 14523.0195, -14210.0000,   5962.0215,  ...,  -4573.0176,\n",
      "          -30064.0039,  24204.9980],\n",
      "         [-24226.0039, 103030.9922, -16044.9883,  ...,  88486.0078,\n",
      "           -5843.9873,  34068.0000]]]),) and output (tensor([[[-24082.0000,  33387.0000,  15374.9961,  ...,  18359.9980,\n",
      "           -7500.0020,  11519.0020],\n",
      "         [ 19172.0078,  11465.9668,   -590.0098,  ...,   -765.0303,\n",
      "          -12810.9805,  26894.0000],\n",
      "         [ 29191.9727,  47747.9883, -25140.0156,  ...,  -4090.9795,\n",
      "          -38256.0469,   8127.9863],\n",
      "         ...,\n",
      "         [  8781.0137,  15644.9727,  21569.9902,  ...,  10533.9980,\n",
      "           23871.0215,  34363.0078],\n",
      "         [ 20237.0195, -16940.0000,   9082.0215,  ...,  -8779.0176,\n",
      "          -35129.0039,  24597.9980],\n",
      "         [-27658.0039,  95399.9922, -17613.9883,  ...,  91244.0078,\n",
      "           -2869.9873,  28790.0000]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[-24082.0000,  33387.0000,  15374.9961,  ...,  18359.9980,\n",
      "           -7500.0020,  11519.0020],\n",
      "         [ 19172.0078,  11465.9668,   -590.0098,  ...,   -765.0303,\n",
      "          -12810.9805,  26894.0000],\n",
      "         [ 29191.9727,  47747.9883, -25140.0156,  ...,  -4090.9795,\n",
      "          -38256.0469,   8127.9863],\n",
      "         ...,\n",
      "         [  8781.0137,  15644.9727,  21569.9902,  ...,  10533.9980,\n",
      "           23871.0215,  34363.0078],\n",
      "         [ 20237.0195, -16940.0000,   9082.0215,  ...,  -8779.0176,\n",
      "          -35129.0039,  24597.9980],\n",
      "         [-27658.0039,  95399.9922, -17613.9883,  ...,  91244.0078,\n",
      "           -2869.9873,  28790.0000]]]),) and output (tensor([[[-14869.0000,  32284.0000,   3742.9961,  ...,  12458.9980,\n",
      "           -5452.0020,   7590.0020],\n",
      "         [ 29047.0078,  16280.9668,  11952.9902,  ...,   2478.9697,\n",
      "           -6321.9805,  16127.0000],\n",
      "         [ 24455.9727,  43618.9883, -21848.0156,  ...,  -3478.9795,\n",
      "          -30102.0469,  -1685.0137],\n",
      "         ...,\n",
      "         [ 15859.0137,  18431.9727,  19761.9902,  ...,  15073.9980,\n",
      "           27735.0215,  36511.0078],\n",
      "         [ 21078.0195,  -9964.0000,   9991.0215,  ...,  -2306.0176,\n",
      "          -44963.0039,  28391.9980],\n",
      "         [-29565.0039,  94624.9922, -16210.9883,  ...,  88145.0078,\n",
      "           -3327.9873,  35490.0000]]]),)\n",
      "[Debug] Layer names being passed: ['model.layers.0', 'model.layers.1', 'model.layers.2', 'model.layers.3', 'model.layers.4', 'model.layers.5', 'model.layers.6', 'model.layers.7', 'model.layers.8', 'model.layers.9', 'model.layers.10', 'model.layers.11', 'model.layers.12', 'model.layers.13', 'model.layers.14', 'model.layers.15', 'model.layers.16', 'model.layers.17', 'model.layers.18', 'model.layers.19', 'model.layers.20', 'model.layers.21', 'model.layers.22', 'model.layers.23', 'model.layers.24', 'model.layers.25', 'model.layers.26', 'model.layers.27', 'model.embed_tokens']\n",
      "[Debug] Trying to access layer: model.layers.0\n",
      "[Debug] Successfully found layer: model.layers.0\n",
      "[Debug] Trying to access layer: model.layers.1\n",
      "[Debug] Successfully found layer: model.layers.1\n",
      "[Debug] Trying to access layer: model.layers.2\n",
      "[Debug] Successfully found layer: model.layers.2\n",
      "[Debug] Trying to access layer: model.layers.3\n",
      "[Debug] Successfully found layer: model.layers.3\n",
      "[Debug] Trying to access layer: model.layers.4\n",
      "[Debug] Successfully found layer: model.layers.4\n",
      "[Debug] Trying to access layer: model.layers.5\n",
      "[Debug] Successfully found layer: model.layers.5\n",
      "[Debug] Trying to access layer: model.layers.6\n",
      "[Debug] Successfully found layer: model.layers.6\n",
      "[Debug] Trying to access layer: model.layers.7\n",
      "[Debug] Successfully found layer: model.layers.7\n",
      "[Debug] Trying to access layer: model.layers.8\n",
      "[Debug] Successfully found layer: model.layers.8\n",
      "[Debug] Trying to access layer: model.layers.9\n",
      "[Debug] Successfully found layer: model.layers.9\n",
      "[Debug] Trying to access layer: model.layers.10\n",
      "[Debug] Successfully found layer: model.layers.10\n",
      "[Debug] Trying to access layer: model.layers.11\n",
      "[Debug] Successfully found layer: model.layers.11\n",
      "[Debug] Trying to access layer: model.layers.12\n",
      "[Debug] Successfully found layer: model.layers.12\n",
      "[Debug] Trying to access layer: model.layers.13\n",
      "[Debug] Successfully found layer: model.layers.13\n",
      "[Debug] Trying to access layer: model.layers.14\n",
      "[Debug] Successfully found layer: model.layers.14\n",
      "[Debug] Trying to access layer: model.layers.15\n",
      "[Debug] Successfully found layer: model.layers.15\n",
      "[Debug] Trying to access layer: model.layers.16\n",
      "[Debug] Successfully found layer: model.layers.16\n",
      "[Debug] Trying to access layer: model.layers.17\n",
      "[Debug] Successfully found layer: model.layers.17\n",
      "[Debug] Trying to access layer: model.layers.18\n",
      "[Debug] Successfully found layer: model.layers.18\n",
      "[Debug] Trying to access layer: model.layers.19\n",
      "[Debug] Successfully found layer: model.layers.19\n",
      "[Debug] Trying to access layer: model.layers.20\n",
      "[Debug] Successfully found layer: model.layers.20\n",
      "[Debug] Trying to access layer: model.layers.21\n",
      "[Debug] Successfully found layer: model.layers.21\n",
      "[Debug] Trying to access layer: model.layers.22\n",
      "[Debug] Successfully found layer: model.layers.22\n",
      "[Debug] Trying to access layer: model.layers.23\n",
      "[Debug] Successfully found layer: model.layers.23\n",
      "[Debug] Trying to access layer: model.layers.24\n",
      "[Debug] Successfully found layer: model.layers.24\n",
      "[Debug] Trying to access layer: model.layers.25\n",
      "[Debug] Successfully found layer: model.layers.25\n",
      "[Debug] Trying to access layer: model.layers.26\n",
      "[Debug] Successfully found layer: model.layers.26\n",
      "[Debug] Trying to access layer: model.layers.27\n",
      "[Debug] Successfully found layer: model.layers.27\n",
      "[Debug] Trying to access layer: model.embed_tokens\n",
      "[Debug] Successfully found layer: model.embed_tokens\n",
      "[Hook] Layer Embedding(128256, 3072, padding_idx=128004) received input (tensor([[128000,  18590,   4718,    468,   6241,    320,  31255,      8,  17646,\n",
      "           4718,    468,   6241,    574,   6689,   3221,  29492,   4409,     11,\n",
      "          19313,    449,  39970,   7314,    304,   3297,    220,   5162,     23,\n",
      "            323,  13696,    304,   6664,   8032,     17,     60,   7089,  10687,\n",
      "           2997,   6295,  24941,  11940,     11,   7188,    323,   5960,  86857,\n",
      "           5165,  20585,     26,    279,  74564,   1051,  42508,    520,  82910,\n",
      "          31362,    449,  40592,  44146,  46090,     13,    578,   4632,    596,\n",
      "           2926,   8199,    574,    400,    605,   4194,  59413,     11,   1603,\n",
      "            433,   9778,  35717,    311,    400,    508,   4194,  59413,     13,\n",
      "            362,   7446,  20900,    315,    400,   1490,     11,    931,    574,\n",
      "          52872,    311,   7710,   6445,    323,  13941,    311,    279,  39970,\n",
      "           3813,     11,    439,    279,  18585,   9689,    574,   7154,    220,\n",
      "           1399,   8931,   3201,     13,    578,  37067,   3190,   1511,    304,\n",
      "            279,   4632,   2853,    400,     17,     13,     19,   4194,  59413,\n",
      "            311,   1977,   8032,     17,     60]]),) and output tensor([[[-0.0010, -0.0007, -0.0046,  ..., -0.0014, -0.0020,  0.0020],\n",
      "         [-0.0374,  0.0035,  0.0053,  ..., -0.0208, -0.0072,  0.0130],\n",
      "         [-0.0214, -0.0131,  0.0074,  ..., -0.0121, -0.0003, -0.0029],\n",
      "         ...,\n",
      "         [ 0.0145, -0.0280, -0.0145,  ...,  0.0136, -0.0054, -0.0027],\n",
      "         [ 0.0006,  0.0145,  0.0134,  ..., -0.0007, -0.0051,  0.0189],\n",
      "         [-0.0045, -0.0086,  0.0126,  ...,  0.0082,  0.0127,  0.0007]]])\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[-0.0010, -0.0007, -0.0046,  ..., -0.0014, -0.0020,  0.0020],\n",
      "         [-0.0374,  0.0035,  0.0053,  ..., -0.0208, -0.0072,  0.0130],\n",
      "         [-0.0214, -0.0131,  0.0074,  ..., -0.0121, -0.0003, -0.0029],\n",
      "         ...,\n",
      "         [ 0.0145, -0.0280, -0.0145,  ...,  0.0136, -0.0054, -0.0027],\n",
      "         [ 0.0006,  0.0145,  0.0134,  ..., -0.0007, -0.0051,  0.0189],\n",
      "         [-0.0045, -0.0086,  0.0126,  ...,  0.0082,  0.0127,  0.0007]]]),) and output (tensor([[[ 442.9990,  239.9993,  302.9954,  ..., -154.0014,  -15.0020,\n",
      "           185.0020],\n",
      "         [ -29.0374, -345.9965,  200.0053,  ...,  107.9792,  229.9928,\n",
      "           -42.9870],\n",
      "         [  15.9786,  -28.0131,   -4.9926,  ...,  -10.0121,   -6.0003,\n",
      "           -20.0029],\n",
      "         ...,\n",
      "         [ -93.9855,   73.9720, -542.0145,  ..., -289.9864,  100.9946,\n",
      "           222.9973],\n",
      "         [  44.0006,  -57.9855,  -65.9866,  ...,  144.9993,   31.9949,\n",
      "          -194.9811],\n",
      "         [ 159.9955,  146.9914, -337.9874,  ..., -548.9918,  186.0127,\n",
      "           463.0007]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[ 442.9990,  239.9993,  302.9954,  ..., -154.0014,  -15.0020,\n",
      "           185.0020],\n",
      "         [ -29.0374, -345.9965,  200.0053,  ...,  107.9792,  229.9928,\n",
      "           -42.9870],\n",
      "         [  15.9786,  -28.0131,   -4.9926,  ...,  -10.0121,   -6.0003,\n",
      "           -20.0029],\n",
      "         ...,\n",
      "         [ -93.9855,   73.9720, -542.0145,  ..., -289.9864,  100.9946,\n",
      "           222.9973],\n",
      "         [  44.0006,  -57.9855,  -65.9866,  ...,  144.9993,   31.9949,\n",
      "          -194.9811],\n",
      "         [ 159.9955,  146.9914, -337.9874,  ..., -548.9918,  186.0127,\n",
      "           463.0007]]]),) and output (tensor([[[  439.9990,  -144.0007,  -410.0046,  ...,  -765.0015,\n",
      "           1193.9979, -1027.9980],\n",
      "         [ -454.0374,  -303.9965,   112.0053,  ..., -1263.0208,\n",
      "            186.9928,    68.0130],\n",
      "         [  391.9786,  -440.0131,  -142.9926,  ...,  -155.0121,\n",
      "            503.9997, -1271.0029],\n",
      "         ...,\n",
      "         [-1258.9855,  -574.0280,   271.9855,  ..., -1414.9863,\n",
      "           -526.0054,  1057.9973],\n",
      "         [ -654.9994,  -552.9855,  -673.9866,  ...,   571.9993,\n",
      "           -424.0051, -1019.9811],\n",
      "         [  176.9955,  -899.0086,   111.0126,  ...,   443.0082,\n",
      "            381.0127,  -281.9993]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[  439.9990,  -144.0007,  -410.0046,  ...,  -765.0015,\n",
      "           1193.9979, -1027.9980],\n",
      "         [ -454.0374,  -303.9965,   112.0053,  ..., -1263.0208,\n",
      "            186.9928,    68.0130],\n",
      "         [  391.9786,  -440.0131,  -142.9926,  ...,  -155.0121,\n",
      "            503.9997, -1271.0029],\n",
      "         ...,\n",
      "         [-1258.9855,  -574.0280,   271.9855,  ..., -1414.9863,\n",
      "           -526.0054,  1057.9973],\n",
      "         [ -654.9994,  -552.9855,  -673.9866,  ...,   571.9993,\n",
      "           -424.0051, -1019.9811],\n",
      "         [  176.9955,  -899.0086,   111.0126,  ...,   443.0082,\n",
      "            381.0127,  -281.9993]]]),) and output (tensor([[[  551.9990,   993.9993, -1790.0046,  ...,   445.9985,\n",
      "          -1386.0021, -1245.9980],\n",
      "         [ 2708.9626, -1533.9965,  1961.0052,  ..., -1279.0208,\n",
      "          -2748.0073,   567.0130],\n",
      "         [ -955.0214,   860.9869,  1400.0073,  ...,  -547.0121,\n",
      "          -4316.0005,  -998.0029],\n",
      "         ...,\n",
      "         [-1501.9855,  3660.9722,  -446.0145,  ...,  -824.9863,\n",
      "           2010.9946,  -163.0027],\n",
      "         [ -664.9994,  -607.9855, -2001.9866,  ...,    32.9993,\n",
      "          -3304.0051, -1606.9811],\n",
      "         [  846.9955,  1504.9915,  1871.0126,  ...,  5145.0083,\n",
      "          -1908.9873,  -885.9993]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[  551.9990,   993.9993, -1790.0046,  ...,   445.9985,\n",
      "          -1386.0021, -1245.9980],\n",
      "         [ 2708.9626, -1533.9965,  1961.0052,  ..., -1279.0208,\n",
      "          -2748.0073,   567.0130],\n",
      "         [ -955.0214,   860.9869,  1400.0073,  ...,  -547.0121,\n",
      "          -4316.0005,  -998.0029],\n",
      "         ...,\n",
      "         [-1501.9855,  3660.9722,  -446.0145,  ...,  -824.9863,\n",
      "           2010.9946,  -163.0027],\n",
      "         [ -664.9994,  -607.9855, -2001.9866,  ...,    32.9993,\n",
      "          -3304.0051, -1606.9811],\n",
      "         [  846.9955,  1504.9915,  1871.0126,  ...,  5145.0083,\n",
      "          -1908.9873,  -885.9993]]]),) and output (tensor([[[ 2131.9990,  -737.0007, -1071.0046,  ..., -2781.0015,\n",
      "           -885.0021,    86.0020],\n",
      "         [ 2663.9626, -3244.9966,  5827.0054,  ...,  1721.9792,\n",
      "           -122.0073,   542.0129],\n",
      "         [-3704.0215,   367.9869,  -483.9927,  ..., -1774.0121,\n",
      "            208.9995,  5535.9971],\n",
      "         ...,\n",
      "         [  344.0145,  1784.9722, -1295.0145,  ...,   936.0137,\n",
      "          -1529.0054, -2184.0027],\n",
      "         [  976.0006, -1317.9856,   981.0134,  ..., -3333.0007,\n",
      "          -1568.0049, -1390.9811],\n",
      "         [  181.9955, -1809.0085,  6338.0127,  ...,  6801.0083,\n",
      "          -1686.9873, -3778.9993]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[ 2131.9990,  -737.0007, -1071.0046,  ..., -2781.0015,\n",
      "           -885.0021,    86.0020],\n",
      "         [ 2663.9626, -3244.9966,  5827.0054,  ...,  1721.9792,\n",
      "           -122.0073,   542.0129],\n",
      "         [-3704.0215,   367.9869,  -483.9927,  ..., -1774.0121,\n",
      "            208.9995,  5535.9971],\n",
      "         ...,\n",
      "         [  344.0145,  1784.9722, -1295.0145,  ...,   936.0137,\n",
      "          -1529.0054, -2184.0027],\n",
      "         [  976.0006, -1317.9856,   981.0134,  ..., -3333.0007,\n",
      "          -1568.0049, -1390.9811],\n",
      "         [  181.9955, -1809.0085,  6338.0127,  ...,  6801.0083,\n",
      "          -1686.9873, -3778.9993]]]),) and output (tensor([[[  3420.9990,    760.9993,  -5376.0049,  ...,  -4193.0015,\n",
      "            2860.9980,  -4080.9980],\n",
      "         [  5489.9629,  -5760.9966,   3469.0054,  ...,   5306.9795,\n",
      "           -3601.0073,   2176.0129],\n",
      "         [ -2310.0215,   2722.9868,  -1913.9927,  ...,    976.9879,\n",
      "             915.9995,   1230.9971],\n",
      "         ...,\n",
      "         [ -1016.9855,   6505.9722,  -2815.0146,  ...,   4575.0137,\n",
      "          -10276.0059,   1513.9973],\n",
      "         [  5725.0005,  -2578.9856,  -1998.9866,  ...,   -400.0007,\n",
      "           -6704.0049,    635.0189],\n",
      "         [ -8748.0049,   -374.0085,   6559.0127,  ...,   3251.0083,\n",
      "           -2795.9873,  -5107.9990]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[  3420.9990,    760.9993,  -5376.0049,  ...,  -4193.0015,\n",
      "            2860.9980,  -4080.9980],\n",
      "         [  5489.9629,  -5760.9966,   3469.0054,  ...,   5306.9795,\n",
      "           -3601.0073,   2176.0129],\n",
      "         [ -2310.0215,   2722.9868,  -1913.9927,  ...,    976.9879,\n",
      "             915.9995,   1230.9971],\n",
      "         ...,\n",
      "         [ -1016.9855,   6505.9722,  -2815.0146,  ...,   4575.0137,\n",
      "          -10276.0059,   1513.9973],\n",
      "         [  5725.0005,  -2578.9856,  -1998.9866,  ...,   -400.0007,\n",
      "           -6704.0049,    635.0189],\n",
      "         [ -8748.0049,   -374.0085,   6559.0127,  ...,   3251.0083,\n",
      "           -2795.9873,  -5107.9990]]]),) and output (tensor([[[  2742.9990,  -1512.0007, -10493.0049,  ...,  -9945.0020,\n",
      "           -2017.0020,  -1802.9980],\n",
      "         [  6547.9629,  -7633.9966,   3158.0054,  ...,   1698.9795,\n",
      "             734.9927,  -1817.9871],\n",
      "         [  5549.9785,  -3817.0132,  -8371.9922,  ...,   2801.9878,\n",
      "            5684.9995,   1122.9971],\n",
      "         ...,\n",
      "         [ -4101.9854,   5833.9722,  -4645.0146,  ...,  10007.0137,\n",
      "          -17930.0059,   9230.9971],\n",
      "         [  6066.0005,  -6072.9854,  -2321.9866,  ...,  -5508.0010,\n",
      "           -9365.0049,   3913.0190],\n",
      "         [-13381.0049,   1025.9915,   5293.0127,  ...,  -2688.9917,\n",
      "           -3284.9873,   -747.9990]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[  2742.9990,  -1512.0007, -10493.0049,  ...,  -9945.0020,\n",
      "           -2017.0020,  -1802.9980],\n",
      "         [  6547.9629,  -7633.9966,   3158.0054,  ...,   1698.9795,\n",
      "             734.9927,  -1817.9871],\n",
      "         [  5549.9785,  -3817.0132,  -8371.9922,  ...,   2801.9878,\n",
      "            5684.9995,   1122.9971],\n",
      "         ...,\n",
      "         [ -4101.9854,   5833.9722,  -4645.0146,  ...,  10007.0137,\n",
      "          -17930.0059,   9230.9971],\n",
      "         [  6066.0005,  -6072.9854,  -2321.9866,  ...,  -5508.0010,\n",
      "           -9365.0049,   3913.0190],\n",
      "         [-13381.0049,   1025.9915,   5293.0127,  ...,  -2688.9917,\n",
      "           -3284.9873,   -747.9990]]]),) and output (tensor([[[  3032.9990,  -3420.0007, -10291.0049,  ...,  -6511.0020,\n",
      "           -1953.0020,   2001.0020],\n",
      "         [  8464.9629,  -4114.9966,   5099.0054,  ...,   3301.9795,\n",
      "            2794.9927,  -3276.9871],\n",
      "         [  7721.9785,  -4945.0132, -10845.9922,  ...,   4108.9878,\n",
      "            1903.9995,    966.9971],\n",
      "         ...,\n",
      "         [ -5588.9854,   5892.9722,  -8138.0146,  ...,  19231.0137,\n",
      "          -30628.0059,   3799.9971],\n",
      "         [   889.0005,  -3289.9854,  -2813.9866,  ...,  -6646.0010,\n",
      "          -12008.0049,   3525.0190],\n",
      "         [-10978.0049,  -1932.0085,  -1681.9873,  ...,   1101.0083,\n",
      "           -2141.9873,  -2167.9990]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[  3032.9990,  -3420.0007, -10291.0049,  ...,  -6511.0020,\n",
      "           -1953.0020,   2001.0020],\n",
      "         [  8464.9629,  -4114.9966,   5099.0054,  ...,   3301.9795,\n",
      "            2794.9927,  -3276.9871],\n",
      "         [  7721.9785,  -4945.0132, -10845.9922,  ...,   4108.9878,\n",
      "            1903.9995,    966.9971],\n",
      "         ...,\n",
      "         [ -5588.9854,   5892.9722,  -8138.0146,  ...,  19231.0137,\n",
      "          -30628.0059,   3799.9971],\n",
      "         [   889.0005,  -3289.9854,  -2813.9866,  ...,  -6646.0010,\n",
      "          -12008.0049,   3525.0190],\n",
      "         [-10978.0049,  -1932.0085,  -1681.9873,  ...,   1101.0083,\n",
      "           -2141.9873,  -2167.9990]]]),) and output (tensor([[[ 3.1330e+03, -1.9700e+03, -1.1544e+04,  ..., -5.7800e+03,\n",
      "          -2.4860e+03, -1.9220e+03],\n",
      "         [ 1.0657e+04, -2.9820e+03,  1.2980e+03,  ...,  2.7630e+03,\n",
      "           7.8499e+02, -1.1760e+03],\n",
      "         [ 4.2420e+03, -4.8780e+03, -1.2805e+04,  ...,  7.8710e+03,\n",
      "           2.3000e+01,  8.3050e+03],\n",
      "         ...,\n",
      "         [-2.9199e+02,  2.7930e+03, -9.3480e+03,  ...,  1.8241e+04,\n",
      "          -3.4452e+04,  7.1690e+03],\n",
      "         [ 7.3000e+01, -6.7130e+03, -2.0150e+03,  ..., -8.7500e+02,\n",
      "          -1.1236e+04,  2.8080e+03],\n",
      "         [-5.8140e+03,  9.4499e+02, -1.7099e+02,  ...,  5.0301e+02,\n",
      "          -6.1110e+03, -8.6380e+03]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[ 3.1330e+03, -1.9700e+03, -1.1544e+04,  ..., -5.7800e+03,\n",
      "          -2.4860e+03, -1.9220e+03],\n",
      "         [ 1.0657e+04, -2.9820e+03,  1.2980e+03,  ...,  2.7630e+03,\n",
      "           7.8499e+02, -1.1760e+03],\n",
      "         [ 4.2420e+03, -4.8780e+03, -1.2805e+04,  ...,  7.8710e+03,\n",
      "           2.3000e+01,  8.3050e+03],\n",
      "         ...,\n",
      "         [-2.9199e+02,  2.7930e+03, -9.3480e+03,  ...,  1.8241e+04,\n",
      "          -3.4452e+04,  7.1690e+03],\n",
      "         [ 7.3000e+01, -6.7130e+03, -2.0150e+03,  ..., -8.7500e+02,\n",
      "          -1.1236e+04,  2.8080e+03],\n",
      "         [-5.8140e+03,  9.4499e+02, -1.7099e+02,  ...,  5.0301e+02,\n",
      "          -6.1110e+03, -8.6380e+03]]]),) and output (tensor([[[  1046.9990,   2403.9993, -14486.0049,  ..., -10922.0020,\n",
      "           -1419.0020,  -2983.9980],\n",
      "         [ 11004.9629,  -4954.9966,   1446.0054,  ...,   9818.9795,\n",
      "            5556.9927,   1954.0129],\n",
      "         [ -1321.0215,  -2642.0132, -11884.9922,  ...,   6215.9883,\n",
      "           -6005.0005,  10460.9971],\n",
      "         ...,\n",
      "         [  2934.0146,   5144.9722, -14367.0146,  ...,  20964.0137,\n",
      "          -42966.0078,   2313.9971],\n",
      "         [  -365.9995,  -6212.9854,   2811.0134,  ...,    177.9990,\n",
      "          -16836.0039,    505.0190],\n",
      "         [ -5224.0049,   8103.9912,  -2763.9873,  ...,   3498.0083,\n",
      "          -12158.9873, -10669.9990]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[  1046.9990,   2403.9993, -14486.0049,  ..., -10922.0020,\n",
      "           -1419.0020,  -2983.9980],\n",
      "         [ 11004.9629,  -4954.9966,   1446.0054,  ...,   9818.9795,\n",
      "            5556.9927,   1954.0129],\n",
      "         [ -1321.0215,  -2642.0132, -11884.9922,  ...,   6215.9883,\n",
      "           -6005.0005,  10460.9971],\n",
      "         ...,\n",
      "         [  2934.0146,   5144.9722, -14367.0146,  ...,  20964.0137,\n",
      "          -42966.0078,   2313.9971],\n",
      "         [  -365.9995,  -6212.9854,   2811.0134,  ...,    177.9990,\n",
      "          -16836.0039,    505.0190],\n",
      "         [ -5224.0049,   8103.9912,  -2763.9873,  ...,   3498.0083,\n",
      "          -12158.9873, -10669.9990]]]),) and output (tensor([[[-3.7670e+03,  5.3080e+03, -1.4221e+04,  ..., -1.5035e+04,\n",
      "          -3.6300e+03,  4.1680e+03],\n",
      "         [ 1.2549e+04, -1.3620e+03, -1.6099e+02,  ...,  1.7754e+04,\n",
      "           1.0892e+04, -2.2999e+02],\n",
      "         [ 7.7998e+02,  4.2987e+01, -1.2708e+04,  ...,  9.3570e+03,\n",
      "          -4.3570e+03,  9.9010e+03],\n",
      "         ...,\n",
      "         [ 2.2050e+03, -1.0120e+03, -1.2282e+04,  ...,  1.8719e+04,\n",
      "          -5.0028e+04,  5.1290e+03],\n",
      "         [-4.4370e+03, -7.5840e+03,  2.4130e+03,  ..., -3.1400e+02,\n",
      "          -1.5826e+04,  6.6502e+02],\n",
      "         [ 4.3430e+03,  4.1810e+03, -5.6870e+03,  ...,  2.8570e+03,\n",
      "          -1.2144e+04, -1.3522e+04]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[-3.7670e+03,  5.3080e+03, -1.4221e+04,  ..., -1.5035e+04,\n",
      "          -3.6300e+03,  4.1680e+03],\n",
      "         [ 1.2549e+04, -1.3620e+03, -1.6099e+02,  ...,  1.7754e+04,\n",
      "           1.0892e+04, -2.2999e+02],\n",
      "         [ 7.7998e+02,  4.2987e+01, -1.2708e+04,  ...,  9.3570e+03,\n",
      "          -4.3570e+03,  9.9010e+03],\n",
      "         ...,\n",
      "         [ 2.2050e+03, -1.0120e+03, -1.2282e+04,  ...,  1.8719e+04,\n",
      "          -5.0028e+04,  5.1290e+03],\n",
      "         [-4.4370e+03, -7.5840e+03,  2.4130e+03,  ..., -3.1400e+02,\n",
      "          -1.5826e+04,  6.6502e+02],\n",
      "         [ 4.3430e+03,  4.1810e+03, -5.6870e+03,  ...,  2.8570e+03,\n",
      "          -1.2144e+04, -1.3522e+04]]]),) and output (tensor([[[ -1326.0010,   1223.9990, -16643.0039,  ..., -13240.0020,\n",
      "           -5677.0020,   9639.0020],\n",
      "         [ 11246.9629,   -245.9966,  -1788.9946,  ...,  20670.9805,\n",
      "           12140.9922,   2126.0129],\n",
      "         [  8000.9785,   6157.9868, -14066.9922,  ...,   6186.9883,\n",
      "           -2871.0005,   9174.9971],\n",
      "         ...,\n",
      "         [  7137.0146,  -2793.0278,  -6449.0146,  ...,  18885.0137,\n",
      "          -54704.0078,  -3095.0029],\n",
      "         [ -8494.0000,  -3099.9854,   2197.0134,  ...,     78.9990,\n",
      "          -12331.0039,  -1394.9810],\n",
      "         [  4149.9951,   7264.9912,  -2647.9873,  ...,   8753.0078,\n",
      "          -17099.9883,  -9289.9990]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[ -1326.0010,   1223.9990, -16643.0039,  ..., -13240.0020,\n",
      "           -5677.0020,   9639.0020],\n",
      "         [ 11246.9629,   -245.9966,  -1788.9946,  ...,  20670.9805,\n",
      "           12140.9922,   2126.0129],\n",
      "         [  8000.9785,   6157.9868, -14066.9922,  ...,   6186.9883,\n",
      "           -2871.0005,   9174.9971],\n",
      "         ...,\n",
      "         [  7137.0146,  -2793.0278,  -6449.0146,  ...,  18885.0137,\n",
      "          -54704.0078,  -3095.0029],\n",
      "         [ -8494.0000,  -3099.9854,   2197.0134,  ...,     78.9990,\n",
      "          -12331.0039,  -1394.9810],\n",
      "         [  4149.9951,   7264.9912,  -2647.9873,  ...,   8753.0078,\n",
      "          -17099.9883,  -9289.9990]]]),) and output (tensor([[[ -5120.0010,   2246.9990, -15993.0039,  ..., -19818.0020,\n",
      "           -9385.0020,  11924.0020],\n",
      "         [  7271.9629,  -4910.9966,  -3216.9946,  ...,  24535.9805,\n",
      "           12275.9922,   6100.0127],\n",
      "         [  8311.9785,   8916.9863, -14510.9922,  ...,  10765.9883,\n",
      "            1014.9995,   5812.9971],\n",
      "         ...,\n",
      "         [  3952.0146,   2129.9722,  -5112.0146,  ...,  20525.0137,\n",
      "          -61344.0078,  -1667.0029],\n",
      "         [   253.0000,    260.0146,   1987.0134,  ...,   -594.0010,\n",
      "          -10897.0039,  -6369.9810],\n",
      "         [  4949.9951,   5562.9912,    738.0127,  ...,  14427.0078,\n",
      "          -16101.9883, -14015.9990]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[ -5120.0010,   2246.9990, -15993.0039,  ..., -19818.0020,\n",
      "           -9385.0020,  11924.0020],\n",
      "         [  7271.9629,  -4910.9966,  -3216.9946,  ...,  24535.9805,\n",
      "           12275.9922,   6100.0127],\n",
      "         [  8311.9785,   8916.9863, -14510.9922,  ...,  10765.9883,\n",
      "            1014.9995,   5812.9971],\n",
      "         ...,\n",
      "         [  3952.0146,   2129.9722,  -5112.0146,  ...,  20525.0137,\n",
      "          -61344.0078,  -1667.0029],\n",
      "         [   253.0000,    260.0146,   1987.0134,  ...,   -594.0010,\n",
      "          -10897.0039,  -6369.9810],\n",
      "         [  4949.9951,   5562.9912,    738.0127,  ...,  14427.0078,\n",
      "          -16101.9883, -14015.9990]]]),) and output (tensor([[[ -6685.0010,  -1278.0010, -15553.0039,  ..., -19846.0020,\n",
      "          -10418.0020,  13376.0020],\n",
      "         [ 11409.9629,  -9828.9961,   -767.9946,  ...,  27825.9805,\n",
      "           10437.9922,  10812.0127],\n",
      "         [  6798.9785,   6682.9863, -14264.9922,  ...,   6959.9883,\n",
      "            4282.9995,   4390.9971],\n",
      "         ...,\n",
      "         [   999.0146,   -365.0278,  -7161.0146,  ...,  24103.0137,\n",
      "          -64118.0078,  -4649.0029],\n",
      "         [  4420.0000,   1497.0146,   3557.0134,  ...,    691.9990,\n",
      "          -15973.0039,  -6359.9810],\n",
      "         [  8733.9951,   8312.9912,    466.0127,  ...,  10405.0078,\n",
      "          -22513.9883, -18121.0000]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[ -6685.0010,  -1278.0010, -15553.0039,  ..., -19846.0020,\n",
      "          -10418.0020,  13376.0020],\n",
      "         [ 11409.9629,  -9828.9961,   -767.9946,  ...,  27825.9805,\n",
      "           10437.9922,  10812.0127],\n",
      "         [  6798.9785,   6682.9863, -14264.9922,  ...,   6959.9883,\n",
      "            4282.9995,   4390.9971],\n",
      "         ...,\n",
      "         [   999.0146,   -365.0278,  -7161.0146,  ...,  24103.0137,\n",
      "          -64118.0078,  -4649.0029],\n",
      "         [  4420.0000,   1497.0146,   3557.0134,  ...,    691.9990,\n",
      "          -15973.0039,  -6359.9810],\n",
      "         [  8733.9951,   8312.9912,    466.0127,  ...,  10405.0078,\n",
      "          -22513.9883, -18121.0000]]]),) and output (tensor([[[ -8422.0010,    339.9990, -15289.0039,  ..., -20884.0020,\n",
      "           -6908.0020,  11014.0020],\n",
      "         [ 13543.9629,  -9934.9961,  -4327.9946,  ...,  28840.9805,\n",
      "            7997.9922,   5223.0127],\n",
      "         [ 10714.9785,   8652.9863, -17851.9922,  ...,    171.9883,\n",
      "            8954.0000,  -5694.0029],\n",
      "         ...,\n",
      "         [ -3018.9854,   6061.9722,  -7086.0146,  ...,  19054.0137,\n",
      "          -70452.0078,   -170.0029],\n",
      "         [  4346.0000,   1358.0146,   7371.0137,  ...,   2869.9990,\n",
      "          -17055.0039, -11908.9805],\n",
      "         [  6890.9951,   9469.9912,   2158.0127,  ...,  14929.0078,\n",
      "          -19579.9883, -16018.0000]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[ -8422.0010,    339.9990, -15289.0039,  ..., -20884.0020,\n",
      "           -6908.0020,  11014.0020],\n",
      "         [ 13543.9629,  -9934.9961,  -4327.9946,  ...,  28840.9805,\n",
      "            7997.9922,   5223.0127],\n",
      "         [ 10714.9785,   8652.9863, -17851.9922,  ...,    171.9883,\n",
      "            8954.0000,  -5694.0029],\n",
      "         ...,\n",
      "         [ -3018.9854,   6061.9722,  -7086.0146,  ...,  19054.0137,\n",
      "          -70452.0078,   -170.0029],\n",
      "         [  4346.0000,   1358.0146,   7371.0137,  ...,   2869.9990,\n",
      "          -17055.0039, -11908.9805],\n",
      "         [  6890.9951,   9469.9912,   2158.0127,  ...,  14929.0078,\n",
      "          -19579.9883, -16018.0000]]]),) and output (tensor([[[-10989.0010,   7034.9990, -13964.0039,  ..., -23968.0020,\n",
      "           -3749.0020,  11730.0020],\n",
      "         [ 13752.9629,  -6387.9961,  -8999.9941,  ...,  34072.9805,\n",
      "            8815.9922,  -3599.9873],\n",
      "         [ 11205.9785,   6961.9863, -22584.9922,  ...,   8136.9883,\n",
      "            9162.0000,  -6642.0029],\n",
      "         ...,\n",
      "         [  -716.9854,   6474.9722,  -5497.0146,  ...,  16479.0137,\n",
      "          -76271.0078,   3278.9971],\n",
      "         [  5303.0000,  -3964.9854,   9564.0137,  ...,   7039.9990,\n",
      "          -14256.0039, -13147.9805],\n",
      "         [  7444.9951,   7447.9912,   2992.0127,  ...,  12789.0078,\n",
      "          -21908.9883, -13995.0000]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[-10989.0010,   7034.9990, -13964.0039,  ..., -23968.0020,\n",
      "           -3749.0020,  11730.0020],\n",
      "         [ 13752.9629,  -6387.9961,  -8999.9941,  ...,  34072.9805,\n",
      "            8815.9922,  -3599.9873],\n",
      "         [ 11205.9785,   6961.9863, -22584.9922,  ...,   8136.9883,\n",
      "            9162.0000,  -6642.0029],\n",
      "         ...,\n",
      "         [  -716.9854,   6474.9722,  -5497.0146,  ...,  16479.0137,\n",
      "          -76271.0078,   3278.9971],\n",
      "         [  5303.0000,  -3964.9854,   9564.0137,  ...,   7039.9990,\n",
      "          -14256.0039, -13147.9805],\n",
      "         [  7444.9951,   7447.9912,   2992.0127,  ...,  12789.0078,\n",
      "          -21908.9883, -13995.0000]]]),) and output (tensor([[[ -7159.0010,   6621.9990, -10967.0039,  ..., -17477.0020,\n",
      "           -2510.0020,  11888.0020],\n",
      "         [  8630.9629,   -867.9961, -11261.9941,  ...,  43388.9805,\n",
      "           10866.9922,   3442.0127],\n",
      "         [  6116.9785,   9069.9863, -17716.9922,  ...,  12392.9883,\n",
      "            9376.0000,  -1272.0029],\n",
      "         ...,\n",
      "         [  4987.0146,   7756.9722,  -2181.0146,  ...,  18666.0137,\n",
      "          -78886.0078,   4532.9971],\n",
      "         [ 13538.0000,  -9012.9854,   4812.0137,  ...,  10637.9990,\n",
      "          -15663.0039, -12964.9805],\n",
      "         [  8236.9951,  10903.9912,   2843.0127,  ...,  10191.0078,\n",
      "          -20440.9883, -18614.0000]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[ -7159.0010,   6621.9990, -10967.0039,  ..., -17477.0020,\n",
      "           -2510.0020,  11888.0020],\n",
      "         [  8630.9629,   -867.9961, -11261.9941,  ...,  43388.9805,\n",
      "           10866.9922,   3442.0127],\n",
      "         [  6116.9785,   9069.9863, -17716.9922,  ...,  12392.9883,\n",
      "            9376.0000,  -1272.0029],\n",
      "         ...,\n",
      "         [  4987.0146,   7756.9722,  -2181.0146,  ...,  18666.0137,\n",
      "          -78886.0078,   4532.9971],\n",
      "         [ 13538.0000,  -9012.9854,   4812.0137,  ...,  10637.9990,\n",
      "          -15663.0039, -12964.9805],\n",
      "         [  8236.9951,  10903.9912,   2843.0127,  ...,  10191.0078,\n",
      "          -20440.9883, -18614.0000]]]),) and output (tensor([[[-4.8220e+03,  3.1250e+03, -3.8850e+03,  ..., -1.9007e+04,\n",
      "          -5.5100e+02,  1.6285e+04],\n",
      "         [ 6.7930e+03, -5.3320e+03, -1.2075e+04,  ...,  4.7627e+04,\n",
      "           7.3930e+03,  4.1100e+03],\n",
      "         [ 1.0584e+04,  1.0571e+04, -1.6899e+04,  ...,  1.4614e+04,\n",
      "           3.8680e+03,  1.3330e+03],\n",
      "         ...,\n",
      "         [ 6.5160e+03,  2.8097e+02, -1.5440e+03,  ...,  1.7679e+04,\n",
      "          -8.4624e+04,  1.0800e+02],\n",
      "         [ 9.9680e+03, -9.3490e+03,  5.5030e+03,  ...,  1.4792e+04,\n",
      "          -1.2362e+04, -1.3487e+04],\n",
      "         [ 6.5540e+03,  1.2605e+04,  8.4013e+01,  ...,  1.1354e+04,\n",
      "          -1.8923e+04, -2.3004e+04]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[-4.8220e+03,  3.1250e+03, -3.8850e+03,  ..., -1.9007e+04,\n",
      "          -5.5100e+02,  1.6285e+04],\n",
      "         [ 6.7930e+03, -5.3320e+03, -1.2075e+04,  ...,  4.7627e+04,\n",
      "           7.3930e+03,  4.1100e+03],\n",
      "         [ 1.0584e+04,  1.0571e+04, -1.6899e+04,  ...,  1.4614e+04,\n",
      "           3.8680e+03,  1.3330e+03],\n",
      "         ...,\n",
      "         [ 6.5160e+03,  2.8097e+02, -1.5440e+03,  ...,  1.7679e+04,\n",
      "          -8.4624e+04,  1.0800e+02],\n",
      "         [ 9.9680e+03, -9.3490e+03,  5.5030e+03,  ...,  1.4792e+04,\n",
      "          -1.2362e+04, -1.3487e+04],\n",
      "         [ 6.5540e+03,  1.2605e+04,  8.4013e+01,  ...,  1.1354e+04,\n",
      "          -1.8923e+04, -2.3004e+04]]]),) and output (tensor([[[ -7235.0010,   6583.9990,  -4212.0039,  ..., -19104.0020,\n",
      "            3087.9980,  17084.0020],\n",
      "         [ 10696.9629,  -9092.9961, -13475.9941,  ...,  53203.9805,\n",
      "            9724.9922,   6907.0127],\n",
      "         [ 10717.9785,  12602.9863, -19518.9922,  ...,  15324.9883,\n",
      "           -4450.0000,   5261.9971],\n",
      "         ...,\n",
      "         [  4649.0146,  -4437.0278,  -4070.0146,  ...,  20224.0137,\n",
      "          -89403.0078,   4671.9971],\n",
      "         [ 10567.0000, -13464.9854,  -1007.9863,  ...,   9136.9990,\n",
      "          -11442.0039, -10338.9805],\n",
      "         [  6500.9951,  12196.9912,   8942.0127,  ...,  14816.0078,\n",
      "          -20042.9883, -12935.0000]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[ -7235.0010,   6583.9990,  -4212.0039,  ..., -19104.0020,\n",
      "            3087.9980,  17084.0020],\n",
      "         [ 10696.9629,  -9092.9961, -13475.9941,  ...,  53203.9805,\n",
      "            9724.9922,   6907.0127],\n",
      "         [ 10717.9785,  12602.9863, -19518.9922,  ...,  15324.9883,\n",
      "           -4450.0000,   5261.9971],\n",
      "         ...,\n",
      "         [  4649.0146,  -4437.0278,  -4070.0146,  ...,  20224.0137,\n",
      "          -89403.0078,   4671.9971],\n",
      "         [ 10567.0000, -13464.9854,  -1007.9863,  ...,   9136.9990,\n",
      "          -11442.0039, -10338.9805],\n",
      "         [  6500.9951,  12196.9912,   8942.0127,  ...,  14816.0078,\n",
      "          -20042.9883, -12935.0000]]]),) and output (tensor([[[-5.5860e+03,  7.6460e+03, -5.0500e+02,  ..., -1.5346e+04,\n",
      "          -6.2300e+02,  1.2986e+04],\n",
      "         [ 1.3488e+04, -6.4780e+03, -1.1754e+04,  ...,  5.1514e+04,\n",
      "           1.1358e+04,  2.9013e+01],\n",
      "         [ 5.6860e+03,  5.7260e+03, -2.3459e+04,  ...,  1.4622e+04,\n",
      "          -7.4700e+02,  1.4252e+04],\n",
      "         ...,\n",
      "         [ 2.4280e+03, -2.6030e+03,  3.3060e+03,  ...,  1.5046e+04,\n",
      "          -9.7148e+04,  1.4100e+03],\n",
      "         [ 2.8995e+04, -8.9800e+03,  9.8101e+02,  ...,  9.7920e+03,\n",
      "          -1.0031e+04, -1.4123e+04],\n",
      "         [ 1.0409e+04,  7.6320e+03,  9.3520e+03,  ...,  2.2546e+04,\n",
      "          -2.1236e+04, -1.1985e+04]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[-5.5860e+03,  7.6460e+03, -5.0500e+02,  ..., -1.5346e+04,\n",
      "          -6.2300e+02,  1.2986e+04],\n",
      "         [ 1.3488e+04, -6.4780e+03, -1.1754e+04,  ...,  5.1514e+04,\n",
      "           1.1358e+04,  2.9013e+01],\n",
      "         [ 5.6860e+03,  5.7260e+03, -2.3459e+04,  ...,  1.4622e+04,\n",
      "          -7.4700e+02,  1.4252e+04],\n",
      "         ...,\n",
      "         [ 2.4280e+03, -2.6030e+03,  3.3060e+03,  ...,  1.5046e+04,\n",
      "          -9.7148e+04,  1.4100e+03],\n",
      "         [ 2.8995e+04, -8.9800e+03,  9.8101e+02,  ...,  9.7920e+03,\n",
      "          -1.0031e+04, -1.4123e+04],\n",
      "         [ 1.0409e+04,  7.6320e+03,  9.3520e+03,  ...,  2.2546e+04,\n",
      "          -2.1236e+04, -1.1985e+04]]]),) and output (tensor([[[ -8916.0010,   2362.9990,  -4833.0039,  ..., -12075.0020,\n",
      "           -1124.0020,  12438.0020],\n",
      "         [ 16469.9629,  -9309.9961, -11567.9941,  ...,  53081.9805,\n",
      "             262.9922,  -7243.9873],\n",
      "         [  6116.9785,   4523.9863, -22103.9922,  ...,  14167.9883,\n",
      "             809.0000,  17757.9961],\n",
      "         ...,\n",
      "         [  6310.0146,   1998.9722,   6304.9854,  ...,   9046.0137,\n",
      "          -94322.0078,   2709.9971],\n",
      "         [ 25949.0000,  -6830.9854,  -2350.9863,  ...,  20387.0000,\n",
      "           -8784.0039, -12941.9805],\n",
      "         [  7816.9951,  10800.9912,  10337.0127,  ...,  29673.0078,\n",
      "          -20258.9883, -11358.0000]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[ -8916.0010,   2362.9990,  -4833.0039,  ..., -12075.0020,\n",
      "           -1124.0020,  12438.0020],\n",
      "         [ 16469.9629,  -9309.9961, -11567.9941,  ...,  53081.9805,\n",
      "             262.9922,  -7243.9873],\n",
      "         [  6116.9785,   4523.9863, -22103.9922,  ...,  14167.9883,\n",
      "             809.0000,  17757.9961],\n",
      "         ...,\n",
      "         [  6310.0146,   1998.9722,   6304.9854,  ...,   9046.0137,\n",
      "          -94322.0078,   2709.9971],\n",
      "         [ 25949.0000,  -6830.9854,  -2350.9863,  ...,  20387.0000,\n",
      "           -8784.0039, -12941.9805],\n",
      "         [  7816.9951,  10800.9912,  10337.0127,  ...,  29673.0078,\n",
      "          -20258.9883, -11358.0000]]]),) and output (tensor([[[-2.0230e+04,  7.8780e+03,  1.6200e+02,  ..., -1.0250e+04,\n",
      "           6.7700e+02,  7.0950e+03],\n",
      "         [ 1.7522e+04, -6.0430e+03, -1.2288e+04,  ...,  5.8186e+04,\n",
      "           3.4840e+03,  3.0013e+01],\n",
      "         [ 1.7806e+04,  3.1040e+03, -2.2693e+04,  ...,  1.9495e+04,\n",
      "          -5.2910e+03,  1.4740e+04],\n",
      "         ...,\n",
      "         [ 4.3990e+03, -1.2103e+02,  2.9720e+03,  ...,  9.3640e+03,\n",
      "          -9.0957e+04,  1.1341e+04],\n",
      "         [ 2.7488e+04, -1.3013e+04,  1.7290e+03,  ...,  1.6927e+04,\n",
      "          -7.2060e+03, -1.3213e+04],\n",
      "         [ 1.1441e+04,  8.5380e+03,  6.9850e+03,  ...,  2.2669e+04,\n",
      "          -2.1875e+04, -1.3593e+04]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[-2.0230e+04,  7.8780e+03,  1.6200e+02,  ..., -1.0250e+04,\n",
      "           6.7700e+02,  7.0950e+03],\n",
      "         [ 1.7522e+04, -6.0430e+03, -1.2288e+04,  ...,  5.8186e+04,\n",
      "           3.4840e+03,  3.0013e+01],\n",
      "         [ 1.7806e+04,  3.1040e+03, -2.2693e+04,  ...,  1.9495e+04,\n",
      "          -5.2910e+03,  1.4740e+04],\n",
      "         ...,\n",
      "         [ 4.3990e+03, -1.2103e+02,  2.9720e+03,  ...,  9.3640e+03,\n",
      "          -9.0957e+04,  1.1341e+04],\n",
      "         [ 2.7488e+04, -1.3013e+04,  1.7290e+03,  ...,  1.6927e+04,\n",
      "          -7.2060e+03, -1.3213e+04],\n",
      "         [ 1.1441e+04,  8.5380e+03,  6.9850e+03,  ...,  2.2669e+04,\n",
      "          -2.1875e+04, -1.3593e+04]]]),) and output (tensor([[[-25670.0000,  13293.9990,   9099.9961,  ...,  -6547.0020,\n",
      "           -5658.0020,   4192.0020],\n",
      "         [ 16075.9629,  -1484.9961, -14697.9941,  ...,  61454.9805,\n",
      "            9409.9922,   2800.0127],\n",
      "         [ 17240.9785,   9937.9863, -22494.9922,  ...,  18301.9883,\n",
      "           -7142.0000,  14410.9961],\n",
      "         ...,\n",
      "         [  2191.0146,  -1195.0278,   8866.9854,  ...,  16684.0137,\n",
      "          -97237.0078,   -562.0029],\n",
      "         [ 30386.0000,  -9713.9854,    893.0137,  ...,  26925.0000,\n",
      "          -12537.0039, -16304.9805],\n",
      "         [ 10495.9951,   7832.9912,   9270.0127,  ...,  22713.0078,\n",
      "          -17054.9883, -16856.0000]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[-25670.0000,  13293.9990,   9099.9961,  ...,  -6547.0020,\n",
      "           -5658.0020,   4192.0020],\n",
      "         [ 16075.9629,  -1484.9961, -14697.9941,  ...,  61454.9805,\n",
      "            9409.9922,   2800.0127],\n",
      "         [ 17240.9785,   9937.9863, -22494.9922,  ...,  18301.9883,\n",
      "           -7142.0000,  14410.9961],\n",
      "         ...,\n",
      "         [  2191.0146,  -1195.0278,   8866.9854,  ...,  16684.0137,\n",
      "          -97237.0078,   -562.0029],\n",
      "         [ 30386.0000,  -9713.9854,    893.0137,  ...,  26925.0000,\n",
      "          -12537.0039, -16304.9805],\n",
      "         [ 10495.9951,   7832.9912,   9270.0127,  ...,  22713.0078,\n",
      "          -17054.9883, -16856.0000]]]),) and output (tensor([[[-25652.0000,  13180.9990,  15728.9961,  ...,  -2831.0020,\n",
      "             158.9980,   2204.0020],\n",
      "         [ 19462.9629,   6514.0039, -15112.9941,  ...,  54675.9805,\n",
      "           10210.9922,  -4149.9873],\n",
      "         [ 20774.9785,   6503.9863, -27844.9922,  ...,  18037.9883,\n",
      "             463.0000,  11084.9961],\n",
      "         ...,\n",
      "         [  4233.0146,   1459.9722,   2802.9854,  ...,  12615.0137,\n",
      "          -99086.0078,   -525.0029],\n",
      "         [ 32847.0000,  -4807.9854,   2008.0137,  ...,  25242.0000,\n",
      "          -13544.0039, -24976.9805],\n",
      "         [ 10096.9951,  18452.9922,  15225.0127,  ...,  24585.0078,\n",
      "          -17269.9883, -21770.0000]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[-25652.0000,  13180.9990,  15728.9961,  ...,  -2831.0020,\n",
      "             158.9980,   2204.0020],\n",
      "         [ 19462.9629,   6514.0039, -15112.9941,  ...,  54675.9805,\n",
      "           10210.9922,  -4149.9873],\n",
      "         [ 20774.9785,   6503.9863, -27844.9922,  ...,  18037.9883,\n",
      "             463.0000,  11084.9961],\n",
      "         ...,\n",
      "         [  4233.0146,   1459.9722,   2802.9854,  ...,  12615.0137,\n",
      "          -99086.0078,   -525.0029],\n",
      "         [ 32847.0000,  -4807.9854,   2008.0137,  ...,  25242.0000,\n",
      "          -13544.0039, -24976.9805],\n",
      "         [ 10096.9951,  18452.9922,  15225.0127,  ...,  24585.0078,\n",
      "          -17269.9883, -21770.0000]]]),) and output (tensor([[[-14791.0000,  15299.9990,  16129.9961,  ...,   3187.9980,\n",
      "           -2552.0020,   5626.0020],\n",
      "         [ 15256.9629,  -5311.9961,  -5580.9941,  ...,  56620.9805,\n",
      "            5110.9922,  -6148.9873],\n",
      "         [ 16542.9785,  -2503.0137, -26591.9922,  ...,  20950.9883,\n",
      "           -3594.0000,  17557.9961],\n",
      "         ...,\n",
      "         [ -2442.9854,  -1750.0278,   2428.9854,  ...,  20290.0137,\n",
      "          -97616.0078,  -5482.0029],\n",
      "         [ 30655.0000,  -5849.9854,   3893.0137,  ...,  24020.0000,\n",
      "           -5834.0039, -26963.9805],\n",
      "         [ 14773.9951,  19598.9922,  12815.0127,  ...,  25137.0078,\n",
      "          -22914.9883, -11280.0000]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[-14791.0000,  15299.9990,  16129.9961,  ...,   3187.9980,\n",
      "           -2552.0020,   5626.0020],\n",
      "         [ 15256.9629,  -5311.9961,  -5580.9941,  ...,  56620.9805,\n",
      "            5110.9922,  -6148.9873],\n",
      "         [ 16542.9785,  -2503.0137, -26591.9922,  ...,  20950.9883,\n",
      "           -3594.0000,  17557.9961],\n",
      "         ...,\n",
      "         [ -2442.9854,  -1750.0278,   2428.9854,  ...,  20290.0137,\n",
      "          -97616.0078,  -5482.0029],\n",
      "         [ 30655.0000,  -5849.9854,   3893.0137,  ...,  24020.0000,\n",
      "           -5834.0039, -26963.9805],\n",
      "         [ 14773.9951,  19598.9922,  12815.0127,  ...,  25137.0078,\n",
      "          -22914.9883, -11280.0000]]]),) and output (tensor([[[ -13284.0000,   26639.0000,    7467.9961,  ...,    4113.9980,\n",
      "            -6608.0020,    5336.0020],\n",
      "         [  17708.9629,   -5511.9961,   -4457.9941,  ...,   61547.9805,\n",
      "             1730.9922,    2647.0127],\n",
      "         [  16965.9785,    7266.9863,  -25708.9922,  ...,   24229.9883,\n",
      "            -5837.0000,   14304.9961],\n",
      "         ...,\n",
      "         [  -5637.9854,   -7958.0278,   -1642.0146,  ...,   17814.0137,\n",
      "          -101178.0078,   -9794.0029],\n",
      "         [  29909.0000,  -10244.9854,    4996.0137,  ...,   32584.0000,\n",
      "            -1533.0039,  -26754.9805],\n",
      "         [  18696.9961,   12148.9922,   20386.0117,  ...,   16723.0078,\n",
      "           -24138.9883,   -1364.0000]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[ -13284.0000,   26639.0000,    7467.9961,  ...,    4113.9980,\n",
      "            -6608.0020,    5336.0020],\n",
      "         [  17708.9629,   -5511.9961,   -4457.9941,  ...,   61547.9805,\n",
      "             1730.9922,    2647.0127],\n",
      "         [  16965.9785,    7266.9863,  -25708.9922,  ...,   24229.9883,\n",
      "            -5837.0000,   14304.9961],\n",
      "         ...,\n",
      "         [  -5637.9854,   -7958.0278,   -1642.0146,  ...,   17814.0137,\n",
      "          -101178.0078,   -9794.0029],\n",
      "         [  29909.0000,  -10244.9854,    4996.0137,  ...,   32584.0000,\n",
      "            -1533.0039,  -26754.9805],\n",
      "         [  18696.9961,   12148.9922,   20386.0117,  ...,   16723.0078,\n",
      "           -24138.9883,   -1364.0000]]]),) and output (tensor([[[-21532.0000,  32796.0000,  16281.9961,  ...,  18208.9980,\n",
      "           -4282.0020,   5756.0020],\n",
      "         [ 28175.9629, -13254.9961,  -2207.9941,  ...,  62036.9805,\n",
      "            1431.9922,   6219.0127],\n",
      "         [ 18652.9785,  -2342.0137, -27323.9922,  ...,  26172.9883,\n",
      "           -9242.0000,  15235.9961],\n",
      "         ...,\n",
      "         [ -4981.9854,  -9374.0273,  -2510.0146,  ...,  11481.0137,\n",
      "          -88936.0078,   2888.9971],\n",
      "         [ 34817.0000,  -3584.9854,   7040.0137,  ...,  28507.0000,\n",
      "            9263.9961, -21965.9805],\n",
      "         [ 24799.9961,   6909.9922,  32554.0117,  ...,  21393.0078,\n",
      "          -20107.9883,  -1937.0000]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[-21532.0000,  32796.0000,  16281.9961,  ...,  18208.9980,\n",
      "           -4282.0020,   5756.0020],\n",
      "         [ 28175.9629, -13254.9961,  -2207.9941,  ...,  62036.9805,\n",
      "            1431.9922,   6219.0127],\n",
      "         [ 18652.9785,  -2342.0137, -27323.9922,  ...,  26172.9883,\n",
      "           -9242.0000,  15235.9961],\n",
      "         ...,\n",
      "         [ -4981.9854,  -9374.0273,  -2510.0146,  ...,  11481.0137,\n",
      "          -88936.0078,   2888.9971],\n",
      "         [ 34817.0000,  -3584.9854,   7040.0137,  ...,  28507.0000,\n",
      "            9263.9961, -21965.9805],\n",
      "         [ 24799.9961,   6909.9922,  32554.0117,  ...,  21393.0078,\n",
      "          -20107.9883,  -1937.0000]]]),) and output (tensor([[[-24082.0000,  33387.0000,  15374.9961,  ...,  18359.9980,\n",
      "           -7500.0020,  11519.0020],\n",
      "         [ 27982.9629, -12990.9961,  -4529.9941,  ...,  54972.9805,\n",
      "          -15263.0078,  -1555.9873],\n",
      "         [ 30118.9785,    836.9863, -21855.9922,  ...,  27276.9883,\n",
      "          -16970.0000,  12672.9961],\n",
      "         ...,\n",
      "         [ -1603.9854, -14979.0273,  -2250.0146,  ...,  14910.0137,\n",
      "          -85832.0078,   8824.9971],\n",
      "         [ 21311.0000,  -7651.9854,   6597.0137,  ...,  19915.0000,\n",
      "            7355.9961,  -9852.9805],\n",
      "         [ 23606.9961,  14259.9922,  22818.0117,  ...,  25746.0078,\n",
      "          -21129.9883, -11177.0000]]]),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): BitLinear()\n",
      "    (k_proj): BitLinear()\n",
      "    (v_proj): BitLinear()\n",
      "    (o_proj): BitLinear()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): BitLinear()\n",
      "    (up_proj): BitLinear()\n",
      "    (down_proj): BitLinear()\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      ") received input (tensor([[[-24082.0000,  33387.0000,  15374.9961,  ...,  18359.9980,\n",
      "           -7500.0020,  11519.0020],\n",
      "         [ 27982.9629, -12990.9961,  -4529.9941,  ...,  54972.9805,\n",
      "          -15263.0078,  -1555.9873],\n",
      "         [ 30118.9785,    836.9863, -21855.9922,  ...,  27276.9883,\n",
      "          -16970.0000,  12672.9961],\n",
      "         ...,\n",
      "         [ -1603.9854, -14979.0273,  -2250.0146,  ...,  14910.0137,\n",
      "          -85832.0078,   8824.9971],\n",
      "         [ 21311.0000,  -7651.9854,   6597.0137,  ...,  19915.0000,\n",
      "            7355.9961,  -9852.9805],\n",
      "         [ 23606.9961,  14259.9922,  22818.0117,  ...,  25746.0078,\n",
      "          -21129.9883, -11177.0000]]]),) and output (tensor([[[-14869.0000,  32284.0000,   3742.9961,  ...,  12458.9980,\n",
      "           -5452.0020,   7590.0020],\n",
      "         [ 17636.9629, -24437.9961,  -2486.9941,  ...,  48441.9805,\n",
      "          -16782.0078,  -1205.9873],\n",
      "         [ 30914.9785,  -3500.0137, -23827.9922,  ...,  26644.9883,\n",
      "          -22645.0000,  11836.9961],\n",
      "         ...,\n",
      "         [  -540.9854,  -7368.0273,  -4250.0146,  ...,  21271.0137,\n",
      "          -78893.0078,  12876.9971],\n",
      "         [ 21630.0000, -19638.9844,  11635.0137,  ...,  16644.0000,\n",
      "            7383.9961,  -7771.9805],\n",
      "         [ 31170.9961,  17092.9922,  30909.0117,  ...,  35471.0078,\n",
      "          -42206.9883,  -9133.0000]]]),)\n"
     ]
    }
   ],
   "source": [
    "logit_lens.plot_topk_logit_lens(\n",
    "    model=dh3b_bitnet_fp32_ptsq,\n",
    "    tokenizer=dh3b_tokenizer,\n",
    "    inputs=nq_answers,\n",
    "    start_ix=0, end_ix=15,\n",
    "    topk=5,\n",
    "    topk_mean=True,\n",
    "    plot_topk_lens=False,\n",
    "    #entropy=True,\n",
    "    block_step=1,\n",
    "    token_font_size=18,\n",
    "    #json_log_path=None,\n",
    "    json_log_path='logs/nq_answers/dh.3b-ptsq.fp32.json', # 20 samples\n",
    "    #save_fig_path=None,\n",
    "    #save_fig_path='Outputs/LogitLens/DH3B/logits_3b_fp32_math.jpg',\n",
    "    model_precision=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dh8b_bnb8_float32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_path = 'logs/gsm8k/llama.8b-1.58.fp32'\n",
    "\n",
    "with open(json_path, 'r') as f:\n",
    "    log_data = json.load(f)\n",
    "\n",
    "# Convert the loaded JSON data to a DataFrame\n",
    "df = pd.json_normalize(log_data)\n",
    "\n",
    "\n",
    "print(df)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('logs/gsm8k/dh.3b-ptsq.fp32', 'r') as f:\n",
    "    log_data = json.load(f)\n",
    "df = pd.json_normalize(log_data)\n",
    "\n",
    "# Ensure each row's layer_names and entropy have matching length\n",
    "num_layers = len(df.loc[0, 'layer_names'])\n",
    "sum_entropy = [0.0] * num_layers\n",
    "valid_rows = 0\n",
    "\n",
    "for _, row in df.iterrows():\n",
    "    layer_names = row['layer_names']\n",
    "    entropy = row['entropy']\n",
    "    \n",
    "    if isinstance(entropy, list) and len(entropy) == num_layers:\n",
    "        sum_entropy = [s + e for s, e in zip(sum_entropy, entropy)]\n",
    "        valid_rows += 1\n",
    "\n",
    "# Compute average\n",
    "if valid_rows > 0:\n",
    "    avg_entropy = [e / valid_rows for e in sum_entropy]\n",
    "    layer_labels = df.loc[0, 'layer_names']\n",
    "\n",
    "    # Plotting\n",
    "    plt.figure(figsize=(14, 6))\n",
    "    plt.plot(layer_labels, avg_entropy, marker='o', linestyle='-', color='b')\n",
    "    plt.xlabel('Layer Name')\n",
    "    plt.ylabel('Average Entropy')\n",
    "    plt.title(f'Average Entropy Across Layers (n = {valid_rows} samples)')\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No valid rows matched expected layer length.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['entropy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['normalized_entropy']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dictionary Learning: SAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch._dynamo\n",
    "torch._dynamo.config.suppress_errors = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dh3b_bitnet_fp32_ptsq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary_learning.plot_sae_heatmap(\n",
    "    model=dh3b_fp32,\n",
    "    tokenizer=dh3b_tokenizer,\n",
    "    inputs=gsm8k_questions_sae,\n",
    "    plot_sae=True,\n",
    "    do_log=True,\n",
    "    top_k=5,\n",
    "    tokens_per_row=30,\n",
    "    target_layers=[2, 9, 16, 20, 26],\n",
    "    log_path='logs/sae_logs/DH3B/fp',\n",
    "    log_name='dh.3b-ptsq.fp32',\n",
    "    fig_path=None,\n",
    "    deterministic_sae=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary_learning.plot_sae_heatmap(\n",
    "    model=olmo1b_fp32,\n",
    "    tokenizer=olmo1b_tokenizer,\n",
    "    inputs=Texts.T1.value,\n",
    "    do_log=False,\n",
    "    top_k=5,\n",
    "    tokens_per_row=30,\n",
    "    target_layers=[5, 10, 15],\n",
    "    log_path=None,\n",
    "    log_name=None,\n",
    "    fig_path=None,\n",
    "    deterministic_sae=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary_learning.plot_comparing_heatmap(\n",
    "    models=(dh3b_fp32, dh3b_bitnet_fp32_ptsq),\n",
    "    tokenizer=dh3b_tokenizer,\n",
    "    inputs=gsm8k_questions_sae,\n",
    "    top_k=5,\n",
    "    tokens_per_row=30,\n",
    "    target_layers=[2, 9, 16, 20, 26],\n",
    "    fig_path=None,\n",
    "    deterministic_sae=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary_learning.plot_comparing_heatmap(\n",
    "    models=(llama8b_fp32, hfbit1_fp32),\n",
    "    tokenizer=llama8b_tokenizer,\n",
    "    inputs=gsm8k_questions_sae,\n",
    "    top_k=5,\n",
    "    tokens_per_row=30,\n",
    "    target_layers=[2, 9, 16, 23, 30],\n",
    "    fig_path=None\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# QA on Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_dict = {\n",
    "    #'dh.3b-llama.fp32': dh3b_fp32,\n",
    "    #'dh.3b-bnb4bit.fp16': dh3b_bnb4_fp16,\n",
    "    #'dh.3b-1.58.ptdq': dh3b_bitnet_fp32, \n",
    "    #'dh.3b-1.58.ptsq': dh3b_bitnet_fp32,\n",
    "    #'dh.8b-llama.fp32': dh8b_fp32,\n",
    "    #'dh.8b-bnb4bit.fp16': dh8b_bnb4_fp16,\n",
    "    #'dh.8b-1.58.ptdq': dh8b_bitnet_fp32,\n",
    "    #'dh.8b-1.58.ptsq': dh8b_bitnet_fp32,\n",
    "    #'llama.8b-instruct.fp32': llama8b_fp32,\n",
    "    #'llama.8b-bnb4bit.fp16': llama8b_bnb4_fp16,\n",
    "    #'llama.8b-1.58.fp32': hfbit1_fp32,\n",
    "    #'llama.8b-1.58.ptdq': llama8b_bitnet_fp32,\n",
    "    #'llama.8b-1.58.ptsq': llama8b_bitnet_fp32,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PARAMS:Dict = {\n",
    "    'context': Contexts.C1.value,\n",
    "    'prompt': MiscPrompts.Q2.value,\n",
    "    'max_new_tokens': 100,\n",
    "    'temperature': 0.8,\n",
    "    'repetition_penalty': 1.1,\n",
    "    'sample': True,\n",
    "    'device': None\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chatbot_analysis.run_nq_analysis(\n",
    "    model=dh3b_bnb4_float32,\n",
    "    tokenizer=dh3b_tokenizer,\n",
    "    model_name='dh.3b-bnb4bit.fp32',\n",
    "    dataset=nq_dataset['train'],\n",
    "    save_path='logs/nq_logs/DH3B',\n",
    "    num_samples=10,\n",
    "    deterministic_backend=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chatbot_analysis.run_gsm8k_analysis(\n",
    "    model=dh3b_bnb4_float32,\n",
    "    tokenizer=dh3b_tokenizer,\n",
    "    model_name='dh.3b-bnb4bit.fp32',\n",
    "    dataset=gsm8k_dataset['train'],\n",
    "    save_path='logs/gsm8k_logs/DH3B',\n",
    "    num_samples=10,\n",
    "    deterministic_backend=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chatbot_analysis.run_chatbot_analysis(\n",
    "    models=chat_dict,\n",
    "    tokenizer=dh3b_tokenizer,\n",
    "    deep_thinking=False,\n",
    "    full_path='logs/chatbot_logs',\n",
    "    deterministic_backend=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chatbot_analysis.plot_chatbot_analysis(\n",
    "    json_logs='logs/gsm8k_logs',\n",
    "    parallel_plot=True,\n",
    "    reference_file='logs/gsm8k_logs/llama.8b-1.58.fp32.json'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chatbot_analysis.plot_chatbot_analysis(\n",
    "    json_logs='logs/gsm8k_logs',\n",
    "    parallel_plot=False,\n",
    "    reference_file='logs/gsm8k_logs/llama.8b-1.58.fp32.json',\n",
    "    title=\"Model Metrics ('What is y if y=2*2-4+(3*2)')\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting Analysis Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dir = \"logs/gsm8k_logs\"\n",
    "\n",
    "# Load all JSONs into a DataFrame\n",
    "all_results = []\n",
    "for filename in os.listdir(results_dir):\n",
    "    if filename.endswith(\".json\"):\n",
    "        with open(os.path.join(results_dir, filename), 'r') as f:\n",
    "            data = json.load(f)\n",
    "            all_results.append(data)\n",
    "\n",
    "df = pd.DataFrame(all_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Timestamp</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Perplexity</th>\n",
       "      <th>CPU Usage (%)</th>\n",
       "      <th>RAM Usage (%)</th>\n",
       "      <th>GPU Memory (MB)</th>\n",
       "      <th>Activation Similarity</th>\n",
       "      <th>Last Layer Mean Activation</th>\n",
       "      <th>Last Layer Activation Std</th>\n",
       "      <th>Mean Logits</th>\n",
       "      <th>Logit Std</th>\n",
       "      <th>Token Count</th>\n",
       "      <th>Sample Response</th>\n",
       "      <th>Latency (s)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>dh.3b-bnb4bit.fp32</td>\n",
       "      <td>2025-05-10T20:12:09.162364</td>\n",
       "      <td>float32</td>\n",
       "      <td>11.444397</td>\n",
       "      <td>5.7</td>\n",
       "      <td>36.2</td>\n",
       "      <td>19585.819336</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.009166</td>\n",
       "      <td>1.586409</td>\n",
       "      <td>-0.948672</td>\n",
       "      <td>2.422672</td>\n",
       "      <td>131</td>\n",
       "      <td>You are a helpful assistant that solves math p...</td>\n",
       "      <td>29.31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>dh.3b-bnb8bit.fp32</td>\n",
       "      <td>2025-05-10T20:04:05.667883</td>\n",
       "      <td>float32</td>\n",
       "      <td>11.222205</td>\n",
       "      <td>4.7</td>\n",
       "      <td>31.5</td>\n",
       "      <td>16550.925293</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.012208</td>\n",
       "      <td>1.591362</td>\n",
       "      <td>-0.832603</td>\n",
       "      <td>2.409796</td>\n",
       "      <td>131</td>\n",
       "      <td>You are a helpful assistant that solves math p...</td>\n",
       "      <td>29.76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>dh.3b-llama.fp32</td>\n",
       "      <td>2025-05-10T19:56:13.397589</td>\n",
       "      <td>float32</td>\n",
       "      <td>11.388047</td>\n",
       "      <td>4.9</td>\n",
       "      <td>27.0</td>\n",
       "      <td>12350.043457</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.011879</td>\n",
       "      <td>1.589604</td>\n",
       "      <td>-0.858537</td>\n",
       "      <td>2.404649</td>\n",
       "      <td>131</td>\n",
       "      <td>You are a helpful assistant that solves math p...</td>\n",
       "      <td>29.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>dh.3b-ptsq.fp32</td>\n",
       "      <td>2025-05-10T19:48:21.909389</td>\n",
       "      <td>float32</td>\n",
       "      <td>707535.812500</td>\n",
       "      <td>13.7</td>\n",
       "      <td>59.1</td>\n",
       "      <td>12349.960449</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.005222</td>\n",
       "      <td>1.653364</td>\n",
       "      <td>-0.144418</td>\n",
       "      <td>1.811667</td>\n",
       "      <td>131</td>\n",
       "      <td>You are a helpful assistant that solves math p...</td>\n",
       "      <td>132.84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>llama.8b-1.58.fp32</td>\n",
       "      <td>2025-05-10T17:04:43.893415</td>\n",
       "      <td>float32</td>\n",
       "      <td>12.292847</td>\n",
       "      <td>6.5</td>\n",
       "      <td>16.1</td>\n",
       "      <td>5790.931152</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.018133</td>\n",
       "      <td>2.256841</td>\n",
       "      <td>-1.710588</td>\n",
       "      <td>2.603533</td>\n",
       "      <td>131</td>\n",
       "      <td>You are a helpful assistant that solves math p...</td>\n",
       "      <td>35.12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>llama.8b-bnb4bit.fp32</td>\n",
       "      <td>2025-05-10T18:27:53.371856</td>\n",
       "      <td>float32</td>\n",
       "      <td>9.718791</td>\n",
       "      <td>7.0</td>\n",
       "      <td>35.8</td>\n",
       "      <td>18578.961914</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.003210</td>\n",
       "      <td>2.194873</td>\n",
       "      <td>-0.547328</td>\n",
       "      <td>2.445930</td>\n",
       "      <td>131</td>\n",
       "      <td>You are a helpful assistant that solves math p...</td>\n",
       "      <td>57.15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>llama.8b-bnb8bit.fp32</td>\n",
       "      <td>2025-05-10T18:11:25.176637</td>\n",
       "      <td>float32</td>\n",
       "      <td>8.825519</td>\n",
       "      <td>7.1</td>\n",
       "      <td>22.9</td>\n",
       "      <td>10801.597168</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.009007</td>\n",
       "      <td>2.191093</td>\n",
       "      <td>-0.420831</td>\n",
       "      <td>2.406327</td>\n",
       "      <td>131</td>\n",
       "      <td>You are a helpful assistant that solves math p...</td>\n",
       "      <td>25.12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>llama.8b-instruct.fp32</td>\n",
       "      <td>2025-05-10T17:50:02.438236</td>\n",
       "      <td>float32</td>\n",
       "      <td>8.636900</td>\n",
       "      <td>5.7</td>\n",
       "      <td>52.1</td>\n",
       "      <td>30743.801270</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.010518</td>\n",
       "      <td>2.188369</td>\n",
       "      <td>-0.353522</td>\n",
       "      <td>2.394554</td>\n",
       "      <td>131</td>\n",
       "      <td>You are a helpful assistant that solves math p...</td>\n",
       "      <td>150.89</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Model                   Timestamp Precision  \\\n",
       "0      dh.3b-bnb4bit.fp32  2025-05-10T20:12:09.162364   float32   \n",
       "1      dh.3b-bnb8bit.fp32  2025-05-10T20:04:05.667883   float32   \n",
       "2        dh.3b-llama.fp32  2025-05-10T19:56:13.397589   float32   \n",
       "3         dh.3b-ptsq.fp32  2025-05-10T19:48:21.909389   float32   \n",
       "4      llama.8b-1.58.fp32  2025-05-10T17:04:43.893415   float32   \n",
       "5   llama.8b-bnb4bit.fp32  2025-05-10T18:27:53.371856   float32   \n",
       "6   llama.8b-bnb8bit.fp32  2025-05-10T18:11:25.176637   float32   \n",
       "7  llama.8b-instruct.fp32  2025-05-10T17:50:02.438236   float32   \n",
       "\n",
       "      Perplexity  CPU Usage (%)  RAM Usage (%)  GPU Memory (MB)  \\\n",
       "0      11.444397            5.7           36.2     19585.819336   \n",
       "1      11.222205            4.7           31.5     16550.925293   \n",
       "2      11.388047            4.9           27.0     12350.043457   \n",
       "3  707535.812500           13.7           59.1     12349.960449   \n",
       "4      12.292847            6.5           16.1      5790.931152   \n",
       "5       9.718791            7.0           35.8     18578.961914   \n",
       "6       8.825519            7.1           22.9     10801.597168   \n",
       "7       8.636900            5.7           52.1     30743.801270   \n",
       "\n",
       "   Activation Similarity  Last Layer Mean Activation  \\\n",
       "0                    1.0                    0.009166   \n",
       "1                    1.0                    0.012208   \n",
       "2                    1.0                    0.011879   \n",
       "3                    1.0                    0.005222   \n",
       "4                    1.0                    0.018133   \n",
       "5                    1.0                    0.003210   \n",
       "6                    1.0                    0.009007   \n",
       "7                    1.0                    0.010518   \n",
       "\n",
       "   Last Layer Activation Std  Mean Logits  Logit Std  Token Count  \\\n",
       "0                   1.586409    -0.948672   2.422672          131   \n",
       "1                   1.591362    -0.832603   2.409796          131   \n",
       "2                   1.589604    -0.858537   2.404649          131   \n",
       "3                   1.653364    -0.144418   1.811667          131   \n",
       "4                   2.256841    -1.710588   2.603533          131   \n",
       "5                   2.194873    -0.547328   2.445930          131   \n",
       "6                   2.191093    -0.420831   2.406327          131   \n",
       "7                   2.188369    -0.353522   2.394554          131   \n",
       "\n",
       "                                     Sample Response  Latency (s)  \n",
       "0  You are a helpful assistant that solves math p...        29.31  \n",
       "1  You are a helpful assistant that solves math p...        29.76  \n",
       "2  You are a helpful assistant that solves math p...        29.20  \n",
       "3  You are a helpful assistant that solves math p...       132.84  \n",
       "4  You are a helpful assistant that solves math p...        35.12  \n",
       "5  You are a helpful assistant that solves math p...        57.15  \n",
       "6  You are a helpful assistant that solves math p...        25.12  \n",
       "7  You are a helpful assistant that solves math p...       150.89  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "def normalize_name(name):\n",
    "    # Remove .json suffix if present\n",
    "    if name.endswith('.json'):\n",
    "        name = name[:-5]\n",
    "\n",
    "    # Fix prefix ldh -> dh\n",
    "    if name.startswith('ldh'):\n",
    "        name = 'dh' + name[3:]\n",
    "\n",
    "    # For names starting with 'dh.3b', replace dots with dashes after first two parts\n",
    "    if name.startswith('dh.3b'):\n",
    "        parts = name.split('.')\n",
    "        if len(parts) > 2:\n",
    "            # Join first two parts with dot, rest joined by dash\n",
    "            name = parts[0] + '.' + parts[1] + '-' + '-'.join(parts[2:])\n",
    "    \n",
    "    return name\n",
    "\n",
    "def plot_flat_metrics_by_model(\n",
    "    df,\n",
    "    metrics,\n",
    "    model_col='Model',\n",
    "    model_name_map=None,\n",
    "    title=\"Model Performance & Resource Metrics\",\n",
    "    font_sizes=None,\n",
    "    log_metrics=None,\n",
    "    col_wrap=5,\n",
    "    figsize=(22, 10),\n",
    "    save_path=None\n",
    "):\n",
    "\n",
    "\n",
    "    if font_sizes is None:\n",
    "        font_sizes = {\n",
    "            \"title\": 20,\n",
    "            \"subtitle\": 22,\n",
    "            \"xlabel\": 20,\n",
    "            \"ylabel\": 20,\n",
    "            \"xtick\": 14,\n",
    "            \"ytick\": 14,\n",
    "            \"legend\": 22\n",
    "        }\n",
    "\n",
    "    if log_metrics is None:\n",
    "        log_metrics = [\"Perplexity\"]\n",
    "\n",
    "    df = df.copy()\n",
    "    df[model_col] = df[model_col].str.strip()\n",
    "\n",
    "    # Normalize df model names\n",
    "    df['NormalizedModel'] = df[model_col].apply(normalize_name)\n",
    "\n",
    "    # Normalize model_name_map values\n",
    "    model_name_map_fixed = {k: normalize_name(v) for k, v in model_name_map.items()}\n",
    "\n",
    "    # Reverse map: normalized model name -> friendly key\n",
    "    json_to_key = {v: k for k, v in model_name_map_fixed.items()}\n",
    "\n",
    "    # Map normalized df model names to friendly keys for legend\n",
    "    df['FriendlyName'] = df['NormalizedModel'].map(json_to_key).fillna(df[model_col])\n",
    "\n",
    "    friendly_names = df['FriendlyName'].unique()\n",
    "    plt.rcParams['font.family'] = 'Times New Roman'\n",
    "    cmap = cm.get_cmap('tab20', len(friendly_names))\n",
    "    color_map = {name: cmap(i) for i, name in enumerate(friendly_names)}\n",
    "\n",
    "    n_metrics = len(metrics)\n",
    "    nrows = int(np.ceil(n_metrics / col_wrap))\n",
    "    ncols = min(col_wrap, n_metrics)\n",
    "    fig, axes = plt.subplots(nrows=nrows, ncols=ncols, figsize=figsize)\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    plt.rcParams['font.family'] = 'Times New Roman'\n",
    "\n",
    "    for i, metric in enumerate(metrics):\n",
    "        ax = axes[i]\n",
    "        if metric not in df.columns:\n",
    "            ax.set_visible(False)\n",
    "            continue\n",
    "\n",
    "        for j, friendly_name in enumerate(friendly_names):\n",
    "            vals = df.loc[df['FriendlyName'] == friendly_name, metric].values\n",
    "            if len(vals) == 0:\n",
    "                continue\n",
    "            val = vals[0]\n",
    "            if metric in log_metrics:\n",
    "                val = np.log1p(val)\n",
    "\n",
    "            label = friendly_name if i == 0 else None\n",
    "            ax.bar(j, val, color=color_map[friendly_name], label=label)\n",
    "\n",
    "        ax.set_title(metric + (\" (log)\" if metric in log_metrics else \"\"), fontsize=font_sizes[\"subtitle\"])\n",
    "        ax.set_xticks([])\n",
    "        ax.grid(True)\n",
    "\n",
    "    for i in range(n_metrics, len(axes)):\n",
    "        fig.delaxes(axes[i])\n",
    "\n",
    "    handles = [plt.Rectangle((0, 0), 1, 1, color=color_map[name]) for name in friendly_names]\n",
    "    labels = list(friendly_names)\n",
    "\n",
    "    fig.legend(handles, labels, loc='upper center', ncol=min(5, len(labels)),\n",
    "               fontsize=font_sizes[\"legend\"], frameon=False, bbox_to_anchor=(0.5, 1.05))\n",
    "\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.93])\n",
    "\n",
    "    if save_path:\n",
    "        fig.savefig(save_path, bbox_inches='tight', dpi=300)\n",
    "\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAACIgAAAQMCAYAAAD3FzDKAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAPYQAAD2EBqD+naQABAABJREFUeJzs3XdYVEfbBvCbJogoiBIVS2yxxK6IvWCJJREBUUGigrFFIbYYo8YaK2JvqIhgr4gRsSAqimJFlGDHBkQp0nvb7w++Pe+uuwssRRTv33V5yZ4yO9vOc86ZZ2ZURCKRCERERERERERERERERERERERUbqmWdQWIiIiIiIiIiIiIiIiIiIiIqHQxQYSIiIiIiIiIiIiIiIiIiIionGOCCBEREREREREREREREREREVE5xwQRIiIiIiIiIiIiIiIiIiIionKOCSJERERERERERERERERERERE5RwTRIiIiIiIiIiIiIiIiIiIiIjKOSaIEBEREREREREREREREREREZVzTBAhIiIiIiIiIiIiIiIiIiIiKueYIEJERERERERERERERERERERUzjFBhIiIiIiIiIiIiIiIiIiIiKicY4IIERERERERERERERERERERUTnHBBEiIiIiIiIiIiIiIiIiIiKico4JIkRERERERERERERERERERETlHBNEiIiIiIiIiIiIiIiIiIiIiMo5JogQERERERERERERERERERERlXNMECEiIiIiIiIiIiIiIiIiIiIq55ggQkRERERERERERERERERERFTOMUGEiIiIiIiIiIiIiIiIiIiIqJxjgggRERERERERERERERERERFROccEESIiIiIiIiIiIiIiIiIiIqJyTr2sKyApIj4NcSmZZV2NT6pqpQqorVexrKtBRFTuJWUnIj0nrayr8UlpqVVEZfUqZV0NIiL6AmUkJyErPb2sq/FJaWhpQVOncllXg4iIPkNZWYnI+cquJ9XUKkJDg9eTREQkKzkrG+k5uWVdjU9KS00VOhqfVZMqEVGRfTZHs4j4NPRxuoKM7K8rqGiqq+LS772ZJEJEVIqSshNxJNwFOcgp66p8UmpQw8g645kkQkRESslITsK94wcgyvm64qaKmho6WNowSYSIiKRkZSXi7ds9EIm+srioooZ69eyYJEJERFKSs7Lh8SYKOaKyrsmnpaYCWHz7DZNEiKhc+GymmIlLyfzqkkMAICM796sbNYWI8jx9+hR///03OnbsiFu3bim9f3x8PMzNzdG9e3fcv3+/FGpYfqTnpH11ySEAkIOcr27UFCIiKr6s9PSvLjkEAEQ5OV/dqCmfu/v376N79+4wNzdHfHx8WVeHiL5SOTlpX11yCACIRDlf3agpRPRlys3NxbVr1+Dg4IAWLVoUqQyedxZeek7uV5ccAgA5Inx1o6YQUfnFVLevgJ+fH3x9fXHjxg2EhYXJ3UZDQwOamprQ09NDjRo10KRJE3To0AEmJibQ0dH5xDUuOTdu3ICdnV2+2yxevBjW1tYAgOzs7AJPIocMGQInJ6cSqyNJCwsLw7Zt23D9+nXExsZCT08PxsbGmDJlCho3biyzvbu7O0JCQnDx4kWkpKTILVNDQwMVK1ZE1apV0bBhQ3Ts2BGmpqYwMDAo1ddy9epVTJw4ER07dsS+ffuE5Xfv3sX69etx9+7dYpV/8+ZNPHr0CADg5eWFdu3aFas8oqJgjGGM+dx169YNMTEx+W6zZ88edO3aNd9trl69igkTJuS7TY0aNXD16lWl61gY79+/R69evWSW16pVCz4+PtDQ0FCqvPDwcAwYMADZ2dlSy+vVqwcfH59ClREXF4cRI0bgzJkzqFChglLPr4ysrCzs27cPnp6eeP36NdTV1dGkSRNYWVnBzMxMZvt79+7By8sLd+7cwfPnz+WWqaqqCi0tLVSqVAnffvstvv/+ewwaNAjt27cvtddBBADr16+Hs7Nzvtt4e3ujUaNGAAoXaxYsWIAXL17g2rVrCA8PV6o+o0ePxl9//aVwfWRkJNzd3XHkyBHcu3dP7jahoaE4dOgQHjx4gIcPH8rdRkVFBVpaWtDW1kbdunXRtGlT9O/fH927d4eKiorC5z99+jSio6MRHR2NW7duYcCAAYV6XRkZGWjdunWB2124cAHffvttvtscOnQIixcvzneb9u3b49ChQwU+X//+/fH27VupZSoqKvDy8pJ7rZOf3NxcDBo0CK9fv5ZZ5+fnh5o1aypVHhFRWSipY+yIESPw4MGDfLdZtmwZhg8frmwVC2X+/Pk4fvy4zHLJ68GSKM/R0RFDhw4tVBmOjo4wNDTEzz//rPTzF6Rnz56IjIxUuL5NmzY4evRoiT8vKS8iIgJ79uzBtWvX8O7dO6iqqqJu3bro3bs37OzsoK+vL2ybmZmJdevW4fXr17hy5QpEIvkZAJqamqhYsSJq1KiB7777Dt27d8egQYOgpaVVYvUODAzE3r17ERgYiNjYWGhpaaFx48YYMmQIRo4cCXX1/zVrHT16FLt375Z7TqSMop53EpUkxkXlymNcJGV9qXGxNKiIFL2iT+zfiAT8tNm/rKtRJrwcuqNlbd1Sf560tDQMHDgQ79+/BwAMHz4cJiYmMDQ0hJaWFmJjY/Hs2TP4+vri2rVrAABtbW2MGTMGU6dOLdUb/6VFJBIhMzMTwcHBmDt3rnAzrmrVqlizZg2MjIygpaUldVMyKysLYWFhcHR0xOXLl4Xlc+fOhZmZGapUqQJV1c9m8J1y5dGjRxg9ejSSk5Nl1mlra8PV1VVhEsSjR49gbm4uPD5y5Ajq168PHR0dZGRkICwsDDdu3IC7uzvev3+PChUq4Lfffiuwsa+oYmNjMWTIEMTExMDY2FgqQSQlJQVaWlrYtGmT0ECwd+9edOrUSanniI+Ph52dHaKjo7Flyxa0bdtW7naBgYFo3LgxqlT5eoeFjc6IhMe7vWVdjTJhUWsMDDRrlPrzMMYwxnyusrOzERkZCScnJ3h7ewvL+/fvj19//RX16tWDjo5Ovg2UQN7nnZGRgYcPH+KPP/7Au3fvAOQlQM2ePRv9+vVDtWrVSvXkPzk5GcHBwViwYIFUQtaKFSswbNgwpcr6+++/sX//fuHxTz/9hBkzZqBmzZpSN9vys2XLFmzevBmrV6+Wm6hREjIzMzFp0iTcuHFD7npbW1vMnTtX7rrs7GyMGDECISEhAIAJEybAxsYG+vr6UFFRwYcPH/Dw4UMcOXIE169fBwB0794da9askbog/Bolx0Tjwamv8+ZFm6EjoFO99JKIc3NzkZqaCl9fXyxYsAAZGRl5z9umDZYuXYqGDRtKxUTJWDN//nzhxreenh5Wr14NY2NjVKxYESoqKsjIyICpqamwzebNm2XOndPT0xEREQEPDw+cOnUKlpaWWL58uUw9nz17BldXV3h5eSErKwtA3uh3+RGJRJg6dSp8fX0BAEOHDoWDgwOqV68OdXV1xMfH49GjR/Dw8MC5c+cAAC1btsT69etRr149uWUGBQXB3t4e1atXh5ubG/T09ORu5+fnJ5NEl5WVhbdv32LJkiVSo/WNHDkSo0ePRq1atQqVqCoSiZCamopbt27hjz/+QFJSEgCgcuXKmDdvHrp164aqVasW6lwmPT0db968wZo1a4TzIQAwMzPD6tWrC9xf0vnz5/Hbb78Jj5s1a4YVK1bgu++++yLPq4g+Z+npkQgPP1DW1SgTderYQEur9K4nS+oYm5OTgw8fPsDFxQXu7u7C8g4dOmDWrFlo3LgxKleuXGrXWllZWYiKisLBgwfh4uIiLK9bty7Onz8PNTW1QpcVFRWFPn36CPFXV1cXq1evRseOHVGpUqUCr1uAvOuG3r17Q19fH+fOnSvx152dnY33799j69at8PDwEJbb2tpi3LhxqF69ulKvuTTIOzf4UqSlpSE4OBjGxsbFKufGjRuwt7eHgYEBZs+ejXbt2iE7OxvXr1/H6tWroa6ujt27d6NZs2Yy+7q5uWHlypUAgCZNmsDJyQm1a9eGlpYWUlJS8OLFC1y8eBEHDx5Eeno6DAwMsHLlSvTo0aNYdQaAnTt3Yt26dejWrRumTp2Kxo0b48OHD/D09ISLiwtatmyJXbt2Cfc5k5KSULFiRdjZ2eH27dsACj5vlac4551fm5j0TJwOy78jTnk1pG51VNcqvXNtxkVZjIsl40s+dn3tcbG0sAXiK1KxYkWpXlRDhgxB37590bx5czRo0AAdOnSAtbU1XFxccPToUdSvXx+pqalwdnbGqFGjkJCQUIa1LxoVFRVoamrCyMgII0aMEJYPGjQIPXr0EG6mStLQ0EDDhg0xZcoUYVmjRo1ga2sLPT09NtyVkri4OEyaNEnoiXzr1i0cOHAAHTt2BACkpqZi3rx5yM2VP4zb999/D13d/yVatW3bFnp6elBXV0elSpXQrFkzjBs3DmfOnIGRkREyMzPh5OSEbdu2lcrrmTdvnsIe65UqVYKamlqxR/zQ09PDyZMn4e/vrzA5BABWr16NxMTEYj0XUUEYYxhjPlfq6uqoXbs2Zs+eLSzT0NDA2rVr0aJFC1SuXLlQF5Pi3u/GxsawtLQUlg8ePBhjx44VLgpKk46ODrp06YKJEydKLd+1a5fC+ChPbGwsTpw4IbXszz//RJ06dQqdHJKRkYGDBw8CgNTNhpK2ZMkSZGZmwtXVFTdv3oS3tzcmT54s1NPd3R2BgYFy91VXVxfOIwCgYcOGqFWrFjQ1NVGhQgXUqlULAwYMgKurq5Bk4u/vj7FjxyItjcOpU+lQVVWFjo4Ohg4dKpUcbGdnh2bNmsnc6JOMNZK9uwYNGoTevXtDW1tbOIZpamri+++/F7bR1dWFgYGB1L+6deuic+fOcHR0xKhRo+R+1588eYIbN26gW7duSh3XVFRU0LlzZ+Fx3bp1UbduXVSsWBEaGhowMDBAr169sHHjRmzYsAFqamr4999/YWNjo/C8uW3btvD394enp6fCm/Tx8fFwdHSUWa6hoYFGjRrBwcFBWGZoaIilS5fiu+++K/QoZioqKqhUqRL69OmD/v37C8tHjx4NCwsL1KhRo9AJGVpaWmjatKnMqC1eXl6IiIgoVBliu3btkno8ceJEtGjRgskhRPRFKaljrJqaGr755hvMnj1bamS91atXo0OHDtDV1S3Vay0NDQ3hmqNGjf8l1ISFhUklqRfGnj17hEYwABg4cKAw+mZhrlsA4NixY0hKSsKbN2+kOiaUFHV1ddSpUwe///67sExTUxN//PEHatSoUeaNYIrODb4Up06dKtJU1JJiYmLw22+/QUtLCwcOHBA6NNSoUQMWFhbYsGEDYmJiMHXqVGRmZsrsL3lOp6enh6ZNm0JHRwfq6urQ1dVFhw4dMGfOHHh4eMDAwADR0dGYPHkybt68Wax6X7t2DWvXroWxsTF27dqF9u3bo0qVKmjQoAFmzJiBiRMnIigoCEuWLBH2qVy5MtTV1dGyZctiPXdxzjuJSgrjoizGxeL70o9dX3NcLE1shfjKaGtrF2o78ZBH4hOr4OBgTJ48WWYY8i+J5ImdopO84mxPxePs7IwlS5Zgzpw5aNy4MfT09GBkZIQ9e/YIN7pfvnyZ73CBlSpVKvB5dHR0sGHDBuFm97Zt24Re4CXlwIEDePr0qdxMQ0mampol+rzyXL58GUFBQaX+PEQAY4y8v0tqeyq+b775RvhbX1+/WMdgyYvbWrVqFateRVG7dm0AEBo3X716VehpYQBg3759yMjIkPrNVqtWTak6eHp64sOHDwDyRvEq7oWaPCEhIUhPT4e7u7vQS6ZRo0aYMWMGZs6cCSCvd01+F/eFOTcA8np1iG++PHv2TGrkL6LSomwskEyGVrS9Msc2BwcHqRttYs2aNYOtrS2GDBmCgQMHFro8oPC/uUGDBmH06NEA8nqEbdq0SannkbRjx458k7okj//FnXZFsixDQ8Mil/PxcTw7Oxuurq6F3j8gIADBwcFSSS5f+8hHRPTlK4ljrIaGhtTxsCzO1Q0NDaWOzzt37lQ4JPnHkpKScOTIEan9lT1Pz87Oxt69/xtB1c3NTan9lSF5PqKvr1/mDWBiBZ0bfM5SU1Oxc+fOYpdz6tQpJCUloV+/fqhevbrM+i5duqBWrVoIDw+Xey1X2HO6Ro0aCcka2dnZUokbRSEe5XLkyJFyG68tLCwAAGfPnkVqaqrUuk9xn/VL/m7Rl4dxkXGxpHzJx66vPS6WJiaIfGUKm1EH5N2A3Lx5s3BQCwwMVOqm1edG8mBcmOxIyW0+lwN5eWZqaoo+ffrILNfQ0MDIkSOFx/n1kC7s99vAwEAYjiorKwtXr15VsraKhYaGwsnJCWvWrClwSpfSHikgKioKCxYsKNXnIJLEGJOHMebzJDkyRmFHyVBE8jMrbllFIX5OKysrYdmOHTsKtW9qaioOHjyIAQMGoGrVqsJyZWKSSCTCnj170KRJE2FZaVxga2lpYfny5XLfYxsbG6HOOTk5CstQ5rg0ePBg4e9Lly4pUVOiolE2dkhuXxKxQ19fH7NmzSpwG2Uo85v78ccfhb+L+psLCAgo8PhTksdsyf2L8xmIe/GNGDFCeM+OHz8uJN4VZNeuXahZsyZ69+4tLOMoZET0pSupY2xJnvcXhYaGBnr37i0kJT579qzQvZUPHDiA1NRUqftgyh7fz549i5iYGNSvXx8AcPv2bTx69EipMgpL2XOZT6Ew5wafs7///lvpUcXkefPmDYD8fwPixmd5PaWVOaczMTEROh+8fPkSr169UqaqUsT1VnQMENc5JydH5jqwtO+tfOnfLfryMC4yLpaEL/3Y9bXHxdL0eXxD6bNlaGiIcePGCY9dXFyQkpJShjWi8qpFixYK14kzW+vXr4+GDRuWyPNJDpctnsuvuDIzMzFz5kzY2trCyMioRMosqvDwcNjZ2SE6OrpM60GUH8YYouKxsLAQst5DQkLg7+9f4D5Hjx5FfHy8zDQ1yrh06RLevn2Lbdu2oW7dugCAK1euCBdbJaVRo0YKp7fQ0tISEszkJZgWRWmcGxB97sQ3yRQpzRuIkr08k5OTld4/ICAAU6dOVWqKrc/Nd999BxMTEwAQRkwqyKNHj3D9+nXY2dlJDRdNRESfDw0NDdjZ2QmPC9PzNSMjA/v27UOfPn3QuHHjIj/3nj17MHToUEyaNElY9iU3DCnjSz43yMnJwbJly+Dh4VEi5YlHvbx48SLS09Nl1ufm5iIiIgI6Ojro0KFDsZ5LVVVVasqL4lxLievt5eUld/3bt28BAB06dEDlypWL/DzK+pK/W0SfA8bFsvElH7sYF0sfE0SoQNbW1kLmW0JCAnx9fWW28fPzw+TJk9G1a1e0bNkSvXv3xrx58/JtKIiLi8P69evx008/oW3btmjXrh2srKxw9OhRmQNWVlYWzp07h3HjxmHMmDEA8kZHWLhwIbp37462bdvCysoKFy9eLMFXXngvXrzA3Llz0adPH7Rs2RKdOnXCpEmTEBAQILNtZmYmvLy8MHr0aGFY5bt378LMzAzt27fHli1bhG3v3buHWbNmSc2heP36dVhZWaF169bo06cPtm/fLvV++fv7w87ODu3bt0fnzp2xYMGCfBtcs7KycPDgQVhbW8PIyAitWrXC4MGDsWHDBoUHrnPnzmH48OFo3bo1mjZtKvUvNjZW6fevIK9evYKKigrmzZtXIpmXOTk5CA4OFh6XVNLJ2rVrUbFiRUydOrXIZRw/fhympqZo3bo1+vbti7Vr1yq8YR4WFob169ejR48eUoHywoULMDMzw4sXL4Rlffv2FT6j0pgGgKioGGMKxhhTujGmLOXk5ODgwYOwsrKCkZERWrZsie7du2PixImF+r5pampi7NixwuOCRhHJysqCm5sbunfvLkzfVhSurq4YNGgQ6tatK/xmcnNzC9WwWVLi4uIQFxeHH3/8scSSMiWnZCupcwOiz9GKFSsQHh5eqG2V6SWjrML85gIDAzF37ly0bdtWqs7r1q3DuHHjhBgUEREhFS++JJI3Kg8ePFjgzaNdu3ZBT08PI0aMKO2qERF91UJDQ6Wuw9q3b4/Bgwdj9erVhbouGTFihJDQfP/+fdy+fTvf7T08PBATE1OsRO6AgAA8evQIdnZ2+Omnn2BgYAAA8Pb2RmRkZJHLLY7Y2Fjs3r0bP/zwAzZv3gwAiIyMxKJFi9C9e3d06NAB9vb2CuuXmpqK1atXw8TEBM2bN5eK95IxtLDnBu/evcPmzZvRs2dPeHh4QCQSYcuWLejSpQv69++PnTt3ylyHiusNAImJiTLr//zzT4WvPzAwEDNmzECPHj3QsmVLdO3aFdOnT8eTJ0+k3iNLS0upaS63bNlSqPIV+fHHH6GqqorIyEi5w9tfuXIFMTEx+P3334s97e2rV68QHx8PIK9RrEGDBkUu66effgIA+Pj4yG0UPHHiBLS0tDB//vwCy0pLS8P69evRp08ftG7dGmZmZjhy5IjCqS2+hvNOouJgXCwZjIuMi58LJohQgapUqYLmzZsLj2/evCn8nZ2djblz52Lr1q2ws7PD+fPnceLECbRs2RInTpyAmZmZ3AasoKAgmJubo2LFitixYwf8/Pzw119/ITQ0FAsWLMCUKVOQnZ0NAFi/fj169+6NadOm4fr16xCJRAgLC4OlpSWOHj2K6OhopKWl4f79+5g6dSp27dpV+m+KhMOHD2PixIno2rUrjh8/Dh8fH1hZWeHq1auws7MTsiFFIhEcHR3Rs2dPzJo1SwiAQUFB+OWXX/D48WOkpKRg8+bN8Pb2hqmpKUaNGgUvLy9kZWVBJBJh+fLlmDx5Mt69e4eMjAxERERgw4YNWL9+PQBg69atmDBhAl69eoXMzEzExcXh6NGjmDZtmty6x8TEwMrKCkFBQViyZAmuXLmCnTt3QlVVFdu3b8ewYcNkApG7uzvmzJkDGxsbXLt2Db6+vpg+fXqp9V5LS0vDkSNHMH/+fPTq1atEytyzZw/evXsHIK/XZM+ePYtd5vXr13HixAmsWbOmSD0tc3NzMXv2bMyfPx9Pnz5FRkYGwsPDsXPnTlhZWSEmJkbY9uXLl5g0aRJ++OEHODs7IyoqSqqsfv364ebNm1i+fLmwzMfHByEhIQgJCRGm1yH6HDDG5I8xpnRjTFnKzs7GxIkTsWTJErRt2xYXLlzAxYsXYWFhAT8/P0ydOhVHjhwpsJxRo0YJPadu374t1eD6sdOnT+Pdu3fFurh++PAh7t69K4z+M2zYMGFKtZMnTyIhIaHIZSvD3d0dHTt2xLJly0qkvNevX+PQoUPCY3HiC1F5lN9x4lOJjY2VSmr7+Dd3+fJlWFhYwNraGh4eHjLzJU+bNg3BwcEwMzMDANSuXVs41w0JCSn1+pektm3bCufnSUlJUseij719+xbnz5/Hzz//LAxZS0REJc/f3x/m5ua4desWHB0dcfPmTTg7OyMnJweurq6wsrKSiU0f09bWxs8//yw8zi+ZW1yusbEx2rZtW+R6u7q6wsTEBI0aNUKFChUwatQoAP9L3P+UYmJisGTJEvTv3x+Ojo5CB4979+7B3NwcFy5cQHp6OpKTk+Hj44Nx48YJ1+limZmZsLW1xZ07d7Bp0ybcuXMHx44dQ/fu3WWer6Bzg9DQUEyePBl9+/bFli1bhGvRVatWYfPmzYiNjcXbt28RGBiIgIAAqdFOJVWpUgW3b9/GypUr8339ubm5WL16NcaPH49u3brh9OnTuHr1KgYNGoSzZ8/C0tJS6CCjr6+PY8eOSZ3DTJkyRaj7ihUrCvemS6hfv77QUOjh4YEZM2YIPabDw8OxbNkyLFmyBNbW1kqXLe91ipmZmRVrZA9zc3PhvGj+/PnYvXu3sO7KlSs4e/Ys9uzZk+9I1EDeuaaVlRWcnZ0RERGBjIwMPH78GAsXLsTMmTOlOsJ8TeedREXFuFh8jIuMi58bJohQoUjOMS85KoGTkxMePHgANzc3dOrUCZUrV0bTpk2xadMmtGjRAqmpqZg+fbpUL6j//vsPEydOhL29PSZPnozatWtDV1cXw4YNw6pVqwDknZiJG73s7Oxw7NgxofEhOzsbc+bMgb29PQIDA3Hv3j38/vvvwsgS69atQ2BgYKm/J0DeEOurVq3Cjh07MGTIEOjr66NWrVqYMWMGxowZA5FIhLVr1+Lu3btQUVHB6NGjceDAAeFmXkZGBjZv3ozTp09j4cKF0NXVhbGxMdq0aQMXFxepXt2Ojo6oXr06AgIC4OfnB29vb2FYpP3792P9+vV4/fo1Ll26hCtXruDevXvo378/AODatWtSI2YAeUFwypQpaNSoERwdHdGkSRPo6OigS5cu2LNnD7S0tPDmzRvMmTNH2Cc5ORnr16+HpaUlzMzMoKurizp16uDXX3/FvHnzSvz9jYuLg4ODAypVqoSBAwcqte/HPdpzcnLw9OlTLFy4EE5OTgCAmjVrYsuWLcUeOjsuLg5z5szBwoULhaH2lbVx40akpaVh7969uHDhAlauXClktD5//lzqc6hbty62bt2qMOiqqqpCXV1darQV8TJ1dfVS7QlKVBSMMfIxxpRujClrR44cgb+/P6pWrYo///wT+vr6qFmzJmbOnIl27doByLuQLYiOjo7UxYuzs7Pc7UQiEXbv3o22bduiU6dORa737t270aVLF+GGXKVKlTB8+HAAeb0Yjh49WuSyC2v//v3YvXs3JkyYoFQDqbybFTExMThy5AhGjRqFxMREqKioYO7cuejYsWNJVpnosyAetejBgwef5PnS09NlRu2Kj4/HmTNnMGLECGEeYTs7OwwdOlRquy5dusDDwwPDhg2TW7aamprMea34XLcs5tUuLsnEPTc3N7lD3gJ5x2BNTU2pG6tERFSycnJy8OeffyIjIwPjxo2DkZERdHR0YGxsjAULFgDIm8e+MCP+jR49Wjhf9ff3V9iYfP78ebx9+7ZYidwvXrzAtWvX8MsvvwjLrK2thWkUDx8+rDC+lAZdXV3MnTsXe/bsEZaFhIRg06ZN2L59OwICAnDnzh1h5MsXL17IjCj6zz//4MGDB1i8eDFatWoFHR0dtG7dGjt37hSumcQKOjeoUaMGVqxYIdXB4caNG8jIyMDly5cxaNAgaGhowMTEBPr6+rC3t8/3tVlYWOTbw3jdunVwdXWFk5MTLC0toaenB319ffzxxx/Q0NBAVlYW5s2bh8zMTKm6iknexyvqaMrTp08XGvS8vb0xatQo3L59G05OTti2bRusrKwKVU5WVhaysrKklqWnp+P27duwtbXF5cuXAQDGxsbFvnZXV1fH9u3bYWxsjNzcXDg6OmLOnDm4cOECfHx8cPLkSbRv377AcqZNm4auXbvi5MmTOHPmDBwcHISOJ97e3lKdf762804iZTEulgzGRcbFzw0TRKhQJH/Y4p6hb968gbu7O0aOHClzc15VVRWdO3cGkHcT8OzZs8K6DRs2QEVFBRYWFjLP07VrV+Hvw4cPC89taGiIRo0aAQCePHmCmTNnYsSIEdDW1oaOjg4mTJggHMhyc3MVNo6Iubq6olu3bvn+s7S0zLeMnJwcLF++HN26dcN3332X72sR9wKrVasWGjVqJMyZFhwcjGnTpqFevXqwsbHBrVu3sG/fPtSuXRvffPON1Anv4MGDMWnSJOjo6AAAGjVqJAwrn5qaCm1tbaxZswa1atUCkDfs/PTp04X9P268O3XqFB48eCA388/AwEB4TQEBAXj9+jWAvGHE0tLShNE3JI0cOVJoTCyuoKAgrF+/HgMHDhQaHvv166fUfGPt27dH+/bt0aVLF3Tq1AmtWrWCqakpjhw5AhUVFYwbNw6enp5yPztlzZ8/H126dIGpqWmRy+jWrRu2bNmCTp064dtvv4WFhQUOHz4s/Pb8/f1x//59AHlz9qmrqxdregCizwljjCzGGGklGWM+F8+fPweQdxH1sVatWgGA3PdCnrFjx0JTUxNAXq+qZ8+eyWzj6+uLFy9eYMKECUWtMsLCwuDj4yN1cQ3k9fwXX7Tt379fpodDSYiMjMSxY8cwbNgw/P3338jMzMSECRNgb2+PjIyMQpWxdOlStGrVCp06dUKXLl3Qpk0bdOvWDQsXLsSHDx/Qpk0bHD16FLa2tiVef6KCODg4FBg7JEeHK0qZbdq0kTuUamlxcXFB69atYWxsjK5du6Jdu3bo1KkTZs6cibCwMDRo0AB79uzBn3/+KZPALL5p97Wc7/bo0UNIvPvw4QNOnDghs01MTAxOnjyJ4cOHo2rVqp+6ikREX434+HhER0cDkD1Xb926tfB3Yc7VP54STFFvaRcXFzRv3hw9evQoSpUB5CURtmnTRmr6xapVqwpJmPHx8Th58mSRy1eWhoYGKlSoIHU9m5mZiV27dqFNmzYA8qaymzJlinAe8PDhQ6kyxNeZ79+/l1qupqam9PTOOjo60NfXR79+/YRlL168wIIFC2BoaIgNGzYgODgYI0eOBJCXCF8QRds8efIELi4uaNOmDfr06SO1TlNTU/geJScnl3rj5Jw5c7Bo0SJoa2sjJCQEo0ePRlRUFPT19Qtdxv3799G6dWsYGRmha9eu6NixI9q2bYvRo0fj1q1bqFy5MpYuXQpXV9cS6SWto6OD3bt3w8LCAhoaGvD09ISDgwPU1dWFexYFsbe3x5w5c/D999+jcePGsLe3x6ZNm4T1Li4uQgeCr+28k0hZjIslg3GRcfFzwxRHKhQ1NTXh7woVKgAAPD09kZubi61btwo9sSWlpqYKf4sbKtLT0+Ht7Q0ABR7cIyMjER8fLzQcirN8W7ZsKXeu+XHjxsHNzQ1xcXFCppu4seRjVlZWCodEEnv37p3QI1ae27dvIzw8HLGxsejWrZvMeskMso8basTvYdu2baWC6Mc3RiUbRcWNRZIk5+qWN9RWnTp1hL8TExOl1omD38eNPGKSQ8Q/e/YM9evXFz4LX19fbNmyBZMmTRI+FzU1Nfzwww9yy1JWdnY2GjRogD59+uDs2bNIS0tDeno65s2bhypVqkgFLUXOnj2LypUrQyQSISEhAVFRUbhx4wZOnz6NyMhI7N27F2/evMG0adOKNV/k4cOH8ezZM3h6eha5DAByp32pU6cOHBwc8PfffwPIa/STzAQVX8AQfekYY2QxxpRejPlcWFhY4MmTJ8JQj5LEN73EWfMFqV69OiwsLHDo0CGIRCLs3LlTGC1LzMXFBY0bN0bfvn2LXGd3d3c0btxY5vdVs2ZNDBw4EF5eXnj//j3OnTsnzB1dUrKysqCrq4vevXsjOTlZSCzy8fHBzJkzsXXr1gLLmD59OiwsLKCqqoq0tDTExsbi33//xfnz53H79m08ePAAc+fOhZ2dXYFJXEQlbdmyZTI9fj7m7e2t1FCqH5eZnJwMLy8vbNmypcj1VIa1tTUmT54MNTU1ZGRkIDY2Fs+ePcPFixdx+fJlvHr1Cn/99RdsbGwwduxYuT0wFcXa8mj8+PGYMWMGgLybmSNHjpR6T/bu3Yvc3NwCzzGIiKh4qlWrBmtra4SFhclci0k2Thf2XN3Ozg4HDhxAVlYWfHx88PLlS6lrLXEPavHUnkURHR0NLy8vrF27Vmadra0tjh49CpFIBHd3d1hZWX3SkWXF16cA0K5dO6nHQN4w8pUrV0ZiYqLU6KDA/zqTLF68GHp6elLX6l27doWXl1ex6mNjYyN1P6Kk3pf9+/dDJBLJHfIfADZv3oxz586hSZMmwmimpSUrKwuxsbHYu3cv7t+/j1WrVuHevXsYNmwYnJ2dC5yqBQBatGiBTZs2QVNTEzk5OYiLi0NYWBj8/Pzg5eWFpKQkrFmzBk+fPoW9vb1SjWyKJCYmokqVKvD19cXatWtx6tQp4R7s1q1bC3wOeaNm9unTBwMGDMD58+eRmJiIwMBAqd/413TeSaQMxsWSxbgoi3GxbDBBhAolPj5e+FvcW0k8msGiRYvkNm5LqlixIgDg33//RVZWFlq0aJHvHGNiyhwMKlSogF69esHT0xNZWVl48+aN1LQFkrS1tYXpOxQpqDeq+PUPGzZMmLtKEcmDKgBhCKSChqEraH1Bw6pLJhBINibm5OQI2YfHjh0TGuAUEX8O3377LYyNjXH79m1s3rwZnp6emDBhAszNzVGhQgX89ddfUvvFxsYiLi5OYbmGhobCd0OSkZERjIyMYGZmhtmzZ2PhwoXw8fGBSCTCmjVr0Ldv3wKDk+RJxTfffIPvvvsO3bp1w9SpU/H333/Dw8MDvr6+8Pf3x86dO4XRCIC8KSrymzNPPNLAy5cvsWbNGri6uhY6g11ZgwYNEhJEQkNDpdYVdSgtos8NY4wsxpjSizGfi9atWwsj2QB579vVq1dx4sQJ3Lp1C0DetDCF9csvv+Do0aPIycmBt7c3pk2bJkx7dvfuXdy/fx+rV68u8sVdQkICTpw4gUWLFsldb2trK1yIurm5KUwQycrKwtu3bxU+T+XKlfHNN9/ILK9Tpw7q1KmDH374AVOnTsWRI0ewYsUKZGZm4uLFi7h37x46dOiQ72uoUaOG1Eg09erVQ9u2bfHzzz/jwoULmDVrFl68eIH58+fj4cOHWLp0ab7lEZUkXV3dAmOHsj1PPi7TwMAADg4OCAoKKkoVlVatWjXUrFlTeFynTh20bt0alpaWuHv3LqZOnYqIiAipOaw/jmkfPy7PBg4ciI0bN+L169eIiIiAl5eXkESYnJyMQ4cOYciQIVLvKRERlY7FixdLPX737h1OnDiB8+fPC8sKe65es2ZNmJqa4sSJE8jNzYWLi4tUwueuXbvw7bffYsCAAUWu7759+2BoaCi3Q1XDhg3Rq1cvXLlyBa9evcLVq1fRq1cvueVERUXJNEZJqlevXoHXdx8rTCwXN4R93Lg4dOhQ7N69G9HR0bCxsYGJiQkmTZqEdu3aQU1NDatXr1aqLoB0Y1dpnWfcvn0bABTG7GrVqsHGxkbpcgt7v1IsOTkZU6ZMgYWFBVq1aoVWrVrh22+/xbRp0xAVFYUxY8Zgz549Uh1L5KlUqZJUB5GaNWuiefPm+OGHHzBlyhT89ttv+Pfff3HgwAFcuXIFhw8flrqm+/h+pqSKFSvC0NBQalloaCimTJmCHTt2oEaNGnB0dETdunWxZcsWBAYGYsyYMdi/f3++UxkoMnDgQOF3HBoaKtXY/TWddxIpi3FRPsbFwmFcPCz3XmdZY4IIFYrkiZx4uLWYmBhhWUE3M8XE+6SmphZ6H2VIDs+U34G7JIhfS2ZmZqm8ltKUkJAgBBcNDQ2l6r9x40bMmTMHV69eRVhYGBYuXIhNmzZh0qRJGDVqlFSD44EDB/Ltobh37165Gd2S9PX1sXHjRlhbW+PBgwd4/fo13r17J3PxUFja2tpYtmwZXr16hfv37yMjIwNz587FhQsXhGA+Z84cIWjJ8/TpU2RlZeH333+Hubk5atSoITOsF/C/rNnMzExhvY6OjlLJJNWqVYOuri4SEhKkRkwgKk8YY2QxxnyaGPM5SEtLw6FDh3D06FEYGRlh4sSJaNq0qdI9/OvWrYtBgwbBy8sLOTk5cHFxEaaS2LlzJwwNDYs1qsehQ4eQmpqK1atXY82aNfluGxwcrDBhIzIyEoMHD1a4r7m5OVatWpVv+aqqqrC2tkZOTo6QRBkQEFBggkh+fvjhB/z5559CUsiRI0fwww8/KOzdQPQla9asWVlXAUZGRnB0dBTmlL569SqOHz8uDF/7NVJVVcUvv/wizOO9a9cuDB06FCoqKjh8+DCSkpIwfvz4Mq4lEdHX5eHDh9i1axfCw8MxcuRIHDp0qEjnnOPHj8fJkyeRm5uLf/75Bw4ODqhVqxaCg4Nx8+ZNLF26tMiNMqmpqTh8+DAyMjIUjqQpeT/Jzc1NYUPYunXr8h1u39fXV6pBpLQ1bNgQO3bswJw5cxAdHY3Lly/j8uXLMDIywuzZs+WOdPk5iIyMBCDdkaIkFOZ+pVhOTg5+/fVXfPjwQWrUyl69esHFxQXjx49HcnIyHBwccObMmSJ3fKtduzacnZ0xcOBAJCcnIyIiAqtXr5bqtZ/f9Z+xsTH27dsnPI6MjMTYsWPRr18/1K9fX1ju4OAAHR0drFq1Cs+fP8fcuXOxfft2pesr2aGQ91mJlMe4KI1xsXAYF1fLHc2mrLH7ORUoNTVVmNsKyBuqCPjfj/nx48eFLks8J/3bt2+RkpJSgrXMI9kbvLTndhK/lidPnpTq85QGcd0B5T4/IC9hY9euXdi2bZtUQ+7y5cthY2MjNRJASVFTU5MaSjkqKqrY5Y0ZM0Z4/N9//yndk/L+/fsICQnBvn370KtXL7n/xGUGBQUJy/bs2aN0fcW99D+e44+oPGCMkY8x5tPFmLJ048YNDBw4ENevX8eePXuwbNmyArPU8zNx4kQh89/DwwPR0dF4+vQp/Pz8YGdnV+CoMYpkZmZi//79+O233/DPP//A09NT7j/JHh9ubm5Ffh2FNWLECKHXWHHPDQBg+PDhUr/tM2fOFLtMos/R7NmzP+lNLEV69eoldZOevznAzMxM6Fn04sUL+Pr6IjMzE+7u7ujXr59MDygiIiodKSkp+OuvvzB27FiYmJjg5MmTsLKyKnJjQcOGDdG/f38Aede6rq6uAPKSAQ0MDGBubl7kup44cQKqqqo4d+6cwvP0CxcuoHHjxgDyrkG+pOvMbt264dy5c3BwcBCuye/evQsrKyts27atjGsnn7gXfX4jJ5Y2T09P3L59W+4Uo0ZGRli7di1UVFTw/v17HD9+vFjPZWBgINUZ4cKFC4WeauJj69atQ3R0tNx629nZCfeHL126hEePHildvuQoo7zPSlR4jIufD8bFovlS42JpYoIIFej48eNCQ139+vWFnpTiaQDEU3/kJzAwUGqfnJwc+Pr65rtPcHCw0j8a8fbq6uqoXbu2UvsqS/xagoOD8d9//+W77b1790q1LsrS09MTpiC4cOFCvttGRkYiLCxMZnnfvn1x8uRJbNmyBfXq1QOQlwixcOFCYRsHBwc8ffpU4T9lenZLTjFRrVq1Qu+nyMdTQ0RHRwt/79u3L996A8oN+19cycnJAGSHxCIqDxhj5GOM+bQxJj+5ubmIjY0tkbJiY2ORk5MDALh48SJ++eUX1KxZEzt27ECtWrWKXX7Tpk3Ru3dvAHnfVzc3N+zatQtVq1bF8OHDi1zu6dOnkZqaijFjxsDAwEDhPzMzM+G3cfHiRbmfbZ06dfL93AoaPURShQoVhN4R1atXL/LrkyxPspea5LkBEZUOyXPyz+03l56eLpyHF1dhX1uFChVgZ2cnPN6xYwdOnTqFqKgoYbQVIiKSVlLxQ1xORkYGfvnlFxw7dgzr16+HhYVFiZQveRw/duwYAgMD4ePjA1tbW1SoUKFIZebk5MDd3R2jRo1CrVq18j1XHzt2rLCfu7u73PJWrVqV77l6WSWY6ujowN7eHpcuXcKkSZOgoaEBkUiEjRs34vLly2VSp/yI71v6+/vnu116ejoePHhQ6HILc79S7J9//gEAhSMw9+3bF3369AGAfHtfF5bkOV1mZiYSExOFx/nVWXL0kOzsbHh7e+db7xkzZgjXfkWpt+RosOLGYaLyhnGRcfFzw7goHRc/F0wQoXzFxsZi586dwuMZM2YIjT5t2rQBALx+/Rqenp4Ky/Dz88PDhw8BAC1bthT23759e76Ncxs2bJDb0zU3N1fhPuIMtDZt2qBSpUoKtysJ4h6+ubm52Lx5s8LtPnz4gIMHD5ZqXZRVoUIFYWjpf/75By9fvlS4rbu7u3Dy/PjxY6nvAwD0798fp06dgpGREYC8xtz09PQSr7P486xTpw7q1q1b7PI+nh5C2cbeTp065Rt8nj59KiS1GBsbC8scHByUep53794JIyHImzOP6EvGGKMYY0yeTxVj8uPt7S01yk1xODo6QkVFBbm5uVi8eDFyc3Px008/FXlkD3kkL7APHjyIs2fPYvTo0VI9pZQhEomwZ88ejBgxosCRc9TU1PDzzz8DyPvuSt7sKy3a2toAgC5dupRIeZLnB6WdCEZEkErA+Nx+c3v37hWGwi2O3NxcpYaTHTlypNCj9eHDh3B0dETnzp2LNcIUEVF5pewxVpGgoCD4+PgAyOt5fP/+fejp6QnJ1yWhZcuWwoiZaWlpmDRpEnR0dGBlZVXkMi9cuICoqCjY2NgUuO3QoUOhr68PAPDy8pKa1vVztWrVKqlYXLlyZcycORMHDx4URts9ffp0qT2/uIGyoCHxP76PIL6f8eLFC1y7dk3hfvv37y+V0U+B/42wmN99kc6dOwOA0ImhOCTP6SpWrFikzn3x8fFCfRW95xUqVEC7du0AFK3eL168AJA3emn79u2V3p/oc8e4yLjIuCjflxgXSxsTRL4yyox6kJGRgZkzZwqZglZWVhg4cKCw3tTUVPh76dKlQg9uSXFxcXB0dBTmGtTV1RXm83r58iXmzJkj92Di5uaGBg0aCA19khISEhTWWXxwGT16tMw6yYNSfg2A8raRd0Do3r278KP28PDA3r175ZaxePFiqfdN0XPIU5KjVHxclvjzy8jIgL29vdwA+ODBA9y+fVsY5h/Ia+z7uCxtbW3MnDkTQN5rSktLK7F6i4m/X/n1nFPm/RJnpANAvXr10KJFi6JXrhSdP38eADBgwACZ+eIlX6+81y45T5/kvHqfunGVvh6MMbJ/K8IY83nFGEUyMzNx4MAB4YK1OG7fvg0dHR2oqqoiLi5O+O7Le28kv7fyvh/iz1beZ9i+fXshoSY1NRUVKlQQkjYUlaOoLCBvPtUXL14U6uIaACwsLIQLxmPHjpXqtEC5ubl48OAB2rVrh44dO8rdRpnv+aNHj/D69WvhcX5zZROVBGVjh+TxQNENC8nlJXGcL+h8M7/tCxIVFYU7d+4Ij+X95gp7vit5Eyk3N7fYw7fGxcXh+vXrJTKC36lTp9C0aVOpZfkdxytVqiR1zE1MTFR4DVSY4zgRUXkm7xirLJFIhE2bNmHAgAEAgGfPngHIiy2S91KAgs/Tgbxjs6JjsuTxPDExEaNGjZI7PH9hju8ikQg7duzAjz/+WKhGB01NTZiZmQHIu86Rd42prIKuaz/epiAfv9acnByhx6+k1q1bY8iQIQAg8xkpc25QUN3ECZuS1whiDx8+RFxcHADZ+2xDhw4V/l60aJHca76HDx/i5MmTMiNfiusv+boyMjLyrac84t+FvHspYuJrtVatWsmsU+a8Ijc3F+fOnRMeDxw4UJj6VBnVqlWDgYEBgPxHSs2v3gUR32e1t7eXum8KlN15J1FJYlxkXGRclO9LjIuljQkiX5mPDw6KhIeHY8yYMQgICAAA2NraYtGiRVLbtGjRQhhOSjzs+MqVK3H37l0EBwfjwIEDMDc3x+DBg4V5lAHg999/F3qxent7w9LSEp6ennj8+DGuXbuG33//HS4uLpgyZYrcur148ULuHIOnTp3Cy5cv0bNnT7mNZZIHncI0VkjOZS9ve01NTcyePVt4vHz5ckydOhVXrlzB48ePcfbsWVhbWyMpKUmYT01MfIB8//59vnWQHHZI3vDGkgdCeQ1mkgf5jzPvrKyshKH0QkNDYWpqCldXVwQHB+Pu3btYv3497OzspF4jADx//lxub3ZxwGvRooUwNYIy7t+/j1u3bskNnOnp6VizZg0GDx6MkSNHKiyjsN/vixcvCj3u1dTUMH/+/BLtvV0U4eHhMsvev3+P7du3o0GDBli8eLHMeslezvK+H3p6esLf4obtixcv5jsaA1FxMMbkYYz5/GJMQfWVJzc3F0uXLkXDhg2hoaGhsKzCfO/fvXuHP//8U/juVK1aVcjsP3DgAO7fvw8g7zNbsWIFDhw4IOz7/v17XL58Gbdu3RKWffjwQer/j0leYFtZWcmdWzk3N1fquydvCNKMjAw4OTmhSZMmhR69S09PT+gZkJqamu8IOAWJjIzE5cuXFU7x4+LigszMTKxZs0ZhGYU9LsXExEh9H3/88ccSm6aISJGSjh1FKbMgksf/wiToFfY3l5KSglmzZgmxpn379lI3jcQkz3flHbvFx7f4+HgEBwdDJBJhzZo1UlNcKXv8z8zMxB9//CH3GKDs8f/p06dYsWKFzLmD+HNSdByXHPmpRYsW6Natm9ztJPf/Enq8ERHlp6SOscoe97dt24bc3FyhMUk89WNWVhaWLFmCjIwMYRpTyV7NkZGRSElJwaZNm6TKi4mJUXhM7tKlizAilJaWFsaMGSN3O8nzX0VleXh44PHjx8Jw6IUh7lAB5A3LXtB0pgWRvIZITEyU23gibiwCCj6XkHddum3bNplh4oH/XSN+HCMLOjeQvM4t6JpZ3Jns0qVL8Pb2RnZ2Nj58+IAtW7Zg48aNwsii//77L5KTk4XPrXfv3sL0uRERETA3N8fBgwcREhKCGzduYMWKFRg1ahRmzJghk6Qgrv+tW7eQmZmJDx8+SE21WlgTJkyAhoYGLl68KIy6Kik5ORmnTp2Cnp4eRo0aJbO+sOd0IpEITk5Own2UatWqKT16spiKigomTZoEANi9e7fc4fj//fdfBAUFwcjISGEnAQBypzv18/PDpUuXMGDAAFhbW8usL4nzTqKSxLj4P4yL/8O4+PXExdLGBJGvSHJystQX/8aNG/j3338RGxuLjIwMREZG4urVq1i4cCEGDRqEoKAgNG/eHFu3bsXcuXPl9rRetGgRTExMAOQFCDc3N9jY2MDS0hJLly6FkZERfv31V6l9GjdujM2bNwtDgj958gRz5syBmZkZxo8fj6tXr2Lr1q3C8E4f09bWxqRJk3DmzBnExsYiLi4OBw4cwIIFC9C2bVs4OTkJ2VgikQhpaWm4e/cujh49KpTh7e2NS5cuITU1VeYgnZWVhdDQUGzYsEFY9vLlS7i5uSEuLk4qi87c3BzTpk0THl+8eBGTJk2CmZkZpk+fjrS0NKxbt05Yn5KSAl9fXzx+/BhAXiOpq6urzM3b7OxsPH/+HBcuXBCWubm5CQEhJycHkZGROHLkiLDew8MDkZGRQiBISkqCq6ursN7HxwdPnjwRsjorVqwIZ2dn1K9fH0DezcXVq1fD0tISNjY22LlzJ2bPni33xujWrVsxa9YsPHz4EMnJyQgKCsKSJUugp6eH5cuXy2xfkNDQUFhZWWHMmDEYNGgQjhw5gnfv3iE1NRUBAQEYO3YsOnfunO/waI8fP5Z6H8+ePYvQ0FAkJCQgMzMTMTEx8Pf3x+zZs2Fvb4/s7Gzo6urCycmpRIdGU0b9+vWFRrQFCxZg1apVCA0NRWpqKvz9/WFjY4NGjRrB3d1d5veQkJAgNXS/h4cHIiIipBJs2rRpI/zO1qxZg169emHbtm0lNk8gkSTGGMaYzzXGiElOdRMfHw9vb28kJycjOztb+JeWlob379/jypUrGDt2LI4dOya3R/urV6+Evy9fvozXr18jIyNDKCcrKwvJyckIDQ3F3r17YWlpiezsbHTo0AEAoKqqihEjRgjvpZWVFYyNjdG3b19UrFhR6nP/8ccfcfLkSRgbGyMzMxOPHz+Gi4sLAMDZ2Rlv376VGSmnV69eaNasGTQ0NGBrayu1Ljc3F1FRUXBycpK6EF2/fj3evXsnfMYvX77EzJkz8erVK4SHh+Pq1asFjkCVkZGB4OBgqZ4EBw4cwJYtW/D+/Xule7c7ODhg8uTJMDExwbJly/D48WOkpKTgzZs3WLZsGXx9fXHkyBGFyStZWVm4e/eu8Pj+/fsICgpCTEwMMjIykJycjMePH2PHjh0YMmQIXrx4ARUVFZibm2P16tVK1ZWosHJzc5GYmIhTp05JzWu7a9cu/PvvvzK9eMSx5ubNm1LH5XPnzgmxJicnBwkJCfDw8JAakUNRmYWRmZmJFy9eSM1fvG/fPsTGxirsCSUSiaSS2R4/fow7d+4gKioK6enpSElJwYsXL7Bv3z4MGTJEeP09e/bErl27pG4E5eTk4N27dzh58qSwzN3dXSamSU4vZW1tjW7duiEjI0Nq5A/JY/bz589x8+ZNpKamSh3/U1NTER4ejrNnz2LEiBG4evUqBg0aJPMaJcvy9vbG+/fvkZmZKZSTmZmJhIQEPH36FNu2bYO1tTUaNWok3FTNyclBREQEHB0dAeQN1xwUFCTTA0pfXx+WlpYAZEdQFIlESExMxD///CP1fru5ueHJkyfsxUpEX6ziHmOBvHslkslzhw8fRlJSErKysoRyMjIyEBUVhYCAANjb22PTpk1S5/ympqZCA4enpyeMjIxgZGSEtWvXYsWKFcK1pIeHB/r164cePXogNzcXsbGxcHNzw+vXr3Hnzh2cO3cOycnJMufA4uP6sGHDZHo4p6en4969ezh8+LCw7MKFC7h27RpSUlIgEomQmpoKb29vLFu2DEDeOUFBDVoikQixsbG4cuWKsCw1NRWTJ0/G7du3C93oIZadnY2IiAipRO3U1FSsWbMGUVFRUteN7u7uwjbnzp1DaGioEKvS09Nx8eJFof63bt1CYGCgTKPozz//DDc3N4SFhSEhIQGHDx/GqVOn0KNHD5mpCBSdGzRs2BBRUVFS9Tl27Bju3bunsCeyra0tVFVVkZWVhRkzZqB169bo2rUr7t+/j82bNwsjJ0ZERMDU1FTo1a2iooK1a9cK9/yioqKwZMkSWFhYwM7ODvv378cff/whdyppcf1DQkKEDiria0dltGjRAqtWrYKKigrGjx+PY8eOITY2FrGxsfDz84ONjQ1SU1Ph7Ows9/6IuEOPuP5+fn4IDw9HcnIy0tPTERERgTNnzmDUqFHYvXs3AKBBgwbYs2dPsaYNHD16NEaPHo2IiAjY2NgI3/3//vsPx44dw4QJE9CkSRNs3rxZpje2sbExatasCQCwtLTEvn37EBkZicTERBw6dAjTp0/HyJEj4eTkJHUPqiTPO4lKEuMi4yLjIuNiaVIRfSbjkEbEp6GP0xVkZBd+eJ3yQFNdFZd+743aekWbF74wrl69isuXL8Pf3x9v376Vu42KigoqV64MXV1dGBoawsjICJ07d4axsXGB5YtEIpw6dQpHjx7FkydPkJubi++++w5WVlawsLBQOHROREQEduzYgatXryImJgZ6enro2bMnpk6dKvcHM3r0aNy+fRvGxsYYP348tm/fjidPnkBFRQUNGzbE0KFDYW1tLdXL98aNG7Czs8u3/osXLxayhrOzswucamTIkCFwcnKSWnbv3j24urri3r17SE5ORu3atfHjjz/il19+EYInkHfzU9E81r6+vqhTpw6AvOkUJHsPS/L09MStW7ewcuVKuetnzZqFHj16CMNjfWzMmDGYP3++8Dg1NRV79uzB2bNn8fbtW2hqaqJdu3aYOHGiMES92OPHj2XKVVVVhYGBAUxMTDBlyhTUqFFD7vPmJzc3Fxs3boSXlxciIyMhEomgq6uLunXronPnzrC0tFTY+CPONLxw4YLczHIxNTU1Ya6v+vXrw9jYGCNGjECVKlWUrm9hSX5nJZM5JIlEIvj7+8PT0xOBgYGIjo5GpUqV0LJlS2F0hI8bzuV9DmJz586Vagz08/PDihUr8OHDB/Tu3Rvz5s1T2DBeniVlJ+JIuAtyUPz5474kalDDyDrjUVm99L7njDGMMZI+xxhz8+ZNvHnzBi4uLgq/o4pUrVoV/v7+UFdXR2xsLO7fv48HDx5g9+7dyM7OVqqssWPHYt68ecLjzMxMbN68Gf/88w8SEhLQunVr/PbbbzAyMkJMTAxsbW0RHR0NKysr/Pbbb4iOjpbq2SCpQoUKCA4Ollp25swZ3LhxQyap5pdffoG/v7/CetarVw8LFy7E+PHj5a6/fv06qlevLned+HekyIEDB2Q+9/zcuHED69evR2hoKNLS0lCpUiVUr14d7dq1w4ABAxQmeD548ABnz57FzZs3hYQpeVRVVVGhQgVUqVIFtWvXFmLv5zrt3KeUkZyEe8cPQFQC865+SVTU1NDB0gaaOpVL7TnWr18PZ2fnfLfx9vYWbjQXJtbUqFFD4fFf7MKFC/j2228LVcfo6Gihd48848aNw5w5c4THL1++xPHjxxEUFJTvcOAqKirQ0NBAlSpVUKtWLXz//fcYPHiwMM+vJDc3N4WxyNPTE82bNxce79y5E66urlBXV8fIkSMxZcoUqKio4Nq1a3jz5g22b9+ucCQiRZo3by6Muvfu3Tv8+++/uH79Og4dOqRUOQAwb948jB07FgDQv39/hbHoyJEjaNu2rfD43bt3mDBhAv755x+p64GdO3cWOK+4n5+f0EBCRMWXlZWIt2/3QCT6yuKiihrq1bODhkbpXU+W1DE2KCgIL1++xP79+xESEqJUGerq6vD395caqfDhw4dYuXIlHj9+DD09PZiammLKlCnQ0tKCi4sLtm7diqZNm2LBggVo0aIF5s+fj+PHj8stf9asWVLJfiKRCKampti+fbtwjQYAQUFB+Y6aCwCOjo44ePAggoKCZNZZW1vLHX0WyGtgUtQrG8hrfHFzc8v3uSXld+0J5HVYcnZ2lmqUktS9e3ds2rQJ7du3V7h+9+7dWL58ucyQ/xoaGvjuu+8wfPhwWFlZye1sIu/c4NatWwrPqfK7b3fx4kVs2rQJr1+/hqGhIUaMGIGxY8dCTU0Nffv2Rf369WFtbQ0TExOZXs9ZWVnYv38/Tp48idevX6NixYro2LEjJk6cKPSY/9iHDx8wf/583Lx5Ew0bNsTs2bMVvo+FERoaCjc3NwQEBCAyMhJqamqoW7cuTExMMHbsWKnG2MzMTGzduhWvXr2Cj49PvlMNaGhooFKlSvjmm2/QuHFj9OrVCz/99FOJjdB8/fp1HDx4EA8ePEBcXBwqVqyIJk2a4Mcff8Tw4cOFRsiPpaenw8vLC+fOnUNISAiSkpJQtWpVGBsb4+eff0a7du1k9inueefHn/vXIDkrGx5vopDzWbQsfjpqKoDFt99AR6P0RiJnXGRcVLSecfHrjoul4bNJEAHykkTiUr6u3i5VK1Uo1eSQ8qQwje1ERIokZSciPafgodHLEy21iqWaHFKeMMYQEUnLSE5CVgGjxpQ3GlpapZocQkREX66srETkfGXXk2pqFUs1OYSIiL5cyVnZSM/5ujp7a6mplmpyCBHRp/RZHc1q61VksgQREZWKyupVmCxBRERUSJo6lZksQURE9P80NKowWYKIiOj/6WioQ0ej4O2IiOjzJDvWDBERERERERERERERERERERGVK0wQoS9GamoqACAlJaWMa0JEROUNYwwREREREREREREREZV3TBChz15qaip8fHzw9OlTAMCzZ89w8eJFNuIREVGxMcYQEREREREREREREdHXQkUkEonKuhJEioSHh6Nv374K1wcEBEBfX/8T1oiIiMoLxhgiIiIiIiIiIiIiIvqaMEGEiIiIiIiIiIiIiIiIiIiIqJzjFDNERERERERERERERERERERE5RwTRIiIiIiIiIiIiIiIiIiIiIjKOSaIEBEREREREREREREREREREZVzTBAhIiIiIiIiIiIiIiIiIiIiKueYIEJERERERERERERERERERERUzjFBhIiIiIiIiIiIiIiIiIiIiKicY4IIERERERERERERERERERERUTnHBBEiIiIiIiIiIiIiIiIiIiKico4JIkRERERERERERERERERERETlHBNEiIiIiIiIiIiIiIiIiIiIiMo5JogQERERERERERERERERERERlXNMECEiIiIiIiIiIiIiIiIiIiIq59TLugJS4sOA1A9lXYtPS7saoFe3rGtBRFTuJcWmIz05q6yr8Ulp6Wigsr5WWVeDiIi+QPHx8UhNTS3ranxS2tra0NPTK+tqEBERERERERERlZrPJ0EkPgzY0gHIzijrmnxa6pqA/T0miRARlaKk2HQcWHgTOdm5ZV2VT0pNXRU2SzszSYSIiJQSHx+PLVu2IDs7u6yr8kmpq6vD3t6eSSJERERERERERFRufT5TzKR++PqSQ4C81/y1jZpCRACAp0+f4u+//0bHjh1x69YtpfePj4+Hubk5unfvjvv375dCDcuP9OSsry45BABysnO/ulFTiOjzxtj3ZUhNTf3qkkMAIDs7+6sbNYWIvky5ubm4du0aHBwc0KJFiyKVcf/+fXTv3h3m5uaIj48v2QoSERERERHRZ+vzGUGESo2fnx98fX1x48YNhIWFyd1GQ0MDmpqa0NPTQ40aNdCkSRN06NABJiYm0NHR+cQ1Ljk3btyAnZ1dvtssXrwY1tbWAPJuChd0c2XIkCFwcnIqsTqStLCwMGzbtg3Xr19HbGws9PT0YGxsjClTpqBx48Yy27u7uyMkJAQXL15ESkqK3DI1NDRQsWJFVK1aFQ0bNkTHjh1hamoKAwODEqt3REQE9uzZg2vXruHdu3dQVVVF3bp10bt3b9jZ2UFfX1/Y9u7du1i/fj3u3r1brOe8efMmHj16BADw8vJCu3btilUeUVEwxjDGfEm+1BgjlpqaihMnTmDPnj1YuXIlOnXqVCLl3r17FzY2NnLXqaqqwsfHB3Xq1JFZx9hHpLxDhw5h8eLF+W7Tvn17HDp0KN9tRowYgQcPHuS7zbJlyzB8+HBlq1go8+fPx/Hjx2WWS8a9kijP0dERQ4cOLVQZjo6OMDQ0xM8//6z08xekZ8+eiIyMVLi+TZs2OHr0aIk/LylPmdiUmZmJdevW4fXr17hy5QpEIpHcMjU1NVGxYkXUqFED3333Hbp3745BgwZBS6t0R+lzcnLCrl27YG9vDwcHB2H50aNHsXv3brx+/bpY5Z8+fRrR0dGIjo7GrVu3MGDAgGLWmIiIiIiIiL4EKiJFV8Cf2n9BwM5eZV2LsjHRDzBsW+pPk5aWhoEDB+L9+/cAgOHDh8PExASGhobQ0tJCbGwsnj17Bl9fX1y7dg1A3jzcY8aMwdSpU1GhQoVSr2NJE4lEyMzMRHBwMObOnYu3b98CAKpWrYo1a9bAyMgIWlpaUFFREfbJyspCWFgYHB0dcfnyZWH53LlzYWZmhipVqkBV9fMZfKc8efToEUaPHo3k5GSZddra2nB1dVXYEPTo0SOYm5sLj48cOYL69etDR0cHGRkZCAsLw40bN+Du7o7379+jQoUK+O233zBhwoRi1/vGjRuwt7eHgYEBZs+ejXbt2iE7OxvXr1/H6tWroa6ujt27d6NZs2YAgJSUFGhpaWHTpk1wdnYGAOzdu1fphr74+HjY2dkhOjoaW7ZsQdu2beVuFxgYiMaNG6NKlSrFep1fsui3STi64k5ZV6NMjJjXEQb1Kpf68zDGMMZ87r7UGAMAHz58wP79+3Hw4EGhh29R4oYiv/zyC/z9/eWuMzExEWKVJMa+8u2///7Dzp07y7oaZWLixIkwNDQstfJFIhFSU1Nx69Yt/PHHH0hKSgIAVK5cGfPmzUO3bt1QtWrVAuNiTk4OPnz4ABcXF7i7uwvLO3TogFmzZqFx48aoXLlyqcWUrKwsREVF4eDBg3BxcRGW161bF+fPn4eamlqhy4qKikKfPn2QlZU36pmuri5Wr16Njh07olKlSlJxVJHk5GT07t0b+vr6OHfuXIm/7uzsbLx//x5bt26Fh4eHsNzW1hbjxo1D9erVlXrNpcHPzw+9en2Z91TS0tIQHBwMY2PjYpWjbGyS5ObmhpUrVwIAmjRpAicnJ9SuXRtaWlpISUnBixcvcPHiRRw8eBDp6ekwMDDAypUr0aNHj2LVWZHbt29j7NixyM3NlUkQSUpKQsWKFWFnZ4fbt28DyBuhS1lBQUGwt7dH9erV4ebmpnB6rS/5u0VERERERESy2ALxFalYsSJat24tPB4yZAj69u2L5s2bo0GDBujQoQOsra3h4uKCo0ePon79+khNTYWzszNGjRqFhISEMqx90aioqEBTUxNGRkYYMWKEsHzQoEHo0aMHKlasKHPDUUNDAw0bNsSUKVOEZY0aNYKtrS309PTYcFdK4uLiMGnSJIwYMQJnzpzBrVu3cODAAXTs2BFAXq/pefPmITdX/jQh33//PXR1dYXHbdu2hZ6eHtTV1VGpUiU0a9YM48aNw5kzZ2BkZITMzEw4OTlh27Ztxap3TEwMfvvtN2hpaeHAgQPo168fqlWrhho1asDCwgIbNmxATEwMpk6diszMTABApUqVoKamVuxez3p6ejh58iT8/f0VNpABwOrVq5GYmFis5yIqCGMMY8zn7EuNMUBeo+fhw4fRrFkz1K5du9jlfSw4OBjXr19HgwYN5P4bM2aMzD6MfURFp6KigkqVKqFPnz7o37+/sHz06NGwsLBAjRo1CpU0qaamhm+++QazZ8+GhoaGsHz16tXo0KEDdHV1SzWmaGhooHbt2pg9ezZq1KghLA8LC4O3t7dSZe3Zs0dIDgGAgQMHCqOMFSY5BACOHTuGpKQkvHnzRioBs6Soq6ujTp06+P3334Vlmpqa+OOPP1CjRo0yTw6Jj4+Ho6NjmdahOE6dOlWkacckFSU2SercubPwt56eHpo2bQodHR2oq6tDV1cXHTp0wJw5c+Dh4QEDAwNER0dj8uTJuHnzZrHqLU9CQgL++OMPhecllStXhrq6Olq2bFms52nbti38/f3h6empMDnkS/9uERERERERkSy2QnxltLW1C7WdeIhc8Q2H4OBgTJ48+Yuei1zyhoeimx/F2Z6Kx9nZGUuWLMGcOXPQuHFj6OnpwcjICHv27MH3338PAHj58mW+w+hWqlSpwOfR0dHBhg0bhOGAt23bhnfv3hW53qdOnUJSUhL69euH6tWry6zv0qULatWqhfDwcJmbnpqamkV+3sK6fPkygoKCSv15iADGGHl/l9T2VDxfaowRlzl16lQMGDAAtra2xSpLHmdnZ/zwww84d+6c3H9du3aV2Yexj6hkfPPNN8LfRR21RENDQ2rKjFq1ahW7XsoyNDSUmjJu586dCqfq+FhSUhKOHDkitX+1atWUev7s7Gzs3btXeOzm5qbU/sqQjNn6+vplnhgitmPHDqSlpZV1NYokNTW1REYsKk5sAgoX54G8xN4lS5YAyPvuif8uSQsXLkTz5s0L3O5TxNQv+btFRERERERE8jFB5CtT2B5YQN7Qvps3bxZuggUGBsLV1bWUalb6JG/eFaY3neQ2n8uNv/LM1NQUffr0kVmuoaGBkSNHCo8V9aICCv/9NjAwEIYvzsrKwtWrV5Ws7f+8efMGQF6vQkXEN/8/7qlW2iMFREVFYcGCBaX6HESSGGPyMMZ8fr7UGPMxyUbgkvD8+XNcvnwZ9vb2Su3H2EdUMiR/Q8WJBZLl5Pe7LC0aGhro3bs3atasCQB49uxZoUfxOHDgAFJTU6WOxcoeJ86ePYuYmBjUr18fQN7UHI8ePVKqjMJSNt5/CgEBAaWaFFPa/v77b0RERBS7nOLEJkC581gTExMhMfrly5d49eqVMlXN14kTJ3Dv3j0sX768wG1L+xzyS/9uERERERERkXyfxx0N+mwZGhpi3LhxwmMXFxekpKSUYY2ovGrRooXCdeKekPXr10fDhg1L5PnEvbsBCHO/F4V4SO2LFy8iPT1dZn1ubi4iIiKgo6ODDh06FPl5lBUeHg47OztER0d/suckUhZjDH0qX2qM+VhJN/w6OzvDwMAAz58/R1hYWKH3Y+wjoo9paGjAzs5OeFyYESEyMjKwb98+9OnTB40bNy7yc+/ZswdDhw7FpEmThGVfS6N2QEAApk6dmm+C4+cqJycHy5Ytg4eHR4mU9yljk6qqqtRUUCUV69++fYsVK1bA0dGxxJNClfUlf7eIiIiIiIgof0wQoQJZW1sLPVMSEhLg6+srs42fnx8mT56Mrl27omXLlujduzfmzZsn9OKRJy4uDuvXr8dPP/2Etm3bol27drCyssLRo0dlbkJkZWXh3LlzGDduHMaMGQMgr4fowoUL0b17d7Rt2xZWVla4ePFiCb7ywnvx4gXmzp2LPn36oGXLlujUqRMmTZqEgIAAmW0zMzPh5eWF0aNHY/To0QCAu3fvwszMDO3bt8eWLVuEbe/du4dZs2ZJzS18/fp1WFlZoXXr1ujTpw+2b98u9X75+/vDzs4O7du3R+fOnbFgwYJ8G1yzsrJw8OBBWFtbw8jICK1atcLgwYOxYcMGhTe6zp07h+HDh6N169Zo2rSp1L/Y2Fil37+CvHr1CioqKpg3b16J9NTLyclBcHCw8Lg4DYI//vgjVFVVERkZKXd44StXriAmJga///57oaaROH78OExNTdG6dWv07dsXa9euRXJystxtw8LCsH79evTo0UPqxuqFCxdgZmaGFy9eCMv69u0rfEbFnd+bqCQxxhSMMebrjTEfU6Z3c0Hevn2Ls2fP4v3795g5cyb69esHCwsLHD16FDk5Ofnuy9hHVD6FhoZKxZv27dtj8ODBWL16daGOvyNGjBB+8/fv38ft27fz3d7DwwMxMTGYOHFikescEBCAR48ewc7ODj/99BMMDAwAAN7e3oiMjCxyucURGxuL3bt344cffsDmzZsBAJGRkVi0aBG6d++ODh06wN7eXmH9UlNTsXr1apiYmKB58+ZScVAyCWbdunUYN26cEIcjIiKkthV79+4dNm/ejJ49e8LDwwMikQhbtmxBly5d0L9/f+zcuVMm3orrDQCJiYky6//880+Frz8wMBAzZsxAjx490LJlS3Tt2hXTp0/HkydPpN4jS0tL7Nu3T1i2ZcuWQpWvSEnHpvy8evUK8fHxAPKSRRo0aFCs8oC86WpmzZqFkSNHyp3erTDS0tKwfv169OnTB61bt4aZmRmOHDmicMqnwMBAzJ07F23btkV4eLiwvLDfLSIiIiIiIvoyMUGEClSlShWp+W9v3rwp/J2dnY25c+di69atsLOzw/nz53HixAm0bNkSJ06cgJmZmdwGrKCgIJibm6NixYrYsWMH/Pz88NdffyE0NBQLFizAlClTkJ2dDQBYv349evfujWnTpuH69esQiUQICwuDpaUljh49iujoaKSlpeH+/fuYOnUqdu3aVfpvioTDhw9j4sSJ6Nq1K44fPw4fHx9YWVnh6tWrsLOzE3rPiUQiODo6omfPnpg1a5ZwwzQoKAi//PILHj9+jJSUFGzevBne3t4wNTXFqFGj4OXlhaysLIhEIixfvhyTJ0/Gu3fvkJGRgYiICGzYsAHr168HAGzduhUTJkzAq1evkJmZibi4OBw9ehTTpk2TW/eYmBhYWVkhKCgIS5YswZUrV7Bz506oqqpi+/btGDZsmMyNS3d3d8yZMwc2Nja4du0afH19MX36dGhoaJTK+5uWloYjR45g/vz56NWrV4mUuWfPHrx79w5AXo/xnj17Frms+vXrCzdqPTw8MGPGDKHHWnh4OJYtW4YlS5bA2to633Jyc3Mxe/ZszJ8/H0+fPkVGRgbCw8Oxc+dOWFlZISYmRtj25cuXmDRpEn744Qc4OzsjKipKqqx+/frh5s2bUsMS+/j4ICQkBCEhIcLUB0SfA8aY/DHGfN0xpjTt3LlTJhEkJCQECxYswNChQ6USLT7G2EdU/vj7+8Pc3By3bt2Co6Mjbt68CWdnZ+Tk5MDV1RVWVlZIS0vLtwxtbW38/PPPwuMdO3Yo3FZcrrGxMdq2bVvkeru6usLExASNGjVChQoVMGrUKAD/S1D8lGJiYrBkyRL0798fjo6OQiLrvXv3YG5ujgsXLiA9PR3Jycnw8fHBuHHjhPMRsczMTNja2uLOnTvYtGkT7ty5g2PHjqF79+4yzzdt2jQEBwfDzMwMAFC7dm3hmBcSEoLQ0FBMnjwZffv2xZYtW4SYu2rVKmzevBmxsbF4+/YtAgMDERAQIDWqm6QqVarg9u3bWLlyZb6vPzc3F6tXr8b48ePRrVs3nD59GlevXsWgQYNw9uxZWFpaConA+vr6OHbsGEJCQoT9p0yZItR9xYoVhXvTJZRUbCqI+HWKmZmZoXLlysUqEwA2b96MnJwczJgxo0j7x8bGwsrKCs7OzoiIiEBGRgYeP36MhQsXYubMmVIJv5cvX4aFhQWsra3h4eEh89su6LtFREREREREXzYmiFChNGnSRPhbssHAyckJDx48gJubGzp16oTKlSujadOm2LRpE1q0aIHU1FRMnz5dqpfwf//9h4kTJ8Le3h6TJ09G7dq1oauri2HDhmHVqlUA8m5YiBu97OzscOzYMVSpUgVAXoPhnDlzYG9vj8DAQNy7dw+///670Ot33bp1CAwMLPX3BAAuXbqEVatWYceOHRgyZAj09fVRq1YtzJgxA2PGjIFIJMLatWtx9+5dqKioYPTo0Thw4IAwX3FGRgY2b96M06dPY+HChdDV1YWxsTHatGkDFxcXqV7djo6OqF69OgICAuDn5wdvb29hGN39+/dj/fr1eP36NS5duoQrV67g3r176N+/PwDg2rVrUr2ZgbybplOmTEGjRo3g6OiIJk2aQEdHB126dMGePXugpaWFN2/eYM6cOcI+ycnJWL9+PSwtLWFmZgZdXV3UqVMHv/76K+bNm1fi729cXBwcHBxQqVIlDBw4UKl9P+7RnpOTg6dPn2LhwoVwcnICANSsWRNbtmwp9pD906dPF26oent7Y9SoUbh9+zacnJywbds2WFlZFVjGxo0bkZaWhr179+LChQtYuXKl0APy+fPnUp9D3bp1sXXrVoU3aVVVVaGuri7VE168TF1dvUR7oBOVBMYY+RhjGGNK07Rp03Du3Dns3bsXc+fORceOHYV1z58/x/Dhw3H//n2F+zP2EZUfOTk5+PPPP5GRkYFx48bByMgIOjo6MDY2xoIFCwAAb968KdRIWqNHjxbikL+/v8LG5PPnz+Pt27fFGj3kxYsXuHbtGn755RdhmbW1tTDN1+HDh+VONVJadHV1MXfuXOzZs0dYFhISgk2bNmH79u0ICAjAnTt3hBG+Xrx4ITNy2j///IMHDx5g8eLFaNWqFXR0dNC6dWvs3LkT7dq1k9pWTU1N5vgmPuapq6ujRo0aWLFihVQi540bN5CRkYHLly9j0KBB0NDQgImJCfT19WFvb5/va7OwsMh35I1169bB1dUVTk5OsLS0hJ6eHvT19fHHH39AQ0MDWVlZmDdvHjIzM6XqKiZ5zC7qiF4lEZuAvPOYrKwsqWXp6em4ffs2bG1tcfnyZQCAsbFxiZyj3L17F/v378fatWuLnBQ7bdo0dO3aFSdPnsSZM2fg4OAglOXt7S2V5NylSxd4eHhg2LBhcssq6LtFREREREREXzYmiFChSN4ISkhIAJB3k9Dd3R0jR44UbgKKqaqqonPnzgCA+Ph4nD17Vli3YcMGqKiowMLCQuZ5JIdSPXz4sPDchoaGaNSoEQDgyZMnmDlzJkaMGAFtbW3o6OhgwoQJwo2v3NxcODs75/t6XF1d0a1bt3z/WVpa5ltGTk4Oli9fjm7duuG7777L97UcOnQIAFCrVi00atRImGM7ODgY06ZNQ7169WBjY4Nbt25h3759qF27Nr755hu0b99eKGPw4MGYNGkSdHR0AACNGjXC2LFjAeQNQ6ytrY01a9agVq1aAABNTU1Mnz5d2P/jxrtTp07hwYMHcnuKGRgYCK8pICAAr1+/BpA37HRaWprQM1rSyJEjhcbE4goKCsL69esxcOBAoeGxX79+Ss1P3b59e7Rv3x5dunRBp06d0KpVK5iamuLIkSNQUVHBuHHj4OnpKfezK4o5c+Zg0aJF0NbWRkhICEaPHo2oqKhCzx3drVs3bNmyBZ06dcK3334LCwsLHD58WPjt+fv7Cw11GhoaUFdXx/fff18idScqa4wxshhjpH3tMaY0GBgYoEGDBujUqRNsbW2xf/9+7N69W5gSJzU1FVOmTBGG0JeHsY+ofIiPj0d0dDSAvEQASa1btxb+lnd8/pienh5GjBghPFY0ioiLiwuaN2+OHj16FKXKAIDdu3ejTZs2MDIyEpZVrVoVQ4cOBZD3uk6ePFnk8pWloaGBChUqSB37MzMzsWvXLrRp0wZA3lRhU6ZMERreHz58KFWGOJ6+f/9earmamhqmTp2qVH10dHSgr6+Pfv36CctevHiBBQsWwNDQEBs2bEBwcDBGjhwJAKhUqVKBZSra5smTJ3BxcUGbNm3Qp08fqXWamprC9yg5ObnUk3aKG5uAvCmSWrduDSMjI3Tt2hUdO3ZE27ZtMXr0aNy6dQuVK1fG0qVL4erqWuzRQ5KSkvDHH3/gzz//LNZUNfb29pgzZw6+//57NG7cGPb29ti0aZOw3sXFRRgpRJxExZhKRERERET0dWLqPxWKmpqa8HeFChUAAJ6ensjNzcXWrVuFntiSUlNThb+fPXsGIK/Xjbe3NwAUeDMwMjIS8fHxQiOBuPdLy5YtpW4Cio0bNw5ubm6Ii4sTekZpamrKLdvKykrhELpi7969w/DhwxWuv337NsLDwxEbG4tu3brJrJfscSR+/WLi97Bt27ZSN10/7t0q2SjaqlUrmecQN+KIy/pYnTp1hL8TExOl1olvlkr2uJMkbqQV179+/frCZ+Hr64stW7Zg0qRJwueipqaGH374QW5ZysrOzkaDBg3Qp08fnD17FmlpaUhPT8e8efNQpUoVqZucipw9exaVK1eGSCRCQkICoqKicOPGDZw+fRqRkZHYu3cv3rx5g2nTppXIPMpZWVmIjY3F3r17cf/+faxatQr37t3DsGHD4OzsjBYtWuS7v7yh7+vUqQMHBwf8/fffAPLmzZbsOSi+sUf0pWOMkcUYwxhTFrp3747Dhw/Dzs4OISEhiI2NhaurK2bOnCl3e8Y+ovKhWrVqsLa2RlhYmEzMEScOAhBGfiiInZ0dDhw4gKysLPj4+ODly5dSMUU8soh4CrOiiI6OhpeXF9auXSuzztbWFkePHoVIJIK7uzusrKw+6ShC4jgMAO3atZN6DORNr1K5cmUkJiZKjYIG/C9pdvHixdDT05M6J+natSu8vLyKVR8bGxup866Sel/2798PkUgkdyocIG/6lHPnzqFJkybCqG2lpbixCQBatGiBTZs2QVNTEzk5OYiLi0NYWBj8/Pzg5eWFpKQkrFmzBk+fPoW9vb1SyScfW7RoEVq2bJnveWFhdOrUSWZZnz59MGDAAJw/fx6JiYkIDAyU+o0rOpclIiIiIiKi8o0JIlQokr1Hq1atCgBCj85FixYVOK97xYoVAQD//vsvsrKy0KJFi3znpBZT5uZRhQoV0KtXL3h6eiIrKwtv3ryRmrZAkra2tjCEuSIZGRn5rhe//mHDhglzHSsieRMOgDBkbkHDsxa0/uNe9R+TbESRbEzMyckReqsdO3aswGFsxZ/Dt99+C2NjY9y+fRubN2+Gp6cnJkyYAHNzc1SoUAF//fWX1H6xsbGIi4tTWK6hoaHw3ZBkZGQEIyMjmJmZYfbs2Vi4cCF8fHwgEomwZs0a9O3bt8CbmZI3ob/55ht899136NatG6ZOnYq///4bHh4e8PX1hb+/P3bu3CmMRgDkTVGR3xzr4pEGxJKTkzFlyhRYWFigVatWaNWqFb799ltMmzYNUVFRGDNmDPbs2SPVUFtYgwYNEhrJQkNDpdYVdehlos8NY4wsxhjGmLKiq6uL3bt3Y8iQIYiOjoaPj4/cBBHGPqLyZfHixVKP3717hxMnTuD8+fPCMpFIVKiyatasCVNTU5w4cQK5ublwcXHBihUrhPW7du3Ct99+iwEDBhS5vvv27YOhoaHcpL6GDRuiV69euHLlCl69eoWrV6+iV69ecsuJioqSSdKQVK9ePaWn/Pg4LssjThD5OOlm6NCh2L17N6Kjo2FjYwMTExNMmjQJ7dq1g5qaGlavXq1UXQDpJJDC1K0obt++DSDvs5enWrVqsLGxUbrcsrouq1SpklQibM2aNdG8eXP88MMPmDJlCn777Tf8+++/OHDgAK5cuYLDhw/jm2++Ebb/OHZJqlixIgwNDQHkJUQHBgbi1KlT+danOAYOHCj8jkNDQ6USRErr+0BERERERESfNyaIUKFI3uAQD0MaExMjLCuoIUxMvE9qamqh91GG5HC++d3oKwni15KZmVkqr6U0JSQkCDcjNTQ0lKr/xo0bMWfOHFy9ehVhYWFYuHAhNm3ahEmTJmHUqFFSDY4HDhzAli1bFJa1d+9euT2dJOnr62Pjxo2wtrbGgwcP8Pr1a7x79064qaYsbW1tLFu2DK9evcL9+/eRkZGBuXPn4sKFC8LN3zlz5gg3OeV5+vSp8HdOTg5+/fVXfPjwAWZmZsLyXr16wcXFBePHj0dycjIcHBxw5swZqV6YhVGtWjXo6uoiISFBasQEovKEMUYWYwxjTFmqWrUqJk6ciOXLlyMiIkJmPWMfUfn18OFD7Nq1C+Hh4Rg5ciQOHTqEDh06KF3O+PHjcfLkSeTm5uKff/6Bg4MDatWqheDgYNy8eRNLly4tcuN0amoqDh8+jIyMDIUjhkkeO9zc3BQmiKxbty7faWh8fX2lEgVKW8OGDbFjxw7MmTMH0dHRuHz5Mi5fvgwjIyPMnj1b7ohen4PIyEgA0gmjJeFzui4Tq127NpydnTFw4EAkJycjIiICq1evlhrNZvDgwQr3NzY2xr59+xAWFoa///4bixcvRlpaWr6JMMnJycK0Q/r6+jKj0uRHMqmVMZWIiIiIiIgAgF3wqECpqanCXMhA3tC2wP9u/jx+/LjQZWVnZwMA3r59i5SUlBKsZR7J3uDFnQu4IOLX8uTJk1J9ntIgrjug3OcH5N2Q2rVrF7Zt2ybVkLt8+XLY2NhIjQRQUtTU1KSma4iKiip2eWPGjBEe//fffwgKCipSWZ6enrh9+zb69u0rs87IyAhr166FiooK3r9/j+PHjxfpOcS99D+eE56oPGCMkY8xhjGmrInjWqVKlWTWMfYRlT8pKSn466+/MHbsWJiYmODkyZOwsrIqciN6w4YN0b9/fwB5Md3V1RVA3ughBgYGMDc3L3JdT5w4AVVVVZw7dw6enp5y/124cAGNGzcGANy4ceOLiqfdunXDuXPn4ODgIJx73L17F1ZWVti2bVsZ104+8egyb9++LbM6fIrYJGZgYICffvpJeHzhwoVCT8EkdurUKSQnJ+P3339Hr1695P4TEyc59erVSxhlrrAkR1NjTCUiIiIiIiKACSJUCMePHxca6urXry/MKyyeBkA8LHt+AgMDpfbJycmBr69vvvsEBwcrfZNFvL26ujpq166t1L7KEr+W4OBg/Pfff/lue+/evVKti7L09PSEIdovXLiQ77aRkZEICwuTWd63b1+cPHkSW7ZsQb169QAAQUFBWLhwobCNg4MDnj59qvBfQT27JUlOMVGtWrVC76fIx1NDREdHC3/v27cv33pL+ueffwBAYW/zvn37ok+fPgCQb++3/CQnJwMo+2kHiEoDY4x8jDGMMWVNPPJLs2bNZNYx9hGVLcljSkmUk5GRgV9++QXHjh3D+vXrYWFhUSLlT5w4Ufj72LFjCAwMhI+PD2xtbZUaAUFSTk4O3N3dMWrUKNSqVQsGBgYK/40dO1bYz93dXW55q1atyveY/ClHD5Gko6MDe3t7XLp0CZMmTYKGhgZEIhE2btyIy5cvl0md8iOOnf7+/vlul56ejgcPHhS63M/tukySZKzPzMxEYmKi8Di/Ou/btw9A4adsKi7JUe/ESVNERERERET0dWOCCOUrNjYWO3fuFB7PmDFDaPRp06YNAOD169fw9PRUWIafnx8ePnwIAGjZsqWw//bt2/NtnNuwYYPUUPJiubm5CvcR91hq06aN3B6vJUk8b3Fubi42b96scLsPHz7g4MGDpVoXZVWoUEFo8Pnnn3/w8uVLhdu6u7sLN5UeP34s9X0AgP79++PUqVMwMjICkNeYm56eXuJ1Fn+ederUQd26dYtd3sfTQxS1sVfc0zy/73Lnzp0B5N3QVta7d++EkRDkzbFO9CVjjFGMMSbP1x5jypI4vg0bNkzhOsY+ok8vNzdXaiqLogoKCoKPjw+AvBE57t+/Dz09PfTu3bvYZYu1bNlSGBksLS0NkyZNgo6ODqysrIpc5oULFxAVFQUbG5sCtx06dCj09fUBAF5eXlLT132uVq1aJUzXAuSNWDZz5kwcPHhQGFnp9OnTpfb84sSdgqaK+fh8SXze9uLFC1y7dk3hfvv37y+VUd6A0o9NHxMnMgJ5o3Qom2BaUKKrZAKMvb19kRJggbzPBMgbpa19+/ZK7UtERERERETlExNEvjLK9FLJyMjAzJkzhZ5lVlZWGDhwoLDe1NRU+Hvp0qVCD25JcXFxcHR0FObg1dXVFYZKffnyJebMmSP35pObmxsaNGggNPRJSkhIUFhn8c2o0aNHy6yTvImVXwOgvG3k3UDq3r27cBPIw8MDe/fulVvG4sWLpd43Rc8hT0n2Kvq4LPHnl5GRAXt7e7k3TB88eIDbt28Lw/wDeY19H5elra2NmTNnAsh7TfnNn1xU4u+XZE/Ejynzfnl7ewt/16tXDy1atChSvZo2bSpVP3nEUyK0atVK6fLPnz8PABgwYIBML27J1yvvtUvO6y4533RpNK4SAYwx8v5WhDGGMaYoCjrulyQvLy907NgRP/74o8w6xj6isnPq1CnhN1hUIpEImzZtwoABAwAAz549A5A3zYzk7waQThRQ1Kiem5ur8JgkeVxNTEzEqFGj5E5bIxmzFJUlEomwY8cO/Pjjj4VqjNfU1ISZmRmAvKQBebFUWQXF74+3KcjHrzUnJ0cYCUNS69atMWTIEACQ+YzExz3JxIvc3Fy5iRIF1U08Bcnr169l1j18+BBxcXEAZI+pQ4cOFf5etGiR3Lj/8OFDnDx5UibBQVx/ydeVkZGRbz3lKW5sUiau5ubm4ty5c8LjgQMHQkVFpdD7f0rimGpvby8VI4HCx9TCfLeIiIiIiIjoy8EEka/MxzeTFAkPD8eYMWMQEBAAALC1tcWiRYuktmnRooUw/HBqairGjBmDlStX4u7duwgODsaBAwdgbm6OwYMH45tvvhH2+/3334V5cL29vWFpaQlPT088fvwY165dw++//w4XFxdMmTJFbt1evHiBR48eySw/deoUXr58iZ49e8ptLJO8SSW+MZQfcQ8kRdtrampi9uzZwuPly5dj6tSpuHLlCh4/foyzZ8/C2toaSUlJwvzbYuIbau/fv8+3DpLD1Er2UBKTvHEmr8FM8sbNxz21rKyshCFmQ0NDYWpqCldXVwQHB+Pu3btYv3497OzspF4jADx//lxub3bxDdIWLVoIUyMo4/79+7h165bcG63p6elYs2YNBg8ejJEjRyoso7Df74sXLwo97tXU1DB//ny5IwkUxoQJE6ChoYGLFy8KoxhISk5OxqlTp6Cnp4dRo0YpLCc8PFxm2fv377F9+3Y0aNAAixcvllkv2UNd3vdDT09P+FvcsH3x4sV8R2MgKg7GmDyMMYwxJRVjPib5ORSU8JCYmIiRI0eiQ4cOOHbsmNS62NhYuLm5KRxi//79+7hw4QI2bdokt8GLsY+oZEgeRwtzjHn69ClWrFghE4fyOx7Ls23bNuTm5gpJFrVq1QKQlwyyZMkSZGRkCNO1SY72ERkZiZSUFGzatEmqvJiYGIWjc3Tp0kUYFUtLSwtjxoyRu11sbKxUefJ4eHjg8ePHwjQhhSFOHAXypispaNq2gkhO75OYmCi3UV2cRAHIj5+S5B3Htm3bJnfKMXEM69atm9RycVJHfHw8goODIRKJsGbNGmEKN8l4XtC5gTih8dKlS/D29kZ2djY+fPiALVu2YOPGjcKIW//++y+Sk5OFz613797CNIEREREwNzfHwYMHERISghs3bmDFihUYNWoUZsyYIZOkIK7/rVu3kJmZiQ8fPkhNKVdYxY1NhY3zIpEITk5OwvlitWrV4ODgoHR9S5q8Kfv8/Pxw6dIlDBgwANbW1jLrJWOqvGNHQd8tIiIiIiIi+jIxQeQrkpycLHWj5MaNG/j3338RGxuLjIwMREZG4urVq1i4cCEGDRqEoKAgNG/eHFu3bsXcuXPl9rRetGgRTExMAOTdUHRzc4ONjQ0sLS2xdOlSGBkZ4ddff5Xap3Hjxti8eTO0tbUBAE+ePMGcOXNgZmaG8ePH4+rVq9i6daswHPDHtLW1MWnSJJw5cwaxsbGIi4vDgQMHsGDBArRt2xZOTk5CY4ZIJEJaWhru3r2Lo0ePCmV4e3vj0qVLSE1Nlbmpl5WVhdDQUGzYsEFY9vLlS7i5uSEuLk6q15W5uTmmTZsmPL548SImTZoEMzMzTJ8+HWlpaVi3bp2wPiUlBb6+vnj8+DGAvIYRV1dXmcbB7OxsPH/+HBcuXBCWubm5CTcQc3JyEBkZiSNHjgjrPTw8EBkZKdw4TEpKgqurq7Dex8cHT548EXoBVqxYEc7Ozqhfvz6AvGkKVq9eDUtLS9jY2GDnzp2YPXu23CFst27dilmzZuHhw4dITk5GUFAQlixZAj09PSxfvlxm+4KEhobCysoKY8aMwaBBg3DkyBG8e/cOqampCAgIwNixY9G5c+d8h9N+/Pix1Pt49uxZhIaGIiEhAZmZmYiJiYG/vz9mz54Ne3t7ZGdnQ1dXF05OTsUaSrtFixZYtWoVVFRUMH78eBw7dgyxsbGIjY2Fn58fbGxskJqaCmdnZ5nvdP369YXhmBcsWIBVq1YhNDQUqamp8Pf3h42NDRo1agR3d3eZfRMSEoT5q4G8zz8iIkKq8bNNmzbC72zNmjXo1asXtm3bVmLzyhNJYoxhjGGMKfkYI5aVlYXIyEicPHlSWObh4YH//vtPYS/egIAABAUFITk5WabX/PHjx7Fy5UqMHj0aU6dOxb1795CcnIywsDBs374dR48elRt7xBj7iErGq1evhL+9vb3x/v17ZGZmIjs7G9nZ2cjMzERCQgKePn2Kbdu2wdraGo0aNRISOoC838WHDx+Ex4cPH0ZSUhKysrKEcjIyMhAVFYWAgADY29tj06ZNwuhbQN6oT+KGf09PTxgZGcHIyAhr167FihUrhN+ih4cH+vXrhx49eiA3N1dINnv9+jXu3LmDc+fOITk5WSb2iUcRGTZsmMzIH+np6bh37x4OHz4sLLtw4QKuXbuGlJQUiEQipKamwtvbG8uWLQMAnDt3rsBED5FIhNjYWFy5ckVYlpqaismTJ+P27duFTgYQy87ORkREBNasWSNV3po1axAVFSUVH93d3YVtzp07h9DQUOFYnZ6ejosXLwr1v3XrFgIDA2WShX7++We4ubkhLCwMCQkJOHz4ME6dOoUePXrITNHTpUsX4W9ra2t069YNGRkZaNiwIaKioqTqc+zYMdy7d0/hCB22trZQVVVFVlYWZsyYgdatW6Nr1664f/8+Nm/eLExBExERAVNTU2G0ExUVFaxdu1Y4vkdFRWHJkiWwsLCAnZ0d9u/fjz/++EPutGHi+oeEhAiJuCNGjMj385CnOLEJgJC4LK6/n58fwsPDkZycjPT0dERERODMmTMYNWoUdu/eDQBo0KAB9uzZU2ZTyRkbG6NmzZoAAEtLS+zbtw+RkZFITEzEoUOHMH36dIwcORJOTk5S59o5OTl49+6d1HmFu7u7zDmjou9Wo0aNSveFERERERERUalSEZX2+NSF9V8QsLNXgZuVSxP9AMO2pVb81atXcfnyZfj7++Pt27dyt1FRUUHlypWhq6sLQ0NDGBkZoXPnzjA2Ni6wfJFIhFOnTuHo0aN48uQJcnNz8d1338HKygoWFhYKh1qNiIjAjh07cPXqVcTExEBPTw89e/bE1KlT5d5gGT16NG7fvg1jY2OMHz8e27dvx5MnT6CiooKGDRti6NChsLa2hoaGhrDPjRs3YGdnl2/9Fy9eLPSmyc7OLnAY+CFDhsDJyUlq2b179+Dq6io0sNSuXRs//vgjfvnlF+FmKwD07NlTak5pSb6+vqhTpw6AvOkUDhw4IHc7T09P3Lp1CytXrpS7ftasWejRo4cwnPLHxowZg/nz5wuPU1NTsWfPHpw9exZv376FpqYm2rVrh4kTJ8LIyEhq38ePH8uUq6qqCgMDA5iYmGDKlCmoUaOG3OfNT25uLjZu3AgvLy9ERkZCJBJBV1cXdevWRefOnWFpaYm6devK3VfcM+3ChQtSveE/pqamJswNXb9+fRgbG2PEiBGoUqWK0vWVJzQ0FG5ubggICEBkZCTU1NRQt25dmJiYYOzYsQqHwRaJRPD394enpycCAwMRHR2NSpUqoWXLlsLoCB83nMv7HMTmzp0LW1tb4bGfnx9WrFiBDx8+oHfv3pg3b57CBr/yLPptEo6uuFPW1SgTI+Z1hEG9yqVWPmMMY4wkxpjSiTEDBw6UakiW1KJFC3h4eMgsT0hIwMSJE/HixQvMnTsXlpaWwjpxcpGvry9iYmKgoaEBQ0NDdOnSBaampmjZsmWh6sXYV379999/2LlzZ1lXo0xMnDgRhoaGpVb+u3fv8O+//+L69es4dOiQ0vvPmzcPY8eORVBQEF6+fIn9+/cjJCREqTLU1dXh7+8vNSLTw4cPsXLlSjx+/Bh6enowNTXFlClToKWlBRcXF2zduhVNmzbFggUL0KJFC8yfPx/Hjx+XW/6sWbOkppYRiUQwNTXF9u3bhVgEAEFBQfmO3AQAjo6OOHjwIIKCgmTWWVtbyx1pCMhLvFA0WgmQ1/Dt5uaW73NLyi/GAnnJac7OzlIN6pK6d++OTZs2oX379grX7969G8uXL5dJ6tPQ0MB3332H4cOHw8rKSm5S7c6dO+Hq6gp1dXWMHDkSU6ZMwa1btxSepxgbG0sl3Em6ePEiNm3ahNevX8PQ0BAjRozA2LFjoaamhr59+6J+/fqwtraGiYmJzGggWVlZ2L9/P06ePInXr1+jYsWK6NixIyZOnCiMJPOxDx8+YP78+bh58yYaNmyI2bNnK3wfC0OZ2JSZmYmtW7fi1atX8PHxyXcKHg0NDVSqVAnffPMNGjdujF69euGnn34qsVHC5BFPm2Nvb69wlJL09HR4eXnh3LlzCAkJQVJSEqpWrQpjY2P8/PPPaNeuncw+bm5uCs/1PD090bx5c+GxvO/Wx587ERERERERfVk+nwSR+DBgSwcgW/m5Zr9o6pqA/T1AT37DBP2PZOOdoptZRETyJMWm48DCm8jJLvyc8OWBmroqbJZ2RmV9rbKuymePMYaI6H/i4+OxZcsWZGdnl3VVPil1dXXY29tLTVVERERERERERERUnpReVwdl6dXNS5RI/VDwtuWJdjUmhxARlbLK+lqwWdoZ6clZZV2VT0pLR4PJIUREpDQ9PT3Y29srPQ3Hl05bW5vJIUREREREREREVK59PgkiQF6iBJMliIioFFTW12KyBBERUSHp6ekxWYKIiIiIiIiIiKickZ28lugzJe7BmJKSUsY1ISKi8oYxhoiIiIiIiIiIiIiIyjsmiNBnLzU1FT4+Pnj69CkA4NmzZ7h48SIb8YiIqNgYY4iIiIiIiIiIiIiI6GuhIhKJRGVdCSJFwsPD0bdvX4XrAwICoK+v/wlrRERE5QVjDBERERERERERERERfU2YIEJERERERERERERERERERERUznGKGSIiIiIiIiIiIiIiIiIiIqJyjgkiREREREREREREREREREREROUcE0SIiIiIiIiIiIiIiIiIiIiIyjkmiBARERERERERERERERERERGVc0wQISIiIiIiIiIiIiIiIiIiIirnmCBCREREREREREREREREREREVM4xQYSIiIiIiIiIiIiIiIiIiIionGOCCBEREREREREREREREREREVE5xwQRIiIiIiIiIiIiIiIiIiIionKOCSJERERERERERERERERERERE5RwTRIiIiIiIiIiIiIiIiIiIiIjKOSaIEBEREREREREREREREREREZVzTBAhIiIiIiIiIiIiIiIiIiIiKueYIEJERERERERERERERERERERUzjFBhIiIiIiIiIiIiIiIiIiIiKicY4IIERERERERERERERERERERUTnHBBEiIiIiIiIiIiIiIiIiIiKico4JIkRERERERERERERERERERETlHBNEiIiIiIiIiIiIiIiIiIiIiMo5JogQERERERERERERERERERERlXNMECEiIiIiIiIiIiIiIiIiIiIq55ggQkRERERERERERERERERERFTOMUGEiIiIiIiIiIiIiIiIiIiIqJxjgggRERERERERERERERERERFROccEESIiIiIiIiIiIiIiIiIiIqJyjgkiREREREREREREREREREREROUcE0SIiIiIiIiIiIiIiIiIiIiIyjkmiBARERERERERERERERERERGVc0wQISIiIiIiIiIiIiIiIiIiIirnmCBCREREREREREREREREREREVM4xQYSIiIiIiIiIiIiIiIiIiIionGOCCBEREREREREREREREREREVE5xwQRIiIiIiIiIiIiIiIiIiIionKOCSJERERERERERERERERERERE5RwTRIiIiIiIiIiIiIiIiIiIiIjKOSaIEBEREREREREREREREREREZVzTBAhIiIiIiIiIiIiIiIiIiIiKueYIEJERERERERERERERERERERUzjFBhIiIiIiIiIiIiIiIiIiIiKicY4IIEZWI3Nzcsq5Cvj73+hERUfn0KeMPYx0REZUlxjwiIiqvvvS486XXn4iIiEoWE0Toq5WQkIB9+/ZhyJAh+PPPP8u6OoIDBw6gffv2+P3338u6KoWSkJCA1atX4+LFi8KyrKwsnDt3Dr/88gv69etXhrX7n8TERPz+++94/vx5WVeFiIi+Ajk5Odi/fz+2bt36yZ4zMDAQy5YtQ1xc3Cd7TiIiIgA4ffo0/v7770/2fG/evMH8+fPx33//fbLnJCKir8+DBw8wY8YMJCUllXVVioX3RYnKh3Xr1qFdu3ZYu3ZtWVel0DIzM2FnZ4eOHTtKtSF9LS5evAhjY2PY2toiMzPzkz///fv30b17d5ibmyM+Pl5mfVRUFJydndGvXz9s3rz5k9cP+PLaRMsLFZFIJCrrStDnZf369XB2dla4Xl1dHZUqVUKNGjXQsmVLmJqaokuXLp+whsWTmZmJRYsWwcfHRzi5Nzc3x6pVq8q4Znl++ukn4WT95s2bqFq1ahnXSLGHDx9i2bJl+OOPP2BkZAQA2LVrFw4dOoSIiAgAQO3atXHp0qWyrKYgKioKv/32GwYPHowxY8aUdXWI6DNw48YNnDt3DkFBQYiIiEB6ejqqVKmCxo0bo2fPnjAzM4OBgQG8vb0RGxuLn3/+GW/evMEPP/ygsExVVVVoampCX18fjRs3Rq9evTBs2DBoaWkJ27i5uWHlypX51m3r1q0ySXYZGRlo3bq1wn28vb3RqFEjheunTJkCX1/ffJ/3zp07qFKlitSyq1evYsKECXK3r169Oq5fv55vmV+bmJgYzJ49G/3798eoUaOk1j18+BDbtm3DvXv3kJ2djbZt22LGjBn5fq6STp8+DUdHR5w6dQr6+voy64OCgjB//nwsXrwYHTt2LJHXQ0Rlz8/PD76+vggICMDbt29l1qupqUFTUxO6urqoW7cu2rdvj1GjRqFGjRrFet4XL17gzz//xPHjxwu9z/fff4+cnBypZdra2rh06ZLS1zYpKSno3bs3EhMTpZarqanh0aNH+e67fPly7N27N99tPD090bx5c6lloaGhGDx4sMJ9Hj58CE1NzQJq/vVISUnB/Pnz0bBhQ0ydOhVqamrCulevXmHLli24fv060tLS0KxZM0ydOhU9e/YsVNl37tzBr7/+iuPHj6N+/foy61++fIkZM2bg119/xcCBA0vqJRFREYWEhODMmTO4d+8e3rx5g+TkZGhqaqJ69epo1qwZevbsiT59+qBq1apwcHCQaoQ4duwY/vrrL4Vlq6mpoWLFiqhRowaaN2+OQYMGoW/fvlBRURG2Katrnfnz58uNk4sXL4a1tXW++ypTnqOjI4YOHap0eVQ0IpEIO3fuxMOHD7F8+XLo6el90s+6SpUquHz5Mvz9/YV7rB9TU1ODlpYWqlSpgm+//RatW7fGjz/+iGbNmsndnvdFiZTj6OgIQ0ND/Pzzz2VdFUG7du2QmpoKbW1t3L9/v6yrUyjBwcGwtLQEAJiYmOTb9vg5yszMxNGjR3HmzBk8f/4cGRkZMDAwQKtWrdC7d2/06NEDvr6+yMrKkvtdmTRpEq5cuQIAOHHiBFq2bPlJ67906VIcOHAAALBp0yYMGDAAABAbG4ulS5fC19dXSFyxt7eHg4PDJ60f8GW1iZYrIqKP5OTkiBITE0X//POPqEWLFqImTZqImjRpIurWrZvI1tZWNGHCBFHfvn2F5U2aNBGNHz9elJSUVNZVL7S0tDTRu3fvRE2bNhU1adJENGfOnLKukmD//v2idu3aiWbNmqVwm9zcXJGfn98nrJWsGzduiDp16iQKCQmRWp6WlibKysoSDRo0SNSkSRORiYlJGdVQvoSEBJGpqano77//LuuqEFEZunfvnsjU1FTUpEkT0fDhw0UnT54UvXjxQpSSkiL68OGD6MaNG6L58+eL2rRpI7K1tRW1a9dOtHnzZmH/jIwM0aNHj0Tm5uZCLGzdurVo+PDhokmTJomGDx8uat68uVQMvXPnjrB/bm6uKDk5WXTlyhVRu3bthO26du0qun79uig1NVWUm5srt+6ZmZmi0NBQka2trVD2jRs3ROnp6QW+7pycHFFsbKzIxcVFKo6PGDFC9OzZM1FmZqbc/XJzc0VpaWmiO3fuiExMTERNmjQRWVtbi54/fy7KyspS8t0v396/fy8yMTERHT16VGadn5+fqEWLFiJLS0vRmzdvRI8fPxb16tVL1Lp1a9GDBw8KLPv169ciIyMj0a1bt/Ld7tGjR6JOnTqJvLy8ivw6iOjzlJaWJurdu7dw/N69e7fowYMHolevXonu3LkjWrlypXAN17JlS9GJEyeK9Xx//vmnqEmTJlIxrCDJycmiBw8eiCwsLKRizcaNG5V+/t27d0uVMXbsWNGLFy8UxitJ4utaT09PqZj8ww8/iB48eCDKyMhQuG9GRoYoJCRENHToUFGTJk1EgwYNKnCfr1FSUpLI3NxctG7dOpl1jx8/FrVt21bUv39/0aNHj0Rv374VmZqaipo1ayby8fEpsOzY2FhRjx49RJ6envluFxERIerdu7fIxcWlyK+DiIonPDxcNH78eFGTJk1EAwcOFLm4uIgeP34sio2NFaWkpIjevn0rOn36tMjOzk70/fffi0xMTERdu3aVKkPy+sjY2Fg4ZhsbG4t+/vln0aRJk4T7TOJ/w4YNE0VGRgpllNW1TmZmpig8PFzk6Ogo9bx9+/YVZWdnK/VeRkZGSt2L7dixo+jSpUuipKQkhdeHVDoWLFggGj16tNT3piw+65SUFFH37t2F7dzd3UWRkZGijIwMUVpamujt27eikydPiiwtLYVtZs6cKUpNTZX7vLwvSlQ4SUlJog4dOoj69+8vysnJ+WTPW1C7z9q1a0Vt27YVrV279pPVqTDev/8/9u47rury///484CgIIkLceGWUsyR5sKPmmVq5mxomeZK09KcpWU50jT3NjVXamnutHIvyl3SUEwxMlPABSoyhfP7w995fzmy5wF83G83bzfOe74OB9/Xua7rdV1XkPns2bOJ7ouKijL37NnTXK9evVTVA3KSoKAgc/v27c21atUyL1y40BwQEGC+e/eu+cyZM+YpU6aYn3zySePZu3r16kSvsWfPHvPTTz9tfvPNN21Snzx9+rTZ29vb3KFDB3NISIixPS4uzhwZGWk+deqU8R7mzp2b7fGZzbmnTzSvIUEEyerVq5fxcDh27JjVvoMHD5obNWpkVeHKbQ1mDRo0yHEJIqlx+PBhm8b8559/mmvXrm3esmVLkscMGjQoRyaImM1mc0BAgLl27drmlStX2joUADawfPly8xNPPGH28vIyb9u2Ldlj/fz8zE2bNjV7enom2oDy3XffGeXgw8/loKAgc+/evY39tWrVMvv6+ia4xsiRI41j4iehpOTkyZNmT09P84YNG1J9TnzxOxhTSjiIb8GCBWZPT0/zv//+m6775mV37twxt27d2jxq1KgE++7fv2/8Lf3yyy/G9q+//trs6elp7ty5c7LXjoqKMnfq1CnVfyM7duww16hRI9G/OQC5m+V7tqenp9nf3z/B/mPHjhkdDo8//rj5+++/T9d9rl27ZlznnXfeSfP5P/30k1XnydNPP20OCwtL9flRUVHm//3vf1bXOH36dJrjMJvN5tdff924xubNm1N93tatW82enp7mI0eOpOu+eVlMTIy5W7du5h49eiTaafnqq6+aPT09zd99952x7dChQ2ZPT09zkyZNUkzy6d+/f6LlaWJOnTplrl69unn37t1pexMAMuzIkSPmunXrmj09Pc3Tp09PsV1w586d5ho1apifeOKJJDvUx4wZYzyzH0509PX1NT/33HPG/ueee84cGhqa4Bq2qus8XG7FfwamxpQpU6zO//jjj9MVBzJmxowZ5vr16yf6t2WRnZ/1O++8k2T7vEVsbKz5s88+M47r1atXkp3atIsCKVu+fLnx/2nv3r3Zdl9b9/uk1/Tp0zM8OCGnuX//vjEwcN++fYke4+fnZyTxJZUgktPdu3fP5gkiqZFb/2/kZHa2nsEEOVuxYsWS3NesWTPNmjXLeO3r66t169ZlR1iZJjdODRwXF6e5c+fa7P4REREaNmyYypQpo/bt2yd5XE7+3VaoUEFvvfWWpk6dqmPHjtk6HADZaM2aNZoyZYri4uL06aefJvsck6QnnnhCy5cvl5OTk27fvp1gf/HixZM8193dXQsXLlSlSpUkPXh+jhs3LsFxpUqVsjontUqUKCHpwVJe6VGyZMlEf06Jm5ubJKl06dLpum9eNn78eP33338aPHhwgn2nT59WUFCQJMnT09PY/uSTT0qS/vzzT129ejXJa0+dOlUFCxbUwIEDUxVL27ZtVbduXb377ru6fv16Wt4GgBzO2dk52f0NGjQwlrcym82aOnWq4uLi0nyf1atXKyYmRpK0b98+Xb58OU3nW8onFxcXSdLt27e1fv36VJ//3XffKTg42DhfSr5+mpz4ZW1ayryMlrV52bx583Ty5EkNHz7caokHSQoKCpKvr6+kxMu8a9euGfsTs2LFCl26dEkff/xxqmKpW7euOnbsqPfff1/+/v5peyMA0u2XX35Rv379dPfuXfXt21fDhw+Xo6Njsue0atVK06ZNU1xcnG7evJnoMck962vVqqUvvvjCuM+///6rhQsXJjjOVnWd0qVLW5VbS5YskTmVq7vfvXtX69evz5RyD+n3888/a8mSJerbt69cXV2TPC47P+uCBQumeE07Ozt98MEH8vLykvTgffz444+JHku7KJC8+/fvWy1VuXLlymy5r637fdLr2rVrxhImecnu3bt15swZVahQQS1atEj0mCeeeMKqjzQ3ysn9eBa59f9GTkeCCJIVf/3gxDRo0EC1a9c2Xu/ZsyeLI8pcKb2/nGjhwoX6/fffbXb/uXPn6p9//lHv3r1lZ5f0IySn/2579Oghe3t7jR07NsEa6QDypnPnzmny5MmSpEaNGqlTp06pOq9y5crq169fogkiyT0HpQdfst98803j9dmzZ/Xff/9ZHZMvXz7j57Q8Oy33TimGpMS/b1quYYkxpz/ns9uBAwe0fft2tW3b1qoj0iI4ONj4OX7nbvw10C0JJA/bu3evvv/+e02fPj1Nn9Vbb72la9euaebMmak+B0De0LJlS+PnwMBABQQEpOn88PBwrVu3zujcj4uL06pVq9J0DUs507VrV2PbihUrjPWNk2M2m7Vs2TLVrFlT1atXN7Y/nIiQ1liktJV5GS1r86pz585p6dKlql+/vmrWrJlgf/zyLK1l3h9//KH58+dr1qxZKSZDxde3b1+Fh4drwoQJqT4HQPqFhoZq8ODBio6Olqenp4YNG5bqc1u3bq2mTZvqxo0bie5PqZ5RuXJlq3IusbZIW9V1HBwc1Lx5cyMp5fz58zpw4ECqzl27dq3Cw8PVpUsXYxvlT/aKiorShx9+qAIFCqhbt27JHpudn3Vqv//Y2dmpTZs2xuv9+/cneSztokDSfvzxR924cUMVKlSQJJ04cUJnz57N8vvaut8nPe7fv68PPvhA9+7ds3Uome6XX36RpBQHW9SrV0/169fPjpCyRG5o382N/zdyA75lIsMso4CkB9mCyDpffvml5s2bZ7P7BwcHa+3atbKzs1Pz5s1tFkdmcHFxUdOmTfXPP/9o69attg4HQDb47LPPdP/+fUlS796903Rujx49FBUVla77PtxxQlmZ95jNZs2ePVuS9OyzzyZ6TNGiRY2fw8PDE/05sRFqgYGB+uijjzRlypQ0zTAjSQ0bNlSRIkW0bds2/fPPP2k6F0Du9vAMV4klOSZn48aNMpvN+vLLL43RrZs2bdLdu3fTHEvTpk31xBNPSHpQBm7ZsiXFc/bu3au///5b/fv3T/P9kPVmz56t2NjYVJV58RuLIyIijJ/jJ4tYhIWFaejQoRo+fLjxN5NaFStWVPXq1XX8+HFGQwPZYObMmUaCx6BBg9LcufDxxx9naHaM+HWsnFa/cnBwUK9evYzXS5YsSfGcqKgorV69Wi1atFCVKlWyMjwkY+3atQoKClKjRo1SlaSYEz/rAgUKGD8n972NdlEgaStWrFCHDh2s6iJZPYuIrft90sMy0/yRI0dsHUqW+vfff7Vp06Zkj2nVqlU2RfPoyY3/N3ILEkSQYfGnIEpq6r1Dhw7p7bffVuPGjVWjRg01b95cH374oS5dupTg2Fu3bmn58uVq1aqV8R9//fr1atasmZo0aWJkY9++fVtfffWVXnzxReO4P//8U2+//bbq1aunp59+Wu+8847++uuvDL2/kJAQzZo1Sy+++KJq166tOnXqqGvXrvr222+tsgc/+eQTPf744wn+WUY1XLp0KcG+h6fBjImJ0a5du9S3b18999xzxvbY2Fh1795d06ZNM7Zt2bLFuE737t3Vv3//RO8/dOhQq3scP348wTF//PFHqn4Xq1atUlRUlGrVqmXV6JcRhw4d0oABA9SsWTM9+eSTeuaZZ/TBBx+kKiPw6NGjGjBggBo2bKhq1aqpevXqqlWrlho2bChvb295e3urSZMmmjJlSqLn16tXT5K0YMGCdE27DSD38PPz0/HjxyU9mJ7V29s7Tee7uLikOIIoKQ9P1ZfcNLV5wc6dO/XKK6+oZs2aCcqbW7duJTj+4sWLGj16tFq0aKEaNWroqaee0gsvvKDPP/880ePj27Vrl3r27Kn69eurWrVq8vLyUu3atdWoUSOrciCxivzp06c1dOhQ/e9//1ONGjXk7e2tIUOG6MyZM2l+z4cPH9a5c+fk4OCgxo0bJ3pM3bp1VbhwYUmymv7eMgrF3d3dWI7I4v79+xo2bJg6d+6sZs2apTkue3t71alTR7GxsVq0aFGazweQe8WftUhK2xIpsbGxWrVqlV577TW5u7vr5ZdflvQgoe3bb79NVzz9+vUzfv7yyy9THKm6dOlSVa5cOckEhJzi6NGj6t69u+rUqZOqOlZQUJA+/fRTtWrVSjVr1lTt2rX1/PPPa9y4cSku4ZORuo+/v79VWdugQQP1799fR48eTfN7vnDhglEnT6psKleunNHpdfHiRWO7pYx1cnJSnTp1Epz38ccfq3r16sYSSWlVt25dSdL8+fPTdT6A1AkODtbmzZslPUj2euaZZ9J8jXLlyqU5+Tm++EvZ5MT61auvvmp89z99+rROnDiR7PGbN2/WjRs3rMrL1EhLe+udO3e0cuVKq/bWu3fvavLkyWrSpImeeuop9e7dW+fOnTPOuXv3rqZPn65nn31WNWvWVOfOnXXo0KFkY7p+/brmzZun9u3bq06dOnr66af16quvavny5UkOukipLXjbtm2Jtnk+/vjjOnz4sNW1Ro0aZbX/xRdfTNXvMjY2VsuXL5ekNA2Ky67POrXiL+H2cP3yYbSLAgkdPXpUZ8+eVa9evfTiiy8aS4/98MMPCepYKfH399eHH36Y4Dt4/OdEavp9LCIiIrR582a99tprVtv9/PySfEaeOnXKKiZvb2+r/Z9//rlVLF9//bW6du2qevXqqUaNGmrSpIn69eunvXv3Wl0nICBA7du3165du4xto0ePNq4bv0P/2rVrWrx4sVq2bJlsR396y49ly5bp+eefN64dHByssWPHqkmTJsbSy2n97CTrZNSPP/5Y8+fPNwYePsyyvNfD4uLi5OPjo8GDB6tGjRoJ9sfExOiHH35Q9+7djc80Li5Oa9euVevWrVWzZk116NDBara0uLg4ff311+rYsaNq1aqlFi1aaNmyZUm+j19//VWjR49W7dq1E8xqnVrpqcceP35cw4YNM953YGCg+vfvrzp16mjAgAHGzKI5vU80LyNBBBkWvyPf0iBjcf/+fY0ePVoLFixQr169tGvXLm3atEk1atTQpk2b1LFjR6NR6tq1axo+fLiaNWumzz//3Bjp+tVXX+mTTz5RUFCQrl+/rqlTp2r06NFq2rSpJk2apAsXLkiSDh48qK5du+rQoUO6e/eu7ty5o71796pLly5Gp2Ba+fr6qlOnTnJyctLixYt16NAhjRkzRhcvXtTHH3+sgQMHGoXCmDFjtHbtWlWuXNk4v3Tp0tq4caMkqXz58lq9erWkBw20X331ld566y3j2CVLluj555/X4MGD5ePjY/XF3N7eXitWrNCZM2eMxt2OHTvqzJkzOnPmjFauXKnJkyerR48eVvF/+eWXVg9QSapfv74OHDigggULqnjx4lq/fn2ihdPDzGazvv/+e0mymuY5vSIjIzVixAi99957at68ubZt26Z9+/bptdde0/fff68uXbok27A3d+5c9ezZ0ygwf/rpJ33wwQeKjY1VSEiIbt26pUWLFmnr1q167733Er2G5X1fuXIl3X8jAHKH3bt3Gz/XqFEjXdPnPf/88+m6d/xyslixYqpYsWK6rpMbrFq1Sh988IG6desmHx8f7du3T0OGDJGDg0Oix//000/q1KmTjh8/bqx//MUXXxiNc127drUabWxhNps1evRoDR48WKVLl9bmzZt18OBBvfXWW4qIiNCtW7cUHh6ur7/+Wlu3bk3Q0TR79mx9+OGHat++vXbs2KEdO3bo2Wef1Y8//qguXbpo27ZtaXrf3333naQHazkntT60o6OjPvjgA0kPpkYMDw/XtWvXtHTpUplMJo0ePTrB1MFz585VTExMmqbsfphlpredO3fmySk/ASRux44dxs9PPfVUmjrhdu3apWvXrhkNVJYpyCVpzZo16ZqGvHXr1ipXrpykByOwdu7cmeSxx48f12+//aa+ffume0mZ7LBnzx717dtXLVq00L59+3T48GGNHTvWmHHlYX5+fmrfvr2+++47ffjhhzpy5IjWrFkjV1dXffPNN3r55ZeTHAWfkbrPunXr1K9fPzVu3FgbN27Unj171LVrVx0+fFi9evVK1Wjn+Cx/W87Ozsl+pxkzZozs7e21bNkyhYSEKCwszFg3evDgwQlmEFm/fr1+++03TZw4MU3xxGcp806ePKl///033dcBkLwffvhBMTExkh50niT1XT8rxW/Uf7gtMidwdnbWG2+8YbxevHhxksda6j7169e3WsY7OWlpb71165Y++ugjNW3aVJMnTzbaWy9cuKAXXnhB33//ve7du6d79+7p559/Vs+ePRUcHKzLly/rpZde0jfffKPIyEhFRUXpzJkzGjBggDHt/sOOHTumdu3aydfXV1OmTNFPP/2kL774QnFxcfr888/Vvn17q46k1LYFL1u2TJs2bdLjjz9unPv444/Lx8dH//vf/6ximDhxosaMGSPpwYju1C6Rd+zYMV2/fl1S2to9s/qzTotff/3V6Kx1cHBIMeGSdlEgoeXLl+uZZ55R5cqV5ejoaPw/iomJ0ddff53q66xcuVKdO3dW5cqV9e233+rIkSPq3bu3Dh48qB49ehjXSk2/j/RgVuRnn31Wo0eP1q+//mp1ryeeeELfffedVQJ29erVdfToUSMRzGL//v1GPeuzzz7TkCFDJD0oV/r166fx48erdu3a2r17t/bu3WskBr7zzjtav369cZ0KFSroxx9/tEpcmDRpkhH3O++8o5s3b2rIkCFq0aKFZs6cmez387SWHzdu3ND48ePVsmVLTZ061UiM/OWXX9SpUyft3r1bkZGRCgsL0549e9S7d+8kkzuS0rp1a1WtWlXSg+f3vHnz1LFjx0RnTKlTp45VWSA9mJWzbdu26tu3r3bt2mV8d7KYN2+ennnmGQ0dOtRIGgoLC1P37t01c+ZM3b59W1FRUTp37pwGDx6sQ4cOKSoqSgMGDNDkyZMVGhqqqKgoXblyRVOnTk1Qrztw4IA6d+6s1157TZs3b060fTU10lqP3bJli1q3bq0ePXro+++/V0xMjEJCQtSzZ08dPHhQ4eHh2r9/vw4dOpTj+0TzOhJEkCFHjx41Hl5OTk4J/jNOnz5dv/32m1auXKkGDRroscce0+OPP665c+fKy8tL4eHhGjJkiO7evStXV1eNHj1akyZNMs739/fXqVOndPjwYb3++utydHRUy5YtNWbMGM2aNcs47sqVK5o9e7YWL16sP//8U4cPHzYyxCMiIjR8+PA0T4d89epV9evXT++++67efvttlSlTRq6urnrppZeMUVkHDhwwHryOjo6qV6+eVqxYYWSW3rp1S3fu3DGuefbsWbm5uWn16tVq0KCBVSW6R48e2r17d5KNbPny5bNaQ9VkMhnb7O3tVbRoUX300UdWo7g8PT2tzrGc5+rqqoiICA0aNEi1a9dOVcPrH3/8YawTHT8JJr3GjRun7du3a9KkSerSpYsKFy6sEiVKqF+/fho7dqzi4uI0b948rVixIsG5u3bt0oIFC1SsWDHNmjVLlStXVrFixfTmm29q4MCBkh5kUu7bt0/FixeXk5NTojGUL1/e+Dm5dTkB5H7xGxAtHVTZITw83OoL+ltvvZVn15EOCwvTrFmz9PLLL6tjx45ydXVV2bJlNWDAAH344YcJjo+NjdWoUaMUFRWl3r17q169enJxcVH9+vX18ccfS3ow+9bDoxSkB9N9bt68WZ6enpo4caLKli0rd3d3DRkyxGqk+/Hjx1W8eHGrEYarV6/Wt99+q1WrVumZZ56Rq6urKlSooAkTJqhly5aKiYnRRx99lOJIbou4uDhjJHVKI7Q6d+6sOXPm6OrVq6pfv76aN28uR0dHLV682GqtaEk6cuSIvvnmG82aNStDje6Wsi4yMjJdI8UB5D4//PCDMQWus7Ozxo4dm6bzV6xYofbt2xt1mjJlyhizIl69etVqlFhq2dvbq0+fPsbr5JISli5dqlKlSqldu3Zpvk92MZvN+uyzz/S///1PvXr1UtGiReXu7q7XX3/dqp4a38cff6zbt2/r5ZdfVrNmzeTi4qIaNWpo6tSpkqTQ0FBjRH58Gan77N+/X1OmTNHixYvVrl07FS1aVKVKldLQoUPVo0cPmc1mzZgxI8HIwuRYyuWUyrxGjRppxYoVsre3V5MmTdSwYUNdv35dn3/+eYKl/s6fP6+pU6dq5syZiS49k1rU74DsEX/kc4UKFVI8/u+//zZmOkrs34QJE9J0/wsXLhjJavb29urbt2+azs8u3bt3N5Yp+emnn5KcqXDXrl36999/0zSjRFraWwsWLKgBAwZY/Z4vXLigWbNmaf78+frpp5906tQpo5wOCQnRggULNHLkSA0ZMkQnTpzQzz//rFWrVqlAgQKKjY1NtBy/ePGi+vXrJzc3Ny1evFjVq1dXwYIFVbduXa1atUoVK1bUP//8ox49ehhL36W2LfiZZ55RjRo1tGDBAuN3WqxYMZUoUSJBm6alndTJyUmffvppqpcyil/vTKmMe1hWftYPS6yD7+rVq1q6dKn69u2rmJgYOTg4aOrUqSm2fVBuAtb8/f3l4+NjVW957bXXjKWb1q1bp8jIyBSv880332jy5MkaOXKk+vTpo+LFi6tQoULq37+/SpcubdQlLElpKfX7SNKwYcO0e/fuRL8rm0wmY7Z6y6Cl4sWLJzoDvGWW49atW+ull14yXq9fv14//fSTihQpolGjRqlo0aIqWbKkhg0bZiSeWGZZih9j/DZOOzs7I247OzsVKVJEU6dOTXF5noyUH/H7js6cOaO5c+dq0aJFOnr0qE6ePGkMevD399e+ffuSjeNhjo6OWrhwocqWLWtsu3Dhgnr16qWePXumOAN+27Zt9cMPPyQ503C7du301VdfGX9f9+7d04cffqhu3brpxIkTOnr0qObPn698+fIpLi5OX3zxhYYPH646dero6NGjOnjwoA4cOGB8F1uyZInVYI5GjRpp8+bNeumll9L0vh+W1nqst7e31q5da5WAMX36dI0dO1bLly9XhQoVVKJECdWoUSPH94nmdXmzhwJZ4saNG0aWXVRUlDZs2GA0SDk5OWnWrFkqVaqUcfylS5e0atUqdenSJcG6jXZ2dmrYsKGkBw+QH3/8Ufnz51fx4sWtRmj/+uuv+uyzz+Tu7q6xY8fq999/17Bhw1SwYEE1adLEOM7X11dLliyRt7e37O3t5e7urunTpxvHXL9+3SrDMTVmz54tk8mkzp07J9gX/6G+bt06q32We9vZ2SkyMlKjR4+W2WzWX3/9pfnz52v+/PmJTvFcoEABOTg4yNPTM01xPmz06NHGF4cNGzYkeszOnTvl4uKiDh06pPq6mdm5evjwYW3ZskUlS5ZU27ZtE+x/5ZVXjC8eM2fO1NWrV632W6bMevLJJxP8bXXt2tX4+e+//042Djc3N+P8pEY/AMgbLAlukozpXzPb3bt3FRYWZrz+/fff1aNHD+NZ1KlTJ/Xs2TNL7p0TXLx4UREREQoMDEywr0uXLglGr4eGhhqV4YenhY4/jePD1zObzUal9Omnn06QcJNcOXD37l1j2bgSJUokiLNRo0aSHowMscwAlpKLFy8aM3PEb2BLSuvWrbVjxw6dPHlSJ0+e1LZt2xJM0X/jxg2NHDlS48ePl4eHh7H97Nmzeu+999SyZUu9/vrrxswlyYl/flo6AAHkLlFRUTp16pRGjRqlYcOGKS4uTlWqVNHq1av1xBNPpPo6J0+e1B9//JGgA79Xr17Gz6kdifuwzp07G0kn586d08GDBxMcc+7cOfn4+KhXr142GZGeWjdv3tTVq1cTLfOaNm2a6Khgy8yXD5d5FStWNBp6E7teeus+sbGxmjRpkry9vY2Rb/HFr9N+8803CfYnJiwszLhPasq8Bg0aaMOGDTp9+rSOHj2q3bt3q2PHjlbHREREaOjQoRowYIDV7+3ff//V6NGj1apVK7388stavXp1irPXxK+nUr8Dsk786cmTmjUpvnLlymndunUaNGiQbt++rRs3bujGjRsymUyaMmVKkrO+Sg+et5YpyO/fv6/du3frzTffVHR0tPLly6exY8da1R1yksKFC+vVV181Xic1s8SXX36patWqJZgJIynpaW8tW7as1bIpMTExmjdvnmrVqiXpQaLNkCFDjDLqp59+0ty5c/XCCy8YbYwNGzY0rpHYtOwffvihoqKi1KNHjwQdMwULFtRHH30k6UEyw8yZMyUp1W3BlqniPTw8jDr18ePHk0zq/+GHH9S+ffs0LT9k6ehzc3NLclbIpGTVZ52Y/v37q1atWmrQoIEaNWpkLNc9ffp03bt3T82aNdN3332nF154IcVr0S4KWFu2bJlq1aplNetGkSJFjD6U0NBQbdmyJdlrXL9+XVOmTJG7u7tee+21BPst146JiVFoaGiqYytQoIBcXFys2ngeVrRoUWPQ1NGjR3Xjxo0Ex0RHR2vr1q0J6ntJ1VWk/5ulL7G6SnLs7Ozk6OioatWqJXtcesoPBwcHOTo6WtVzoqOjtXTpUqNsM5lMGjhwoJEIkFJCR2LKlSunDRs2JBjUdfToUb3yyisaPHhwkmWRk5OTTCZTkvXxChUqqFKlSkaCR2hoqMaNG2dV9rZs2dLo4/z111/Vp08fvf3228b3r1KlShmD9u/evWvMxCXJSDzJ6GoAaa3HlihRQsWKFTO+j0gPBpw3btxY3t7e2rVrlw4fPqxSpUrl+D7RvI4EEaTa8uXL1bp1azVs2FB169bVmDFj5OLioldeeUVbt25NsObo1q1bFRcXpwULFiQ6QiB+I9T58+eNn+OP8u3cubNVZTN+Vlf849q2bZugo8dkMmn48OHGa8vo3tSIjIzUDz/8oHv37ul///tfgtjjr4UVHBycoDBv2LChkQ1+8uRJLV26VIMHD9bQoUNTnELQkrWZXhUrVjSSLlavXm3VWWmxdu1avfTSS0nOrJGYv/76y/g5I6O6pAdTU0vJTwPapUsXSQ8K9oeTcCyxJNZoXLRoUSM7Nqm16eIrXry4pJSTSQDkbvGn8UvP8jKpcebMGXXr1k2NGjXSU089pVdeeUUXLlxQo0aNNHfuXE2ZMiVPZydbEm/27dun+fPnJ/idP7xET7FixfTaa6+pSZMm8vb2ttoXv+y3NAhb3Lp1y0gsSawcqFKlivHzw+XArl27dO/ePW3atCnR7yaWiqZkXe4lJ/5xjz32WKrOkR5UFBNreDSbzXr//ff17LPPWjXqHT16VF27dtWtW7e0YMECNWvWTCNHjkxxKv74348uXryY6vgA5A7dunXT008/rdq1a6tbt27asmWLmjZtqiVLlmj79u1pnjZ12bJlat68eYIZA2vXrm0kcPv6+ur06dNpjtXR0VFvvvmm8TqxzpOlS5cm6GTJiQoVKiR7e3udO3dOn3zyicLDw632P9yAKEn9+vVTvXr11Lp16wT7LOXBw2WelP66z4kTJ/Tff//pyJEjiZZ5I0eONI6NXx9PzoULF4xpf9NS5jk6OiZ5/KeffqqSJUtajdT866+/9Morr+iPP/7QzJkz1bVrV02ePFmDBg2S2WxO8j5FixY1GpUp84CsE/9Z9fB06YnJly+fPDw81LVrV6vZobp27ar//e9/yXbif/fdd2rXrp0aNmyoOnXqaNCgQYqLi1Pbtm21fv16o+0op4qf8Lhnz54EbU+W2SbSMqNEettb4yeTVK9ePUG92NHR0RglXaZMmUQT6i3748+WLD3odPP19ZWUdFtfkyZNjPM3b95s1V6Z2rZgSerZs6dcXFwUGxub6HeJ8+fP6+TJk+rWrVuicSTF8rtKS/kWX1Z81omZMmWK9uzZox07duibb77RypUrNWLECHl5eUmSDh06pPfffz/RmTgTQ7so8MD169e1Y8cOq++kFj179jSeRatWrUr2++jGjRsVGRmpRo0aJUh2kB7Mqj5hwgR98cUXiSZxp8TS6Z+U7t27y97ePsklcXbu3CkPD48EyZWdO3dWnTp1rAYGWFieyYnVVTIac2aWH3Xq1LF6LT2oH1ie62ldYSD+NWbPnq1ly5Yl+Mx27dqlF198UatXr07y/JT6+yzlc5kyZRKd9SX+rFbxlxGyiD/DiWWGlbTcPyXprcfG/yweXnni4bI9p/aJ5nUJn1BAEt5//301aNBA9+/f1927d+Xk5JTsw93SaDh27FjVr18/2WvH/08ZfyRwYoVoWlSvXl2lS5fW1atX5e/vn+rz/vzzT8XExMjLyyvZtSMtEkuYGDRokI4fP67Tp09rxowZevHFF1NVOcmMjssBAwZox44dCg0N1erVqzVgwABj3+nTp+Xn56fZs2en6ZohISHGz2nNpI8vPDxcP/30kyQlO81j/L+Zh9fBLFiwoCIjI4215R5mmekm/tqkSbH87UVEROj27dtpGl0AIPeI/5xOS4Z+WjRs2FBTpkyR2WxWWFiYzGazHnvssRSTQjKaNJLe8zM7WaV8+fKqX7++Tpw4oXnz5mnr1q1666231KlTJzk6OhrrQMc3btw4q9eBgYHatGmT1fIFD1e84zdsJlYOxF9T9OFywPLdZMCAASlmjD9cqUxKZpWPFosXL9b169e1cOFCY1t0dLQ++ugjRUVF6dNPP1WFChXk6empXbt2afXq1WrRokWSU1bG/44VHByc4fgA5Czz5s1ThQoVtHPnTiNh7PLly6pbt26alzT7+++/dfDgQSOZ+2E9e/Y0nqMrV65MtHEqJa+99pqWLFmiO3fu6Ndff9WpU6eMUXSXL1/Wjz/+qIEDB2Z6o01ml7WOjo5q3769tmzZovXr12vfvn3q3bu3unTpIhcXl0RnDHvnnXf0zjvvGK9v3bqlbdu2afv27UbiY2KNzemt+1g+q5deekn9+/dP9v2ltg6a2WXejh07dPjwYW3bts3qd/zJJ58oNDRUM2fOlJeXl7y8vHT48GHt2rVLGzdu1CuvvJLkNQsUKKCwsDDKPCALFSlSxBihmtb/a/GTDh6eYTAxvXr1UufOnRUbG6u7d+/KwcEhVc+fnJKYX7JkSbVv316bNm1SXFycvvzyS3322WfG/qVLl6p8+fJq1apVqq+Z3vbW1LSxpvS7tbQDP5wYtGfPHuNnS8LBw0wmk+rXr6///vtP0dHROn36tDGTRlragl1dXfXGG2/oiy++0NatW/X2229bdY6tWbNG9evXT1WboEVYWJjRuZXe8i0rPuvElC5d2vh/5ObmpgoVKqhu3brq27evvv76a3366af6448/9M4772jo0KF6++23k70e7aLAA6tXr1bp0qWtBgZbVKpUSc2aNdPBgwcVEBCgw4cPJ5gJ1sKyDFvJkiUT3V+wYMEMJTem9L3dw8NDzz77rHbv3q1vvvlGb7/9tlX71po1a6yS9i1q1qxpNUg3NjZWhw8f1qZNm4y+meQSY9Ibc0bLj9TUYx577DHduXMn3QkuFk2aNNG2bdu0efNmzZs3z/gOFBkZqYkTJ+r27dt69913E5yXUowplXsPzxb2sPh9tIkl7ma0vzG99di0lO05tU80r2MGEaRZvnz5VKRIkRSzFeNPYeXm5pbsv9RMSZlelqy+xLLGkmKJPTw8PMXY3dzcEm18zZcvn2bMmGFkKP73339WnVZZqVKlSsao45UrV1q99zVr1qhp06ZpXiYmqczMtPrvv/+MqYGTa7QuU6aMUUm5du2a1T5LJ9iFCxcSTA126dIl3blzRw4ODqkaeRi/spyaNQQB5E7xs61v3ryZpfcymUx67LHHVKhQoVQ1TMb/kmwZlZsali/96Z2CP/5901LJs6xpnJg5c+aoadOmkh509H3yySd65pln9NVXXyVbBv7+++8aNGiQBg4cqOLFiyc71b2Tk5OeeuopSdKRI0cSTHFpme64cOHCCZYxs5TvcXFxKZbtqW0Yy6zyUXowXeTSpUs1a9Ysq+9ZP//8s65cuWI0AFpYlsRJbhk9yjkgb3N1dZWbm5u6d+9uLN3x999/a8SIEWkqUyRpxYoVMpvNeu+99xIdkTx+/Hjj2D179ujKlStpjtfFxUWvv/668fqLL74wfl6+fLny58+vN954I83XTUlWlLVjx441fuc3btzQ1KlT1bx5c82fP18RERFJXvPvv//W6NGj9cYbbxgdSMl1kqa37mMp86Kjo1Ms8xIbqZaYzCzzLl26pPHjx2vatGlWAwfOnz8vX19fmUwmq+m9LVMUp7R0rKXco8wDsk78acD//PPPNJ0b/3mcls4Ae3t7FS5cONWd91lR10mvvn37Gu1f3333nVF/+eOPP3Ts2DH16dMnTb+LnNLeGl/8JMbk2vri18sfbutLi169eqlgwYKKiYmx+i5x9+5dbd++Pc2zh1iWDJUyVr5l9medFiaTSd26dbNaNmLOnDkpzqhFfRF40Ae0bt06BQUFJTqbvLe3t5H4IT3ob0mKJWkgNTNsZRVLAsitW7eslib+888/FRgYmGCG3/giIiK0fPlytW3bVvv27VO/fv0SzP6QmbK7/Mgoe3t7vfLKK9q9e7cGDx5s1Xa3YMECnTt3zmaxZbW01mOzW1b0ieZ1JIggy1gKQT8/P5vGYUnQSEulyNKJ9e+//1pVEtIqKCjIKNh8fX01d+7cdF8rrQYOHCg7OzsjY056UInctWtXuhpd41dgMlJhiD/9cvwRaImxfGYPz9AyePBgFS5cWGazWaNGjTKmXb569apGjx4tOzs7jR8/Ptk1+Szid95mdCorADlX/HUPfX190531nhXiZ4KnJZnR8jxNb6NfRu6b1D2LFi2qpUuXauHChcYalzdu3NCkSZPUrVu3BLO33Lt3T2PGjNGbb76pZ555Rlu2bFHXrl1TfE8ffPCB8ufPr6ioKA0bNsxYB93f31+TJk1S/vz5NX369ATTA1vK98z8bhK/fEzN0mZJuX37toYPH67Ro0dbLZMjyZhu8+ERKKVKlZL0ILEkKZRzwKNj/PjxxmjZAwcOaM6cOak+1zIKaPLkydq6dWui/7777jsNGjRI0oMRZclNo5ucN99802hI8/HxkZ+fn27duqXNmzfr1VdfNZYsy0zxy7y01O+SK2udnJz0+eefa+3atcYI7rt372revHnq2LGjUTZZxMTEaNasWerQoYMqVaqk7777Tn369EkxOSO9dR9LmZeZjZTxO1wzUuZFR0dr6NCheuONN4xkRwtLmVekSBGrcstS5v3555/Jjv6zlHuUeUDWsSSES9I///yTI5emyIq6TnpVqlRJLVu2lPSgLFi+fLmkBzNKuLm5qVOnTmm6Xk5pb40vftmaXFtf/Pa9jMxUUbhwYaNtc+vWrUaZu2nTJhUqVCjRGQCSk1ltnpn9WadH/OUh4uLi9OOPPyZ7PPVF4MGzw87OTjt37kyyLrR7926jrebIkSNJfse2JKP/+++/2Rb/w+rVq2csNbpq1Spj+5o1a/Taa68lmQh55MgRtW7dWj///LNWrFihiRMnJliKJrNld/mRFitXrjTqJg8rUKCA3nnnHW3YsMGY1SkuLk4bN27MltiyU3rrsbaQ2X2ieR0JIsgyRYoUkfRgdFlKHXHJdWxklKXiFH/EbUosscfGxmrfvn3JHvvHH38k2kB17do1DRs2TDNnzjRGcy1dulRHjx5NdRwZUblyZWPta0vG3Pr161WmTBk1adIkzdeLX/BmpLJkadiTHoyCS42KFStavS5XrpzWrFmjJ554Qg4ODnrppZdUp04dPf/888qfP79WrVqll156KVXXtnwJsYz4B5A3tWrVyqhMXL9+Xb/99puNI/o/8adQTMsUzYGBgTKZTFbP1fTeNygoKE33LV26dLLHPPvss9qyZYvmz59vZGf7+vrqk08+MY6JiopSnz59tGHDBs2aNUudO3dOdQy1a9fW8uXLVaZMGUVGRur5559XnTp19PLLL6t8+fL69ttvjekm47N0Ovr4+CQ7uluSfvnll1TFEr8jM6VrJmf06NF66qmn9PLLLyfYZ6kkP9xgZ+lgTa4SHb+yzXTBQN5WoEABzZs3z/hOu3jxYqslu5KzZs0aubm5qUOHDsmORO7Ro4fR6bZhw4Y0dbpZFC1a1OpZt3jxYq1atUqxsbGJrnmdGTJS5hUuXDjZaX3r1aun1atX66uvvjKSI//55x8NGTLEqAebzWYNHz5cX3zxhUaNGqW33nor1cupprfuY6nT/vHHH7p69Wqy90htmRe/HMlImTd16lQ5OTklOgVzSmWe2WxOVblHmQdknRYtWli1sX399de2CyYJWVnXSY9+/foZP2/YsEG//vqr9uzZo549e6Z5xoqc0t4aX/w66fnz55M8Ln68D7f1pVWvXr3k7OxszCJiNpv19ddfq2vXrmlesrxQoULGAL+MzqKRmZ91ejw8M5hl+v+k0C6KR11sbKxWrVql119/XaVKlUq2LhR/aZb4iRfxWcqfEydOJJvUbDabrWYlyWyWJS/Pnz+vo0eP6tatW9q7d6+6du2a6PF79+5Vnz59VLJkSS1evDjdbY1pZYvyIy0sy7olxdPTU5MnTzZeW5bgyysyUo+1hczuE83rSBBBlqlVq5akBw/FrVu3JnncoUOHEkyVm1bJTRNsydZ8eGRScmrUqGFUDBYtWpRsYT579uwED8WYmBgNHjxY3bp1U5MmTfThhx+qUqVKiouL08iRI3Xr1q1Ux5IR8TPmVq5cqfXr16tbt27pWos1fuX/zp076Y7J3d3dmBLMz88vySnBYmNjjZHmlod6fGFhYapevbq2bdum3377TQcOHNBvv/2mFStWpLgGa3yWilDZsmWzbJpHALbn7Oys/v37G6/jT0ObWlOnTs2S9ezjr4185syZVJ/3+++/q3Llyuke4ZaR+1rK+Pj8/Py0ZMkSq20tW7bUtm3bjCni9+zZYzS4bdq0SadPn1bhwoXVvHnzNMcfGhqqF154QVu2bNFvv/2mgwcP6tdff9XChQv1xBNPJHqOJe67d+9q2bJlSV77/Pnzqe5ULV++vPHz3bt30/AO/s9XX30lf39/q+Ub4rMkNz28TI9l9HZyHZfxE0SYShHI+8qXL6/PP/9cJpMpwYwTSYmMjNTXX3+tnj17pvh9uFChQkZCX1hYmDZt2pSuOHv37m3UoXbt2qU1a9aoffv2Sa7VnVGZXebduXPHKulRkho0aKANGzYYdZc//vhDAQEBkqTDhw8b5UqHDh3SHH966j6W0X5xcXGaN29ekte+efNmqjt3M6PM27dvn7Zv364ZM2Yk+veWVJkXv06eVLlnNpuNxBXKPCDr2NnZ6cMPPzTaltatW5fjZhHJ7Od+RtWoUcNYMiwiIkL9+/eXi4tLkh11ycnO9tbUsrw36UGZlxRLe6inp6cqV66coXsWKVLEWEpm69at+vrrrxUYGKguXbqk+VqOjo5GJ2VG2jylzP2s0yM2NtYqibNMmTLJHk+7KB51u3fv1rVr11K1NFWHDh2MBKwdO3ZYLfllYXlG3759W5s3b07yWjt37rRaXiWztWnTxlj+Y+XKldqwYYNatmyZ6KwPcXFxGjdunOLi4vTiiy9mawKALcqPtNi7d2+KxzRp0sQYPJbXktQzWo+1hczsE83rSBBBsuInXqR1Dev27dsbP0+YMCHRrPWQkBBNnTrVWBsqufsn5/bt24luDw4O1vnz5+Xg4KDXXnstwf74I7ric3V1VbNmzSQ9WFvrgw8+SHTduJUrV6pixYoJ1kebOHGi3NzcjKxxJycnzZw5Uw4ODrp+/bo++OCDJLP8k4rJwvJlPf5SLdHR0Yn+rqpUqaJWrVpJerAG2t27d9M0Qjs+y7RkkhJMl5yY5N5H9+7djX3ffPNNouefP39eMTExKlu2rJ555hmrfRcuXFCfPn307LPPSvq/tWjTWpGJi4szvshZRvsByLt69epldKIcOHAgTZ1aX331lUqXLp1gbcWMlJMWlStXNipEp0+fTtVamnfu3NHmzZsz9OW8bdu2RsVw9+7dqVp2x9fXV76+vkne97vvvktwHWdnZw0bNkzSg9+RpbHKMjLg3r17VmWaZL1Wa2xsbIL7HD16VO+9957atm0rSXJwcJCrq2uy65VKUuvWrY1RW4sWLdLu3bsTHBMZGalPPvkk1b9bLy8v476pKR8fdvbsWc2ePVszZ85MMtnHy8tLUsLRX5a/lYeXpIkvflKT5ToAcrf4z8XEnt3PPvus3nrrLUkP6gwDBgxItPHSYt26dYqKikp1PSF+B8fKlSuTXF87ufpAmTJljGd4XFycwsPD1bdv30SvE798Te8ScY0bNzYaEg8fPpyq0cGXL1/W3r17kywPdu3aleA6+fLl0+jRo43XlvIt/mi4hz8Ls9lsJEMkVualt+7TpEkTFStWTJK0efNmffXVVwmOsTQGt27dOtlrWZQtW9b47pCeMi8wMFAffvihJk+enGQykKWsCgkJsUoSsZRnJUuWTHKE87Vr14y/F+p3QNZq1qyZBg4cKOnBd/ehQ4ema1apxGRGHSsr6jqpERcXl+S94s8scefOHb3++uuJfv9PqdxLb3trZi+zGv96zz33nPFc37FjR5JJFn/++aek/2sXTExaPvPevXsbs4hMnDhRbdq0SfeU95Z2zxs3bqTqe0J2fNap2fewAwcOGHXufPnyGe3CiaFdFI86s9msxYsXq23btsb35uTkz59fHTt2lPSgPyax79fxn9EzZszQxYsXExzz77//aubMmQm+g6em3yelviOLfPnyGUtqHDp0SCtXrkzy2RsSEmK0NyVWb0yujS5+Mkn8uOM/R+PH+nDcGS0/0lJmpKccPHXqlHx8fJI9Jjo62hjAZelTTOq+icWQmeVzStdP6/6M1GPjS+lzyql9onkdCSJI1s2bN42fk2tUTIyXl5fxHy88PFw9evTQ5MmTderUKf3xxx9au3atOnXqpBdeeMFYp0uyLjxSOw3kvn37El1Leu7cuYqNjdXgwYMTdOpJ/5cVntjopxEjRsjJyUmS9MMPP+jll1/W1q1b5efnJx8fH40YMUJffvmlUSG2WLt2rQ4dOmQ1tZQkVatWzWj0PHz4sBYvXpzoe7HElFTF2pKFePr0aYWFhSk8PFyjR49O8iE8cOBAmUwmxcXFqWPHjukeaf70008b69OlZg09S9JOYu+jS5cueuqppyRJy5cvT/SL0urVq2UymTRx4sQEUzAuXLhQ9+7d06ZNm3Tq1Cn5+/vr77//VkBAgC5fvqzg4OBkZ32xuHLlilF4p2WGGQC5k729vb744gvj+TNmzBgtXrw42S/icXFxmj9/vm7evJnoWoXxZ4RKazkZ35gxY1SwYEFFR0drzJgxyT7Drl27pn79+qlkyZLGlJHpUahQIY0aNUomk0l//fWXFi1alOzxv/32m95991298cYbql27dqLHXLhwIdFRypYyysvLy5gS2TJCKyYmRuPHj1dUVJSxtFv8zsfg4GDdu3dPc+fONbbNmjVL9+/f15o1a+Tr65ugHLh+/XqCkceSVKJECWMmmfv372vw4MEaPXq0jhw5ojNnzmjLli3q3LmzKlWqlOpkChcXF2OUdlrXmA0LC9PQoUP13nvvWSViPqx58+YqXry4rly5YvXd6NixY5KkF198Mclz44/mbNiwYZriA5AzxS9vkpqZcMiQIcb/+StXrqhnz56J1q1u3bqlL774Qg0bNlTBggVTdf+qVasa0+9fvXrVWNs3qTjj1ynje+utt4xRPC1btjRmGXxYRuqkFvny5dPYsWPl4OCgGzduaNKkSck2UgUEBKhfv35q3LhxkoMZQkNDNXbs2ATbLeWPm5ubMYI9/tTJn376qe7cuSOz2azjx4/rjTfeMD7HoKAgxcTEaPr06cbx6a375M+fXyNHjjReT5o0Se+8844OHjwoPz8//fjjj3rttdd09+5dtWzZMrlfnxVLvSmtZV5sbKyGDx+uDh06qEWLFkkeV7NmTT3++OO6f/++1ZKAaS3zqN8BWW/w4MEaMWKEJOncuXPq3r17ilOrJ/Yd/WGZ8dzPirpOaty4cSPJmBs1amTUGwoUKKAePXokelxKdcz0trfG73RLrA1V+r8ZCh9O4LeIX97Ev4ajo6MmTJggOzs7hYWFJWgTlaRLly7Jx8dH9evX1yuvvGK1Lz1twdKDZessAwLj4uISrbOnlmXQhtlsTlUSZHZ81hZJfR4P++effzRu3Djjde/eva1m/3oY7aJ41G3evFl+fn7Jfjd9WPwEgNWrVydYyvHxxx/Xq6++KunBc7dLly5aunSpfv/9d504cUJz585Vhw4d1L179wRJz6np90mp7yi+Ll26yMnJSWazWRUrVkwyEaxIkSLGco5r1641llUJCgrSZ599prVr1xrHBgUF6cCBAzp+/LikB+WtpU73008/yWw269KlS5o2bVqCmBOLO6PlR/ylJ1NaAjO9iawjRozQqVOnkty/cuVKRURE6Kmnnkq0rpLc+4+/P6WyWUq8PEiqbLaI3/eZ2P74g+8f7ifNSD02LWV7Tu0TzetIEEECcXFxun37trZs2WI86CVpxYoVOnfuXKo63i3Gjh1rzP4QExOjlStXqlu3bnr55Zc1YcIE1atXTwMGDDCOv3Xrlr788kvj9e7du3Xo0KEUvwiHhYWpV69eOnHihMLCwvTff/9p4sSJ2rhxo7p27WqMoLOIjIzUunXrjAfOsWPHdPz4cauHbZUqVTRv3jxj+tpz587pgw8+UMeOHdW3b18dPnxYCxYsMDLTIyMjtWrVKk2cOFGFCxdOdDS0m5ub8Xr27Nlavny58QCOjo6Wr6+v8Tu/c+eOvvnmmwTXsXxhv3btmpo3b66mTZvq2WefNZI3Hubp6annn39ekjJUUXJxcTG+ACW3HlxMTEyC97F69Wqrh7u9vb0WLVqkmjVrKjIyUj169NCPP/6oO3fuKDAwUJMnT9YPP/yg2bNnJ1pBsTzQ9+/fr27duqlt27Zq06aNWrdureeee05NmzbVU089pV69euncuXNJxmpJTEkpox5A3lGwYEGtXLlSAwYMUL58+TRz5kx16tRJX3/9tfz9/RUeHq67d+/qwoUL+uqrr/Tmm2+qbNmyGjp0qNV1oqOjdebMGatkvxMnTmjHjh26e/dumke6Va5cWV9//bVKlSqlQ4cOqWPHjtq0aZOuXLmiiIgIBQUF6fjx4xo3bpzatGmjwoULa/ny5Rlew7hDhw6aPn268ufPrzlz5qhv3746ePCgrl+/roiICP3777/avXu33n33XXXr1k0vvfSSPvroo2SvuWDBAg0fPly///67wsLC5Ovrq/Hjx6tw4cKaNGmScVz79u2NzsitW7eqXr16qlevnmbMmKHPPvvMKF83b96s5557Tv/73/+Mcy3lwMaNG9WlS5cE5UCTJk1Ur149vfvuuwka9wYOHGhMO2w2m7V582b16tVLnTt31qhRo+Tm5pZg2YCUWDoPL168mGLWfHzjxo1TxYoVrdaxTYyTk5M+/fRTOTg4aOLEibpx44ZWr14tX19f1a9f32h8SIylrKtWrVq2TsMJIHNZ6mgbN27UyZMnje1LlixRQEBAglk87O3tNXPmTCNJ/sKFC2rfvr1mzZql33//XRERETp79qzeffddhYSE6MyZM/r1119TrOuFh4cba1hbzJo1S2vXrjU6NmJiYhQQEGAk9q1atUrnz59PcO2qVasadcWH62txcXG6deuWVq5caSzTIj1Ilrh06VKSs5Ykp0GDBlq6dKlcXV317bff6rXXXtMPP/yg4OBgRURE6MqVKzp8+LDef/99dejQQU899ZTmzJmT7FS0W7duVZ8+fXTq1CnduXNHf/31l0aOHKkCBQro888/N+pozZs3N0bG/fTTT2rUqJHq1aunESNG6L333jOWRjtx4oQaNmxodCpJGav7dOrUSe+9957xeu/everfv786duyoIUOGKCIiQjNnzkzT79FS5t26dSvBzFbJmTt3riIjI43O5ORMnDhRzs7OmjZtmoKCgrRz507t2rVLlStXtmo/eJglQaRo0aJW01UDyDpvvfWWvvnmG9WtW1dnz57Viy++qI8++kg///yzQkJCFBUVpaCgIO3Zs0dDhw7V0qVLlS9fPnXo0MGqvcdsNuvevXvav3+/du7caWzfsGGDfvnlF0VFRaV5hG1W1HUSE7/M+ueff3Ty5Ent3LlTYWFhCWK2zCzx0ksvJRitHhkZqV9++UXr1q0ztu3evVs+Pj66d++e1bXS2t4aFhZmtcTm7t27dfHiRaM8jYiI0KFDh3T27FlJD743xG+PvX//vs6fP281A+PSpUutOr2aNWtm1Fk2b96s999/X3///bfCw8Pl4+OjPn366Omnn9aCBQusytb0tgVb9OnTR05OTqpZs6ZV+ZlWzz//vFFuJ9XuaYvP+t69e1ZLBR09elRnzpzRrVu3FB0drdu3b+u3334z2jauX7+ufPnyqW/fvho+fHiy75l2UTyqwsPD9cMPP2jixImSHiz38nCix8PMZrNu3bqlgwcPWl3n7bff1okTJ6yeWWPGjDGSTu7evavp06frlVdeUffu3bVgwQJ17do10cSx5Pp9oqOjdeDAAfn7+0t68Jzeu3evVX/Ww1xdXY0ZT5KbucnOzs5oV7p79666du2q+vXr69lnn5WTk5NVfaJt27basmWLMUOzs7OzsazOgQMH1KRJE73++uvG/cLCwrRixQrj/F27dunChQtWCaPpLT/u3r2rVatWGa937typixcvGvXOyMhI7d271/hsjx8/nqo678NCQ0PVo0cPjR49WidOnNDNmzeNZ+/IkSM1c+ZM1a9fX4sWLbKa2fj+/fu6ePGi9uzZY2z78ssvjX6yqKgo7d271/hMz58/r0OHDhmJFdHR0bpw4YJV2bt69WqFhobKbDYrLi5OwcHB+vbbb43933zzjYKDgxUXF6fY2FgFBgZqy5Ytxv5Vq1YpNDTUeP3w57Nz506rPuD01GOjo6N19uxZq7hnzJihq1evJmgvz+l9onmdyZzZ88sh15s1a5a++OKLZI/54YcfUt3JYDabtW3bNn377bc6d+6c4uLiVLVqVXXt2lWdO3c2HuqXLl0y/tM+rEyZMtq/f3+C7ZYRWe+8844ee+wxffvtt7p8+bKcnZ315JNPqlu3bolmgDZt2tRqynWLqlWraseOHVbbrly5osWLF+vw4cO6ceOGChcurKZNm+qdd96xWsdx0aJFmj17tvHa3t7eqFhJSf9eH3vsMZ06dUoDBw7Uvn37ktxvER4ernHjxmnPnj0qVaqUBg0aZKxznZQpU6bo3LlzWrlyZbLHpcTX11ddunTRY489phMnTiQ6lf+IESO0ffv2RM8/evSo1VSPsbGxWr9+vb777jtdvHhR9+/fV5kyZfS///1P3bp1U9myZRO9TkxMjAYNGqSAgADFxcXpzp07Cg8P1/379xMUMoUKFdL27dsTncJ48uTJWrlypV588UXNmDEjLb8KAHmApcPh4MGDunz5sm7evKn79++raNGievzxx9W8eXN16NAhQZZxcuWVxVtvvZWqDpCHhYeHa8uWLTp8+LAuXLigmzdvKjo6WgUKFFDp0qVVs2ZNdejQIdNngggKCtKGDRt0/Phx/fPPP7p9+7ZiY2Pl4uKicuXK6emnn9arr76qihUrJnkNPz8/o+JpYWdnJzc3Nz3zzDMaOHBggtm8fv/9d02ePFl+fn4qXLiw2rdvr4EDB6pAgQL68ssvtWDBAj3++OP6+OOPrWb0CAsLU8+ePRUZGamIiAjduXNHkZGRiomJSdAwV6ZMGW3fvj3ByPgDBw5o9erV+uOPPxQVFaUKFSqoc+fO6tatW5IVjKSEhYXpueeeU0hIiDZt2pTsbCAWGzdu1Lx587Rt2zZjrdKUnDp1SnPmzNGff/4pFxcXtWvXToMHDzZGeiSmU6dOOnv2rCZMmJCu9bgB5AzTp0/X0qVLkz3m4e/a0oMRNt27d091QkWBAgWsZm14WIsWLXTlypUk9x86dEgtWrRIMlnup59+skqa9/X11Zw5c6wapSTpk08+0fr165O8z8N1rbQIDQ3Vxo0b9fPPP+vixYsKDQ3V/fv35ezsrDJlyqhu3brq3Llzss/yO3fu6Omnn7baZjKZVKRIETVp0kQDBw5MUGYGBARo0qRJOn36tAoUKKCWLVtqyJAhKly4sHbs2KFx48apVKlSGjVqlLy9vY3zMqPu88svv2j58uX65ZdfFBYWZizx06dPn1TPHGMRFxendu3ayd/fX3Pnzk1Vh9LRo0c1aNAgbdq0KdmRzPH99ddfmjVrlk6ePCkHBwc9//zzGjp0qDETWWLeffdd7dmzR/379zeWtwOQfX7//XcdPnxYx48fV1BQkEJCQhQRESEnJycVL15cnp6eatiwoZ5//nkVL17c6twNGzZozJgxyV5/6dKlatq0aZrjyoy6TnI++ugjbdy4MdF9w4cPt1puxGw2q3379lq0aJFVm5elvS05U6dOtVr+JrXtrbdu3Upydohnn31WCxcuVL169RKdXdnR0VF//PFHkm2WknTy5EkVKlTIeO3v768VK1bo6NGjun79ugoXLixPT0+9/PLLev75562WSEtPW/DDYmJi9Mwzz2jkyJEZWh5I+r/vH2+88YY+/vjjBPuz87MuUqSIDh8+LB8fn2Rn5rGzs1P+/PlVpEgRlS1bVrVq1dLLL7+sChUqJP9mRbsoHl1dunSRr69vgu2vvfaa1Sw88R0/fjzJ2YCkBx3Y8fteLAOS1q9fr/Pnz8ve3l61a9dWr1691KRJk0SvkVy/T4cOHRIdCPvEE09o27ZtScYVEBCg3r17a8+ePVbLwTwsOjpa8+bN03fffafbt2+rZs2aGjx4sOrVq6cbN26oZ8+eun79urp27arBgwcneJaPHj1aZ8+e1ZNPPqkPP/xQ1apVS7TOZNGjR48ESZlpKT+SK9uaNGmiuXPnGjNIJ7Y/ftJkUlauXCk3Nzc1btxYJ0+e1E8//aRff/1VgYGBioyMVKFCheTl5aUOHTqobdu2CfrKJk2alOgyRNKDsvP111/XhQsXEuxLqb9QktavXy9fX99EZ1yRHgze+++//5Lcv3XrVlWtWjXJ2ZObN29uDIpMaz02uf7ByZMnWy33khv6RPMyEkSQq1kSRN59910NGjTIxtHkTBEREWrWrJk+++wzPffccxm+Xv/+/XXw4EGtXr3ayBTNbgcOHNDy5cu1bNmyREfPx8TE6Pbt2/L19dVHH32kwYMHq1u3bgmOa9WqlS5fvqwdO3YkOaU1ACDnWbdunY4cOZLkyO7o6GiFhITo6NGj+vjjjzV//vxE1wHNTMuWLdPUqVM1aNAgvfvuu1l6r9QKDg5Ws2bNVKZMGe3cuTPNiS8AANvLrLpPZtq5c6fee+89de7cOclGx+wWHR2tBg0ayM7OTvv27Ut18iUAIHfbsWOHPvvsMx08eDDDM2wGBwerZcuWcnNzS7JTLi+hXRQAkBdldp9oXsUSM0Aet2XLFrm4uBhTT2bUsGHD5OjomGTGfFb7/fff9d5772nkyJFJVvwcHBxUvHhxPffcc3r66acT7Tz09fXVP//8ozfffJNKEADkInv37tWECRP08ccfJzntv6Ojo9zd3dWxY0dVqVIlW+Lq1q2bypUrp61bt6ZqbfXssG3bNpnNZn300UckhwBALpRZdZ/M1qpVKz311FPavXu31fICtrRnzx6Fh4dr6NChJIcAwCNk1apVevXVVzOcHCJJ7u7u6tWrl/777z8dPXo0E6LLuWgXBQDkVZndJ5pXkSAC5GG3b9/W4sWL1adPH6spuDLi8ccf14QJE7Rjx45EpzXLSmazWSNHjlRUVFSqOrouXrwoX1/fRLMEZ8+eLU9PTw0dOjQrQgUAZIGwsDB99NFHsrOzS1W5dvLkSYWEhGT6kjyJKVCggBYsWKCbN29arSVtK5Z1xrt06ZLocnsAgJwtM+s+mc1kMmnOnDlydnbWokWLsvx+KYmNjdW8efPUpEmTLJ89BQCQc+zcuVP+/v564403Mu2agwcPVqNGjTRz5swES7nlJbSLAgDyoqzoE82rSBBBrhUeHp7oz4+yTz75RLVq1VLnzp01ffp0devWTU5OTimubZlWnTp1Uvfu3TVy5EhFRkZm6rWTExYWpv/++0+S1LdvX61Zs0aBgYEJjrt586bWrl2r7t2766OPPlKJEiWs9m/ZskWXLl3SkiVLMmWEAQAgewQGBio0NFQxMTHq0qWLtmzZops3byZ63BdffKFBgwZp+vTpyp8/f7bE5+npqWnTpmnmzJm6ePFittwzKZMmTVKNGjVSXMsdAJAzZVbdJ6uUKFFC8+fP17p163TixIlsuWdSFi1aJCcnJ82ePTtbZlABAGS/4OBgtW7dWnXr1tW7776ryZMna9SoUXrrrbdUvHjxTLuPvb29Zs+erdu3b+uLL77ItOvmJLSLAgDyiuzqE82LTGaz2WzrIIC0MJvNun37tr766istWLBAklSmTBktXLhQlSpVeqS/2NarV0937941Xru7u2vNmjUqV65cpt/LbDZr/vz58vX11cKFC7Ot8+27777ThAkTrN6ni4uLChcuLHt7e925c0chISEqU6aMpk6dqnr16lmd//PPP2vatGmaO3dulvxeAABZa9myZZozZ46ioqKMba6uripUqJBMJpNCQ0N1584dPf7445o+fbo8PT2zPcYDBw5oxowZ+uKLL1S2bNlsv//8+fN17tw5TZs2TU5OTtl+fwBA5sho3Sc7+Pr6avTo0Zo+fbq8vLyy/f6bN2/Wxo0bNX/+fBUtWjTb7w8AyB579+7VO++8Y7XtxRdf1LRp02Rnl/ljYG/cuKF3331XL730kl555ZVMv76t0C4KAMhLsrNPNK8hQQS5zubNmzV69OhE91WtWlU7duzI5ohyji1bthgVoxYtWmjw4MGZmkWfmAMHDmjLli0aNWqUSpcunaX3sggNDdXmzZt16NAh+fv76/bt23JwcFDRokVVo0YNPfvss2rTpo3VVMxxcXFavHixAgMDNWrUKDk7O2dLrACAzBccHKwNGzboyJEjunjxosLCwpQ/f365ubmpVq1aatWqlVq0aGHTUcQXL17UnDlz1K1bNzVo0CBb7nn79m1NmTJFnp6e6tWrV7bcEwCQtdJT98luQUFBmjFjhp555hm98MIL2XLPqKgozZo1S46Ojho8eLDy5cuXLfcFANhGdHS0Ro4cqcOHD6ts2bJ644039Morr2RJcohFVFSU5s2bJwcHB7377ru5eqp62kUBAHmRLfpE84pcmSBy+vRpmc1mmzaAAPg/UVFRCgoKUvny5W0dSpLCw8N148YNMgdtKCYmRiaTSXXq1LF1KDkW5RuQ9/j7+6tKlSrZcq+rV6+qYMGCcnV1zZb74QHKt9ShjAPyvgsXLqhq1arZcq+bN28qLi5Obm5u2XK/RxVlXMoo34C878qVKypSpEiuTqqgXdQa5VvKKN8AIPdJS/mWK4dYmM1m5cK8FiDPyp8/f45ODpEkZ2dnKkE2xnM7ZZRvQN6TXckhkrJtJi9Y47mdOpRxQN6XXckhklSsWLFsu9ejjOd2yijfgLyvTJkytg4hw2gXtcZzO2WUbwCQ+6TluZ3pCSLnz5/XkiVLVKlSJQ0cODDB/ujoaHXt2lVvvPGGOnfunK57WLIWn3zyyQzFCgDIPn/88YetQ8jxKN8AIPehfEsdyjgAyH0o41JG+QYAuQ/lW8oo3wAg90lL+Zapi/SFh4crNDRUx44dU2xsbKLHzJ07V5cuXcrM2wIAAAAAAAAAAAAAACAZmTqDiLOzs+rXr5/kUhPff/+9nnzySf3www8ZvpfZbFZ4eHiGrwMAyB5ms1kmk8nWYQAAAAAAAAAAAACPpExfYkaS7OwSTkzy999/y9/fX++9954+//zzDN8jJiZGfn5+Gb4OACD7ODo62joEAAAAAAAAAAAA4JGUJQkiD4uMjNSKFSv0ySefZNo1HRwcVKVKlUy7HgAga/n7+9s6BAAAAAAAAAAAAOCRlS0JInPnztXAgQPl4OCQadc0mUxydnbOtOsBALIWy8sAAAAAAAAAAAAAtpNwLZhMFhgYqK+++krt2rVTvXr1VK9ePV29elXjx49X//79s/r2AABkmvPnz2vEiBFauHBhovujo6PVuXNnbd68OZsjAwAAAAAAAAAAAJKX5TOIuLm5aefOnVbbXn/9db355ptq3759Vt8eAIBMER4ertDQUB07dkzly5dP9Ji5c+fq0qVL2RwZAAAAAAAAAAAAkLIsSRAxm80ym80PbpAvn8qWLWt903z5VKRIEbm5uWXF7QEAyHTOzs6qX79+kskh33//vZ588kn98MMP2RwZAAAAAAAAAAAAkLJMTRCJjY3V3r17dfHiRcXGxqpx48aqV69eZt4CAACbsrNLuDrb33//LX9/f7333nv6/PPPM3R9s9ms8PDwDF0DAJB9zGazTCaTrcPIsCtXrmjHjh0qU6aMypUrp5o1a+rSpUtavHix3NzcFBISohEjRqhQoUK2DhUAAAAAAABAOmVqgoi9vb1atWqlVq1aJXvc/v37M/O2AADYTGRkpFasWKFPPvkkU64XExMjPz+/TLkWACB7ODo62jqEDDl69Ki+/vprTZkyRQULFpT0YGm1Pn36aNmyZSpfvrz27t2r4cOHa+nSpTaOFgAAAAAAAEB6ZckSMwAAPCrmzp2rgQMHysHBIVOu5+DgoCpVqmTKtQAAWc/f39/WIWTI+fPnNWHCBG3YsMFIDpGkjRs3qmjRosbSas2aNdOwYcPk6+ur2rVr2yhaAAAAAAAAABlBgggAAOkUGBior776St9++62xLSwsTOPHj9euXbu0ePHiNF/TZDLJ2dk5M8MEAGSh3L68zMSJE1WtWjUtXbpUv/76q/73v/+pb9++8vHxkYeHh3Gcg4ODPDw8dOTIkXQniLCMGgDkLnllGTUAAAAAwP8hQQQAgHRyc3PTzp07rba9/vrrevPNN9W+fXsbRQUAQOpcvnxZx48f17x58/T888/r3Llz6tKli+7fv6/AwEBj9hCLggULKigoKN33Yxk1AMh9cvsyagAAAAAAaySIAACQBmazWWazWZKUL18+lS1b1mp/vnz5VKRIEbm5udkiPAAAUu3cuXOSJG9vb0nSE088oeeee04bN25UwYIFlT9/fqvjY2JiMrSkGsuoAUDuktuXUQMAAAAAJESCCAAAqRAbG6u9e/fq4sWLio2NVePGjVWvXj1bhwUAQLrdv39f0oMyzqJ69erav3+/KleurNu3b1sdHxYWJnd393Tfj2XUACB3YXkZAABs7/z581qyZIkqVaqkgQMHJnnc77//rvXr16tIkSKKjo7WiBEjmAkMAJAoEkQAAEgFe3t7tWrVSq1atUr2uP3792dTRAAAZEz16tUlSQEBAapVq5akB+Wdp6enmjVrph07dhjHRkdH6+rVq8ZsIwAAAACArBUeHq7Q0FAdO3YswRKg8QUHB2vIkCHasmWLXF1dtWrVKk2aNEnjx4/PxmgBALkFCSIAAAAA8AgqX7682rRpo+3btxsJIr/88ov69u2rRo0aaenSpbp27ZpKlCihQ4cOydvbW15eXjaOGgAAAAAeDc7Ozqpfv36yySGStHz5ctWqVUuurq6SpJYtW+q5557TwIED0z0LpNlsVnh4eLrOtchJs5FZlgxPTE6KU8o9sSYXp0Ss6ZVXYs1JcUq5J9aUPv+Uzk3teyFBBI+suDiz7Oxs/58+p8QBAMgbclK5kpNiAZC4zz77TJ9++qkWLFggs9mshg0bqmXLlpKkhQsXatasWfLw8NDNmzc1c+ZMG0eLR11OKVdyShwAAADZKc5sll0O6ETLKXFkJzs7u2T3+/j4GPU4SSpdurQcHR117NgxdejQIV33jImJkZ+fX7rOlSQHBwdVq+4lh3z26b5GZom5Hyu/s2cUExOTYJ+Dg4NqVK8mu3wONogsobj7MfrzrF+SsXp5ecne3va/09jYWJ05k/jvVPr/sVb3kn0O+Pxj78fqTBKfv5T7fq/Vq1dXvny2796/f/++zp49m+P/VqXkf68PYq0he/vkn7PZITY2TmfO/Jnk558aqV1azPZ/QYCN2NmZtGf5Gd0KvGezGIqWKqiWvRmFCQDIPDmhfJMo44DcwtnZWZMnT050n5eXV5L7AFvICWUc5RsAAHhU2ZlMOhQUotvR920Wg6tjPjUrWcRm98+pAgMDVbhwYattBQsWVHBwcLqv6eDgoCpVqqT7fJPJJId89npv3Wn5XwtL93UyqkoJF83pWkdVq1ZNdGS+yWR6kByyqa9047wNIoynuKfsXvoy2Vjt7e217/oOhcbctEGADxR2KKZn3V5MMk7p/8eazz7H1N9SjNXeXucP7lF46K1sjvD/OBcuKs/mLVOMNV++fNq0aZNu3LiRzRH+n+LFi+ull15K8W81KOgHxcTY7ncqSQ4ORVWy5AspxGqXY8q35D7/lPj7+6f6WBJE8Ei7FXhPNy7b7ssJAABZgfINAJBXUcYBAADYzu3o+7oZlf6Rzcg6+fPnt3odExOToVkGTCaTnJ2dMxqW/K+F6czVOxm+TkY5OTklf8CN81Lgb9kTTApSijU05qZuRF/LpmiSluLvVDmn/paaWMNDb+neTdslXVikJtYbN24oMDAwG6JJXkqxxsTcUlSU7f9WpZRjzSnlW2o+/6SkZakc28+XAgAAAAAAAAAAACDNSpUqpdu3bxuvzWazwsPD5e7ubsOoAAA5FQkiAAAAAAAAAAAAQC7UrFkzq6UFLl++rLi4ODVs2NCGUQEAcioSRAAAAAAAAAAAAIAcyGw2y2w2G68jIiI0ffp03bx5U5L0+uuv69dff1VkZKQkaf/+/eratauKFStmk3gBADlb+hcgAwAAAAAAAIB08Pf315gxY3Tu3Dl5enpq8uTJqly5siTp0qVLWrx4sdzc3BQSEqIRI0aoUKFCxrlbtmzRmTNnJEkVKlTQG2+8YeyLiIjQ1KlT5erqqqCgIPXu3Vuenp7G/t9//13r169XkSJFFB0drREjRsjR0TGb3jUAAKkXGxurvXv36uLFi4qNjVXjxo1Vr149hYaGatu2bWrZsqWKFSsmDw8PTZw4UZMmTZK7u7siIiI0evRoW4cPAMihSBABAAAAAAAAkG2ioqK0fft2zZ07V/fu3dN7772nsWPHas2aNQoPD1efPn20bNkylS9fXnv37tXw4cO1dOlSSQ9GRW/evFmrV6+WJL3zzjsqWrSoXnjhBUnSBx98oObNm6tz5866ceOGunbtqs2bN6tQoUIKDg7WkCFDtGXLFrm6umrVqlWaNGmSxo8fb7PfBQAASbG3t1erVq3UqlUrq+2lSpWSj4+P1TZvb295e3tnZ3gAgFyKJWYAAAAAAAAAZBuTyaRBgwapRIkSqlixorp06SI7uwfNlBs3blTRokVVvnx5SVKzZs10/Phx+fr6SpLmzZtnJINIUsuWLTV37lxJ0rlz57Rv3z6jI6148eIqXbq01q1bJ0lavny5atWqJVdXV+PcDRs2KDg4OFveNwAAAADYGjOIAAAAAAAAAMg2Dy/pcuXKFWMqfB8fH3l4eBj7HBwc5OHhoSNHjqhs2bI6e/asypUrZ+yvVKmSAgICFBQUJB8fHxUtWlQFCxY09leuXFlHjhxRv3795OPjo5YtWxr7SpcuLUdHRx07dkwdOnRI13sxm80KDw9P17kAkFuYTCY5OTnZOgxDRESEzGZzus41m80ymUyZHBEAALkHCSIAAAAAAAAAsp2fn5/Wrl2rEydOqHnz5pKkwMBAY/YQi4IFCyooKEiBgYGSpMKFC1vtk2Tsj78v/rmWaye2PyMziMTExMjPzy/d5wNAbuDk5KTq1avbOgxDQECAIiIi0n3+w4mKAAA8SkgQAQAAAAAAAJDtypQpo9atW+v8+fN66623tHv3bplMJuXPn9/quJiYGDk4OBgjvgsUKGC1T5Ly5csnk8lktS/+uRaJXTtfvvQ3kTo4OKhKlSrpPh8AcoOcNuNGxYoV0z2DiL+/fyZHAwBA7kKCCAAAAAAAAIBsV6hQITVp0kS1atVS8+bNdfr0aZUsWVK3b9+2Oi4sLEzu7u4qVaqUJCk0NNRqnyS5u7urZMmS8vHxSXBuiRIlJEmlSpWyurZleRh3d/d0vweTySRnZ+d0nw8ASLuMLHeT05JdAADIbna2DgAAAAAAAADAo+uxxx5TxYoVVbp0aTVr1sxqdHd0dLSuXr0qb29vFStWTF5eXlb7AwIC5OnpKTc3NzVv3lxXr17VvXv3rPY3adJEkhJc+/Lly4qLi1PDhg2z4V0CAAAAgO2RIAIAAAAAAAAg20REROj33383Xv/9999yc3NTzZo11bFjRwUGBuratWuSpEOHDsnb21teXl6SpL59+2rfvn3Gufv379fAgQMlSVWrVlWjRo10+PBhSdK1a9cUHBysV155RZL0+uuv69dff1VkZKRxbteuXVWsWLGsf9MAAAAAkAOwxAwAAAAAAACAbHP+/HkNHDhQxYoVU4MGDeTq6qoZM2ZIklxcXLRw4ULNmjVLHh4eunnzpmbOnGmc+8ILLygoKEiff/65JKlFixZq06aNsX/q1KmaNm2a/vnnHwUGBmrJkiVycXGRJHl4eGjixImaNGmS3N3dFRERodGjR2fjOwcAAAAA2yJBBAAAAAAAAEC2qVWrln7++eck93t5eWny5MlJ7u/du3eS+4oUKaLPPvssyf3e3t7y9vZOXaAAAAAAkMewxAwAAAAAAAAAAAAAAEAeR4IIAAAAAAAAAAAAAABAHkeCCAAAAAAAAAAAAAAAQB5HgggAAAAAAAAAAAAAAEAeR4IIAAAAAAAAAAAAAABAHkeCCAAAAAAAAAAAAAAAQB5HgggAAAAAAAAAAAAAAEAeR4IIAAAAAAAAAAAAAABAHkeCCAAAAAAAAAAAAAAAQB5HgggAAAAAAAAAAAAAAEAeR4IIAAAAAAAAAAAAAABAHkeCCAAAAAAAAAAAAAAAQB5HgggAAAAAAAAAAAAAAEAeR4IIAAAAAAAAAAAAAABAHkeCCAAAAAAAAAAAAAAAQB5HgggAAAAAAAAAAAAAAEAeR4IIAAAAAAAAAAAAAABAHkeCCAAAAAAAAAAAAAAAQB5HgggAAAAAAAAAAAAAAEAeR4IIAAAAAAAAAAAAAABAHkeCCAAAAAAAAAAAAAAAQB6X6Qki58+f14gRI7Rw4UKr7atXr1bz5s319NNPa/jw4QoJCcnsWwMAAAAAAAAAAAAAACARmZogEh4ertDQUB07dkyxsbHG9sOHD+uff/7RokWLNG7cOP3000/6+OOPM/PWAAAAAAAAAAAAAAAASEK+zLyYs7Oz6tevr/LlyyfYZ0kIqVatmkJCQjRjxozMvDUAAAAAAAAAAAAAAACSkKkJIhZ2dtYTkzRt2tTqtYeHh8qWLZuhe5jNZoWHh2foGnh0mUwmOTk52ToMQ0REhMxms63DALKU2WyWyWSydRgAAAAAAAAAAADAIylLEkRScurUKfXp0ydD14iJiZGfn18mRYRHjZOTk6pXr27rMAwBAQGKiIiwdRhAlnN0dLR1CAAAAAAAAAAAAMAjKdsTRIKDgxUYGKhhw4Zl6DoODg6qUqVKJkWFR01Om8WgYsWKzCCCPM/f39/WIQAAAAAAAAAAAACPrGxNEImOjtaSJUs0bty4DHfQm0wmOTs7Z1JkgG3lpOVugKyS0xKz0uP8+fNasmSJKlWqpIEDBxrbV69erWXLlunevXtq2rSpxowZoyJFitgwUgAAAAAAAAAAAMCaXXbdKDY2VgsXLtSAAQPk4uKSXbcFACBThIeHKzQ0VMeOHVNsbKyx/fDhw/rnn3+0aNEijRs3Tj/99JM+/vhjG0YKAAAAAAAAAAAAJJQlM4iYzWar5TJiYmI0ZcoUtWnTRpGRkbp8+bIuX76skJAQtW3bNitCAAAgUzk7O6t+/foqX758gn2WhJBq1aopJCREM2bMyO7wAAAAAOQQcWaz7HLADIo5JQ4AAAAAQM6RqQkisbGx2rt3ry5evKjY2Fg1btxY9erV04ABA+Tj46M1a9YYx9rZ2engwYOZeXsAALKcnZ315FtNmza1eu3h4aGyZcum+/pms1nh4eHpPh+PNpPJlOOWLYuIiLBKHAbyGrPZnCeWUQMAZB47k0mHgkJ0O/q+zWJwdcynZiVZ9hIAAAAAYC1TE0Ts7e3VqlUrtWrVymr7l19+mZm3AQAgxzp16pT69OmT7vNjYmLk5+eXiRHhUeLk5KTq1avbOgwrAQEBioiIsHUYQJZydHS0dQgAgBzmdvR93YyKsXUYAAAAAABYyZIlZgAAeBQFBwcrMDBQw4YNS/c1HBwcVKVKlUyMCo+SnDiLQcWKFZlBBHmav7+/rUMAAAAAAAAAgFQhQQQAgEwQHR2tJUuWaNy4cRnqpDeZTHJ2ds7EyADbymlL3gCZLScmZgEAAAAAAABAYuxsHQAAALldbGysFi5cqAEDBsjFxcXW4QAAAAAAAAAAAAAJMIMIAABpYDabrZbLiImJ0ZQpU9SmTRtFRkbq8uXLunz5skJCQtS2bVsbRgoAAAAAAAAAAAD8HxJEAABIhdjYWO3du1cXL15UbGysGjdurHr16mnAgAHy8fHRmjVrjGPt7Ox08OBB2wULAAAAAAAAAAAAPIQEEQAAUsHe3l6tWrVSq1atrLZ/+eWXNooIAAAAAAAAAAAASD0SRAAAAADgEXbq1Cl169bNeD1mzBh1795dly5d0uLFi+Xm5qaQkBCNGDFChQoVsmGkAAAAAAAAADKCBBEAAAAAeIRt3rxZK1asMF7XrVtX4eHh6tOnj5YtW6by5ctr7969Gj58uJYuXWrDSAEAAAAAAABkhJ2tAwAAAAAA2MapU6cUGRmpypUrq3HjxmrcuLHy58+vjRs3qmjRoipfvrwkqVmzZjp+/Lh8fX1tGzAAAAAAAACAdGMGEQAAAAB4RH3zzTfavXu3vv/+e7Vq1Urjxo1T0aJF5ePjIw8PD+M4BwcHeXh46MiRI6pdu3a67mU2mxUeHp5JkeNRYzKZ5OTkZOswDBERETKbzbYOAzlQXvpbNZvNMplMmRwRAAAAAMCWSBABAAAAgEfUjBkz9Nlnn+nAgQOaMmWKevbsqY0bNyowMNCYPcSiYMGCCgoKSve9YmJi5Ofnl9GQ8YhycnJS9erVbR2GISAgQBEREbYOAzlQXvtbdXR0zMRoAAAAAAC2RoIIAAAAADzC8ufPr9atW+vJJ59Uu3bttHfvXplMJuXPn9/quJiYGDk4OKT7Pg4ODqpSpUpGw8UjKqfNYlCxYkVmEEGi8tLfqr+/fyZHAwAAAACwNRJEAAAAAAAqU6aM2rZtqytXrqhkyZK6ffu21f6wsDC5u7un+/omk0nOzs4ZDRPIEXLSEiJAcjLyt5rTkl0AAAAAABlHgggAAAAAQJJkZ2enWrVqycnJSTt27DC2R0dH6+rVq/L29rZhdAAAAAAAAAAyws7WAQAAAAAAsl9YWJjWrl2rsLAwSdKff/4pR0dH1a9fXx07dlRgYKCuXbsmSTp06JC8vb3l5eVly5ABAAAAAAAAZAAziAAAAADAIygqKkrr1q3T/PnzVb9+fT399NMaPXq0JMnFxUULFy7UrFmz5OHhoZs3b2rmzJk2jhgAAAAAAABARpAgAgAAAACPoGLFimn79u1J7vfy8tLkyZOzMSIAAAAAAAAAWYklZgAAAAAAAAAAAAAAAPI4EkQAAAAAAAAAAAAAAADyOJaYAQAAAAAAAJCtzp07p3Hjxumvv/5ShQoVNGrUKDVo0MDY/+GHH2rTpk2SpAIFCmjfvn0qXry4JGnLli06c+aMJKlChQp64403jPMiIiI0depUubq6KigoSL1795anp6ex//fff9f69etVpEgRRUdHa8SIEXJ0dMyOtwwAAAAANkeCCAAAAAAAAIBsEx0drTlz5mjQoEFycXHRrFmz9Pbbb2vnzp1yd3fX5cuXVaBAAa1YsUKSVKhQISM5ZP/+/dq8ebNWr14tSXrnnXdUtGhRvfDCC5KkDz74QM2bN1fnzp1148YNde3aVZs3b1ahQoUUHBysIUOGaMuWLXJ1ddWqVas0adIkjR8/3ja/CAAAAADIZiwxAwAAAAAAACDbXLp0SWPHjpW3t7dq1aqlOXPmKDo6WqdPn5YkffHFF3rqqadUt25dNW7cWDVq1DDOnTdvnpEMIkktW7bU3LlzJT2YlWTfvn1q1aqVJKl48eIqXbq01q1bJ0lavny5atWqJVdXV+PcDRs2KDg4OFveNwAAAADYGjOIAAAAAAAAAMg2VatWtXrt6uoqV1dXlS1bVjdu3NDBgwe1ceNGOTs7a/jw4erWrZtMJpNu3Lihs2fPqly5csa5lSpVUkBAgIKCguTj46OiRYuqYMGCxv7KlSvryJEj6tevn3x8fNSyZUtjX+nSpeXo6Khjx46pQ4cO6XovZrNZ4eHh6ToXAHILk8kkJycnW4dhiIiIkNlsTte5ZrNZJpMpkyMCACD3IEEEAAAAAAAAgM38/fffqlKlijFTyM8//6xr165p9erVmjRpku7du6f+/fsrMDBQklS4cGHjXEsySFBQkAIDA632WfYHBQVJUpL7MzKDSExMjPz8/NJ9PgDkBk5OTqpevbqtwzAEBAQoIiIi3ec7OjpmYjQAAOQuJIgAAAAAAAAAsJnly5drwoQJVttKlCih4cOHq1ixYpozZ47eeustY8R3gQIFjONiYmIkSfny5ZPJZLLaZ9nv4OBgvM6fP3+C/fnypb+J1MHBQVWqVEn3+QCQG+S0GTcqVqyY7hlE/P39MzmarBMbG6vZs2fLZDLp+vXr6tSpk+rXr5/osatWrVJQUJDc3d3177//qlu3bqpcuXI2RwwAyA1IEAEAAAAAAABgExs3blSbNm1UoUKFRPe/8cYbmjVrlkJCQlSqVClJUmhoqLE/LCxMkuTu7q6SJUvKx8fH6vywsDCVKFFCklSqVCndvn3b2GdZHsbd3T3d8ZtMJjk7O6f7fABA2mVkuZucluySnOnTp8vZ2VmDBg1SVFSU2rVrp2XLlsnDw8PqOB8fH+3du1erV6+WJF2+fFkDBgzQjh07bBE2ACCHs7N1AAAAAAAAAAAePfv375erq6u8vb2TPMZkMqlcuXIqWrSoihUrJi8vL6vR3wEBAfL09JSbm5uaN2+uq1ev6t69e1b7mzRpIklq1qyZ1bmXL19WXFycGjZsmAXvDgCA9AsJCdHq1av1wgsvSHowA1bdunW1dOnSBMeeP39e4eHhxuv8+fNbJUQCABAfM4gAAAAAAAAAyFa7du3S5cuX1bp1a/33338KDw/Xnj171KRJE924cUPPPvuszGaz5s+fr48++sgY8d23b19t3bpVXbp0kfQgyWTgwIGSpKpVq6pRo0Y6fPiw2rRpo2vXrik4OFivvPKKJOn1119Xjx49FBkZqQIFCmj//v3q2rWrihUrZptfAgAASTh69KhiYmKsZgupXLmy1q9fn+DY5557TvPnz9e0adM0YsQIbdiwIcHSbWllmWUrvUwmU4ZmeslsERERiS5LlNPilHJPrEnFKRFrRuSFWHNanFLuiTW5zz8lZrM51bNkkSACAAAAAAAAINts375dH3zwgWJjYzVt2jRj+/Dhw3X58mV9+umncnd3l5eXl1577TXVrFnTOOaFF15QUFCQPv/8c0lSixYt1KZNG2P/1KlTNW3aNP3zzz8KDAzUkiVL5OLiIkny8PDQxIkTNWnSJLm7uysiIkKjR4/OpncNAEDqBQYGqmDBgnJ0dDS2FSxYUEFBQQmOLV++vJYsWaJ3331XP/30k4YMGaJnnnkmQ/ePiYmRn59fus93cnJS9erVMxRDZgoICFBERESC7TktTin3xJpUnBKxZkReiDWnxSnlnliT+/xTI36ZkRwSRAAAAAAAAABkm3bt2qldu3ZJ7n/xxReTPb93795J7itSpIg+++yzJPd7e3snu6QNAAA5gclkUoECBay2xcTEKF++xLv17ty5o2nTpmn9+vX68MMPtWrVKnl6eqb7/g4ODqpSpUq6z0/tKPbsUrFixSRnD8hpckusScUpEWtG5IVYc1qcUu6JNbnPPyXxl9JMCQkiAAAAAAAAAAAAQA5RsmRJ3blzx2pbWFiY3N3dExz766+/atu2bZo7d668vb01cuRIDR06VN9//326728ymeTs7Jzu83OanLSEREpyS6y5JU6JWLMKsWa+jMSZlmQXu3TfBQAAAAAAAAAAAECmatSokUwmkwICAoxtAQEBatKkSYJjv//+e5UrV06SZG9vr4kTJ+rKlSsKCQnJtngBALkHCSIAAAAAAAAAAABADlGkSBG99NJL2rdvnyQpIiJCvr6+6tWrlyIiIjR9+nTdvHlTkuTl5aU///zTONdkMqlixYoqUqSITWIHAORsLDEDAAAAAAAAAAAA5CCjRo3S1KlTNX/+fF2/fl1TpkxRmTJlFBgYqG3btqlly5YqVqyYOnXqpGvXrmnatGmqWLGigoKCNGPGDFuHDwDIoUgQAQAAAAAAAAAAAHKQAgUK6JNPPkmwvVSpUvLx8TFem0wmvf3229kZGgAgF2OJGQAAAAAAAAAAAAAAgDyOBBEAAAAAAAAAAAAAAIA8jgQRAAAAAAAAAAAAAACAPI4EEQAAAAAAAAAAAAAAgDyOBBEAAAAAAAAAAAAAAIA8jgQRAAAAAAAAAAAAAACAPI4EEQAAAAAAAAAAAAAAgDyOBBEAAAAAAAAAAAAAAIA8jgQRAAAAAAAAAAAAAACAPI4EEQAAAAAAAAAAAAAAgDyOBBEAAAAAAAAAAAAAAIA8Ll9mX/D8+fNasmSJKlWqpIEDBxrbL126pMWLF8vNzU0hISEaMWKEChUqlNm3BwAAAAAAAAAAAAAAwEMydQaR8PBwhYaG6tixY4qNjbXa3qdPH/Xv319Dhw5V06ZNNXz48My8NQAAAAAAAAAAAAAAAJKQqQkizs7Oql+/vsqXL2+1fePGjSpatKixvVmzZjp+/Lh8fX0z8/YAAAAAAAAAAAAAAABIRKYvMSNJdnbWeSc+Pj7y8PAwXjs4OMjDw0NHjhxR7dq103UPs9ms8PDwjISJR5jJZJKTk5OtwzBERETIbDbbOgwgS5nNZplMJluHAQAAAAAAAAAAADySsiRB5GGBgYEJZhUpWLCggoKC0n3NmJgY+fn5ZTQ0PKKcnJxUvXp1W4dhCAgIUEREhK3DALKco6OjrUMAAAAAAAAAAAAAHknZkiBiMpmUP39+q20xMTFycHBI9zUdHBxUpUqVjIaGR1ROm8WgYsWKzCCCPM/f39/WIWTY+fPntWTJElWqVEkDBw40tl+6dEmLFy+Wm5ubQkJCNGLECBUqVMiGkQIAAAAAAAAAAADWsiVBpGTJkrp9+7bVtrCwMLm7u6f7miaTSc7OzhkNDcgRctJyN0BWyWmJWWkVHh6u0NBQHTt2zGpWrPDwcPXp00fLli1T+fLltXfvXg0fPlxLly61YbQAAAAAAAAAAACAtWxJEGnWrJl27NhhvI6OjtbVq1fl7e2dHbcHACDDnJ2dVb9+/QRLpm3cuFFFixY1tjdr1kzDhg2Tr6+vateuneb7mM1mhYeHZ0bIeASZTKYcl3QYERHBLFnI08xmc65PggQAAAAAAADwaMiSBBGz2WzVEdCxY0ctXbpU165dU4kSJXTo0CF5e3vLy8srK24PAECWsbOzs3rt4+MjDw8P47WDg4M8PDx05MiRdCWIxMTEyM/PL6Nh4hHl5OSk6tWr2zoMKwEBAYqIiLB1GECWcnR0tHUIAAAAAAAAAJCiTE0QiY2N1d69e3Xx4kXFxsaqcePGqlevnlxcXLRw4ULNmjVLHh4eunnzpmbOnJmZtwYAwCYCAwMTzCpSsGBBBQUFpet6Dg4OqlKlSmaEhkdQTpzFoGLFiswggjzN39/f1iEAAAAAAAAAQKpkaoKIvb29WrVqpVatWiXY5+XlpcmTJ2fm7QAAsDmTyaT8+WM5eO0AAQAASURBVPNbbYuJiZGDg0O6r+fs7JwZoQE5Qk5b8gbIbDkxMQsAAAAAAAAAEmOX8iEAACApJUuW1O3bt622hYWFyd3d3UYRAQAAAAAAAAAAAAmRIAIAQAY0a9bManmB6OhoXb16Vd7e3jaMCgAAAAAAAAAAALBGgggAAGlgNptlNpuN1x07dlRgYKCuXbsmSTp06JC8vb3l5eVlqxABAAAAAAAAAACABPLZOgAAAHKD2NhY7d27VxcvXlRsbKwaN26sevXqycXFRQsXLtSsWbPk4eGhmzdvaubMmbYOFwAAAAAAAAAAALBCgggAAKlgb2+vVq1aqVWrVgn2eXl5afLkyTaICgAAAAAAAAAAAEgdlpgBAAAAAAAAAAAAAADI40gQAQAAAAAAAAAAAAAAyONIEAEAAAAAAAAAAAAAAMjjSBABAAAAAAAAAAAAAADI40gQAQAAAAAAAAAAAAAAyONIEAEAAAAAAAAAAAAAAMjjSBABAAAAAAAAAAAAAADI40gQAQAAAAAAAAAAAAAAyONIEAEAAAAAAAAAAAAAAMjj8tk6AAAAAACAbX355Zc6dOiQVq9eLUm6dOmSFi9eLDc3N4WEhGjEiBEqVKiQjaMEAAAAAAAAkBHMIAIAAAAAj7BffvlF33zzjfE6PDxcffr0Uf/+/TV06FA1bdpUw4cPt2GEAAAAAAAAADIDCSIAAAAA8Ii6deuWtm/frg4dOhjbNm7cqKJFi6p8+fKSpGbNmun48ePy9fW1UZQAAAAAAAAAMgNLzAAAAADAI8hsNmvOnDkaNmyYvvrqK2O7j4+PPDw8jNcODg7y8PDQkSNHVLt27QzdLzw8PCMh4xFmMpnk5ORk6zAMERERMpvNtg4DOVBe+ls1m80ymUyZHBEAAAAAwJZIEAEAAACAR9CyZcv08ssvy9XV1Wp7YGCgMXuIRcGCBRUUFJSh+8XExMjPzy9D18Cjy8nJSdWrV7d1GIaAgABFRETYOgzkQHntb9XR0TETowEAAAAA2BoJIgAAAADwiDl+/Lgee+wxPfnkkwn2mUwm5c+f32pbTEyMHBwcMnRPBwcHValSJUPXwKMrp81iULFiRWYQQaLy0t+qv79/JkcDAAAAALA1EkQAAAAA4BGzcOFCnTlzRtOmTZMkRUVFKTY2VvXq1VP16tV1+/Ztq+PDwsLk7u6eoXuaTCY5Oztn6BpATpGTlhABkpORv9WcluwCAAAAAMg4EkQAAAAA4BEzffp0RUVFGa9XrVql3377TTNnztTBgwe1Y8cOY190dLSuXr0qb29vW4QKAAAAAAAAIJPY2ToAAAAAAED2cnNzU9myZY1/hQoVUv78+VW2bFl17NhRgYGBunbtmiTp0KFD8vb2lpeXl42jBgAAAAAAAJARzCACAAAAADC4uLho4cKFmjVrljw8PHTz5k3NnDnT1mEBAAAAAAAAyCASRAAAAADgETdo0CCr115eXpo8ebKNogEAPArOnTuncePG6a+//lKFChU0atQoNWjQQJJ06dIlLV68WG5ubgoJCdGIESNUqFAh49wtW7bozJkzkqQKFSrojTfeMPZFRERo6tSpcnV1VVBQkHr37i1PT09j/++//67169erSJEiio6O1ogRI+To6JhN7xoAAAAAbIslZgAAAAAAAABkm+joaM2ZM0eDBg3SypUr5erqqrffflvBwcEKDw9Xnz591L9/fw0dOlRNmzbV8OHDjXP379+vzZs3a8yYMRozZoyOHj2qH374wdj/wQcf6Mknn9SQIUM0YsQIDRw4UHfu3JEkBQcHa8iQIXr//fc1YsQIlSlTRpMmTcr29w8AAAAAtkKCCAAAAAAAAIBsc+nSJY0dO1be3t6qVauW5syZo+joaJ0+fVobN25U0aJFVb58eUlSs2bNdPz4cfn6+kqS5s2bpxdeeMG4VsuWLTV37lxJD2Yl2bdvn1q1aiVJKl68uEqXLq1169ZJkpYvX65atWrJ1dXVOHfDhg0KDg7OrrcOAAAAADbFEjMAAAAAAAAAsk3VqlWtXru6usrV1VVly5bVpk2b5OHhYexzcHCQh4eHjhw5orJly+rs2bMqV66csb9SpUoKCAhQUFCQfHx8VLRoURUsWNDYX7lyZR05ckT9+vWTj4+PWrZsaewrXbq0HB0ddezYMXXo0CFd78VsNis8PDxd5wJAbmEymeTk5GTrMAwREREym83pOtdsNstkMmVyRAAA5B4kiAAAAAAAAACwmb///ltVqlRRjRo1FBgYaMweYlGwYEEFBQUpMDBQklS4cGGrfZKM/fH3xT9XUpL7MzKDSExMjPz8/NJ9PgDkBk5OTqpevbqtwzAEBAQoIiIi3ec7OjpmYjQAAOQuJIgAAAAAAAAAsJnly5drwoQJkh6MUs+fP7/V/piYGDk4OBgjvgsUKGC1T5Ly5csnk8lktS/+uRaJXTtfvvQ3kTo4OKhKlSrpPh8AcoOcNuNGxYoV0z2DiL+/fyZHAwBA7kKCCAAAAAAAAACb2Lhxo9q0aaMKFSpIkkqWLKnbt29bHRMWFiZ3d3eVKlVKkhQaGmq1T5Lc3d1VsmRJ+fj4JDi3RIkSkqRSpUpZXduyPIy7u3u64zeZTHJ2dk73+QCAtMvIcjc5LdkFAIDsZmfrAAAAAAAAAAA8evbv3y9XV1d5e3sb25o1a2Y1ujs6OlpXr16Vt7e3ihUrJi8vL6v9AQEB8vT0lJubm5o3b66rV6/q3r17VvubNGmS6LUvX76suLg4NWzYMCvfJgAAAADkGCSIAAAAAAAAAMhWu3bt0t9//61q1arpv//+0/nz57VgwQJ17NhRgYGBunbtmiTp0KFD8vb2lpeXlySpb9++2rdvn3Gd/fv3a+DAgZKkqlWrqlGjRjp8+LAk6dq1awoODtYrr7wiSXr99df166+/KjIy0ji3a9euKlasWLa9bwAAAACwJZaYAQAAAAAAAJBttm/frg8++ECxsbGaNm2asX348OFycXHRwoULNWvWLHl4eOjmzZuaOXOmccwLL7ygoKAgff7555KkFi1aqE2bNsb+qVOnatq0afrnn38UGBioJUuWyMXFRZLk4eGhiRMnatKkSXJ3d1dERIRGjx6dTe86cbFxZtnb5YzlDnJSLAAAAACyBgkiAAAAAAAAALJNu3bt1K5duyT3e3l5afLkyUnu7927d5L7ihQpos8++yzJ/d7e3lZL2tiavZ1J7607Lf9rYTaNo0oJF83pWsemMQAAAADIeiSIAAAAAAAAAICN+F8L05mrd2wdBgAAAIBHgJ2tAwAAAAAAAAAAAAAAAEDWIkEEAAAAAAAAAAAAAAAgjyNBBAAAAAAAAAAAAAAAII8jQQQAAAAAAAAAAAAAACCPI0EEAAAAAAAAAAAAAAAgjyNBBAAAAAAAAAAAAAAAII8jQQQAAAAAkOPFxpltHYKknBMHAAAAAAAAkFb5bB0AAAAAAAApsbcz6b11p+V/LcxmMVQp4aI5XevY7P4AAAAAHh2xsbGaPXu2TCaTrl+/rk6dOql+/frJnrNt2zbdvXtXpUuX1lNPPaXChQtnT7AAgFyDBBEAAAAAQK7gfy1MZ67esXUYAAAAAJDlpk+fLmdnZw0aNEhRUVFq166dli1bJg8PjwTHRkdH64MPPtCLL76oDh062CBaAEBuka0JIpcvX9bChQtVtWpVRUREyMHBQf369cvOEAAAAAAAACApLi5Odna2X304p8QBAMgbclK5kpNiQe4SEhKi1atXa9u2bZKk/Pnzq27dulq6dKkmTJiQ4PhPPvlEtWrV0rPPPpvdoQIAcplsTRAZMWKEhg8fbkyB9f7772vnzp1q3bp1doYBAAAAAADwyLOzs9OmTZt048YNm8VQvHhxvfTSSza7PwAg78kJ5ZtEGYeMOXr0qGJiYqxmC6lcubLWr1+f4NiTJ09q165devzxxzV8+HBFRUXpvffeU9WqVdN9f7PZrPDw8HSfbzKZ5OTklO7zM1tERITMZnOC7TktTin3xJpUnBKxZkReiDWnxSnlnliT+/xTYjabZTKZUnVstiaInD9/Xvfu3TNeFyhQQHfuMD0wAAAAAACALdy4cUOBgYG2DgMAgExF+YbcLjAwUAULFpSjo6OxrWDBggoKCkpw7MaNG1WtWjW9/PLL6tmzp0aOHKnu3btr9+7dKlSoULruHxMTIz8/v3TH7+TkpOrVq6f7/MwWEBCgiIiIBNtzWpxS7ok1qTglYs2IvBBrTotTyj2xJvf5p0b8MiM52Zog0q5dO40fP17lypWTi4uL7t27l+610DKavYhHW17KCANyi7RkLwIAAAAAAADAo8pkMqlAgQJW22JiYpQvX8JuPT8/P3l7e+uxxx6TJL377rvavn279u7dq86dO6fr/g4ODqpSpUq6zpWU49qBK1asmOTsATlNbok1qTglYs2IvBBrTotTyj2xJvf5p8Tf3z/Vx2ZrgsiYMWM0evRovfrqq2rWrJmmTZsme3v7dF0ro9mLeLTltYwwILdIbfZibnP58mUtXLhQVatWVUREhBwcHNSvXz9bhwUAAAAAAAAgFypZsmSCGfjDwsLk7u6e4NjY2Fjdv3/feF2hQgU5OzsrJCQk3fc3mUxydnZO9/k5TU4aMJyS3BJrbolTItasQqyZLyNxpiXZJVsTRKKjo/XEE0+oTZs2ev/99zVhwgSNHz8+XdfKaPYiHm15KSMMyC3Skr2Y24wYMULDhw9X/fr1JUnvv/++du7cqdatW9s4MgAAAAAAAAC5TaNGjWQymRQQEKCKFStKejDQtEmTJgmOrVatmgICAqy22dvby9PTM1tiBQDkLtmaIDJ06FANGzZM1apV08qVK/Xmm2+qQYMGeuGFF9J8rbyWvYhHW27JXAMyIqclZmWm8+fP6969e8brAgUKJMjwBwAAAAAAAIDUKFKkiF566SXt27dPffv2VUREhHx9fbVy5UpFRERowYIF6tWrl4oVK6Y+ffrojTfeUHBwsNzd3fXXX3+pbNmyiSaTAACQbQkiISEhOnz4sObPny9Jqlmzpnr37q1ffvklXQkiAADkFO3atdP48eNVrlw5ubi46N69e+rQoUO6rmU2mxUeHp7JEeJRYTKZclzSYUREBLNkIU8zm815OgkSAAAAAGAbo0aN0tSpUzV//nxdv35dU6ZMUZkyZRQYGKht27apZcuWKlasmKpVq6ZZs2bp888/V506dfT3339r0aJF1FUBAInKtgSRwoULq0yZMvrjjz9Ur149SQ86MWrXrp1dIQAAkCXGjBmj0aNH69VXX1WzZs00bdo02dvbp+taMTEx8vPzy+QI8ahwcnJS9erVbR2GlYCAAEVERNg6DCBLOTo62joEAAAAAEAeU6BAAX3yyScJtpcqVUo+Pj5W25o2baqmTZtmV2gAgFws2xJETCaTli5dqkWLFumvv/6SyWSSq6ur2rVrl10hAACQJaKjo/XEE0+oTZs2ev/99zVhwgSNHz8+XddycHBQlSpVMjlCPCpy4siQihUrMoMI8jR/f39bhwAAAAAAAAAAqZJtCSKSVLlyZU2fPj07bwkAQJYbOnSohg0bpmrVqmnlypV688031aBBg3QtoWYymeTs7JwFUQK2kdOWvAEyW05MzAIAAAAAAACAxNjZOgAAAHKzkJAQHT58WJUqVZIk1axZU71799Yvv/xi48gAAAAAAAAAAACA/0OCCAAAGVC4cGGVKVNGf/zxh7HNZDKpdu3atgsKAAAAAAAAAAAAeEi2LjEDAEBeYzKZtHTpUi1atEh//fWXTCaTXF1d1a5dO1uHBgAAAAAAAAAAABhIEAEAIIMqV66s6dOn2zoMAAAAAAAAAAAAIEksMQMAAAAAAAAAAAAAAJDHkSACAAAAAAAAAAD+H3t3Hmdj/f9//HmGGWbIMJaxZm0qKimJCBVZSraKLGWvlI9C4dMmJaHsFLJl34lKQvYtIbKkYfhMzGCYwZjtmLl+f8zvXN85Zl/PMo/77dYt51zb8yxzva/rOq/r/QYAAICbo0AEAAAAAAAAAAAAAADAzVEgAgAAAAAAAAAAAAAA4OYoEAEAAAAAAAAAAAAAAHBzFIgAAAAAAAAAAAAAAAC4OQpEAAAAAAAAAAAAAAAA3BwFIgAAAAAAAAAAAAAAAG6OAhEAAAAAAAAAAAAAAAA3R4EIAAAAAAAAAAAAAACAm6NABAAAAAAAAACQtoR4RydI5Cw5AAAAABdU0NEBAAAAAAAAAABOzqOAtKqPFHbacRlKBUgdv3Pc9gEAAAAXR4EIAAAAAAA5KSE+8Uc0R3OWHAAA9xF2Wgr509EpAAAAAGQRBSIAAAAAAOQk7rAGAAAAAACAE6JABAAAAACAnMYd1gAAAAAAAHAyHo4OAAAAAAAAAAAAAAAAgNxFgQgAAAAAAAAAAAAAAICbo0AEAAAAAAAAAAAAAADAzRV0dAAAAAAAOccwEmSxOL4O3FlyIG2BgYH68MMPderUKQUEBGj06NGqXr26JOn8+fOaMWOGSpcurfDwcA0ZMkTFihVzcGIAAAAAAAAAWUWBCAAAAOBGLBYPhYb+JKv1msMyeHr6qWzZ1g7bPjImNjZW69ev1+TJk3Xr1i0NHDhQn3zyiRYuXKioqCj17t1bs2fPVuXKlbV582YNHjxYs2bNcnRsAAAAAAAAAFlEgQgAAADgZqzWa4qNvezoGHByFotFAwYMUMGCiaeFnTp10i+//CJJWrlypfz8/FS5cmVJUpMmTTRo0CAdOXJEDz/8sKMiAwAAAAAAAMgGCkQAAACAdCQkJMjDw/HDpThLDrgHLy8vu8cXLlzQ8OHDJUk7d+5UpUqVzGmenp6qVKmS9uzZk+UCEcMwFBUVlaVlLRaLvL29s7RsboiOjpZhGClOc6WsroT3NefxnuYOd3pfDcOQxWLJ4UQAAAAAAEeiQAQAAABIh4eHh1atWqWwsDCHZShVqpQ6duzosO3DfZ08eVKLFi3SgQMH1LRpU0lSSEiI2XuITZEiRRQaGprl7VitVp08eTJLy3p7e6tmzZpZ3nZOCwoKUnR0dIrTXCmrK+F9zXm8p7nD3d7XO4sJAQAAAACujQIRAAAAIAPCwsIUEhLi6BhAjqtQoYJatmyp06dPq2/fvtq0aZMsFosKFSpkN5/VapWnp2eWt+Pp6akaNWpkaVlnu4O9atWqafYg4kzSyupKeF9zHu9p7nCn9zUwMDCH0wAAAAAAHI0CEQAAAADIx4oVK6ZGjRqpdu3aatq0qQ4fPqyyZcvq+vXrdvNFRkbK398/y9uxWCzy8fHJblyn4EzDR6THlbK6Et7XnMd7mjuy8746W7ELAAAAACD7GMAcAAAAAKC77rpLVatWVfny5dWkSRO7O8fj4uJ08eJFNWzY0IEJAQAAAAAAAGQHPYgAAAAAQD4UHR2tf/75Rw899JAk6ezZsypdurQeeughVatWTbNmzdLly5dVpkwZbd++XQ0bNlStWrUcnBoA4E5Onz6tmTNnqlq1aurfv7/dtP/+979atWqVJKlw4cLasmWLSpUqJUlas2aNjh8/LkmqUqWKunXrZi4XHR2tsWPHytfXV6GhoerVq5cCAgLM6UePHtWyZctUokQJxcXFaciQIfLy8srtlwoAAAAAToECEQAAAADIh06fPq3+/furZMmSevzxx+Xr66uvv/5aklS0aFFNnz5dEyZMUKVKlXT16lWNHz/ewYkBAO4kKipKERER2rdvnypXrmw3LTg4WIULF9bcuXMlJQ6HZisO2bp1q1avXq0FCxZIkt566y35+fmpdevWkqShQ4eqadOm6tChg8LCwtS5c2etXr1axYoV06VLl/TOO+9ozZo18vX11fz58zVq1Ch9+umnefjKAQAAAMBxKBABAAAAgHyodu3a2r17d6rTa9WqpdGjR+dhIgBAfuLj46N69eolKw6RpG+//VYNGjTQo48+qkKFCtlNmzJlil5++WXzcfPmzTV58mS1bt1ap06d0pYtW8z2q1SpUipfvryWLl2qfv36ac6cOapdu7Z8fX3NZZs1a6b+/fvL398/F18tAAAAADgHCkQAAAAAAAAAOISHh4fd47CwMG3btk0rV66Uj4+PBg8erK5du8pisSgsLEwnTpzQ3Xffbc5frVo1BQUFKTQ0VDt37pSfn5+KFCliTq9evbr27Nmjfv36aefOnWrevLk5rXz58vLy8tK+ffvUtm3bLOU3DENRUVFZWtZiscjb2ztLy+aW6OhoGYaR7Hlny5paTsDZvquSe3xfne19zc57ahiGLBZLDicCAMB1UCACAAAAAAAAwCmUKlVKu3fv1uXLl7VgwQKNGjVKt27d0uuvv66QkBBJUvHixc35bcUgoaGhCgkJsZtmmx4aGipJqU6/dOlSlvNarVadPHkyS8t6e3urZs2aWd52bggKClJ0dHSy550ta2o5AWf7rkru8X11tvc1u++pl5dXDqYBAMC1UCACAAAAAAAAwKmUKVNGgwcPVsmSJTVp0iT17dvXvOO7cOHC5nxWq1WSVLBgQVksFrtptumenp7m4zuHrLFarSpYMOuXSD09PVWjRo0sLeuMd7BXrVo11R5EnElqOQFn+65K7vF9dbb3NTvvaWBgYA6nAQDAtVAgAgAAAAAAAMApdevWTRMmTFB4eLjKlSsnSYqIiDCnR0ZGSpL8/f1VtmxZ7dy50275yMhIlSlTRpJUrlw5Xb9+3ZxmGx7G398/y/ksFot8fHyyvLyzcaYhJNLiKjkBie9rbsjOe+psxS4AAOQ1j/RnAQAAAAAAAIC8Z7FYdPfdd8vPz08lS5ZUrVq17O7+DgoKUkBAgEqXLq2mTZvq4sWLunXrlt30Ro0aSZKaNGlit2xwcLASEhJUv379vHtBAAAAAOBAFIgAAAAAAAAAcAjDMOyGCfjzzz+1ZcsWc9rUqVP1wQcfmHd89+nTx5wuSVu3blX//v0lSffcc48aNGigHTt2SJIuX76sS5cu6aWXXpIkdenSRYcOHVJMTIy5bOfOnVWyZMncf6EAAAAA4AQYYgYAAAAOYSQkyOLh+HplZ8kBAACQn8THx2vz5s06c+aM4uPj9cQTT6hu3boKDg7WZ599Jn9/f9WqVUuvvPKKHnroIXO51q1bKzQ0VGPGjJEkPf3002rVqpU5fezYsRo3bpzOnTunkJAQzZw5U0WLFpUkVapUSZ9//rlGjRolf39/RUdHa/jw4Xn7wgEAAADAgSgQAQAAgENYPDx0etuvioq45rAMPsX9FNC0ucO2DwAAkF8VKFBALVq0UIsWLeyef/755/X888+nuWyvXr1SnVaiRAl98cUXqU5v2LChGjZsmLmwAAAAAOAmKBABAACAw0RFXNOtq2GOjgEAAAAAAAAAgNujL20AAAAAAAAAAAAAAAA3R4EIAAAAAAAAAAAAAACAm6NABAAAAAAAAAAAAAAAwM1RIAIAAAAAQD6VYCQ4OoIk58kBAAAAAADgzgo6YqMXLlzQhg0bVKFCBd1999166KGHHBEDAAAAAIB8zcPioS1XNijCetVhGYp7ltQzpZ932PYBAAAAAADyizwvENm7d68WL16sL7/8UkWKFMnrzQMAAAAAgCQirFcVFnfZ0TEAAAAAAACQy/K0QOT06dMaOXKkVqxYQXEIAAAAAAAAAAAAAABAHsnTApHPP/9c999/v2bNmqVDhw7pySefVJ8+feTh4ZHpdRmGoaioqFxIifzAYrHI29vb0TFM0dHRMgzD0TGAXGUYhiwWi6Nj5CqGUAMAAAAAAAAAAICzyrMCkeDgYO3fv19TpkzRs88+q1OnTqlTp066ffu2+vfvn+n1Wa1WnTx5MheSIj/w9vZWzZo1HR3DFBQUpOjoaEfHAHKdl5eXoyPkGoZQAwAAAAAAAAAAgDPLswKRU6dOSZIaNmwoSbrvvvvUrFkzrVy5MksFIp6enqpRo0aOZkT+4Wy9GFStWpUeROD2AgMDHR0h1zCEGgAAAAAAAAAAAJxdnhWI3L59W5IUHx9vPlezZk1t3bo1S+uzWCzy8fHJkWyAoznTcDdAbnG2wqycxBBqcBbONoSalPowas6WNa3h3siadflhGL38MIQaAAAAAAAAAPeQZwUituE8goKCVLt2bUlSgQIFFBAQkFcRAADIcQyhBmfibEOoSakPo+ZsWdMa7o2sWZdfhtFz5yHUAAAAAAAAALiPPCsQqVy5slq1aqX169ebBSJ//PGH+vTpk1cRAADIcQyhBmfijL0YpDaMmrNlTWu4N7JmXX4YRs+dh1ADAAAAAAAA4F7yrEBEkr744gt99tlnmjZtmgzDUP369dW8efO8jAAAQI5iCDUgbc403ElaXCWnRFZn42xFOQAAAAAAAACQmjwtEPHx8dHo0aPzcpMAAOQqhlADAAAAAAAAAACAK/BwdAAAAFxZ0iHUbBhCDQAAAAAAAAAAAM6GAhEAALLpiy++0K1btzRt2jRNnTqVIdQAAAAAAAAAAADgdPJ0iBkAANwRQ6gBAAAAAAAAAADA2dGDCAAAAAAAAAAAAAAAgJujQAQAADcTn2A4OoIk58kBwDklGM6zj3CmLAAAAAAAAACQWxhiBgAAN1PAw6KBSw8r8HKkwzLUKFNUkzrXcdj2ATg/D4tF20PDdT3utkNz+HoVVJOyJRyaAQAAAAAAAADyAgUiAAC4ocDLkTp+8YajYwBAmq7H3dbVWKujYwAAAAAA4HTi4+M1ceJEWSwWXblyRe3bt1e9evXSXCYiIkLt2rXTmDFj9Pjjj+dRUgCAK6FABAAAAAAAAAAAAHAiX331lXx8fDRgwADFxsaqTZs2mj17tipVqpTi/IZhaPz48QoJCcnjpAAAV+Lh6AAAAAAAAAAAAAAAEoWHh2vBggVq3bq1JKlQoUJ69NFHNWvWrFSXmTt3rtq3b59XEQEALooeRAAAAAAAAAAAbiPBSJCHxfH3RmYkR0KCIQ8PSx4lcv4cABLt3btXVqvVrreQ6tWra9myZSnO//vvv6tQoUKqU6dOjmzfMAxFRUVleXmLxSJvb+8cyZIToqOjZRhGsuedLafkOllTyymRNTvcIauz5ZRcJ2tan396DMOQxZKxYzkKRAAAAAAAAAAAbsPD4qEtVzYownrVYRmKe5bUM6WfT3c+Dw+Lfp1zXNdCbuVBqpT5lSui5r1qOWz7AJILCQlRkSJF5OXlZT5XpEgRhYaGJpv36tWr+uWXX/Thhx/m2PatVqtOnjyZ5eW9vb1Vs2bNHMuTXUFBQYqOjk72vLPllFwna2o5JbJmhztkdbackutkTevzz4ikbUZaKBABAAAAAAAAALiVCOtVhcVddnSMDLkWckthwZGOjgHAiVgsFhUuXNjuOavVqoIF7X/WS0hI0JQpUzRkyJAc3b6np6dq1KiR5eUzehd7XqlatWqqvQc4G1fJmlpOiazZ4Q5ZnS2n5DpZ0/r80xMYGJjheSkQAQAAAAAAAAAAAJxE2bJldePGDbvnIiMj5e/vb/fcoUOHtGrVKm3YsMHu+TfeeENt27bViBEjsrR9i8UiHx+fLC3rjJxpCIn0uEpWV8kpkTW3kDXnZSdnZopdKBABAAAAAAAAAAAAnESDBg1ksVgUFBSkqlWrSkoceqBRo0Z28z344IP6+eef7Z575pln9Pnnn6tBgwZ5lhcA4Do8HB0AAAAAAAAAAAAAQKISJUqoY8eO2rJliyQpOjpaR44cUc+ePRUdHa2vvvpKV69eVaFChVSxYkW7/ySpVKlS8vPzc+RLAAA4KQpEAAAAAAAA4NQMI8HRESQ5Tw4AAOD+hg0bposXL2rq1Kn68ssv9eWXX6pChQqKiIjQunXr9O+//zo6IgDABTHEDAAAAAAAAJyaxeKh0NCfZLVec1gGT08/lS3b2mHbBwAA+UvhwoX18ccfJ3u+XLly2rlzZ6rL/f3337kZCwDg4igQAQAAAAAAgNOzWq8pNvayo2MAAAAAAOCyGGIGAAAAAAAAAAAAAADAzVEgAgAAAAAAAAAAAAAA4OYoEAEAAAAAAAAAAAAAAHBzFIgAAAAAAAAAAAAAAAC4OQpEAAAAAAAAAAAAAAAA3BwFIgAAAAAAAAAAAAAAAG6OAhEAAAAAAAAAAAAAAAA3R4EIAAAAAAAAAAAAAACAm6NABAAAIAMSjARHR5DkPDkAAAAAAAAAAIBrKejoAAAAAK7Aw+KhLVc2KMJ61WEZinuW1DOln3fY9gEAAAAAAAAAgOuiQAQAACCDIqxXFRZ32dExAAAAAAAAAAAAMo0hZgAAAAAAAAAAAPKY4STDyDpLDgAAkPvoQQQAAAAA8qFTp05pxIgR+vvvv1WlShUNGzZMjz/+uCTp/PnzmjFjhkqXLq3w8HANGTJExYoVc3BiAAAAwL1YLB4KDf1JVus1h2Xw9PRT2bKtHbZ9AACQtygQAQAAAIB8Ji4uTpMmTdKAAQNUtGhRTZgwQW+88YY2btyou+66S71799bs2bNVuXJlbd68WYMHD9asWbMcHRsAAABwO1brNcXGMpwtAADIGwwxAwAAHCch3tEJEjlLDgDII+fPn9cnn3yihg0bqnbt2po0aZLi4uJ0+PBhrVy5Un5+fqpcubIkqUmTJtq/f7+OHDni2NAAAAAAAAAAsoUeRAAAgON4FJBW9ZHCTjsuQ6kAqeN3jts+ADjAPffcY/fY19dXvr6+qlixolatWqVKlSqZ0zw9PVWpUiXt2bNHDz/8cJa3aRiGoqKisrSsxWKRt7d3lred06Kjo2UYRorTyJp17pLVVbjSe0rWrMvOd9UwDFkslhxOBAAAAABwJApEAACAY4WdlkL+dHQKAMjXzp49qxo1auiBBx5QSEiI2XuITZEiRRQaGpqtbVitVp08eTJLy3p7e6tmzZrZ2n5OCgoKUnR0dIrTyJp17pLVVbjSe0rWrMvud9XLyysH0wAAAAAAHI0CEQAAAADI5+bMmaORI0dKSrz7vVChQnbTrVarPD09s7UNT09P1ahRI0vLOtsd7FWrVk2z9wBnQtbckVZWV+FK7ylZsy4739XAwMAcTpPc6dOnNXPmTFWrVk39+/c3nz9//rxmzJih0qVLKzw8XEOGDFGxYsXM6WvWrNHx48clSVWqVFG3bt3MadHR0Ro7dqx8fX0VGhqqXr16KSAgwJx+9OhRLVu2TCVKlFBcXJyGDBlCIQwAAACAfIMCEQAAAADIx1auXKlWrVqpSpUqkqSyZcvq+vXrdvNERkbK398/W9uxWCzy8fHJ1jqchTMNH5EesuYOV8rqKlzpPc0vWXO72CUqKkoRERHat2+fXc9VUVFR6t27t2bPnq3KlStr8+bNGjx4sGbNmiVJ2rp1q1avXq0FCxZIkt566y35+fmpdevWkqShQ4eqadOm6tChg8LCwtS5c2etXr1axYoV06VLl/TOO+9ozZo18vX11fz58zVq1Ch9+umnufpaAQAAAMBZeDg6AAAAAADAMbZu3SpfX181bNjQfK5JkyZ2d43HxcXp4sWLdvMAAJBdPj4+qlevXrJhzVauXCk/Pz/z+SZNmmj//v06cuSIJGnKlClmMYgkNW/eXJMnT5YknTp1Slu2bFGLFi0kSaVKlVL58uW1dOlSSYk9ZtWuXVu+vr7msitWrNClS5dy9bUCAAAAgLOgBxEAAAAAyId++eUXBQcHq2XLlvr3338VFRWlX3/9Va+99ppmzZqly5cvq0yZMtq+fbsaNmyoWrVqOToyAMANeXjY37+2c+dOVapUyXzs6empSpUqac+ePapYsaJOnDihu+++25xerVo1BQUFKTQ0VDt37pSfn5+KFCliTq9evbr27Nmjfv36aefOnWrevLk5rXz58vLy8tK+ffvUtm3bLOU3DENRUVFZWtZisThdjzTR0dEpDkvkbFlTyymRNTvSyuoqnO09lfi7yg3Z+a4ahuF0Q8IBAJCXKBABAAAAgHxm/fr1Gjp0qOLj4zVu3Djz+cGDB6to0aKaPn26JkyYoEqVKunq1asaP368A9MCAPKTkJCQZL2KFClSRKGhoQoJCZEkFS9e3G6aJHN60mlJl7WtO6Xp2elBxGq16uTJk1la1tvbWzVr1szytnNDUFCQoqOjkz3vbFlTyymRNTvSyuoqnO09lfi7yg3Z/a56eXnlYBoAAFwLBSIAAAAAkM+0adNGbdq0SXV6rVq1NHr06DxMBABAIovFokKFCtk9Z7Va5enpad7xXbhwYbtpklSwYEFZLBa7aUmXtUlp3QULZv0Sqaenp2rUqJGlZZ3xDvaqVaum2tOBM0ktp0TW7Egrq6twtvdU4u8qN2Tnu5p0OE0AAPIjCkQAAAAAAAAAOIWyZcvq+vXrds9FRkbK399f5cqVkyRFRETYTZMkf39/lS1bVjt37ky2bJkyZSRJ5cqVs1u3bXgYf3//LOe1WCzy8fHJ8vLOxpmGkEiLq+SUyArXeV9dJaeUvazOVuwCAEBe80h/FgAAAAAAAADIfU2aNLG7uzsuLk4XL15Uw4YNVbJkSdWqVctuelBQkAICAlS6dGk1bdpUFy9e1K1bt+ymN2rUKMV1BwcHKyEhQfXr18+DVwYAAAAAjkeBCAAAAAAAAACHMAzDbpiAdu3aKSQkRJcvX5Ykbd++XQ0bNlStWrUkSX369NGWLVvM+bdu3ar+/ftLku655x41aNBAO3bskCRdvnxZly5d0ksvvSRJ6tKliw4dOqSYmBhz2c6dO6tkyZK5/0IBAAAAwAkwxAwAAAAAAACAPBUfH6/NmzfrzJkzio+P1xNPPKG6deuqaNGimj59uiZMmKBKlSrp6tWrGj9+vLlc69atFRoaqjFjxkiSnn76abVq1cqcPnbsWI0bN07nzp1TSEiIZs6cqaJFi0qSKlWqpM8//1yjRo2Sv7+/oqOjNXz48Lx94QAAAADgQBSIAAAAAAAAAMhTBQoUUIsWLdSiRYtk02rVqqXRo0enumyvXr1SnVaiRAl98cUXqU5v2LChGjZsmLmwAAAAAOAmHDbEzHfffafu3bs7avMAAAAAAAAAAAAAAAD5hkMKRP744w8tWbLEEZsGACBXUQAJAAAAAAAAAAAAZ5TnQ8xcu3ZN69evV9u2bfX7779neT2GYSgqKioHkyE/sVgs8vb2dnQMU3R0tAzDcHQMIFcZhiGLxeLoGLnKVgBZvnx5R0cBAAAAAAAAAAAA7ORpgYhhGJo0aZIGDRqk77//PlvrslqtOnnyZA4lQ37j7e2tmjVrOjqGKSgoSNHR0Y6OAeQ6Ly8vR0fINTlVAAkAAAAAAAAAAADkhjwtEJk9e7ZefPFF+fr6Zntdnp6eqlGjRg6kQn7kbL0YVK1alR5E4PYCAwMdHSHX5GQBZHZ7yHKlHpLImnWpZXW2nJLrZHWHz19ynazOllPKeo9u+aGHLAAAAAAAAADuIc8KRPbv36+77rpLDz74YI6sz2KxyMfHJ0fWBTias/1AAuQGd/7xLCcLILPbQ5Yr9ZBE1qxLLauz5ZRcJ6s7fP6S62R1tpxS9np0c+cesgAAAAAAAAC4jzwrEJk+fbqOHz+ucePGSZJiY2MVHx+vunXr6ocfflD58uXzKgoAADkmpwsgs9tDlrMV4qTVQxJZsy61rM6WU3KdrO7w+Uuuk9XZckpZ79HNnXvIAgAAAAAAAOBe8qxA5KuvvlJsbKz5eP78+frzzz81fvx4lSlTJq9iAACQo3K6ANLdeshypR6SyJo7XCWrq+SUyJpbsprVGYtdAAAAAAAAACAleVYgUrp0abvHxYoVU6FChVSxYsW8igAAQI6jABIAAAAAAAAAAACuIM8KRAAAcEcUQAIAAAAAAAAAAMAVOKxAZMCAAY7aNAAAAAAAAAAAAAAAQL5CDyIAAOQgCiABAAAAAAAAAADgjDwcHQAAAAAAAAAAAAAAAAC5iwIRAAAAAACAHGIkJDg6gsmZsgAAAAAAAMdjiBkAAAAAAIAcYvHw0Oltvyoq4ppDc/gU91NA0+YOzQAAAAAAAJwLBSIAAAAAAAA5KCrimm5dDXN0DAAAAAAAADsMMQMAAAAAAAAAAAAAAODmKBABAAAAAAAAAAAAAABwcxSIAAAAAAAAAAAAAAAAuDkKRAAAAAAAAAAAAAAAANwcBSIAAAAAAAAAAAAAAABujgIRAAAAAAAAAAAAAAAAN0eBCAAAAAAAAAAAAAAAgJujQAQAAAAAAAAAAAAAAMDNUSACAAAAAAAAAAAAAADg5igQAQAAAAAAAAAAAAAAcHMUiAAAAAAAAAAAAAAAALg5CkQAAAAAAAAAAAAAAADcHAUiAAAAAAAAAAAAAAAAbo4CEQAAAAAAAAAAAAAAADdHgQgAAAAAAAAAAAAAAICbo0AEAAAAAAAAAAAAAADAzVEgAgAAAAAAAAAAAAAA4OYoEAEAAAAAAAAAAAAAAHBzFIgAAAAAAAAAAAAAAAC4OQpEAAAAAAAAAAAAAAAA3BwFIgAAAAAAAAAAAAAAAG6uoKMDAAAAAAAAAAAAAPg/8fHxmjhxoiwWi65cuaL27durXr16yeYzDEMTJ07UypUrFR8fr1atWmno0KEqXLiwA1IDAJwdBSIAAAAAAAAAAACAE/nqq6/k4+OjAQMGKDY2Vm3atNHs2bNVqVIlu/lWrFghT09PzZ49W0eOHNEXX3whLy8vDR8+3EHJAQDOjAIRAAAAAAAAAAAAwEmEh4drwYIFWrdunSSpUKFCevTRRzVr1iyNHDnSbt6yZcvq5ZdfliTdd999On/+vHbu3Jmt7RuGoaioqCwvb7FY5O3tna0MOSk6OlqGYSR73tlySq6TNbWcElmzwx2yOltOyXWypvX5p8cwDFkslgzNS4EIAAAAAAAAAAAA4CT27t0rq9Vq11tI9erVtWzZsmTzNm7c2O5xpUqVVLFixWxt32q16uTJk1le3tvbWzVr1sxWhpwUFBSk6OjoZM87W07JdbKmllMia3a4Q1Znyym5Tta0Pv+M8PLyytB8FIgAAAAAAAAAAAAATiIkJERFihSx+7GvSJEiCg0NTXfZP/74Q7169crW9j09PVWjRo0sL5/Ru9jzStWqVVPtPcDZuErW1HJKZM0Od8jqbDkl18ma1uefnsDAwAzPS4EIAAAAAAAAAAAA4CQsFosKFy5s95zValXBgmn/rHfq1Cn5+fmpXr162d6+j49PttbhTJxpCIn0uEpWV8kpkTW3kDXnZSdnZopdPLK8FQB5xkhIcHQESc6TAwAAAAAAAAAAd1W2bFnduHHD7rnIyEj5+/unuszNmze1YsUKDR06NLfjAQBcGD2IAC7A4uGh09t+VVTENYdl8Cnup4CmzR22fQAAAAAAAAAA8oMGDRrIYrEoKChIVatWlSQFBQWpUaNGKc4fHR2tGTNmaPDgwen2MgIAyN/ydSsRn2CogIfjxxZylhw5IcFIkIfFOTqmcaYsOSEq4ppuXQ1zdAwAAAAAAAAA+ZCRkCCLh+OvtzpLDiA3lShRQh07dtSWLVvUp08fRUdH68iRI5o3b56io6M1bdo09ezZUyVLllRkZKRGjx6t7t2769q1a7p69apOnjypIkWKqGHDho5+KQAAJ5OvC0QKeFg0cOlhBV6OdFiGGmWKalLnOunPmBAveRTI/UDZzOFh8dCWKxsUYb2ah6GSK+5ZUs+Uft6hGQAAAAAAAADAXdDLMZC3hg0bprFjx2rq1Km6cuWKvvzyS1WoUEEhISFat26dmjdvLh8fH7366qs6fvy4Vq5caS5brFgx7dq1y4HpAQDOKl8XiEhS4OVIHb94I/0ZHc2jgLSqjxR22nEZSgVIHb9Ld7YI61WFxV3Og0AAAAAAAAAAgLxCL8dA3ilcuLA+/vjjZM+XK1dOO3fuNB+vXr06L2MBAFxcvi8QcSlhp6WQPx2dAgAAAIAbOX36tGbOnKlq1aqpf//+5vPnz5/XjBkzVLp0aYWHh2vIkCEqVqyYA5MCAAAAAAAAyA4G6gOQoxISEhwdQZLz5AAAAHBmUVFRioiI0L59+xQfH2/3fO/evfX666/r3XffVePGjTV48GAHJgUAAAAAAACQXfQgAiBHeXh4aNWqVQoLc1xXk6VKlVLHjh0dtn0AAABX4ePjo3r16qly5cp2z69cuVJ+fn7m802aNNGgQYN05MgRPfzwww5ICgAAAAAAACC7KBABkOPCwsIUEhLi6BgAAADIIA8P+84ld+7cqUqVKpmPPT09ValSJe3ZsyfLBSKGYSgqKipLy1osFnl7e2dp2dwQHR0twzBSnEbWrHOHrM6WU3KdrO7w+UuulTU9hmHIYrHkcKLMOXjwoLp27Wo+/vDDD9W9e/d0h0Fbs2aNjh8/LkmqUqWKunXrZk6Ljo7W2LFj5evrq9DQUPXq1UsBAQF596IAAAAAwIEoEAGQLxlGgiwW5xhlK70sCYYhDwdflHOmHAAAIPeFhIQk61WkSJEiCg0NzfI6rVarTp48maVlvb29VbNmzSxvO6cFBQUpOjo6xWlkzTp3yOpsOSXXyeoOn7/kWlkzwsvLKwfTZN7q1as1d+5c8/Gjjz5qDoM2e/ZsVa5cWZs3b9bgwYM1a9YsSdLWrVu1evVqLViwQJL01ltvyc/PT61bt5YkDR06VE2bNlWHDh0UFhamzp07a/Xq1XYFJgAAAADgrigQAZAvWSweCg39SVbrNYfm8PT0U9myrdOcx8Ni0fbQcF2Pu51HqZLz9SqoJmVLOGz7AAAgb1ksFhUqVMjuOavVKk9Pzyyv09PTUzVq1MhyHmdStWrVNHsPcCZkzR2pZXW2nJLrZHWHz19yrazpCQwMzOE0mXPw4EHFxMSoevXq8vf3N5///vvv0xwGbcqUKXr55ZfN+Zs3b67JkyerdevWOnXqlLZs2aLRo0dLShyitnz58lq6dKn69euXty8QAAAAAByAAhEA+ZbVek2xsZcdHSNDrsfd1tVYq6NjAACAfKJs2bK6fv263XORkZF2P9BllsVikY+PT3ajOQVnGj4iPWTNHWTNea6SU8o/WR1d7LJkyRJt2rRJP/74o1q0aKERI0bIz88vzWHQKlasqBMnTujuu+82p1erVk1BQUEKDQ3Vzp075efnpyJFipjTq1evrj179mS5QMSdhlCTGJYqN5A1d7jKd1Vynazu8PlnhDMMoQYAgCNRIAIAAAAAsNOkSRNt2LDBfBwXF6eLFy+qYcOGDkwFAMhPvv76a33xxRf67bff9OWXX6pHjx5auXJlmsOghYSESJKKFy9uN02SOT3ptKTLZpU7DaEmMSxVbiBr7nCV76rkOlnd4fPPKEcPoQYAgCPlaYHIqVOnNGLECP3999+qUqWKhg0bpscffzwvIwAAAAAA7mAYht0deO3atdOsWbN0+fJllSlTRtu3b1fDhg1Vq1YtB6YEAOQ3hQoVUsuWLfXggw+qTZs22rx5c5rDoNnuCC9cuLDdNEkqWLCgLBaL3bSky2aVOw2hJjEsVW4ga+5wle+q5DpZ3eHzzwhHD6EGAICj5VmBSFxcnCZNmqQBAwaoaNGimjBhgt544w1t3LgxW90UAwDgaBRAAgBcVXx8vDZv3qwzZ84oPj5eTzzxhOrWrauiRYtq+vTpmjBhgipVqqSrV69q/Pjxjo4LAMinKlSooOeee04XLlxIcxi0cuXKSZIiIiLspkmSv7+/ypYtq507dyZbtkyZMlnO5k5DqEmuM4SSq+SUyJpbyJrzXCWn5NpDqAEA4Gh5ViBy/vx5ffLJJypbtqwkadKkSXriiSd0+PBhtWzZMq9iAACQoyiABAC4sgIFCqhFixZq0aJFsmm1atXS6NGjHZAKAIDkPDw8VLt2bXl7e6c6DFrJkiVVq1YtBQYG6tFHH5WUOAxBQECASpcuraZNm2rSpEm6deuWOfRMUFCQmjdv7pDXBAAAAAB5Lc8KRO655x67x76+vvL19VXFihWztD7DMBQVFZXlPBaLxakqYqOjo9Psvs0VsjpbTsl1srrD5y+5TlZnyym5Tta0Pv/0GIbhlhX6FEACAAAAQM6KjIzUunXr1LZtWxUtWlR//fWXvLy8VK9ePdWsWTPNYdD69OmjtWvXqlOnTpKkrVu3qn///pISr082aNBAO3bsUKtWrXT58mVdunRJL730ksNeKwAAAADkpTwrELnT2bNnVaNGDT3wwANZWt5qterkyZNZ3r63t7dq1qyZ5eVzWlBQkKKjo1Oc5ipZnS2n5DpZ3eHzl1wnq7PllFwna1qff0Z4eXnlYBrnQAFk2tyhqExynazOllNynazu8PlLrpPV2XJKWS+CdNcCSAAAHCk2NlZLly7V1KlTVa9ePT322GMaPny4JKU7DFrr1q0VGhqqMWPGSJKefvpptWrVypw+duxYjRs3TufOnVNISIhmzpypokWL5u0LBAAAAAAHcViByJw5czRy5MgsL+/p6akaNWpkeXlnu4hbtWrVNC/0O5PUsjpbTsl1srrD5y+5TlZnyym5Tta0Pv/0BAYG5nAa50QBpD13KCqTXCers+WUXCerO3z+kutkdbacUvaKIN2xABIAAEcqWbKk1q9fn+r09IZB69WrV6rTSpQooS+++CJb+QAAAADAVTmkQGTlypVq1aqVqlSpkuV1WCwW+fj45FwoB3O2OyjTQtac5yo5JbLmFlfJmp2czlbsklsogLTnDkVlkutkdbackutkdYfPX3KdrM6WU8p6EWR+KYAEAAAAAAAA4PryvEBk69at8vX1VcOGDfN60wAA5CoKIJNzleIniay5xVWyukpOiay5JatZnbHYBQAAAAAAAABS4pGXG/vll1909uxZ3X///fr33391+vRpTZs2LS8jAACQKyiABAAAAAAAAAAAgDPLsx5E1q9fr6FDhyo+Pl7jxo0znx88eHBeRQAAIFf88ssvCg4OVsuWLfXvv/8qKipKv/76q9566y1HRwMAAAAAAAAAAAAk5WGBSJs2bdSmTZu82hwAAHmCAkgAAAAAAAAAAAC4gjwrEAEAwB1RAAkAAAAAAAAAAABX4OHoAAAAAAAAAAAAAAAAAMhdFIgAAAAAAAAAAAAAAAC4OQpEAAAAAAAAAAAAAAAA3BwFIgAAAAAAAAAAAAAAAG6OAhEAAAAAAAAAAAAAAAA3R4EIAAAAAAAAAAAAAACAm6NABAAAAAAAAAAAAAAAwM1RIAIAAAAAAAAAAAAAAODmKBABAAAAAAAAAAAAAABwcxSIAAAAAAAAAAAAAAAAuDkKRAAAAAAAAAAAAAAAANwcBSIAAAAAAAAAAAAAAABujgIRAAAAAAAAAAAAAAAAN0eBCAAAAAAAAAAAAAAAgJujQAQAAAAAAAAAAAAAAMDNUSACAAAAAAAAAAAAAADg5igQAQAAAAAAAAAAAAAAcHMUiAAAAAAAAAAAAAAAALg5CkQAAAAAAAAAAAAAAADcHAUiAAAAAAAAAAAAAAAAbo4CEQAAAAAAAAAAAAAAADdHgQgAAAAAAAAAAAAAAICbo0AEAAAAAAAAAAAAAADAzVEgAgAAAAAAAAAAAAAA4OYoEAEAAAAAAAAAAAAAAHBzFIgAAAAAAAAAAAAAAAC4OQpEAAAAAAAAAAAAAAAA3BwFIgAAAAAAAAAAAAAAAG6OAhEAAAAAAAAAAAAAAAA3R4EIAAAAAAAAAAAAAACAm6NABAAAAAAAAAAAAAAAwM1RIAIAAAAAAAAAAAAAAODmKBABAAAAAAAAAAAAAABwcxSIAAAAAAAAAAAAAAAAuDkKRAAAAAAAAAAAAAAAANwcBSIAAAAAAAAAAAAAAABujgIRAAAAAAAAAAAAAAAAN0eBCAAAAAAAAAAAAAAAgJujQAQAAAAAAAAAAAAAAMDNUSACAAAAAAAAAAAAAADg5igQAQAAAAAAAAAAAAAAcHMUiAAAAAAAAAAAAAAAALg5CkQAAAAAAAAAAAAAAADcHAUiAAAAAAAAAAAAAAAAbo4CEQAAAAAAAAAAAAAAADdXMC83Fh8fr4kTJ8pisejKlStq37696tWrl5cRAADIcbRvAAB3RPsGAHBHtG8AAFeRmTbr6NGjWrZsmUqUKKG4uDgNGTJEXl5eeZwYAOAK8rRA5KuvvpKPj48GDBig2NhYtWnTRrNnz1alSpXyMgYAADmK9g0A4I5o3wAA7oj2DQDgKjLaZl26dEnvvPOO1qxZI19fX82fP1+jRo3Sp59+6qDkAABnlmdDzISHh2vBggVq3bq1JKlQoUJ69NFHNWvWrLyKAABAjqN9AwC4I9o3AIA7on0DALiKzLRZc+bMUe3ateXr6ytJat68uVasWKFLly7laWYAgGuwGIZh5MWGfvrpJ7377rs6duyY2a3Vd999p2XLlunXX3/N1LoOHTokwzDk6emZrUwWi0VXb8XJGp+QrfVkh2cBD5Us4qX0PgaLxSLdCpMSrHmULAUenlKRUmlmtVgsio6PUoIc955Kkoc85F3AJ/2sN61KcODn71HAQ953eWbo87fGRMtIcFxWi4eHPAt7ZyjrrVu3lODArB4eHipSpEi6n398fJQMw7HfVYvFQwUy8F2NiU9QQt7srlPkYbGocAGPdD//tFitVlksFj3yyCM5mMzxaN9S5m7tm+QcbZyrtG9Sxto42rfMyUj7JjlHG+cq7ZuU/TaO9i1jcqKNo33LJDdr3yTnaONcpX2TMtbG0b5lTkbat8T5HN/GcQ6XMtq31GWkjaN9yxx3a98k52jjXKV9k1znGiXtm3PKTJvVunVrNW/eXO+++6753MMPP6xPP/1Ubdu2zfS289s1Sqdo3ySX+Q2O9i13uNs1Smdo3xJzuMY1yrxu3/JsiJmQkBAVKVLEbsyzIkWKKDQ0NNPrslgsdv/PjpJFnGMMtgy9liKlcj9IBqSX1buATx4lSV+6We/K3gFOTsnI5+9Z2DsPkqQvI1mLFCmSB0nSl17WAi70XS1cIM86fEpTdva7FoslR/bbzob2LW3u1L5JztPGuUr7JqWflfYt8zKS1VnaOFdp36Ss73tp3zImp9o42rfMc6f2TXKeNs5V2jcp/ay0b5mXkazO0sZxDmeP9i196b4W2rdMc6f2TXKeNs5V2jfJda5R0r45l8y0WSEhISpevLjdc0WKFMlyDyL58hqlk7Rvkuv8Bkf7ljvc6Rqls7Rvkutco8yr9i3PCkQsFosKFy5s95zValXBgpmPUKdOnZyKBQBAttC+AQDcUU62bxJtHADAOdC+AQBcRWbbrEKFCmV43vTQvgGAe8uzcpiyZcvqxo0bds9FRkbK398/ryIAAJDjaN8AAO6I9g0A4I5o3wAAriIzbVa5cuV0/fp187FhGIqKiqJ9AwCkKM8KRBo0aCCLxaKgoCDzuaCgIDVq1CivIgAAkONo3wAA7oj2DQDgjmjfAACuIjNtVpMmTRQYGGg+Dg4OVkJCgurXr58nWQEAriXPCkRKlCihjh07asuWLZKk6OhoHTlyRD179syrCAAA5DjaNwCAO6J9AwC4I9o3AICrSKvNio6O1ldffaWrV69Kkrp06aJDhw4pJiZGkrR161Z17txZJUuWdFh+AIDzshiGYeTVxmJiYjR27Fj5+fnpypUreuGFF/Too4/m1eYBAMgVtG8AAHdE+wYAcEe0bwAAV5FamxUSEqKXX35ZU6dOVe3atSVJu3fv1saNG+Xv76/o6Gi988478vT0dPArAAA4ozwtEAEAAAAAAAAAAAAAAEDey7MhZgAAAAAAAAAAAAAAAOAYFIgAAAAAAAAAAAAAAAC4OQpEAAAAAAAAAAAAAAAA3BwFIgAAAAAAAAAAAAAAAG6OAhEAAAAAAAAAAAAAAAA3R4EIAAAAAAAAAAAAAACAm6NABAAAAAAAAAAAAAAAwM1RIAIAAAAAAAAAAAAAAODmKBABAAAAAAAAAAAAAABwcxSIAAAAAAAAAAAAAAAAuDkKRAAAAAAAAAAAAAAAANwcBSIAAAAAAAAAAAAAAABujgIRAAAAAAAAAAAAAAAAN0eBCAAAAAAAAAAAAAAAgJujQAQAAAAAAAAAAAAAAMDNUSACAAAAAAAAAAAAAADg5igQAQAAAAAAAAAAAAAAcHMUiAAAAAAAAAAAAAAAALg5CkQAAAAAAAAAAAAAAADcHAUiAHJEQkJCniwDAAAAAAAAAAAAAMg8CkQAZMv169c1ZswYbd68OdPL3rhxQ0OGDNE///yTC8kAAMif3nvvPT3yyCNatGiRo6NkWEREhNq3b69GjRrp8OHDjo4DAE7LVfeXhw8fVqNGjdS+fXtFREQ4Oo5LuXnzphYuXKg2bdpo2LBhjo4DAC5r8+bNqlevnnr06KG4uDhHxwEA5IHx48erTp06+vrrrx0dJdccOXJEw4cPV+3atfXvv/86Og5cREFHB0D+tmTJEo0YMSLNeR555BEtWbIkbwIhU44eParPP/9c77//vurWrZvp5YsXL673339f//nPf9S6dWu9+uqruZAScG/5bT8aGxurhx56KN35Nm3apMqVK+dBItdx8OBBde3aNdnztWvX1vLlyzO9vj/++ENdunRJ9nyDBg00b968rER0SQMHDlS7du301FNPOTqKJOnatWv64YcfJElLly5N8TN3Rvv27dOJEyckSRs2bFCdOnUcnAiujLYxZbSNmWe1WvXcc89p6dKl8vPzc3QcSa67v1y/fr2uXLmiK1euaP/+/WrRooWjI2VKcHCw5s2bp127dikkJEReXl4qX768GjRooKZNm+q+++7ToEGDNHXqVBUpUiRHthkTE6NRo0bpxx9/1K1btyRJtWrVypF1A/nR9OnTNWnSpGTP9+3bV0OGDMmx9Q0cOFD9+/fPUkZnc+bMGbVu3Trd+Y4fP66CBZ3/Z4YVK1bo+vXr2rt3r06fPq0HHnjA0ZEAwOm42/n0ggULFBUVpYULF2rw4MG5so2EhAT98MMPWrt2rU6ePKlbt26pZMmSuv/++9W4cWM1bdpUgYGB2rt3r4YOHZpj2922bZumTp2qY8eO5dg6kX/QgwgcqnPnzjp06JC++eYb3XXXXebzd911l0aPHq0dO3Zo/vz5eZYnMDCQCrsM2rt3r/r166cRI0ZkqTjEpkyZMpo5c6ZWrVqlzz//PAcTAvmDs+1HMyuz+91ChQrpr7/+0k8//aTHH3/cblqnTp20YcMG/fHHH/wAloJHH31UBw4c0LRp01S8eHHz+T///FP79u3L9PpmzZpl97hXr17asWOH5syZk92oLiM4OFi//vprnhfEREdH68CBAylO8/Pz0wsvvCAfHx917tw5T3OlJ62/9/r166tmzZoqXbq02rRpk8fJ4G5oG/8PbWP2/Pjjjzp//nyeX/x01f3l9u3bU532wgsvqHTp0rr//vuTfU+d3c8//6w2bdpo3759ev/99/Xbb7/p559/1rvvvqvTp0+rR48eql+/vvbs2ZPmetJ6f1JSuHBhDR8+XCtXrpTFYsnOSwAgqV+/ftq5c6eGDRtm9ze1ZMkS3bx5M1PriomJ0YIFC8zHXl5eGjNmjA4cOKA333wzxzI7WvXq1XXs2DGtXr1a9957r920/v3765dfftGhQ4dcojhEkl566SX5+vqqQYMGCggISHW+zO6vAcCduPr59J26d+8uHx8fde/ePdV50rrOlp7IyEj17NlTH3zwge69914tWrRI+/bt0+zZs/XAAw9o4sSJeuqpp9S3b98015OVDPXq1dPy5cv1/PPPZyk78jcKROBQFotFRYoU0dNPP63mzZubz3fv3l0dOnSQv7+/vLy88izP9OnTdeHChTzbnqs6fvy4+vfvr2HDhqlmzZrZXl+xYsU0adIkrVq1yqUOLgBn4Gz70czKyn7X09NT1atX14ABA8znypcvr5EjR+qee+5R0aJFczqmW7BYLPL19VWzZs2SFQ7MnDkzU+s6ffq0tm3bZj729vbWe++9J39/f3l45J/Dy/nz5ys+Pl779u3TqVOn8my769at0/79+1OdPm7cOB0+fNjpeg9J6++9ePHiWrNmjXbt2qWHH344b4PB7dA2JqJtzD5b0ePixYvztCt6V9xf3r59W5988kmq0x9++GHt2rVLa9eutStUdXZHjx7VkCFDVLp0aS1ZskTPPPOMSpYsqdKlS+upp57S3LlzMzTsS3rvT2p8fHxUrVo1+fr6ZiU+gCQKFiyoMmXKqGfPnnb7z8jIyEwPzbhq1Spdu3bNfFyvXj21a9dOvr6+blfQ5eXlpVq1aqlXr17mc3Xr1tXAgQNVpUqVHOs1KS80a9ZMBw4c0Lx581I9Fjxz5oxmz56dx8kAwHm4+vn0nQYNGqTDhw9r0KBBqc6T3nW2tLz//vvat2+fhg8fruHDh6tGjRoqWrSoatSoobfffltr1qzRPffck+56spLBx8dHHh4eLtOrJJxL/rmCD6dXpkwZ89/ly5fP8+2fOnVKP//8c55v19VER0dr0KBBqlChgl544YUcW2+VKlXUt29fjR07Nkt3sgNw/H40s7K73036esuWLZsTkfKNChUqSJL5g+Hu3bsz1R3hd999Z3chsHjx4vmqMESSrl+/rlWrVpmP86oXkaioqEwX9DgDjrPgKLSNyKpdu3bp77//liSFhYVpw4YNebJdV91fLl26VCEhIY6OkeOmTp2q27dvq2PHjipWrFiK8/Ts2VMdO3ZMcz3ZfX8KFy6c5WUBJFehQgW74sn58+crJiYmQ8vGx8drzpw5dsuXLFkyxzM6m6THGOXKlXNgktw1adIkGYbh6BgA4BRc7Xw6K7Jzne3o0aPasmWLvL291alTpxTnqVChgqZMmZLm8Xx2r/UVKlQoy8si/8pfV/Lh1JJ2R1igQIE83fatW7f03nvvKSEhIU+364omT56sc+fOqVevXjn+Y+Crr76qAgUK6JNPPlF8fHyOrhvIDxy5H82snNjvJn2NrtKlrbOwvV9JexLJ6InIxYsX9eOPP9otm9+KQ6TErqi9vb3Ni8EbNmzQlStXcn27n332mcv1dsZxFhyJthFZNWfOHLvu5/OiENBV95eBgYH6+uuvHR0jV/zxxx+SlO5n8uabb6baa0BOvD/Ovv8CXI2np6dq1qyphx56SJJ07do1rVixIkPL/vTTT/r333/z3flQfjjGWLNmjX755RdHxwAAp+FK59NZlZ3rbIcOHZKkdAsLq1atqlatWuVKBsl9PxvkLvc/egXSER4ertdff12nT592dBSnd+nSJS1atEgeHh5q2rRpjq+/aNGiaty4sc6dO6e1a9fm+PoBOAf2u86jR48eZreQmzdv1pkzZ9JdZs6cOfLw8FCPHj1yOZ3ziouL08KFC9W9e3e98sorkiSr1arFixfn2jbj4+P1+eefa/Xq1bm2jdzA3zuQMfytOJdTp05p9+7dGjFihOrVqydJ+vvvv7V3795c26arfgdOnTqlnj17KioqytFRcoXtYu/ixYt1+fLlVOerVKlSisOvuvv7A7i6fv36mf+eM2eOrFZrust89913uu+++9S4cePcjIY8tn79en300UeOjgEAyCM5cZ3Ndq4QExOjb775Js15W7RokSsZgKxyz3Jf5GsxMTH6/vvvtXHjRgUFBclqtapkyZKqXbu2unfvrscee8yc948//tDAgQPt7vh99dVXzX+PHj1aHTp0MB9brVatWLFC69ev1z///KPY2FhVqlRJzz77rHr37q277rrLLsuZM2e0fPlyrV27VpMnT9bjjz+uY8eOadq0afr9999VuHBhtWnTRoMHD5anp2eKr8dqtWr58uXasGGDAgMDFRMTo4oVK6ply5bq06eP2cX/66+/rm3btiVbvnXr1powYYL5eP/+/XavUZJWrlypBx98MN33dv78+YqNjVWdOnXk5+eX5rwbN27U7Nmz9ffffys2NtZu2t69e1Ndvm7duvr11181bdo0tW/fPl/chQE4m8zsR5PKyN99Zve7eWXHjh1avHixjh49quvXr6to0aKqXLmyWrdurW7dupkV88eOHdOLL76Y4jp++uknVa9e3XzcvXt3HThwwHzctGlTzZgxw3xsGIY2bNig1atX68SJE7p165bKlSunJk2a6PXXX1fp0qXt1h8SEqKVK1dqxYoVeuedd9S+fXtNmzZNixYtUtGiRfXVV1+pdu3amXrdpUuXVvv27bVs2TIlJCTou+++0+jRo1OdPzw8XKtWrVL79u2T5UtLSEiIZs+erR07digkJESFChVSzZo11bVr1xRPkCTpxx9/1IoVK3Ty5ElFRkbK19dX1atXV/v27VP8jly7dk1r1qzRsmXL1KZNGw0YMECXLl3S9OnTtWXLFkVHR6tBgwb66KOP5O/vn+HsKfnhhx9069YtvfLKK4qPj9esWbMUGxurJUuW6I033shU146HDh3SggULdPDgQYWHh6tYsWKqV6+e3njjDd13333ma+vdu7dOnDhhLjd16lRNnTpVktS+fXt9+eWXkqQbN27ohx9+0IoVK3T//febz2/evFlvvfVWihm2b99uDkURGxtr3klp89FHH6lbt26Scu84Kzg4WCtXrtTq1av17rvvprofCA4O1qJFi7R9+3ZdvHhR3t7euueee9SmTRt17NgxxbsmQkJCtGLFCq1YscJcd1BQkKZOnapdu3ZJkp5++ml98MEHdl2FA0nRNuaftlFK/JGwTp06evTRR9WjRw8z87x589SgQYMMryel1+Pv76+nnnpK/fv3N8+JcmJ/OWrUKH3//ffJMtx999369ddfzcc7duxQ37597eZZv3692VtKaGioZs2apV27dikkJEQeHh4qU6aMnnjiCfXu3VuVKlUyl1u0aJHGjBlj992+9957zX9v2bJFFStWlJTY3q1YsUI///yzNmzYYD5/pz/++EOLFy/W4cOHdeXKFRUvXlx16tRRp06d1LBhwxSXOXbsmJYtW6Yff/xR69evV8WKFbVnzx59++23OnbsmHx9ffXKK6+oX79+qfbykZratWtrz549unLlijp16qSxY8em+vf+wAMP2D3OzPsjJX5fVq9erdWrV5vXG2rWrJlq+w0g+5o1a6bq1avrzJkzunjxojZs2KD27dunOv+OHTt06tQpjR8/PlPbOXz4sL7//nvzmN/X11ePPfaY+vbtq1q1aiWbPyvHHTlxDTIvZWV/b3P8+HHNnj1bBw4c0LVr1yQl3uXu7e1tHp9YLBY9+uijmjRpkqTEnqB2796tFStWaOvWrfrrr7/M9b333nv64YcfzMcHDhww99cVKlTQ1q1bzWlZvd4JAPldfHy8Nm7cqJUrV+qff/7RjRs35O/vryeeeEKvvfaaqlWrlubyv/zyi5YsWaITJ07o5s2b8vDwkKenp7y9vc3fkCwWi/r06WPe1BYdHa2ff/5ZK1asUMGCBbVgwQJJmbvOlpak18+mTZum8PBwvf/++/L29k42b61atezOhbOS4X//+59mz56tnTt3mm1nixYtdPfdd6ebFUjGAJzE5MmTjYCAACMgIMBYtWpVltZx48YNo23btkZAQIAxe/ZsIyIiwvjf//5nDBs2zAgICDDuu+8+Y/v27eb8CQkJhtVqNfbs2WNue8+ePYbVajWsVquRkJBgznvlyhWjQ4cOxnvvvWf8/fffxs2bN409e/YYzz33nBEQEGA0b97cCA0NNQzDMI4cOWK88847xn333Weud9++fcbChQuNWrVqGY0bNzZq1aplTvvoo49SfD0XL140XnjhBeOll14yDh8+bNy6dcs4efKk0apVKyMgIMB47rnnjGvXrhmGYRhXr141Pv/8c3OdAQEBxo4dOwyr1Wq3zoSEBOPChQtGnTp1jCeeeMI4fPiw3etMTUJCgtG4cWMjICDA+PTTT9Ocd968ecZDDz1krFmzxoiIiDCCg4ON6dOnm6/56tWrqS77+++/230WADLOEftRm4z+3Wdmv5ue4OBgcx3dunXL0us1DMOYMmWKERAQYPTo0cMICgoywsPDjTVr1hgPP/ywERAQYAwaNMicNz4+3ti3b5/RpEkTc9u9evUywsPDk603NjbW+PLLL42AgADjs88+M27cuGFOi4yMNPr06WP06dPH+PPPP42bN28aR44cMbp06WIEBAQYDRo0ME6dOmUYhmEEBgYar7/+unH//ffbfb5ffPGF3T7/jTfeyPBrXrVqlREQEGAYhmGcP3/eXHetWrWMCxcupLrc5MmTjfvvv984f/68YRiGue2nnnoq1WU2b95sPPnkk8aiRYuMS5cuGWFhYcacOXOMBx980AgICDA++eSTZMvYvm+DBg0yQkJCjLCwMGPevHlmzq+//tqc98qVK8aIESOMRx55xMwzefJk4+DBg0aDBg2M+vXrG48++qg5rXXr1snaxsxISEgwnnvuOeOzzz4zn/vvf/9rrn/ZsmUZWk98fLzx5ZdfGnXq1DFWrFhhhIeHG1evXjVGjhxpfhabN28257f9jdi2M3HiRPO5+Ph4IzY21hg6dKjd+zB06FBz+du3bxuHDh0yWrZsaU7v06ePcfPmzWTZIiMjjUGDBhkPPvigsW3bNiMuLs4wjNw5zjpz5ozRr18/u2Om1PZfP/74o1G7dm1jyJAhRmBgoHHz5k1j+/btxrPPPmsEBAQYL774onlcZBiGcfbsWWP48OF2x1yrVq0yNm3aZDz88MNGo0aNjNq1a9v9LcP90DZmTX5sG21CQ0ONWrVqGZs2bTIMI/Hzse1n7r33XuPMmTMZWk9ERITx2muvGc8++6yxa9cu49atW0ZQUJD5ep588kmzPc2J/WVUVJSxYcMGu3Zg6tSpRmxsrF2uhIQE48qVK8azzz5rNGvWzPjnn3/M79iJEyeMxx57zKhbt66xbds24+bNm8axY8eMF1980QgICDDq1atnXLp0yVxXfHy8YbVajYkTJ5rbtGW2tbVbt2412rdvb/e5BAcHJ3u/bt++bYwZM8aoWbOm8e233xqXL182rl27ZixZssSoU6eOERAQYHz44YdGfHy8ucy2bduMfv36JVv3+PHjjZo1axpNmjSx+458++23Gfrskjpw4IDde37vvfcaw4YNM8/905KR98fm1q1bxmuvvWY88MADxty5c42rV68aoaGh5mux7TeStu0Asm7o0KFmO2k7NwoICDBatWqVZrvbrVs3o1mzZsbt27eNffv2pXjcfacJEyYYLVu2NLZu3WpEREQYQUFBxkcffWQe869du9Zu/swed+TUNcj0ZPT1picr+/ukli9fbtSsWdN46aWXjL/++ssIDw831q5da9f+bdq0ybh8+bJ5rrNixQq78yDbuXDSTFar1ejatasREBBgdO3a1dxX375925wvO9c7AcAV5cT5tGEYRnh4uNGzZ0+jfv36xk8//WTcuHHDCA4ONs8Na9WqZSxfvjzFZRMSEsw2cPjw4UZwcLARGhpqTJgwwcz28MMPG+fOnTOuXLlinv+MGjXKaNCgQarnx+ldZ8uoHj162LUvTZs2NX788ccMncdnJsPGjRuN2rVrG6+88orx559/GpGRkcbevXuN559/3njggQfSPNcCUsKt+XAr33zzjU6ePKlatWqpV69e8vX1VaVKlfT555+rbNmySkhIsBs72mKxqGDBgna9VHh4eKhgwYIqWLCgeXeR1WpV//79Vb16dY0dO1YBAQEqWrSoGjRooLlz56pw4cI6f/68hg4dKkmqUaOGJkyYoEGDBpnrXb58uY4cOaJNmzZp+/bt2r9/v+rUqSMpsQePsLAwu9dy8+ZN9ejRQzdv3tScOXP08MMPy8fHR/fdd5969+4tSfrnn3/MikI/Pz998MEHatKkibmOgICAZOOCWiwW+fr6Kjo6WgMGDNDDDz+cobuojh07ptDQUEmyuwvwTpGRkZowYYJefPFFtWvXTr6+vqpYsaLefPNN/fe//013O5UrVzb/nbRCH0DeyOx+VMrc331G97t5Jel+dPjw4apSpYqKFy+udu3aqXv37pKkDRs26NKlS2bWxx9/XBMnTjRfQ7ly5VS8ePFk6/by8pKHh4fKlSunYcOG2fUyNWzYMMXExOjbb7/VQw89pKJFi6p27dqaNWuWypYtq6tXr2rgwIGKj4+Xv7+/vvjiCw0cONBcfs+ePYqNjdVvv/2mVq1aydPTM8tDf919991q2bKlpMT2bs6cOSnOFx0drYULF6ply5YZrkz/66+/9M4772jMmDHq0qWLypQpo5IlS6pnz54aMmSIJGnJkiVat26ducz27dvNrhVHjhypsmXLqmTJknrttdfM8Trnz5+v+Ph4SZKvr6+GDx+uuXPnmus4fvy4Jk+erG+++UZ79+7V77//bn6egYGB2rJlS2beIjs7duzQ2bNn7YbY6dmzp/nv+fPnpzv2qCSNHz9ec+bM0VdffaUXX3xRxYsXl5+fn95//315enrKarXqv//9r+Li4iTJ/BuxSfp34+HhIS8vL40YMULr169P8e+oQIECqlOnjqZOnWp+dytUqJBijxlFihRRfHy8unfvriZNmph3GObGcValSpU0bdq0NHuukaR9+/Zp0KBBqlu3rsaNG6fq1aubQ9MtWLBAJUqU0NGjR9W7d2/zPStbtqw+//xzjR071lzPtm3btGrVKq1cuVI7d+7U77//rmeffVaStGvXLrs7CQEb2sb81TbOnz9fFSpU0DPPPCMp8fOx9eZhGEaKvXTcyXb++Ndff2nevHlq2LChfHx8VKVKFQ0YMEBS4vCdX3zxhbmN7O4vvb299dxzz+n99983n6tatao5lJyNxWKRn5+fbt26pQ8//FA1atQwv2MfffSRrl+/rhdffFFNmjRR0aJF9cADD5j70YiICLvuj20Zk+a2Zba1WQ0aNNDq1avVsWPHNN+z6dOna/bs2Ro4cKDZW0yJEiXUuXNnTZkyRVLiObXtPZMSe5+cMWOGOdybJE2cOFExMTHauXOntm3bph07dpjnmLNmzTKPHzLqscce06effmr2UGX8/14+nn32WY0bN07Xr19PddmMvD+2dQ4cOFB79+7Vl19+qR49esjPz0/+/v5699131adPnwwNewEga9q0aaPy5ctLSuyFI2mvS0n9+eefOnDggHr37p1ir3UpWbBggZYvX6758+frqaeekq+vr6pUqaKRI0eqefPmslqt+uCDDxQcHGwuk9njjpy4BpmXsrK/tzl69Kg+/vhjWSwWTZ48WbVq1VLx4sXVtm1bffzxx+Z8GzduVOnSpc1zneeee04//fSTnnjiiRQzFShQwO6Yy9YuFyxY0Pyss3u9EwDyK8Mw9J///Ed79uzR9OnT1apVK911112qWLGihg4dqjfeeENWq1UfffSRNm7cmGz5uXPnavXq1QoICNDnn3+uihUryt/fX++8847Zk2VUVJT279+vUqVKmec/gwYN0qZNm1SsWLEUc6V3nS2jvv76a7vewC5evKh3331XL774onbv3p3mshnNcPDgQQ0ePFiVK1fW3Llz9dBDD6lIkSKqX7++5s6dKx8fnwznBWwoEIFb+eeffyQp2cXIAgUKmDvpkJCQTK933bp1+vPPP9WrV69k00qXLq177rlHUmJXgufOnTOHfQn4/930SomFD+PGjTNPOosUKWL+oBQfH6/jx4/brXfKlCk6d+6c+vbtm+zHm0cffdQ8aUnaDbGUeBHXdvKyYsWKFF/Pxo0bVbRoUbVt2zZjb4ASC0Rs0vph8MyZM4qOjk7xfe7UqVO63fqXLl3abND++OOPDOcDkDOysh/N7t+9I/3zzz/mj/m+vr5205J2E2grkLN5+OGH1aZNG0mJ+9TIyMhk605ISNDGjRv1yiuv2B3s7927V5s2bdJrr72W7MKij4+PHn74YUlSUFCQ9u3bp6JFi8rPz0/NmjUz5wsMDNRHH32k8uXLa+LEiTp27Jg6deqUhXcgUdKxt1esWGF205vU8uXLFRERkaxb+rSMHj1a1atXT7E7/qQX55YsWWL++/Tp05IkT09Psz21sQ2HFhMTo/DwcHM+Ly8vsy2WpLi4OM2aNcscVsBisah///5m23n06NEMv4Y7zZ49Wy1btrTrEr5GjRpq1KiRpMTPZufOnWmu49SpU/ruu+9Uu3ZtPf3003bTChUqZH73IiMjFRMTk+FshQsXVvny5ZN9l5OqXr26uc1NmzaZxRRJXbt2Tdu3b082JF1uHGd5enqqYMGCqlmzZqrzxMXFafjw4TIMI8VjsTJlyuidd96RlFgcZLtgbutmNOnxmI+Pj7755huz2NXT01Ovv/66OT073w24L9rG/+PubWNkZKSWL1+unj172l2Q69Chg/n5r127VhEREWmuZ/HixTp48KA6d+6scuXK2U174IEHzC6H7zyXS0tG9pdSYlfEpUqVkpQ4dExKtm7dquLFi9vdXCD933f9zs+9atWq5oXVzO7nCxcuLElp5j59+rS+/fZbeXp6mkOaJdWwYUM999xzkhJ/bD1y5IgkmccJSY8BGjRooOHDh5td/JcqVUqdO3eWlHgTxrlz5zKVX5JefvllzZ071254nZiYGH333Xd65pln9N1336XYnmbUypUrtWPHDj300EPm60zqzu8jgJzl6elpV/CddOizpGbOnKnSpUtneNi3mzdvasKECXr++edVpkyZZNNt50hWq1UrV640n8/scUdOXIPMK1nd39vMnTtXCQkJqlKlijlEps3zzz9vFp4GBQXZTfP29pbFYjGH8MwKVz62AwBHWrp0qVmoaCtWTOqtt95ShQoVZBiGRowYoVu3bpnTDMMwb2R77LHHkh0T247zJens2bN20woXLqyiRYvaHcPnBj8/Py1atEhdunSxu7njr7/+Uq9evdSjRw+dOnUqy+uPi4vTsGHDZLVaNWjQoGTDWpcqVSpTv/MBNpxhwq106dJFtWvXVpcuXZJNsxVZZOXCzZo1ayRJvXv3VsOGDZP9l3QHb/thS5Ld3VqPP/54svUm7S3j5s2b5r/j4uLM4g7bD05JValSRYsXL9aHH36ojz76yG5a1apV7U6mUrowu2jRInXs2DHFsdBS8/fff5v/Tq3qUvq/E9gtW7Zo6tSpdnc6FShQwLxLNy22C5p3NuoAcl9W9qM58XfvKE888YQaNWqU4gWdpMUJKbUdb775pgoUKKCbN2+aY1gmtW3bNl2+fFkvvfSS3fO2NuXDDz9MsU357bffzHlTa1O6du1q9wNadu8uv++++9S4cWNJiT94zJ8/32767du3NW/ePD355JO6//77M7TO4OBgHTx4UGfPnk3xdSa9IJj0dbZo0UKPPfaY3Y/2NkkLJu/8TJK+P3Xq1El2x7Sfn595wTBpm5sZJ06c0P79+82evJK6sxeRtCxcuFCGYaTYxkuJRaIff/yx5s2bl2abmxrbj3GpsfV+cvXq1RR/PFy2bJmefvrpZH8TuXWclV7mLVu26OLFi5ISi2RT8sILL5gFpnf+PSb9LtSrVy/Z30tqx2OADW3j/3H3tnHFihXy8vJS+/bt7Z739vY2i02io6O1bNmyNNezaNEiSSmfyxUtWlRLly7VBx98oPHjx2c6Y3r7eC8vL7NHjW3btun8+fMp5rP1BpNUv379VLduXbNnsaRsn31W9/N3XshMasmSJbp9+7Zq1aqV6t1vSS/+prWfT+m8O+kNDjdu3Mhw5qQef/xx/fjjjxowYIBdxps3b2rcuHFq165dli78JiQk6Ntvv5WkZEWjNsWLF+eHRyCXvfTSSypRooSkxB909uzZYzf9zJkz2rJli1599dVk5xmp+eWXX3Tr1i2tWrUqxbYtaRuQ9JpbVo+5s3oNMi9ld39ve59sPRwmVaBAAVWpUkWSFBsbm+K602qL0uPKx3YA4Ei2c6O6deumON3Ly8ssvgwPD9eGDRvMadeuXTOL6lPa99eoUcP8d2r7/vTOn3KCt7e3PvnkEy1fvjxZEczevXvVoUMHjR8/XgkJCZle9/r16xUcHKzChQun2hNW0iJRIKMKpj8L4DqeeuopPfXUU+bjuLg4bdq0SWvWrNGhQ4ckKUPdvicVHx9v3k26YsWKFBuipJL+kJPeXT6p/dB17NgxRUVFSVKyinibRx55RI888kiK0958801t2LBBERERWrBggd58801z2uHDh3Xy5ElNnDgxzWx3st2pLSnZHd1JVa5cWfXq1dOBAwc0ZcoUrV27Vn379lX79u3l5eWlDz/8MN1t2QpXoqOjdf369TTvhAaQs7KyH82Jv3tHKV68uGbPnm333F9//aWVK1fa/RiV0gF81apV1bp1a61fv17z5s1T9+7d7fbrixYt0nPPPWfewWpz+PBhSdK0adPSHaol6f426Q9dGe3SODNef/117dixQ1Linc9Je7DasGGDLl68aDdUR3ps35cnn3xSI0aMSHPepK/t7rvv1sKFC+2m79+/X6tWrbLrmePOzyQj78ldd92lGzduZPnHrdmzZ6t+/fp2XUfaNGrUSAEBATp9+rR27dqlf/75x+6O5qQOHDggKfU2vmTJkuratWuWMkrpvxePPfaYatWqpePHj2v+/Pl23f7fvn1bS5cu1aRJk5ItlxvHWTZpHTPZuvm+6667Ur2o6+PjowceeEAHDhzQ5cuXdfbsWVWrVi3dddvWa5OdO8Dhvmgb80fbePv2bX3//ffq2rVrivuarl27as6cObJarVq4cKF69eqV4rnhpUuXzKKM1Pbz9913X5bvYs5ITxJdunTRzJkzFRsbqwULFth9386cOaMTJ05o+vTpyZZ766239NZbb5mPr127pnXr1mn9+vXmhdms7ufT+nxs+/mSJUumOo+t+DMuLk779+/P8LqlnNvPFypUSG+//bY6deqkKVOmaNWqVbp9+7akxPe1W7duWrRoke69994Mr/Ovv/7Sv//+K0nmD5spoQcRIHd5e3ure/fumjx5sqTEXkSS/ggze/ZsFS1aNMWijdTY2rY333wz3Tt7kxZ3ZPWYO6vXIPNSdvf3tmOA//3vfykua9snp7Yfzs6xgisf2wGAo5w9e9bsGSutfX+9evXMf+/fv98szk9aTJhS4bttvy/lzr4/sx566CEtXbpUv/zyi8aPH2/2XhgfH68ZM2boypUr6Q6xfCfbsDsVK1ZM9bdJzhWQFXxr4JYiIiI0adIkPf/88zp27Jj++9//qkWLFlla1/Xr180TJ09PT5UuXTrN/7JTjW5jG89bsm/kMqpatWpq3bq1JGnevHl2vYgsXLhQjRs3TvfC652SriO9uyUmTZpk3okeHBysjz/+WE899ZS+//77DL2epD2bZKZrfQA5J7P70ez+3TuaYRjauHGjXn75ZU2aNElPPvlkhg7Y33zzTXl4eJgFeTZBQUHavXt3ij/y28Z7LlCgQLptSl6OIVm3bl2z8PDGjRtavHixpMT3Zvbs2apTp44ee+yxDK/P9jpjYmLSfZ22nqOSun37tlasWKE2bdpo6dKl6tChg4YMGZIDrzRrLl68qI0bN+ro0aMp3gHYsGFDuwuVafUiYmvnk951ltdee+01SYl34e3du9d8fvPmzSpdurQ5nENKcvI4KyNsJ9TpnfDaho2RpMuXL+daHuRftI3u3Tb+/PPPunjxor7//vsU9/EdOnQwi2IuX76sn3/+OcX1ZPdcLif4+fmZw/2sWrXK7k7xjPQmefbsWQ0fPlzdunVTQkKCvvvuu1zrwSIqKsosPklrP+/p6Wl2Dx0WFpalu+9ySunSpTVy5EitX7/e/BuXEu/IHz58eKbW9ddff5n/zkwPnwByXrdu3cw2Zt++ffrzzz8lJe7Xf/jhB3Xp0iXZENBpsbVtCQkJ6bZtKd0YldfH3LktJ/b3tqKdyMjIZO3wzZs3zR8PM1PIkxmufmwHAHktaVFHWvv+1K7neHt7m9cq9+zZk2yYr2PHjklKvMkhpaEaHaVFixb68ccf9fHHH9u18atXr7a76SIjbMPC5UVPKMhfKBCB29mwYYOeffZZ/fvvv1q+fLmGDx9u18BkVtID/JMnT+ZExHQlvRMgpcrIjOjfv3+yC7NhYWH65ZdfUhznMz1JKy3TK9rw8/PTrFmzNH36dHOs6bCwMI0aNUpdu3ZNd8zupHcC5kTBDYDMycp+NLt/944UHBysLl26aPz48Ro2bJhmzZqlZ555JkMV5tWrV0+xIG/RokWqXbu2HnzwwWTL2IoC8qpNyYx+/fqZ/54/f75iY2O1bds2nT59Wn379s3UumztZ1a6Wz9+/Ljatm2rpUuXasKECZowYYLq16+f7aF0suP7779X1apVtWnTJq1duzbF/zZu3Gh2PfzDDz/o2rVrKa7L1s6ndudbXmjdurU5FnrSYpbUhh2wyenjrIyw9aoWGRmZZlFN0l7c6H0MOY220f3bxrlz5+rll1/Whg0bUt3Pf/fdd+b88+bNS3E9SX/Iyuq5XE6wDScWFRVlDl8aGRmp9evXp9pLldVq1YQJE9S2bVtVq1ZNP/zwg3r37p2sx5eclHSM8aS9VqbEtp8vWrRont0l9+6776Y6rVq1apo1a5aGDRtmPnf8+HGdOHEiw+u/fv26+e+sDn8DIGf4+vraDW8yY8YMSYn7ew8PD7PAOqNs50NZadscccyd23Jif9+jRw+zeOSLL77QwYMHZRiGwsPD9d///ldRUVEaMGBAqsMYZJcrH9sBgCPYrudIae/7k/b4d+dQy0OHDlWhQoUUGxurQYMGmb3vBQYGatSoUSpUqJC++uoru3Xklc2bN+vHH39McVrBggXVtWtXrVu3zm4onOXLl2dqG7bzBYZERk6jQAQuz1Z9LiX2jjF48GDVr19f48aNM3+kyY7ixYubJyObNm1Kc95Lly4pODg429tM2t3Wrl270pz3zz//TLFgo3r16mrVqpWk/7swu2zZMlWoUCHFsbDTk/SHloz26vHMM89ozZo1mjp1qtljyZEjR/Txxx+nuZztpNFisTikYQfym5zcj2b17z4vxcTEmD9WBQcH6+WXX9bp06e1YMGCVIfuSsudBXlRUVFau3Ztqj++2Ma2Tq9NiYmJydQPDDmhadOm5riVYWFhWrVqlWbOnKl77rlHTz/9dKbWZXudYWFh+uOPP9KcN+n0o0ePqkuXLoqOjtb3339vdxLlKJGRkVqxYoV69uyZ5p1/5cqVMy8qx8bGmr2w3MnWzqfXxsfExJh3LuY0T09P8zu6bds2nTt3TqdPn1ZQUJB5/HCn3DjOyohy5cpJSuyS88yZM6nOZyu8KViwoHnhGMgq2sb81Tbu27dPJ0+eVJ8+fdLczz/xxBPmmNLHjx/X77//nmxdmTmXO3PmjHmHeU6755571LBhQ0mJ3+H4+HitWbNGjz/+uCpUqJBsfsMwNHjwYH377bcaNmyY+vbtq4IFc39U4hIlSph3wwUGBqY5hI1tWtWqVXM9l825c+fS/SGzZ8+eev755+2WyaikvRGcPn060/kA5KwePXqY3bdv3bpVf/zxh5YtW6aOHTum2TV+SmzHCzt37lR0dHSa8yY9H3LUMXduuXnzpmJjY3Nkf1+iRAktWLBAjz/+uIoVK6Y+ffrokUceUaNGjXT58mVNmzZNb7/9du69mP/PFY7tAMCRbOfTSYfctA01k5479/0PP/yw5syZowoVKigmJkbPPvus6tSpoxdffFGVK1fW8uXL9eSTT+Zc+EyyDSmXmnLlymny5Mnmb4yZOVeQ/u984d9//7UrtgSyiwIRuLSEhAR9/fXXkhIr6caOHStJateuXY5tw8vLyxwf+ocfftDZs2dTnXf+/Pk5Usn3wAMPmHfnLV68ONWxQePj4zV16tRUu5dKemF23rx5WrZsmbp27ZqlO7CTjoec1p1NJ0+e1MyZM+2ea968udatW2dW8P/6669pFpnYGrqKFSvm6RhxQH6UE/vRnPi7z0vff/+92f37119/rWvXrumJJ57Ictfp1atXV8uWLSUlFuQtXLhQhQoVMp+7U+3atSVJe/fuTTamclKrVq3ShQsXspQpqywWi11PIRMnTtShQ4fUp0+fTLcdDz30kPnvSZMmpXoBMC4uzu7789lnn5knfLYxph1t6dKl8vb2NrvrT0vXrl3Ni8pLlixJsQ23fQcCAwO1c+fOVNe1cOHCXD3569SpkwoXLizDMPT9999r4cKF6ty5c4pDyeXWcVZGNGjQwPz3jh07Up3P1mPLk08+manuv4E70Tbmv7Zxzpw5atasmSpXrpzuvD179jT/nVIvIpUqVTKHTkurNylJmjBhQq4OJ2e72/3ChQvatGmTFi9erFdffTXFeXfs2KFffvlFktS2bdtcy3SnggULmkPYRURE6OjRo6nOa3svUytkzA0JCQnasmVLuvPZes2Rkt/1mJakY6X/8ssv6Q6dEx8fn+F1A8g8f39/s703DENvvPGGYmJi1KtXr0yvy9a23bx5U7Nnz051vtOnT5v7X0cec+eWSZMmKTY2Nsf29zExMSpevLjWrl2rQ4cO6bffftORI0e0bNkyNWvWLHdehFzv2A4AHCXp+fSDDz5o3gC8b9++VH/nSnrOlNK+PyIiQq1bt9aaNWv0559/atu2bTp06JCmT59u/nbnKFu3bk33GL569epmzsz2eGu7kS8+Pj7dGyokOXQoTrgWCkTg0tatW2deUDl//rxiY2MlKcW7sGzdFqd0QSVpEULSbq9sB/UvvPCCpMS7gd9+++0U1//nn3/qwIEDZheDUuZ2xkl/OCtatKh5UnPhwgWNGjUqxWW++uorPfDAA6mus0aNGub4pNOmTdPNmzfVoUOHDGdKKul2bN14peaHH35I9kOgj4+PBg0aJCnxfUnt7omEhATz/U36XgLIHTm1H83s3316+93cEh4ert27d5vd89ru1Ex6p7hN0qEs0tuf9+/fXxaLRREREZowYYJefvnlFH9kl/6vTbHdqZtS4WFwcLAWLlxoN659Utk92Lctn1LRxnPPPaeKFStKSrxAWaFCBbu7YlPKkNJ67rvvPvO7tX//fo0ZMybF+caNG2dXAJDVzySrbW56YmJiNG/evFQLJ+5UpkwZs7eVsLAwrVmzJtk8SX94++STT1L8ezt69Kh5p3dStr+dpH83tr9bG9vrS+91lihRwsyyZs0abdy40a5b7aRy8zjrzqx35n7xxRfNH1CXLVuW6vjef/31lyQlGyInM593ZuaF+6JtzF9t44kTJ7R9+/Y0h9dKqlmzZipdurSkxIuBKfVsZHs9kZGRGj58eIr7rYULF8rHx8euQCS7+8s7NW7c2PxcP/vsM3l6eqpevXopzpu094o7v+uGYZivIb39fNLCxqTtU1q5k773ixYtSjFfeHi4/v33XxUrVizZj6a5dQxgM2vWrHQLNm3TixYtqkcffdRuWlrvzyOPPGL2lPW///0v2Y+Pd+ZO+r0AkHUJCQmp7g/69Olj3uV748YNtWrVKsXe6dI7H2rZsqXZ7n3zzTcp/qgTExOjjz/+2Dwmz85xR27vC7Pi3Llzunz5slk4l939/ZUrV9S9e3c98sgj8vT0lIeHh4oXL24W6KcnvTbUtr9Ouq++ffu22QZm53onAOQXSc+nvby81KlTJ0mJBZM//PBDisvYrufUqVMn2e9de/fu1cCBA/Xcc89JSuwR19fXN8NDTqZ3jSwj19nScuHChQwNG2NrW5o0aZKpDEmvyU6YMCHFGxCSvjbOF5BRFIjAaSStHszITuzvv//WF198Yd6FlrS7qm+++ca8UHf27FkNHTrUvOsnIiJCsbGxWrZsmXnhMWl3jbY7eY8ePapZs2ZJkjp37mx2cX/mzBm98MILmjNnjo4dO6aDBw9qwoQJ6tmzp9577z27jEm7ok3vwq6tS2ebd99917xYuHTpUvXu3Vu//fabTpw4oZ9++kmvvPKKNm7cqN69e6e5XtuF2YSEBLVr1y7Ld9Q+9thj5gnX//73vzTn/eeffzRlypRkz9tOYGvVqmV2I32nCxcumI1f0h8MAaTPkfvRzP7dp7ffzYikrzcjvSzExcXp/ffft/vB3XZB/vDhw1q8eLEMw9CtW7e0YMECffDBB+Z8oaGhOnfunJYsWZLiuu+55x6zIM/DwyPVH9mlxB+W6tevLynxAlfHjh01ZcoUHTlyREeOHNGsWbP04osv6vXXX1ehQoXM5ZKenISGhqb7etNy9epVSSlfdCxQoIDdHXK9evVKsYt52zqkxPYupYuUw4cPN0/Y5s6dq1dffVW//PKLTp48qa1bt6pv377avXu3unTpYi5j+x5u3LjR/M6Fh4dr6tSpmjBhgjlfaGio/vzzT/3000/mPDbpXZS7s81Ny8yZM3XlypVMDbGT9GRv2rRpybbXtGlTc7i3CxcuqH379lq8eLGOHz+uPXv26IsvvlCXLl307rvvJutJy3anwf79+xUXF6erV68m68rY1ptZRl7na6+9JovFoqioKDVp0sS86/1OuXmclTRzSrn9/Pw0dOhQSYlFqt9++22yfAcPHtSpU6fUrl07c0gFm9z6bsA10DamP39+bhsNw9Do0aN11113pVo4cacCBQqY+5mEhASNGzcu2Tx9+/ZVmTJlJCUO49WlSxf9/PPPOnnypDZv3qw33nhDEydO1DvvvGO3XHb3l3eyWCxmjyFXr15NswjG9rlLicUkN27ckGEY2r9/v7p162ZeiAwNDZXVatVXX31lzp/0Ljhb7vnz59v1BpM0953fzSZNmpgXfNevX6+9e/cmy7d48WIlJCRo2LBh8vPzs5uWnfPujDh37pz+85//mON/38lWTCpJ7733nry9ve2mp/X+FCxYUO+//745fcKECRo/fryZ8/r16/r444/N7/ehQ4d0+PDhdLuyBpC2q1evpjrEV5UqVfTss8+aj/v165fifEl/oElpXWXKlNHrr78uKbHI4D//+Y+GDx+uPXv26Pjx41qzZo06dOigatWqqVatWpKyd9yR2/vCzB5T3bhxQ4MGDdIzzzxjPpfd/f38+fN15coVbdy4UXv37tU///yjs2fPKigoSP/73/906dKlNF970h6RU3oPbPvrwMBAs7376KOPzDYsO9c7AcAVZfd8Wkr8fcrWU+OECRNSLEZftGiRvLy8UrxResKECbp9+7YWLlyoI0eOKDAw0Nz3BwcH68qVK6neSCT9374/tbYvI9fZ0jNq1Cj9+uuvqU7/6aefdP78eVWuXNns5TGjGdq3b28W3Fy6dEldunTRoUOHzGV37dqlqVOnmo9Xr16t48ePmz2EAqmhQAROIygoyPz3Tz/9pNDQUMXFxZmV2nFxcbp+/br+/vtvTZ8+Xa+88oqqV69uXsgqU6aMedJx8eJFtW7dWvXq1VO7du304IMP6pVXXpGUeJJUv359nT17VtWqVZMkVatWzew+edGiRWrcuLEGDRpkjpHt7e2tb7/91hxm5erVqxozZoxefPFFde3aVTNnztR7771nXlg1DEOXL1/WihUrzNe0ePFihYWFKT4+XoZh6MaNG1q6dKk5fe3atfr333/Nk4qqVatqypQpZpHIrl279MYbb6h9+/Z69913FRISou+++y7dgo+AgADzxLZbt26Z/VhMRYsWNX/wysjYyNOmTdPgwYN19OhRRUZG6siRI/r0009VvHjxVHtEkWSe/BYsWNC8oAwgYxy5H5Uy93ef3n43s6/3n3/+0b59+xQVFWW+3tu3bysqKkr//vuvfv75Z7388svasWOHXVeFttckSZ9++qkeffRRPf7449q9e7fZHaIkffTRR3r77bfT3C/ZCvKaN2+eZpf8Hh4emjhxotnlcFRUlKZOnapOnTqpU6dO+uqrr9SpUye7ro0vX76s+fPnm+tYsWKF/vjjj0xVtEuJn93Bgwe1bNkySYlDyISGhiYr7njxxRdVqlQp+fn56cUXX7SbFh8fr4sXL2rMmDHmc9HR0Zo8ebLCwsLs7lxr0KCBRo0aZRaYHDhwQP/5z3/Url07vfnmmwoMDNQ333xjd0e57TO5ffu2+vfvr7p166phw4YKDQ3ViBEjzPn69OmjMWPG6JlnntHNmzft3p+NGzfqzJkz5olsTEyMNm/erIsXL0pKPOE6dOhQql1bSoknkIsWLdKMGTMkJV7ATOlu+qQSEhJ06dIl7dmzx3zu0qVLevPNN3Xs2DHz87JYLPr666/N78Dly5f16aefqkOHDurZs6cWLlyo999/P8Uukm3Fk8ePH1fjxo3VsmVLvfzyy+bnsGzZMvMC5r59+/THH3+k+TqrV69ujtWa1g+HuXmcdf36dS1YsMDc1urVq3XhwgW772Xnzp31n//8Rx4eHpoyZYrGjBmjCxcuKDIyUj///LPefvtttW3bVp999pld7vDwcLu7E1etWqWLFy+aFw9u3bqlhQsXmtN//fVX/fPPP3a9JMC10TbSNqbm4sWL+uSTT3TgwAHdunVLGzZsSPcHs7i4OJ05c0bHjx83n/vtt980YsQIBQcHm/stPz8/zZgxQyVLlpSU2NvkO++8o3bt2umtt97SoUOH9O2336p8+fJ268+J/eWd2rVrp+LFi6t48eJmzyYpadq0qfnD5K5du9SgQQPVrVtXQ4YM0cCBA80ukQ8cOKD69evbDSVXv359syB00KBBatSokfbu3avGjRsrPj5eISEhdj1qzZ8/XxEREXbb//LLL/XUU08pISFB/fv317JlyxQeHq6rV69qxowZ+uabb/TBBx+oY8eO5jIJCQkKDg7W+vXr7dYdHh5u9g5w7do1rVq1ypy+dOlSXblyJdM9zuzatUtt2rTRzJkzFRgYqMjISF28eFEbN25Ux44dderUKQ0ZMiTFQqi03h8pcXiapMVCM2bMUIMGDfT000+rUaNGqlChgrlPunbtmsaNG5fmsK8AUma7Frdu3Trt37/fLHKMiIhItk+wFXY0bdrUbigoKbEtOHXqlF3x3oEDB/TTTz/p5s2bdnfx9u/f37xz2jAMrV69Wj179lSHDh00bNgwlS5d2u5HqKwcd1StWjXb1yAzIukxxu+//67jx48rOjraPL6wWq26deuWzp8/r1WrVqlDhw4KDAy0KxCRsra/t7FdAz18+LB69Oih559/Xq1atVLLli3VvHlzNW7cWI888og6depkV6R4+/ZtnTlzxu7Hu++++y5Zu28714qNjVXLli3VqFEj1ahRw67oI6vXOwHAFWX3fFqSihQpojlz5qhy5coKCwtT165dtXPnTkVGRurcuXMaMmSITp48qTlz5pi9HyZl2/evXLlSnTp10nPPPWfu+5s1a6ZGjRqpbt26evvtt+16vo+Li9Nvv/2mwMBASYnnx5s3b052vpjWdbaMiouL09tvv60BAwZo+/btunTpkm7evKkTJ07o888/1/vvv6+AgADNmTMnxSFG08rg5eWlb775xvxtMigoSK+88ooaNWqkJ554QiNHjrS7drt69Wr9/PPPGe5dC/mXxaAfZThQSEiI/vrrL+3evTvVO8/S8t///teu4i4yMlJjx47V5s2bFRMTo3r16mnQoEEKCAhQYGCg+vbtK6vVqr59+yar1Dt69Kg+/vhjnT9/Xo8//rg++OCDZF1IRkVFae7cufr555/1v//9T4UKFVKdOnXUr18/c7xJKfGHqYEDB6aYefDgwbrnnnv0xhtvpDo96d0J//77r6ZPn66dO3cqPDxc5cqVU4sWLdS3b98Mj1f25Zdf6tSpUymOkZ0ZR44cUadOnXTXXXfpwIEDKXbjdfLkyWRdQHp4eKh06dJ66qmn1L9//zQvDo8ePVrz5s3T888/b3cBGkDKnGE/mtW/+4zsd++UkJCgnTt36vz58/rmm29S7FYvLffff7/Wrl1r99zatWs1c+ZM/fvvv2Ylt+3A+u2339bu3bvVuHFjffLJJ8nuYErq2rVratKkiebOnWvXJqTGarVqyZIlWrt2rc6ePSuLxaIHHnhAPXr0sLuItmfPHvXs2TPFddSrV8/uh6K0HDx4MNUfGatWraqNGzfaPTdz5kyzSCOpli1b2p0g3qlBgwbJ2pvTp09r5syZ2rdvnyIiIswLn2+++Way9zQhIUHz5s3T4sWLdfnyZd17771644039Mwzzyg2NlY9evTQ6dOn9dxzz2n48OGKjo5OtcepRo0aafLkyXrkkUdSnZ7aeOCNGjVKsSBkyJAh6tu3b4rLrF69WsOHD09xmpRYeJP0oqHVatXChQu1Zs0anTt3Tt7e3nrsscfUr18/ux/ekrp69ao++OAD7du3T9WqVdN7771nvv66deva3aFt88wzz2j69Omp5tq9e7cmT55sFg6lJjeOs1Laf9gMHz5cPXr0sHvu8OHDWrBggQ4ePKhr166pdOnSqlmzpjp37mwWutgcO3YsWYGTTadOndStWze1adMm1ekjR45M8/2A86JtpG1Mr208e/ZsimNbS4mFJqntg4cNG5bi0GE2Y8eOtRtG7Nq1a/r222+1efNmXb58WSVLltRTTz2lN998M9VzopzaXyY1YcIEJSQkaPDgwanOIyVebBw1apQOHz6swoULq3nz5nrnnXdUvHhxbdiwQSNGjFC5cuU0bNiwZL01rV69WpMmTVJMTIyef/55DR48WD4+Ppo3b55Gjx6d4vbWrl2r+++/3+65DRs2aNWqVTpx4oRiYmJUtmxZPf744+revbvuueceu3lnz56tsWPHprju8ePHKzw8PFnhYNLptrvY0/L6669r9OjRCg8P1549e7R3714dP37cvFPflq9bt27JfkROKrX3J6n9+/dr5syZ+vPPPxUfH68HH3xQb7zxhp544gk988wzuvfee9W3b1/VqVMn3dwAkps+fbomTZqU4rSUjv169+6tN9980679unLlitkTYGoGDhyY7Bzqt99+04IFC8yi8SpVqqhDhw7q2rVrsh9wMnvckZPXIO8UFRWlAwcO6O+//9a3336b6W7rmzVrpmnTpqU4LTP7exvDMPTpp59qx44d8vT01PXr1xUVFSWr1ZqsyMfT01PLly9XzZo1NWrUKH3//fcprvP33383h8CJj4/X2LFjtXLlSvn6+qp3797mOXR2r3cCgKvI6fNpm5iYGH3//ffauHGjzp07J4vFokqVKqlZs2Z65ZVXzOL6O0VGRqpHjx6KiYlRdHS0bty4oZiYGFmt1mTDxlSoUEHr169XkSJF1LZtW506dSrZ+u677z6tW7fOfJzWdbb0bN68WWfPntUrr7yiQ4cOadeuXTp48KAuXLigW7duqWjRogoICFDr1q3VsWPHVIdczUiGyMhIzZo1Sz///LMuXryo0qVLq1WrVurfv782bdqkCRMm6LXXXlPnzp2zPIoA8hcKRAA3Fx0drSZNmuiLL75I8U7kzHr99de1bds2LViwIMPdMGdGixYtFBwcrA0bNtjdfQkAzm7GjBn66aef7E4yAADIz2gbAQBATjl69KhGjBihuXPnpnjT3O3bt3Xjxg39/fff+uijj/Tss8/aDeMFAHA9S5cu1Z49ezRp0iRZLJZk0+Pi4hQeHq69e/fqo48+0tSpU+2GfgaQMoaYAdzcmjVrVLRoUT311FM5sr5BgwbJy8tLK1euzJH1JXXkyBGdO3dOr732GsUhAFxKXFycFi9enK2hvAAAcCe0jQAAIKcEBwerb9++6tOnT6o9KhcsWFB+fn5q0KCBnnnmmRR/SAQAuI7Nmzdr5MiR+uijj1Ldp3t5ecnf31/t2rVTjRo18jgh4LooEAHc2PXr1zVjxgz17t1bBQoUyJF13nvvvRo5cqQ2bNiQYhdd2TFx4kQFBATo3XffzdH1AkBuW7BggSwWi1544QVHRwEAwCnQNgIAgJzy8ccfKyIiItXu+ZMKCwvTpk2b9Pzzz+dBMgBAboiMjNQHH3wgDw+PDP229fvvvys8PFz169fPg3SA6yvo6AAAcs7HH3+sdevWqXr16nriiSe0bds2eXt7q1OnTjm6nfbt2+vUqVN67733tGLFChUuXDjb61yzZo3Onz+vxYsXZ+hkDwAc5eTJk3rjjTcUExOjJ598Uj4+Plq5cqVGjx6tQoUKOToeAAB5jrYRAADkppMnT0qS3n//ffXr10+tWrVS5cqV7ea5efOmtmzZokmTJunVV1/V/fff74ioAIAcEBISooiICElSp06d1L9/fzVu3FglS5ZMNt+6des0b948TZ06lfNPIIMshmEYjg4BIGfUrVtXN2/eNB/7+/tr4cKFuvvuu3N8W4ZhaOrUqTpy5IimT5+erYZ39+7dGjdunCZPnpwrWQEgJ82bN0+jR4+2e65fv34aPHiwgxIBAOBYtI0AHC0+Pl4TJ06UxWLRlStX1L59e9WrVy/FeY8ePaply5apRIkSiouL05AhQ7hRBXBye/bs0fvvv68rV66Yz3l7e8vPz0+enp6KjIxUWFiYSpQooZEjR+rZZ591YFoAQE6YPXu2Jk2apNjYWPM5X19fFStWTBaLRREREbpx44buvfdeffXVVwoICHBgWsC1UCACuJE1a9Zo3Lhx8vDw0NNPP63//Oc/KlWqVK5u87ffftOaNWs0bNgwlS9fPlPLJiQkaMaMGQoJCdGwYcPk4+OTSykBIOdcv35dAwcO1JEjR1S9enX17dtXLVu2dHQsAAAchrYRgKONGTNGPj4+GjBggGJjY9WmTRvNnj1blSpVspvv0qVLeuWVV7RmzRr5+vpq/vz5Onv2rD799FMHJQeQUdHR0Vq3bp22bNmiv//+W9euXZOHh4dKlCihmjVrqnHjxmrXrp28vb0dHRUAkEMuXbqkFStWaM+ePTpz5owiIyNVqFAhlS5dWrVr11aLFi309NNPy2KxODoq4FJcskDk8OHDMgxDnp6ejo4CQFJsbKxCQ0OTde2YnqioKIWFhdFrSD5htVplsVhUp04dR0dxWrRvAOB6aN8yhjYOAFyPq7Rx4eHhevLJJ80hdyVp+PDh8vT01MiRI+3mHT16tC5fvqwJEyZIki5evKhmzZrpt99+k7+/f6a3TfsGAK7HVdo3R6J9AwDXk5n2rWAe5MlxhmHIBetaALdVqFChTBeHSJKPjw/FIfkI++300b4BgOthv50xtHEA4HpcZb+9d+9eWa1Wu95CqlevrmXLliWbd+fOnWrevLn5uHz58vLy8tK+ffvUtm3bTG/b1r7FxcVlLbzkdMPbpPVanClreu85WbPGXbI6U07JdbK6y+eP7OP8DQBcT2b22y5ZIGKrWnzwwQcdnAQAkFHHjh1zdASnR/sGAK6H9i1jaOMAwPW4ShsXEhKiIkWK2P1wWaRIEYWGhqY4b/Hixe2eK1KkiC5dupSlbXt6esowDNWoUSNLy1ssFnl7e2vg0sMKvByZpXXklBplimpS5zqKjo5O8eKyLatW9ZHCTjsg4f9XKkDq+F2qOaX/y7rlygZFWK/mccD/U9yzpJ4p/XyGsv4657iuhdzK44T/x69cETXvVStDWU9v+1VREdfyOOH/8Snup4CmzdP9rq5atUphYWEOSPh/SpUqpY4dO6abNTT0J1mtjntPPT39VLZs6wx9/ttDw3U97nYeJ/w/vl4F1aRsiTSzpicwMJDhKNLB+RsAuJ7MnL+5ZIEIAAAAAAAAAFgsFhUuXNjuOavVqoIFU77sWahQoQzPm9Ht+/j4ZHl5SQq8HKnjF29kax05xdvbO+0Zwk5LIX/mTZg0pJtTUoT1qsLiLudBmrRlJOu1kFsKC3ZskZCUsaxREdd066pjCy+k9LOGhYUpJCQkj9KkLb2sVus1xca6xnf1etxtXY215kGatGUka2ooDgEA5Hcejg4AAAAAAAAAAFlRtmxZ3bhhX1wRGRkpf3//ZPOWK1dO169fNx8bhqGoqKgU5wUAAAAAd0SBCAAAAAAAAACX1KBBA1ksFgUFBZnPBQUFqVGjRsnmbdKkiQIDA83HwcHBSkhIUP369fMkKwAAAAA4GgUiAAAAAAAAAFxSiRIl1LFjR23ZskWSFB0drSNHjqhnz56Kjo7WV199patXr0qSunTpokOHDikmJkaStHXrVnXu3FklS5Z0WH4AAAAAyEtZH2ATAAAAAAAAABxs2LBhGjt2rKZOnaorV67oyy+/VIUKFRQSEqJ169apefPmKlmypCpVqqTPP/9co0aNkr+/v6KjozV8+HBHxwcAAACAPEOBCAAAAAAAAACXVbhwYX388cfJni9Xrpx27txp91zDhg3VsGHDvIoGAAAAAE6FIWYAAAAAAAAAAAAAAADcHAUiAAAAAAAAAAAAAAAAbo4CEQAAAAAAAAAAAAAAADdHgQgAAAAAAAAAAAAAAICbo0AEAAAAAAAAAAAAAADg/7F3/9FR1fe+/197yCSZDDUkMSQBBoxE1EQBhVIwlGhrroVj/MWl5bhaIwSLh8oXlSwEjqJwiWKgAjagGEE5dHnLFbUcrD9OgVuMB9FyJNCjII2OHJCEn0k4MQMZMvP9g8Vcx/wAJpPM3pPnY62u5f7sz2d/XvPDjpl5788nylEgAgAAAAAAAAAAAAAAEOUoEAEAAAAAAAAAAAAAAIhyFIgAAAAAAAAAAAAAAABEOQpEAAAAAAAAAAAAAAAAohwFIgAAAAAAAAAAAAAAAFGOAhEAAAAAAAAAAAAAAIAoR4EIAJicz++PdARJ5skBIHr4fb5IR5BknhwAgOhgps8VM2UBAAAAADMyy99NZsmB6BcT6QAAgPbZDEPbampV33Q2YhkSY2OUl54UsfkBRCfDZtP+v/xZjXUnI5YhoVeyBt2cH7H5AQDRxwyfbxKfcQAAAABwMczwNxx/v6ErUSACABZQ33RWJ854Ix0DAMKuse6kvj1xPNIxAAAIKz7fAAAAAMA6+BsO3QlbzAAAAAAAAAAAAAAAAEQ5CkQAAAAAAAAAAAAAAACiHAUiAAAAAAAAAAAAAAAAUY4CEQAAAAAAAAAAAAAAgChHgQgAAAAAAAAAAAAAAECUo0AEAAAAAAAAAAAAAAAgylEgAgAAAAAAAAAAAAAAEOViIh0AAIBQNTc3a9myZTIMQ8eOHdPdd9+tESNGtNp3z549Wr9+vZKSktTU1KTi4mLFxsYGzh86dEirV6+Wz+fT/PnzA+0+n0/5+fk6dOhQ0PXGjh2rZcuWBXKMGzdOX3/9tSQpJydHb775ZngfLAAAAAAAAAAAANABFIgAACxryZIlSkhI0PTp03XmzBkVFBRo9erVcrlcQf2OHDmihx9+WG+99ZYSExO1du1alZSUBApBfD6fDh8+rMrKSl199dVBYz/88EPdfvvt+vGPf6yYmHMfm6+88orGjh0b6LNp0ybdd999yszMlKQW8wMAAABm5PP5ZLNFfnFZs+QAAAAAACDaUSACALCk2tparVu3Ths3bpQkxcXFadiwYSovL9eCBQuC+q5Zs0ZDhgxRYmKiJCk/P1+33nqrpk2bprS0NNlsNo0YMUJXXXVVi3kyMzM1ZsyYoDa3262bb75Z0rnVQ9avX685c+bouuuu44ttAAAAWIbNZtMbb7yh48ePRyzD5ZdfrvHjx1+wn9/vk2FE/r+1LyaHz++XzTC6KJH5cwAAgNDs27dPTz31lL744gtdccUVmj17tn70ox+12b+wsFA7duyQJKWmpmrr1q1BKygDACBRIAIAsKiPPvpIXq83aLWOgQMHav369S36VlRUKD8/P3Dcp08fxcbGaseOHbrzzjsD7a0Vd3x/NZBdu3bp2muvVVxcnCTpgw8+0L59+zRhwgT17dtXJSUlGjVqVMiPy+/3q7GxMeTxgFUYhiGHwxHpGAEej0d+vz/SMWBBfr9fBj++AbCw48ePq7q6OtIxLsgwbKqpeUde78mIZbDbk5WePu6C/WyGoW01tapvOtsFqVqXGBujvPSkiM0P4NIkZzi79fwAWmpqatLy5cs1ffp09ezZU0uXLtWDDz6o9957T2lpaS3679y5UzfeeKOmTp0qSerduzfFIQCAVlEgAgCwpOrqajmdzqA/dJxOp2pqalrt26tXr6A2p9OpI0eOXPK8f/rTn/QP//APgeNbbrlFu3bt0v79+7V06VJNnjxZ69at0/Dhwy/52pLk9Xq1d+/ekMYCVuJwOJSdnR3pGAFut1sejyfSMWBRfOkGAF3D6z2pM2eORjrGRalvOqsTZ7yRjgHAAnw+v/In50Q6hnw+v2w2Cp8Bszhw4ICefPJJpaenS5KWL1+um266Sbt27dLPfvazFv1ffPFFFRUV6Yc//GFgm2wAAFrDpwQAwJIMw1B8fHxQm9frbfMPoPMrflxM37b4fD598sknmj17dotzgwYN0sqVK/XQQw9p1apVIReI2O12ZWVlhTQWsBKzrbiQmZnJCiIISVVVVaQjAAAAwMLMUpRhlhwAzvn+VtiJiYlKTExUv379WvTdt2+fKisrdf/99ys5OVlPPPGExo278Kpn7WGVY3QXrHKMaHEpqxxTIAIAsKT09HSdOnUqqK2hoaHVJRYzMjJUX18fOD7/B05rfdvzySef6MYbb2yzsMQwDBUVFenxxx+/pOt+/xoJCQkhjwcQGjP9IQhrMVuxEwAAAAAg+nz11VfKysrSdddd1+LcNddco507d+rgwYN68cUX9cgjj6i5uVkFBQUhz8cqx+guWOUY0eRiVzmmQAQAYEmjRo2SYRhyu93KzMyUdO4/nkaPHt2ib15eXtAd3gcPHpTP59PIkSMvac533nlHd9xxR7t9evTooSFDhlzSdQEAAAAAAACgLWvWrNGCBQva7eNyuVRSUiKHw6EXXnihQwUirHKM7sJsN/6wyjFCdSmrHFMgAgCwpKSkJI0fP15btmzRlClT5PF4VFlZqVdffVUej0crVqzQpEmTlJKSonvvvVf33XefTp8+rfj4eG3dulUTJ05USkpK0DXb+w+vs2fPas+ePZo/f35Q+7vvvqsBAwYoOztb3377rV577TXNmjWrUx4zAAAAAAAAgO5lw4YNGjt2rK644oqL6v/AAw/o9ddf79CcrHIMRAarHCNUl1LsRIEIAMCyZs+erdLSUpWVlenYsWNatGiR+vbtq+rqam3cuFH5+flKSUmRy+XSwoULVVJSorS0NHk8Hs2ZMyfoWh988IF2794d+OcxY8YEnd++fXtg1ZLv2rVrl+bOnavrr79egwYN0syZM1sUngAAAAAAAADApdq6dasSExOVm5t70WN69OihwYMHd2IqAICVUSACALCs+Ph4zZs3r0V7RkaGKioqgtpyc3Pb/UNqzJgxLYpCLub83LlzNXfu3EtIDQAAAAAAAADte//993Xw4EH97Gc/06FDh9TY2Kg///nPmjx5ctDqyR9++KFiY2M1YsQIeb1elZWV6amnnop0fACASVEgAgAAAAAAAAAAAJjEpk2b9Nhjj6m5uVmLFy8OtM+cOVN1dXVBqyfv379fZWVluuqqq3T11Vdr0qRJyszMjGB6AICZUSACAAAAAAAAwJIOHDigVatWKTU1VbW1tSouLtZll13Wat9vvvlG8+bN06effqr09HTNmDFDP/vZz7o4MQAAF1ZQUKCCgoI2z3939eTJkydr8uTJXRELABAFbJEOAAAAAAAAAACXqrGxUUVFRZo6daoeeeQRjRkzRjNnzmyz/29/+1v94z/+o9atW6drr71Wjz76qD777LMuTAwAAAAAkRVSgcj+/ftVXFyslStXtnq+qalJ99xzj958882g9rfeeksLFy7UwoUL9fvf/z6UqQEAAAAAAABAGzZsUHJysgYMGCBJysvL08cff6zKysoWfWtrazVp0iTdeuutuu666/Tss88qOTlZO3bs6OLUAAAAQGh8Pl+kI0gyTw6E5pK3mGlsbFRdXZ127NgR+OPr+55//nkdOHAgqG3r1q168803tW7dOknSb37zGyUnJ2vcuHEhxAYAAAAAAADQnVVUVMjlcgWO7Xa7XC6Xtm/frqFDhwb1TUpKUlJSUlDfjIwM9evXr0MZ/H6/GhsbQxprGIYcDkeH5g83j8cjv9/fot1sWdvKKVkrq5VY5Xk1W07JOlm7y79Xfr9fhmGEOREAdA2bzaY33nhDx48fj1iGyy+/XOPHj4/Y/Oi4Sy4QSUhI0IgRI9osDvnTn/6k66+/Xu+8805Q++9+9zv9/Oc/Dxzn5+fr+eefp0AEAAAAAAAAQAulpaX64osv2jy/c+dOTZgwIajN6XSqpqbmgtc+deqUvF6vbrnllg5l9Hq92rt3b0hjHQ6HsrOzOzR/uLndbnk8nhbtZsvaVk7JWlmtxCrPq9lyStbJ2p3+vYqNjQ1jGgDoWsePH1d1dXWkY8DCLrlA5DybreXuNF999ZWqqqo0Y8YMPfvss4H248eP6/PPP1f//v0DbVdeeaXcbrdqamqUnp5+yfN3pDofAKyC6nwAAAAAQHc1a9asds8XFBQoLi4uqM3r9cput1/w2qtXr9YTTzzR4R8J7Xa7srKyQhprxr+RMzMz21zpwEzayilZK6uVWOV5NVtOyTpZu8u/V1VVVWFOg7Y0+/zqYYv8e8csOQDALEIuEPm+06dP65VXXtG8efNanDtfxdSrV69Am9PplKSQC0Q6Up0PAFZBdT4AAAAAAK1LT09XfX19UFtDQ4PS0tLaHbdt2zZlZmZq2LBhHc5gGIYSEhI6fB2zMNNNKu2xSk7JWlmtxErPq1WyWiWn1LGsZit2iWY9bIZm/GGXqo42RCxDVu+eWj7xhojNDwBmFLYCkeeff17Tpk1rtUL//AdufHx8oM3r9Z4LEBNahI5U5wOAVZjtDxaq8wEAAAAAZpGXl6e33347cNzU1KTDhw8rNze3zTGVlZWqqanRL37xi66ICABAt1Z1tEGfHT4V6Rjt8zVLth6RTnGOmbIAHeD3+2QYLXcjiQQzZTGLsBSIVFdX61/+5V/0f/7P/wm0NTQ0aP78+Xr//ff19NNPS5Lq6uqCzku6YEV/W6KtOh8ArIDqfAAAAACAWdx1110qLy/X0aNH1bt3b23btk25ubnKycmRdK4YpLKyUvfff78kaefOnfrLX/6iiRMn6tChQzpz5ozeeecdPfjggxe1LQ0AAIhCth7SG1Ok4/sjm+PyQdL4l9vt4vP7ZDPBD91myQHzMgybamrekdd7MqI57PZkpaePi2gGMwpLgUhqaqree++9oLZ7771XhYWFuuOOO5SSkqKcnBxVVVUFlm50u90aNGiQUlNTwxEBAAAAAAAAQDfSs2dPrVy5UkuXLpXL5dKJEyf03HPPBc7v3r1bmzZt0v33368dO3Zo6tSpOn36tMrLywN9fvGLX1AcAgBAd3d8v1S9O9IpLshm2LTl2Nuq856IWIZe9hT9NPX2iM0P6/B6T+rMmaORjnFBPr9fNhPc4NyVOUIuEPH7/YFtBmJiYtSvX7/gC8fEKCkpKVAAMmXKFP3xj38MLN+4detWTZs2LdTpAQAAAAAAAHRzOTk5euaZZ1o9V1hYqMLCQknSyJEjtXu3+X/4AQAAaE+d94SON5n/R3fAKmyGoW01tapvOhuxDImxMcpLT+qy+S65QKS5uVmbN2/Wl19+qebmZt10000aPnz4BceNGzdONTU1evbZZyVJP/nJTzR27NhLTwwAAAAAAAAAAAAAANBB9U1ndeKMN9IxuswlF4j06NFDt912m2677bZ2+23durVF2+TJky91OgAAAAAAAAAAAAAAAHSQLdIBAAAAAAAAAAAAAAAA0LkoEAEAAAAAAAAAAAAAAIhyFIgAAAAAAAAAAAAAAABEOQpEAAAAAAAAAAAAAAAAohwFIgAAAAAAAAAAAAAAAFEuJtIBAAAAAABdY9++fXrqqaf0xRdf6IorrtDs2bP1ox/9qNW+e/bs0fr165WUlKSmpiYVFxcrNja2ixMDAAAAAAAACBdWEAEAAACAbqCpqUnLly/X9OnT9eqrryoxMVEPPvigjhw50qLvkSNH9PDDD2vWrFkqLi5W3759VVJSEoHUAAAAAAAAAMKFAhEAAAAA6AYOHDigJ598Urm5uRoyZIiWL1+upqYm7dq1q0XfNWvWaMiQIUpMTJQk5efn6/XXX2+1mAQAAAAAAACANbDFDAAAAAB0A1dddVXQcWJiohITE9WvX78WfSsqKpSfnx847tOnj2JjY7Vjxw7deeedIc3v9/vV2NgY0ljAKgzDkMPhiHSMIB6PR36/v0W72bK2lVMia0e0l/VC/H6/DMMIcyIAAAAAQCRRIAIAAAAA3dBXX32lrKwsXXfddS3OVVdXq1evXkFtTqezQyuIeL1e7d27N+TxgBU4HA5lZ2dHOkYQt9stj8fTot1sWdvKKZG1I9rLejFiY2PDmAYAAAAAEGkUiAAAAABAN7RmzRotWLCgzfNxcXFBx16vVzExof8JabfblZWVFfJ4wArMuNpCZmZmmyuImElbOSWydkR7WS+kqqoqzGkAAAAAAJFGgQi6LZ/PL5st8l/cmCUHAAAAuo8NGzZo7NixuuKKK1o9n5GRofr6+sDx+e1h0tLSQp7TMAwlJCSEPB5AaMy03Ul7rJJT6j5ZzVbsAgAAAADoOApE0G3ZbIb+vOYznaz+NmIZkjOcyp+cE7H5AQAA0P1s3bpViYmJys3NbbNPXl5e0J3jBw8elM/n08iRI7siIgAAAAAAAIBOQIEIurWT1d/q+MGGSMcAAAAAusT777+vgwcP6mc/+5kOHTqkxsZG/fnPf9bkyZO1YsUKTZo0SSkpKbr33nt133336fTp04qPj9fWrVs1ceJEpaSkRPohAAAAAAAAAAgRBSIAAAAA0A1s2rRJjz32mJqbm7V48eJA+8yZM1VXV6eNGzcqPz9fKSkpcrlcWrhwoUpKSpSWliaPx6M5c+ZEMD0AAAAAAACAjqJABAAAAAC6gYKCAhUUFLR5vqKiIug4Nze33W1oAAAAAAAAAFiLLdIBAAAAAAAAAAAAAAAA0LkoEAEAAAAAAAAAAAAAAIhyFIgAAAAAAAAAAAAAAABEOQpEAAAAAAAAAAAAAAAAohwFIgAAAAAAAAAAAAAAAFGOAhEAAAAAAAAAAAAAAIAoR4EIAAAAAAAAAAAAAABAlKNABAAAAAAAAAAAAAAAIMrFRDoAAAAAAAAAAAAAzC0xNrI/KUV6fgAAogGfpgAAAAAAAAAAAGiTz+9XXnpSpGPI5/fLZhiRjgEAgGWxxQwAAAAAAAAAAADaZJaiDLPkAADAqigQAQAAAAAAAAAAAAAAiHJsMQMAsKzm5mYtW7ZMhmHo2LFjuvvuuzVixIhW++7Zs0fr169XUlKSmpqaVFxcrNjY2MD5Q4cOafXq1fL5fJo/f36L8StWrNDzzz8fON64caOuueYaSdK2bdu0ZcsWOZ1OORwOTZ8+XQZ3MwAAAAAAAAAAAMBEKBABAFjWkiVLlJCQoOnTp+vMmTMqKCjQ6tWr5XK5gvodOXJEDz/8sN566y0lJiZq7dq1KikpCRSC+Hw+HT58WJWVlbr66qtbzNPQ0KBDhw7plVdekSTFxcUFikM+//xzlZaW6o9//KPsdruefvpplZeX69e//nUnP3oAAAAAAAAAAADg4lEgAgCwpNraWq1bt04bN26UdK5oY9iwYSovL9eCBQuC+q5Zs0ZDhgxRYmKiJCk/P1+33nqrpk2bprS0NNlsNo0YMUJXXXVVq3OtXbtWWVlZGjx4sHr27Bl0buXKlbrllltkt9sD1542bZoKCwsVFxd3yY/L7/ersbHxkscBVmMYhhwOR6RjBHg8Hvn9/kjHgAX5/X5WjQIAAAAAAABgCRSIAAAs6aOPPpLX6w1aLWTgwIFav359i74VFRXKz88PHPfp00exsbHasWOH7rzzzkC7zWZrMbapqUn/+q//qgMHDmjp0qV64IEHNG3aNNntdvl8Pv37v/+7xowZE5Th1KlT+tvf/qbhw4df8uPyer3au3fvJY8DrMbhcCg7OzvSMQLcbrc8Hk+kY8CivrtlGQAAAAAAAACYFQUiAABLqq6ultPpDPpRzul0qqamptW+vXr1CmpzOp06cuTIBeeJjY3V+++/r7q6Or3xxhtavny5Tpw4oQULFqiurk6NjY1B13Y6nZJ0Uddujd1uV1ZWVkhjASsx24oLmZmZrCCCkFRVVUU6AgAA3dqBAwe0atUqpaamqra2VsXFxbrssssuOO7dd9/V4sWLtXXr1i5ICQAAAADmQIEIAMCSDMNQfHx8UJvX61VMTOsfbd/f7qW9vq3p1auXioqK1L9/fz300EOaMWNGYMWR717b6/VK0iVd+7sMw1BCQkJIYwGEzkzb3cBazFbsBABAd9LY2KiioiKtXr1aAwYM0ObNmzVz5kyVl5e3O+7AgQNasWJFF6UEAAAAAPNouZY+AAAWkJ6erlOnTgW1NTQ0KC0trUXfjIwM1dfXB479fr8aGxtb7Xsh+fn5uuKKK/TNN98oKSlJDocj6NoNDQ2SFNK1AQAAAAAXb8OGDUpOTtaAAQMkSXl5efr4449VWVnZ5pgzZ86ovLxcv/zlL7soJQAAAACYByuIAAAsadSoUTIMQ263W5mZmZIkt9ut0aNHt+ibl5cXtAXAwYMH5fP5NHLkyJDm/sEPfqCBAwdKksaMGRN0bbfbrcTERF133XUhXRsAAAAAcHEqKirkcrkCx3a7XS6XS9u3b9fQoUNbHbN8+XJNnTpVf/3rX8OS4fwNCKEwDMN0K9l5PJ5Wt140W9a2ckrWymolVnlezZZTsk7WaHmvXojf72clSABAt0aBCADAkpKSkjR+/Hht2bJFU6ZMkcfjUWVlpV599VV5PB6tWLFCkyZNUkpKiu69917dd999On36tOLj47V161ZNnDhRKSkpQdds7Y/gr776Sv/5n/+p22+/XTabTX/4wx/0y1/+Uk6nU5I0efJkzZs3T4888ogMw9CWLVs0derUkLeYAQAAAACcU1paqi+++KLN8zt37tSECROC2pxOp2pqalrtv2nTJg0bNkwulytsBSJer1d79+4NaazD4VB2dnZYcoSL2+2Wx+Np0W62rG3llKyV1Uqs8ryaLadknazR8l69GLGxsZGOAABAxPDrFQDAsmbPnq3S0lKVlZXp2LFjWrRokfr27avq6mpt3LhR+fn5SklJkcvl0sKFC1VSUqK0tDR5PB7NmTMn6FoffPCBdu/eHfjnMWPGSJKOHTum0tJSrVixQkOGDNHtt98eOCdJQ4cO1ZQpU7Rw4UL17NlTqampKioq6ronAQAAAACi1KxZs9o9X1BQoLi4uKA2r9cru93eou+XX36p//qv/9JvfvObsGa02+3KysoKaawZ72DPzMxsc6UDM2krp2StrFZilefVbDkl62SNlvfqhXx3JWAAALojCkQAAJYVHx+vefPmtWjPyMhQRUVFUFtubq5yc3PbvNaYMWOCCj/O+9GPfqQPP/yw3Rx33HGH7rjjjotMDQAAAAAIh/T0dNXX1we1NTQ0KC0trUXf1atX67333tMrr7wi6VwhyZkzZzR8+HC9+OKLGj58eEgZDMNQQkJCSGPNyEzbXbTHKjkla2W1Eis9r1bJapWcHWW2whwAALoaBSIAAAAAAAAALCcvL09vv/124LipqUmHDx9u9eaA4uJiTZs2LXD8/vvva+3atXrttdeUmpraJXkBoDV2e3K3nh8AAHStkApE9u/fr5deeklXXnll0B9W69at0+rVq/Xtt99qzJgxevzxx5WUlBQ4/9Zbb+mzzz6TJF1xxRX65S9/2cH4AAAAAAAAALqju+66S+Xl5Tp69Kh69+6tbdu2KTc3Vzk5OZKkyspKVVZW6v7771dycrKSk//fj6BJSUmKiYlRv379IhUfAOT3+5SePi7SMeT3+2QYtkjHAAAAXeCSP/EbGxtVV1enHTt2qLm5OdD+wQcf6Ouvv9YLL7ygp556Sh9++KGeeOKJwPmtW7fqzTff1OOPP67HH39cH330kd55553wPAoAAAAAAAAA3UrPnj21cuVKLV26VCtXrtSOHTv03HPPBc7v3r1bmzZtimBCAGifWYoyzJIDAAB0vkteQSQhIUEjRozQgAEDWpw7XxBy7bXXqra2Vr/97W8D5373u9/p5z//eeA4Pz9fzz//vMaNC6061u/3q7GxMaSxgGEYptpT0ePxyO/3RzoGTCia3qt+v589PgEAAAAAYZWTk6Nnnnmm1XOFhYUqLCxs9dw999yje+65pzOjAQAAAIDphLTFjCTZbMEVpWPGjAk6drlcgSUajx8/rs8//1z9+/cPnL/yyivldrtVU1Oj9PT0S57f6/Vq7969ISQHJIfDoezs7EjHCHC73fJ4PJGOAROKtvdqbGxsGNMAAAAAAAAAAAAAuFghF4hcyM6dO1VUVCRJqq6uliT16tUrcN7pdEpSyAUidrtdWVlZHQ+KbslsqxhkZmaygghaFU3v1aqqqjCnAQAAAAAAAAAAAHCxOqVA5MiRI6qurtajjz4q6f/9wBkfHx/o4/V6zwWICS2CYRhKSEjoYFLAHMy0hQjQno68V81W7AIAAAAAAAAAAAB0J7YLd7k0TU1Neumll/TUU08FfgzMyMiQJNXV1QX6NTQ0SJLS0tLCHQEAAAAAAAAAAAAAAADfEdYCkebmZq1cuVL/9E//pJ49ewbaU1JSlJOTE7S9gNvt1qBBg5SamhrOCAAAAAAAAAAAAAAAAPiekAtE/H6//H5/4Njr9erpp5/W6NGjdfr0aR08eFDbt2/Xn/70J0nSlClTtGXLlkD/rVu3atq0aR2IDgAAAAAAAAAAAAAAgIsRc6kDmpubtXnzZn355Zdqbm7WTTfdpOHDh+uf/umfVFFRod///veBvjabTX/5y18kSePGjVNNTY2effZZSdJPfvITjR07NjyPAgAAAAAAAAAAAAAAAG265AKRHj166LbbbtNtt90W1P7yyy9fcOzkyZMvdToAAAAAAAAAAACgW9m3b5+eeuopffHFF7riiis0e/Zs/ehHP2q17549e7R+/XolJSWpqalJxcXFio2N7eLEAAArCHmLGQAAAAAAAAAAAADh1dTUpOXLl2v69Ol69dVXlZiYqAcffFBHjhxp0ffIkSN6+OGHNWvWLBUXF6tv374qKSmJQGoAgBVQIAIAAAAAAAAAAACYxIEDB/Tkk08qNzdXQ4YM0fLly9XU1KRdu3a16LtmzRoNGTJEiYmJkqT8/Hy9/vrrrRaTAABwyVvMAAAAAAAAAAAAAOgcV111VdBxYmKiEhMT1a9fvxZ9KyoqlJ+fHzju06ePYmNjtWPHDt15550hze/3+9XY2BjSWEkyDEMOhyPk8eHm8Xjk9/tbtJstp2SdrG3ltBorPa9WyWq2nJJ1snbk3yu/3y/DMC6qLwUiAAAAAAAAAAAAgEl99dVXysrK0nXXXdfiXHV1tXr16hXU5nQ6O7SCiNfr1d69e0Me73A4lJ2dHfL4cHO73fJ4PC3azZZTsk7WtnJajZWeV6tkNVtOyTpZO/rvVWxs7EX1o0AEAAAAAAAAAAAAMKk1a9ZowYIFbZ6Pi4sLOvZ6vYqJCf0nQLvdrqysrJDHX+xd7F0lMzOzzdUDzMYqWdvKaTVWel6tktVsOSXrZO3Iv1dVVVUX3ZcCEQAAAAAAAAAAAMCENmzYoLFjx+qKK65o9XxGRobq6+sDx+e3h0lLSwt5TsMwlJCQEPJ4szHTFhIXYpWsVslpNVZ6Xskafh3JeSnFLraQZwEAAAAAAAAAAADQKbZu3arExETl5ua22ScvLy/ozvGDBw/K5/Np5MiRXRERAGAxFIgAAAAAAAAAAAAAJvL+++/rq6++0rXXXqtDhw5p//79WrFihTwej5YsWaITJ05Iku699159+umnOn36tKRzRSUTJ05USkpKJOMDAEyKLWYAAAAAAAAAAAAAk9i0aZMee+wxNTc3a/HixYH2mTNnqq6uThs3blR+fr5SUlLkcrm0cOFClZSUKC0tTR6PR3PmzIlgegCAmVEgAgAAAAAAAAAAAJhEQUGBCgoK2jxfUVERdJybm9vuNjQAAJzHFjMAAAAAAAAAAAAAAABRjgIRAAAAAAAAAAAAAACAKEeBCAAAAAAAAAAAAAAAQJSjQAQAAAAAAAAAAAAAACDKUSACAAAAAAAAAAAAAAAQ5SgQAQAAAAAAAAAAAAAAiHIxkQ4AAAAAAABwIT6fXzabEekYpskBAADadvnll0c6gikyAAAAfB8FIgAAAAAAwPRsNkN/XvOZTlZ/G7EMyRlO5U/Oidj8AADgwnw+n8aPHx/pGJLOZbHZWMgdAACYBwUiAAAAAADAEk5Wf6vjBxsiHQMAAJiYmQoyzJQFAABAkvivEwAAAAAAAAAAAAAAgChHgQgAAAAAAAAAAAAAAECUo0AEAAAAAAAAAAAAAAAgylEgAgAAAAAAAAAAAAAAEOUoEAEAAAAAAAAAAAAAAIhyFIgAAAAAAAAAAAAAAABEuZhIBwAAAAAAAACAUBw4cECrVq1SamqqamtrVVxcrMsuu6zdMSdPntSGDRvUu3dv9enTRyNGjOiitAAAAAAQWRSIAAAAAAAAALCcxsZGFRUVafXq1RowYIA2b96smTNnqry8vM0x+/bt0+LFi1VaWqqUlJQuTAsAAAAAkccWMwAAAAAAhJOvOdIJzrmIHD6/rwuCXJhZcgCwlg0bNig5OVkDBgyQJOXl5enjjz9WZWVlq/2PHTum6dOnq6SkhOIQAAAAAN0SK4gAAAAAAEyv2edXD5sR6RgXl8PWQ3pjinR8f9eEas3lg6TxL1+wm82wacuxt1XnPdEFoVrXy56in6beHrH5AVhXRUWFXC5X4Nhut8vlcmn79u0aOnRoi/7PPfec+vfvrw0bNmjnzp3KycnRjBkzFBsbG3IGv9+vxsbGkMYahiGHwxHy3J3B4/HI7/e3aDdb1rZyStbKaiU8r4gWfr9fhhH5vysAAIgUCkQAAJbV3NysZcuWyTAMHTt2THfffXebe0fv2bNH69evV1JSkpqamlRcXBz0JeChQ4e0evVq+Xw+zZ8/P2jsu+++q2XLlunYsWMaPny4nnzySfXt2zcox7hx4/T1119LknJycvTmm2+G/wEDANCN9bAZmvGHXao62hCxDFm9e2r5xBsurvPx/VL17s4NFCZ13hM63nQ00jEAoIXS0lJ98cUXbZ7fuXOnJkyYENTmdDpVU1PTom9jY6PeeecdPfLII7r//vtVU1Oje+65R3V1dSopKQk5o9fr1d69e0Ma63A4lJ2dHfLcncHtdsvj8bRoN1vWtnJK1spqJTyviCYdKQwEAMDqKBABAFjWkiVLlJCQoOnTp+vMmTMqKCjQ6tWrg+4gk6QjR47o4Ycf1ltvvaXExEStXbtWJSUlgUIQn8+nw4cPq7KyUldffXXQ2P3792vz5s1avHixTp48qQULFuihhx7SW2+9FeizadMm3XfffcrMzJSkFvMDAIDwqDraoM8On4p0DABAF5k1a1a75wsKChQXFxfU5vV6ZbfbW/T9+uuvdfr0ad10002SpPT0dE2YMEEvv/yynnrqqVbHXAy73a6srKyQxprxDvbMzMw2VxAxk7ZyStbKaiU8r4gWVVVVkY4AdAs+n182E6wCapYcgJlQIAIAsKTa2lqtW7dOGzdulCTFxcVp2LBhKi8v14IFC4L6rlmzRkOGDFFiYqIkKT8/X7feequmTZumtLQ02Ww2jRgxQldddVWLeY4eParFixfLZrNJOrcM5YMPPqiTJ08qOTlZzc3NWr9+vebMmaPrrrsu0A8AAAAA0LnS09NVX18f1NbQ0KC0tLQWfc+ePSvp3AqQ51177bU6e/as/vu//1vJyckhZTAMQwkJCSGNNSMzbSHSHqvklKyV1Up4XhEqsxU7AdHKZjP05zWf6WT1txHLkJzhVP7knIjND5gVBSIAAEv66KOP5PV6g1brGDhwoNavX9+ib0VFhfLz8wPHffr0UWxsrHbs2KE777wz0N5accfo0aODjvv376/LLrtMl112mSTpgw8+0L59+zRhwgT17dtXJSUlGjVqVMiPqyP7VwNWwv7ViBbsXw0AQOTk5eXp7bffDhw3NTXp8OHDys3NbdF34MCBiouLk9vt1rXXXitJiomJUWpqasjFIQAAAGjbyepvdfxg5LaJBdA6CkQAAJZUXV0tp9MZtGdoW3tNV1dXq1evXkFtTqdTR44cueR5//rXv6qwsFAxMec+Qm+55Rbt2rVL+/fv19KlSzV58mStW7dOw4cPv+RrSx3bvxqwEvavRjRh/2oAACLjrrvuUnl5uY4eParevXtr27Ztys3NVU7OuTtFKysrVVlZqfvvv19Op1O/+tWvtGnTJo0bN06StHPnTk2ZMiWSDwEAAAAAuhQFIgAASzIMQ/Hx8UFtXq83ULjxfa3tS91W37Y0NjZq+/btWrx4cYtzgwYN0sqVK/XQQw9p1apVIReIdGT/asBKzLbiAvtXI1TsXw0AQOT07NlTK1eu1NKlS+VyuXTixAk999xzgfO7d+/Wpk2bdP/990uSHn30US1ZskSlpaVKSkpSz549VVhYGKH0AAAAAND1KBABAFhSenq6Tp06FdTW1l7TGRkZQftSn9/GpbW+7SkrK9M///M/tyg2Oc8wDBUVFenxxx+/pOt+/xrRtH81YBVm2u4G1mK2YicAALqbnJwcPfPMM62eKywsDCoA6dGjhx577LGuigYAAAAApmOLdAAAAEIxatQoGYYht9sdaHO73Ro9enSLvnl5eUF3eB88eFA+n08jR4686PnKy8s1fvz4CxaV9OjRQ0OGDLno6wIAAAAAAAAAAABdgQIRAIAlJSUlafz48dqyZYskyePxqLKyUpMmTZLH49GSJUt04sQJSdK9996rTz/9VKdPn5Ykbd26VRMnTlRKSkrQNf1+f6tbTCxfvlwDBw5UXFycDh06pE8//VR/+MMfJEnvvvuuPv/8c0nSt99+q9dee03FxcWd9rgBAAAAAAAAAACAULDFDADAsmbPnq3S0lKVlZXp2LFjWrRokfr27avq6mpt3LhR+fn5SklJkcvl0sKFC1VSUqK0tDR5PB7NmTMn6FoffPCBdu/eHfjnMWPGSJKefPLJQDHId61fv16StGvXLs2dO1fXX3+9Bg0apJkzZ7YoPAEAwEz279+vl156SVdeeaWmTZvWbt/CwkLt2LFDkpSamqqtW7cqNja2K2ICAAAAAAAACLOQCkTa+kLxwIEDWrVqlVJTU1VbW6vi4mJddtllgfNvvfWWPvvsM0nSFVdcoV/+8pcdjA8A6M7i4+M1b968Fu0ZGRmqqKgIasvNzVVubm6b1xozZkygKOS75s+fr/nz57c5bu7cuZo7d+4lpEYQX7Nk6xHpFObJAQCdrLGxUXV1ddqxY4cGDBjQbt+dO3fqxhtv1NSpUyVJvXv3pjgEAAAAAAAAsLBLLhBp6wvFxsZGFRUVafXq1RowYIA2b96smTNnqry8XNK55fzffPNNrVu3TpL0m9/8RsnJyRo3blyYHgoAALAcWw/pjSnS8f2Ry3D5IGn8y5GbHwC6UEJCgkaMGHHB4hBJevHFF1VUVKQf/vCHiolh8UkAAAAAAADA6i75W762vlDcsGGDkpOTA+15eXl69NFHVVlZqaFDh+p3v/udfv7znwf65+fn6/nnn6dABACA7u74fql6d6RTAEC3YrPZ2j2/b98+VVZW6v7771dycrKeeOKJDv/t5vf71djYGNJYwzDkcDg6NH84eTwe+f3+Vs+RNXTRkNVsOSXrZI2G11+yVtYL8fv9MgwjzIkAAAAAAJEU8m1g3/9CsaKiQi6XK3Bst9vlcrm0fft29evXT59//rn69+8fOH/llVfK7XarpqZG6enplzx/R75cBKLpCxtEt2h6r/LlIgAA1nHNNddo586dOnjwoF588UU98sgjam5uVkFBQcjX9Hq92rt3b0hjHQ6HsrOzQ5473NxutzweT6vnyBq6aMhqtpySdbJGw+svWSvrxWBrMQAAAACILmFbJ7i6urrFqiJOp1M1NTWqrq6WJPXq1SvonKSQC0Q68uUiEG1f2CB6Rdt7lS8XAQCwFpfLpZKSEjkcDr3wwgsdKhCx2+3KysoKaazZikwzMzPbXT3ATMjaOdrKaracknWyRsPrL1kr64VUVVWFOQ0AAAAAINLCViBiGIbi4uKC2rxer+x2e+CP4/j4+KBzkkLey7ojXy4C0fSFDaJbNL1X+XIRAADreuCBB/T666936BqGYSghISFMiSLLTCu8XQhZOwdZw88qOaXuk9Vsf48CAAAAADoubAUi6enpqq+vD2praGhQWlqaMjIyJEl1dXVB5yQpLS0tpPmi6ctFwEpfLqF748tFAAC6px49emjw4MGRjgEAAAAAAACgA2zhulBeXl7Q3eFNTU06fPiwcnNzlZKSopycnKDzbrdbgwYNUmpqargiAAAAdBqf3xfpCJLMkwOAtfn9/qBVwTwej5YsWaITJ05Ikj788EN98sknks6t/lhWVqannnoqElEBAAAAAAAAhEnIK4h8/wvFu+66S+Xl5Tp69Kh69+6tbdu2KTc3Vzk5OZKkKVOm6I9//KN+8YtfSJK2bt2qadOmdTA+AABA17AZNm059rbqvCcilqGXPUU/Tb09YvMDsL7m5mZt3rxZX375pZqbm3XTTTdp+PDhqqur08aNG5Wfn6+UlBTt379fZWVluuqqq3T11Vdr0qRJyszMjHR8AAAAAAAAAB1wyQUibX2h2LNnT61cuVJLly6Vy+XSiRMn9NxzzwXGjRs3TjU1NXr22WclST/5yU80duzY8D0SAACATlbnPaHjTUcjHQMAQtajRw/ddtttuu2224LaMzIyVFFRETiePHmyJk+e3NXxAAAAAAAAAHSiSy4QaesLRUnKycnRM8880+ZYvmAEAAAAAAAAAAAAAADoerZIBwAAAAAAAAAAAAAAAEDnokAEAAAAAAAAAAAAAAAgylEgAgAAAAAAAAAAAAAAEOUoEAEAAAAAAAAAAAAAAIhyFIgAAAAAAAAAAAAAAABEOQpEAAAAAAAAAAAAAAAAohwFIgAAAAAAAAAAAAAAAFGOAhEAAAAAAAAAAAAAAIAoR4EIAAAAAAAAAAAAAABAlKNABAAAAAAAAAAAAAAAIMpRIAIAAAAAAAAAAAAAABDlKBABAAAAAAAAAAAAAACIchSIAAAAAAAAAAAAAAAARDkKRAAAAAAAAAAAAAAAAKIcBSIAAAAAAAAAAAAAAABRjgIRAAAAAAAAAAAAAACAKEeBCAAAAAAAAAAAAAAAQJSjQAQAAAAAAAAAAAAAACDKUSACAAAAAAAAAAAAAAAQ5SgQAQAAAAAAAAAAAAAAiHIxkQ4AAAAAAAAAAKE4cOCAVq1apdTUVNXW1qq4uFiXXXZZq31PnjyppUuXyuVyyTAMnTp1SjNmzFBMDF+RAgAAAOgeWEEEAAAAAAAAgOU0NjaqqKhIU6dO1SOPPKIxY8Zo5syZbfZfuHChbrzxRv3617/WAw88oJiYGK1du7YLEwMAAABAZFEgAgAAAAAAAMByNmzYoOTkZA0YMECSlJeXp48//liVlZWt9t+/f7++/fbbwHF8fLxOnTrVFVEBAAjJ/v37VVxcrJUrV16wb2Fhoa6++mpdffXVGj16tJqamrogIQDAalg/EQAAAAAAAIDlVFRUyOVyBY7tdrtcLpe2b9+uoUOHtuh/xx13aNmyZcrJydFVV12lvXv36sknn+xQBr/fr8bGxpDGGoYhh8PRofnDzePxyO/3t2g3W9a2ckrWymolPK+IFn6/X4ZhRDrGRWlsbFRdXZ127NgRKIZsy86dO3XjjTdq6tSpkqTevXsrNja2K2ICACyGAhEAAAAAAAAAplNaWqovvviizfM7d+7UhAkTgtqcTqdqampa7f/AAw+otrZWv/rVrzRq1CgtW7ZMTqezQxm9Xq/27t0b0liHw6Hs7OwOzR9ubrdbHo+nRbvZsraVU7JWVivheUU0sUrhREJCgkaMGHHB4hBJevHFF1VUVKQf/vCHionhpz8AQNv4lAAAAADQ5Xx+v2wmuWvLTFkAAMD/M2vWrHbPFxQUKC4uLqjN6/XKbre32v/s2bNKSUnRqlWr9Nhjj2nmzJkqKyvr0A9pdrtdWVlZIY014x3smZmZba4gYiZt5ZSsldVKeF4RLaqqqiId4ZLZbLZ2z+/bt0+VlZW6//77lZycrCeeeELjxo3r0JwdWSFLss6qQ2bLKVkna7Ss5kXW0FnlvSpZJ2tHVki7lBWyKBABAAAA0OVshqFtNbWqbzob0RyJsTHKS0+KaAYAABCa9PR01dfXB7U1NDQoLS2t1f4lJSUaPXq0cnNz9dprr+nee+/VK6+8ogceeCDkDIZhKCEhIeTxZmOmL8jbY5WckrWyWgnPK0JltmKncLjmmmu0c+dOHTx4UC+++KIeeeQRNTc3q6CgIORrdmSFLMk6qw6ZLadknazRspoXWUNnlfeqZJ2sHV0h7WJXyKJABAAAAEBE1Ded1Ykz3kjHAAAAFpWXl6e33347cNzU1KTDhw8rNze31f4bN27Ur371K0lS//799eijj+rf/u3fOlQgAgCAWbhcLpWUlMjhcOiFF17oUIFIR1bIksxXiGOVFbIk62SNltW8yBo6q7xXJetk7cgKaZeyQhYFIgAAAAAAAAAs56677lJ5ebmOHj2q3r17a9u2bcrNzVVOTo4kqbKyMrDkviRdd911+s///E8NHDhQ0rkvhIcOHRqh9AAAdI4HHnhAr7/+eoeuwQpZkWOVrFbJKZG1s5A1/DqS81KKXSgQAQAAAAAAAGA5PXv21MqVK7V06VK5XC6dOHFCzz33XOD87t27tWnTpkCByJIlS/Tcc8+ptrZWTqdTtbW1mjJlSoTSAwDQOXr06KHBgwdHOgYAwKQoEAEAAAAAAABgSTk5OXrmmWdaPVdYWKjCwsLAcVpamp599tmuihadLh/UvecHgAjw+/1BWw54PB6tWLFCkyZNUkpKij788EPFxsZqxIgR8nq9Kisr01NPPRW5wAAAU6NABAAAAAAAAADQPl+zNP7lSKc4l8PWI9IpAKDTNTc3a/Pmzfryyy/V3Nysm266ScOHD1ddXZ02btyo/Px8paSkaP/+/SorK9NVV12lq6++WpMmTVJmZmak4wMATIoCEQCAZTU3N2vZsmUyDEPHjh3T3XffrREjRrTad8+ePVq/fr2SkpLU1NSk4uJixcbGBs4fOnRIq1evls/n0/z584PGnjx5Ur/97W+VmpqqI0eO6P/7//4/ZWRkBM5v27ZNW7ZskdPplMPh0PTp0y9pvzcAAAAAAEzPLEUZZskBAJ2sR48euu2223TbbbcFtWdkZKiioiJwPHnyZE2ePLmr4wEALMoW6QAAAIRqyZIlio2N1aOPPqqnnnpKjz/+uA4ePNii35EjR/Twww9r1qxZKi4uVt++fVVSUhI47/P5dPjwYVVWVurMmTNBY30+n37961/rjjvu0MMPP6zCwkJNnTpVXq9XkvT555+rtLRUTzzxhB577DE1NDSovLy8cx84AAAAAAAAAAAAcIlYQQQAYEm1tbVat26dNm7cKEmKi4vTsGHDVF5ergULFgT1XbNmjYYMGaLExERJUn5+vm699VZNmzZNaWlpstlsGjFihK666qoW8/zf//t/VVNTox/96EeSpGuuuUbffvut/u3f/k3/8A//oJUrV+qWW26R3W4PXHvatGkqLCxUXFzcJT8uv9+vxsbGSx5nRYZhyOFwRDpGgMfjCdrP9buslNUqeE67N7O9/lLo7wG/38+qUQAAAAAAAAAsgQIRAIAlffTRR/J6vXK5XIG2gQMHav369S36VlRUKD8/P3Dcp08fxcbGaseOHbrzzjsD7TZby4W1Kioq1K9fv6C2gQMHavv27Ro7dqz+/d//XWPGjAk6d+rUKf3tb3/T8OHDL/lxeb1e7d2795LHWZHD4VB2dnakYwS43W55PJ5Wz1kpq1XwnHZvZnv9pY69B767ZRkAAAAAAAAAmBUFIgAAS6qurpbT6Qz6Uc7pdKqmpqbVvr169QpqczqdOnLkyEXN09rYmpoa1dXVqbGxMei80+mUpIu6dmvsdruysrJCGms1ZrvjPjMzs90VRMykvaxWwXPavZnt9ZdCfw9UVVV1QhoAAAB0VC97SreeHwAAAGgNBSIAAEsyDEPx8fFBbV6vVzExrX+0fX+7l/b6Xsw8drs98APnd6/t9Xol6aKu3dZ8CQkJIY1Fx5htu4v2WCmrVfCcItT3gBmLXQAAALo7n9+nn6beHukY8vl9shktVysFAAAAIoX/OgUAWFJ6erpOnToV1NbQ0KC0tLQWfTMyMlRfXx849vv9amxsbLVva/N8d+z5eXr37q2kpCQ5HI6g8w0NDZJ0UdcGAAAAAADhZ5aiDLPkAAAAAM7jv1ABAJY0atQoGYYht9sdaHO73Ro9enSLvnl5eUFbABw8eFA+n08jR4684DzfH/v9ecaMGRN03u12KzExUdddd90lPyYAAAAAAAAAAACgs4S1QOTgwYOaM2eO1qxZoxUrVuill14KnDtw4IDmzp2rpUuXat68eS3u+gYA4FIkJSVp/Pjx2rJliyTJ4/GosrJSkyZNksfj0ZIlS3TixAlJ0r333qtPP/1Up0+fliRt3bpVEydOVEpK8H7Afr9ffr8/qC0vL0+9evXSnj17JEmff/65EhMT9dOf/lSSNHnyZP3lL38JjNuyZYumTp0a8hYzAAAAAAAAAAAAQGcI669XxcXFmjlzpkaMGCFJmjVrlt577z2NGTNGRUVFWr16tQYMGKDNmzdr5syZKi8vD+f0AIBuZvbs2SotLVVZWZmOHTumRYsWqW/fvqqurtbGjRuVn5+vlJQUuVwuLVy4UCUlJUpLS5PH49GcOXOCrvXBBx9o9+7dgX8eM2aMJMlms+mFF15QWVmZtm/frsOHD+vll19Wjx49JElDhw7VlClTtHDhQvXs2VOpqakqKirq2icCAAAAAAAAAAAAuICwFojs379f3377beA4Pj5ep06d0oYNG5ScnKwBAwZIOnc39qOPPqrKykoNHTo0pLn8fr8aGxvDERvdkGEYcjgckY4R4PF4WqxaAEjR9V71+/0yDCOseeLj4zVv3rwW7RkZGaqoqAhqy83NVW5ubpvXGjNmTKAo5Pv69eunRYsWtTn2jjvu0B133HGRqQEAAAAAAAAAAICuF9YCkYKCAs2fP1/9+/dXz5499e233+rOO+/UQw89JJfLFehnt9vlcrm0ffv2kAtEvF6v9u7dG6bk6G4cDoeys7MjHSPA7XbL4/FEOgZMKNreq7GxsWFMAwAAAAAAAAAAAOBihbVA5PHHH9ecOXP085//XHl5eVq8eLF69Oih6urqwOoh5zmdTtXU1IQ8l91uV1ZWVkcjo5sK9yoGHZWZmckKImhVNL1Xq6qqwpwGAAAAAAAAAAAAwMUKa4FIU1OTrrnmGo0dO1azZs3SggULNH/+fBmGobi4uKC+Xq9Xdrs95LkMw1BCQkJHIwOmYKYtRID2dOS9arZiFwAAAAAAAAAAAKA7sYXzYo888ohGjx6tW2+9Va+++qr+9V//Ve+8847S09NVX18f1LehoUFpaWnhnB4AAAAAAAAAAAAAAACtCFuBSG1trT744ANdeeWVkqTBgwdr8uTJ+o//+A/l5eUFbS3Q1NSkw4cPKzc3N1zTAwAAAAAAAAAAAAAAoA1hKxDp1auX+vbtq7/97W+BNsMwNHToUN11112qrq7W0aNHJUnbtm1Tbm6ucnJywjU9AAAAAAAAAAAAAAAA2hATrgsZhqHy8nK98MIL+uKLL2QYhhITE1VQUCBJWrlypZYuXSqXy6UTJ07oueeeC9fUAAAAAAAAAAAAAAAAaEfYCkQkaeDAgVqyZEmr53JycvTMM8+EczoAAAAAAAAAAAAAAABchLBtMQMAAAAAAAAAAAAAAABzokAEAAAAAAAAAAAAAAAgylEgAgBAlGn2+SMdQZJ5cgAAAAAAAAAAAECKiXQAAAAQXj1shmb8YZeqjjZELENW755aPvGGiM0PAAAAAAAAAACAYBSIAAAQhaqONuizw6ciHQMAAAAAAAAAAAAmQYEIAAAAEEX8fp8MI/I7SZolBwAAgNll9e4Z6QimyAAAAACg81EgAgAAAEQRw7CppuYdeb0nI5bBbk9Wevq4iM0PAABgFc0+v2m252z2+dXDZkQ6BgAAAIBORIEIAAAAEGW83pM6c+ZopGMAAADgAsxUkGGmLAAAAAA6B2s+AwAAAAAAAAAAAAAARDkKRAAAAAAAAAAAAAAAAKIcBSIAAAAAAAAAAAAAAABRjgIRAAAAAAAAAAAAAACAKEeBCAAAAAAAAAAAAAAAQJSLiXQAAAAAAAAAAAhFc3Oz3nvvPa1atUorV65Uv3792u3/8ssv6+TJk2poaNCoUaM0duzYLkoKAAAAAJFHgQgAAAAAAAAASzpw4IDq6+v1xRdfXLDv73//e1VVVWnRokXy+/2aMGGC0tPTdcMNN3RBUgAAAACIPLaYAQAAAAAAAGBJV155pX784x9fsF9zc7PKyso0btw4SZJhGLr55ptVVlbW2REBAAAAwDRYQQQAAAAAAACAZRmGccE+n332mWpra9W/f/9A28CBA/XSSy/p7NmziokJ7WtSv9+vxsbGkMYCVmIYhhwOR6RjBHg8Hvn9/kjHgAX5/f6L+twAACBaUSACAAAAAAAAwHRKS0vb3TpmxowZGjx48EVd6/Dhw5KkXr16BdqcTqfOnDmj2tpapaamhpTR6/Vq7969IY0FrMThcCg7OzvSMQLcbrc8Hk+kY8CiYmNjIx0BAICIoUAEAAAAAAAAgOnMmjUrbNc6f7d4fHx8oM3r9UpSyKuHSJLdbldWVlbHwgEWYLYVFzIzM1lBBCGpqqqKdAQAACKKAhEAAAAAAAAAUS0jI0OSVF9fHygSaWhokMPhUGJiYsjXNQxDCQkJYckI4OKZabsbWIvZip0AAOhqtkgHAAAAAAAAAIDOlJ2drdTUVP39738PtLndbo0cOVI2G1+RAgAAAOge+OsHAAAAAAAAgGWd32bi+9tNVFZW6tVXX5V0bhuZwsJCbdmyRZLk8/lUUVGhBx98sEuzAgAAAEAkUSACAAAAAN3I/v37VVxcrJUrV7bbb8+ePfrnf/5nLVmyRE8//bSampq6KCEAABfvm2++0ZtvvilJeuONN3T48OHAud27d2vTpk2B46KiIvXs2VPLli3T008/rWnTpmno0KFdHRkAAAAAIiYm0gEAAAAAAF2jsbFRdXV12rFjhwYMGNBmvyNHjujhhx/WW2+9pcTERK1du1YlJSWaP39+F6YFAODC+vbtqxkzZmjGjBktzhUWFqqwsDBwbLPZNHPmzK6MBwAAAACmwgoiAAAAANBNJCQkaMSIEe0Wh0jSmjVrNGTIECUmJkqS8vPz9frrr+vIkSNdERMAAAAAAABAJ2AFEQAAAADoZmy29u8VqKioUH5+fuC4T58+io2N1Y4dO3TnnXeGNKff71djY2NIYw3DkMPhCGlsZ/B4PPL7/a2eI2vooiGr2XJK1skaDa+/ZK2sF+L3+2UYRpgTAQAAAAAiiQIRAAAAAECQ6upq9erVK6jN6XR2aAURr9ervXv3hjTW4XAoOzs75LnDze12y+PxtHqOrKGLhqxmyylZJ2s0vP6StbJejNjY2DCmAQAAAABEGgUiAAAAAIAW4uLigo69Xq9iYkL/E9JutysrKyuksWa7gz0zM7Pd1QPMhKydo62sZsspWSdrNLz+krWyXkhVVVWY0wAAAAAAIo0CEQAAAABAkIyMDNXX1weOz28Pk5aWFvI1DcNQQkJCOOJFnJm2j7gQsnYOsoafVXJK3Ser2YpdAAAAAAAd1/7G0wAAAACAbicvLy/ozvGDBw/K5/Np5MiREUwFAAAAAAAAoCMoEAEAAACAbsbv9wdtOeDxeLRkyRKdOHFCknTvvffq008/1enTpyVJW7du1cSJE5WSkhKRvAAAAAAAAAA6ji1mAAAAAKCbaG5u1ubNm/Xll1+qublZN910k4YPH666ujpt3LhR+fn5SklJkcvl0sKFC1VSUqK0tDR5PB7NmTMn0vEBAAAAAAAAdAAFIgAAAADQTfTo0UO33XabbrvttqD2jIwMVVRUBLXl5uYqNze3K+MBAAAAAAAA6EQUiAAALKu5uVnLli2TYRg6duyY7r77bo0YMaLVvnv27NH69euVlJSkpqYmFRcXKzY2NnD+5Zdf1smTJ9XQ0KBRo0Zp7NixkiSfz6f8/HwdOnQo6Hpjx47VsmXLAjnGjRunr7/+WpKUk5OjN998M/wPGAAAAAAAAAAAAAgRBSIAAMtasmSJEhISNH36dJ05c0YFBQVavXq1XC5XUL8jR47o4Ycf1ltvvaXExEStXbtWJSUlmj9/viTp97//vaqqqrRo0SL5/X5NmDBB6enpuuGGG/Thhx/q9ttv149//GPFxJz72HzllVcCBSSStGnTJt13333KzMyUpBbzAwAAAAAAAAAAAJFmi3QAAABCUVtbq3Xr1mncuHGSpLi4OA0bNkzl5eUt+q5Zs0ZDhgxRYmKiJCk/P1+vv/66jhw5oubmZpWVlQWuYxiGbr75ZpWVlUmSMjMz9cgjj2j48OEaOnSohg4dKrfbrZtvvlnSudVD1q9fr+uvv14jR47UTTfdRIEIAAAAAAAAAAAATIcVRAAAlvTRRx/J6/UGFWMMHDhQ69evb9G3oqJC+fn5geM+ffooNjZWO3bsUGZmpmpra9W/f/+g67z00ks6e/Zsi2KPXbt26dprr1VcXJwk6YMPPtC+ffs0YcIE9e3bVyUlJRo1alTIj8vv96uxsTHk8YZhyOFwhDw+3Dwej/x+f6vnyBq69rJaBc9p57DK82q2nFLo7wG/3y/DMDohEQAAAAAAAACEFwUiAABLqq6ultPpVGxsbKDN6XSqpqam1b69evUKanM6nTpy5Eig0OO7551Op86cOaPa2lqlpqYGjfvTn/6kf/iHfwgc33LLLdq1a5f279+vpUuXavLkyVq3bp2GDx8e0uPyer3au3dvSGMlyeFwKDs7O+Tx4eZ2u+XxeFo9R9bQtZfVKnhOO4dVnlez5ZQ69h747mcRAAAAAAAAAJgVBSIAAEsyDEPx8fFBbV6vVzExrX+0nS8E+X7f83d9f/daXq9Xklpcy+fz6ZNPPtHs2bNbXH/QoEFauXKlHnroIa1atSrkAhG73a6srKyQxkoy3V3smZmZ7a7KYSbRktUqeE47h1WeV7PllEJ/D1RVVXVCGgAAAAAAAAAIv04pEPnmm2/09ttvq2/fvurfv78GDx6sAwcOaNWqVUpNTVVtba2Ki4t12WWXdcb0AIBuID09XadOnQpqa2hoUFpaWou+GRkZqq+vDxyf38YlLS1NGRkZkqT6+vpAkUhDQ4McDocSExODrvPJJ5/oxhtvbLMIxTAMFRUV6fHHHw/5cRmGoYSEhJDHm43ZtpBoD1m7N57TzmGl5zXUrGYsdgEAAAAAAACA1oS9QOSjjz7Sa6+9pkWLFsnpdEqSGhsbVVRUpNWrV2vAgAHavHmzZs6cqfLy8nBPDwDoJkaNGiXDMOR2u5WZmSnp3PYAo0ePbtE3Ly8v6A7vgwcPyufzaeTIkUpMTFRqaqr+/ve/B4pL3G63Ro4cKZvNFnSdd955R3fccUe7uXr06KEhQ4Z09OEBAAAAAAAAAAAAYWW7cJeLt3//fi1YsEDPPPNMoDhEkjZs2KDk5GQNGDBA0rkf6j7++GNVVlaGc3oAQDeSlJSk8ePHa8uWLZIkj8ejyspKTZo0SR6PR0uWLNGJEyckSffee68+/fRTnT59WpK0detWTZw4USkpKYqJiVFhYWHgOj6fTxUVFXrwwQeD5jt79qz27NmjYcOGBbW/++67+vzzzyVJ3377rV577TUVFxd36mMHAAAAAAAAAAAALlVYVxBZuHChrr32WpWXl+vTTz/Vj3/8Y02ZMkUVFRVyuVyBfna7XS6XS9u3b9fQoUNDmuv89gBAKAzDMNWS5x6PJ6Q97xH9oum96vf7w74M/+zZs1VaWqqysjIdO3ZMixYtUt++fVVdXa2NGzcqPz9fKSkpcrlcWrhwoUpKSpSWliaPx6M5c+YErlNUVKSlS5dq2bJlamho0LRp01p8Pm3fvj2wasl37dq1S3PnztX111+vQYMGaebMmUpJSQnr4wQAAAAAAAAAAAA6KmwFIgcPHtTHH3+s3/3ud/of/+N/aN++ffrFL36hs2fPqrq6OrB6yHlOp1M1NTUhz+f1erV3796OxkY35XA4lJ2dHekYAW63Wx6PJ9IxYELR9l6NjY0NYxopPj5e8+bNa9GekZGhioqKoLbc3Fzl5ua2eh2bzaaZM2e2O9eYMWM0ZsyYFu1z587V3LlzLyE1AAAAAAAAAAAA0PXCViCyb98+SQr8+HbNNdfo1ltv1YYNG+R0OhUXFxfU3+v1ym63hzyf3W5XVlZW6IHRrYV7FYOOyszMZAURtCqa3qtVVVVhTgMAAAAAAAAA0Wv//v166aWXdOWVV2ratGlt9tuzZ4/Wr1+vpKQkNTU1qbi4OOw36wEAokPYCkTOnj0rSWpubg60ZWdna+vWrRo4cKDq6+uD+jc0NCgtLS3k+QzDUEJCQsjjATMx0xYiQHs68l41W7ELAAAAAAAAAJhVY2Oj6urqtGPHjhar9H/XkSNH9PDDD+utt95SYmKi1q5dq5KSEs2fP78L0wIArMIWrgud3wLB7XYH2nr06KFBgwYpLy8v6M7xpqYmHT58uM2l/gEAAAAAAAAAAIDuKiEhQSNGjGi3OESS1qxZoyFDhigxMVGSlJ+fr9dff11HjhzpipgAAIsJ2woiAwYM0NixY7Vp0yYNGTJEkvQf//EfmjJlikaNGqXy8nIdPXpUvXv31rZt25Sbm6ucnJxwTQ8AAAAAAAAAAABEFZut/Xu9KyoqlJ+fHzju06ePYmNjtWPHDt15550hzen3+9XY2BjSWOncatJmWjnd4/G0unW62XJK1snaVk6JrB0RDVnNllOyTtb2Xv8L8fv9F72Sf9gKRCTp6aef1v/6X/9LK1askN/v18iRIwMfSitXrtTSpUvlcrl04sQJPffcc+GcGgAAAAAAAAAAAOhWqqur1atXr6A2p9PZoRVEvF6v9u7dG/J4h8MR2HnADNxutzweT4t2s+WUrJO1rZwSWTsiGrKaLadknaztvf4XIzY29qL6hbVAJCEhQc8880yr53Jycto8BwAAAAAAAAAAAODSxcXFBR17vV7FxIT+E6DdbldWVlbI4y/2LvaukpmZ2ebqAWZjlaxt5ZTI2hHRkNVsOSXrZG3v9b+Qqqqqi+4b1gIRAPD5fBdc8q475QAAAAAAAAAAoLNkZGSovr4+cHx+e5i0tLSQr2kYhhISEsIRzxTMtIXEhVglq1VySmTtLGQNv47kvJRiFwpEAISVzWbTG2+8oePHj0csw+WXX67x48dHbH4AAAAAAAAAALpCXl5e0J3jBw8elM/n08iRIyOYCgBgVhSIAAi748ePq7q6OtIxAAAAAAAAAACwNL/fH7TlgMfj0YoVKzRp0iSlpKTo3nvv1X333afTp08rPj5eW7du1cSJE5WSkhLB1AAAs6JABAAAAAAAAAAAADCR5uZmbd68WV9++aWam5t10003afjw4aqrq9PGjRuVn5+vlJQUuVwuLVy4UCUlJUpLS5PH49GcOXMiHR8AYFIUiAAAAAAAAAAAAAAm0qNHD91222267bbbgtozMjJUUVER1Jabm6vc3NyujAcAsChbpAMAAAAAAAAAAAAAAACgc1EgAgAAAAAAAAAAAAAAEOUoEAEAAAAAAAAAAAAAAIhyFIgAAAAAAAAAsKTm5mb96U9/0h133KFDhw612/ebb75RUVGRbrjhBo0dO1bvvfdeF6UEAAAAAHOgQAQAAAAAAACAJR04cED19fX64osvLtj3t7/9rf7xH/9R69at07XXXqtHH31Un332WRekBAAAAABziIl0AAAAAAAAAAAIxZVXXim73X7BfrW1tZo0aZKuv/56SdKzzz6rTz75RDt27FBOTk5nxwQAAAAAU6BABAAAAAAAAIBlGYZxwT5JSUlKSkoKHNvtdmVkZKhfv34dmtvv96uxsbFD1wCswDAMORyOSMcI8Hg88vv9kY4BC/L7/Rf1uQEAQLSiQAQAAAAAAACA6ZSWlra7dcyMGTM0ePDgkK596tQpeb1e3XLLLaHGkyR5vV7t3bu3Q9cArMDhcCg7OzvSMQLcbrc8Hk+kY8CiYmNjIx0BAICIoUAEAAAAAAAAgOnMmjWr0669evVqPfHEEx3+kdButysrKytMqQDzMtuKC5mZmawggpBUVVVFOgIAABFFgQgAAAAAAACAbmPbtm3KzMzUsGHDOnwtwzCUkJAQhlQALoWZtruBtZit2AkAgK5mi3QAAAAAAAAAAOgKlZWVqqmp0V133RXpKAAAAADQ5SgQAQAAAAAAAGBZ57eZ+P52E5WVlXr11VcDxzt37tTmzZuVm5urQ4cO6csvv9Tvfvc7eb3erowLAAAAABFDgQgAAAAAAAAAS/rmm2/05ptvSpLeeOMNHT58OHBu9+7d2rRpkyRpx44dKioqUnl5uX7605/qpz/9qcaNG6djx47JbrdHJDsAAAAAdLWYSAcAAAAAAAAAgFD07dtXM2bM0IwZM1qcKywsVGFhoSRp5MiR2r17d1fHAwAAAABTYQURAAAAAAAAAAAAAACAKEeBCAAAAAAAAAAAAAAAQJSjQAQAAAAAAAAAAAAAACDKUSACAAAAAAAAAAAAAAAQ5SgQAQAAAAAAAAAAAAAAiHIUiAAAAAAAAAAAAAAAAEQ5CkQAAAAAAAAAAAAAAACiHAUiAAAAAAAAAAAAAAAAUY4CEQAAAAAAAAAAAAAAgChHgQgAAAAAAAAAAAAAAECUo0AEAAAAAAAAAAAAAAAgylEgAgAAAAAAAAAAAAAAEOUoEAEAAAAAAAAAAAAAAIhyFIgAAAAAAAAAAAAAAABEOQpEAAAAAAAAAAAAAAAAohwFIgAAAAAAAAAAAAAAAFGOAhEAAIAo4vP5Ix0hwExZAAAAAAAAAADo7mIiHQAAgFA1Nzdr2bJlMgxDx44d0913360RI0a02nfPnj1av369kpKS1NTUpOLiYsXGxgbOv/zyyzp58qQaGho0atQojR07Nmj8ihUr9PzzzweON27cqGuuuUaStG3bNm3ZskVOp1MOh0PTp0+XYRid8IiBC7PZDP15zWc6Wf1tRHMkZziVPzknohkAAAAAAAAAAMD/Q4EIAMCylixZooSEBE2fPl1nzpxRQUGBVq9eLZfLFdTvyJEjevjhh/XWW28pMTFRa9euVUlJiebPny9J+v3vf6+qqiotWrRIfr9fEyZMUHp6um644QZJUkNDgw4dOqRXXnlFkhQXFxcoDvn8889VWlqqP/7xj7Lb7Xr66adVXl6uX//61134TADBTlZ/q+MHGyIdAwAAAAAAAAAAmAgFIgAAS6qtrdW6deu0ceNGSeeKNoYNG6by8nItWLAgqO+aNWs0ZMgQJSYmSpLy8/N16623atq0abr88stVVlam0tJSSZJhGLr55ptVVlam1atXS5LWrl2rrKwsDR48WD179gy69sqVK3XLLbfIbrcHrj1t2jQVFhYqLi7ukh+X3+9XY2PjJY87zzAMORyOkMeHm8fjkd/f+jYjZA1dW1nNllOyTtb2Xn8rscrzaracUujvAb/fz6pRAAAAAAAAACyBAhEAgCV99NFH8nq9QauFDBw4UOvXr2/Rt6KiQvn5+YHjPn36KDY2Vjt27FBmZqZqa2vVv3//oOu89NJLOnv2rHw+n/71X/9VBw4c0NKlS/XAAw9o2rRpstvt8vl8+vd//3eNGTMmaOypU6f0t7/9TcOHD7/kx+X1erV3795LHneew+FQdnZ2yOPDze12y+PxtHqOrKFrK6vZckrWydre628lVnlezZZT6th74LtblgEAAAAAAACAWXVagcjLL7+sbdu2ad26dZKkAwcOaNWqVUpNTVVtba2Ki4t12WWXddb0AIAoV11dLafTGfSjnNPpVE1NTat9e/XqFdTmdDp15MiRwCof3z3vdDp15swZ1dbWKjU1Ve+//77q6ur0xhtvaPny5Tpx4oQWLFiguro6NTY2thgrndvWJhR2u11ZWVkhjZVkurvYMzMz212Vw0yiIavZckrWydre628lVnlezZZTCv09UFVV1QlpAAAAAAAAACD8OqVA5D/+4z/0v//3/1afPn0kSY2NjSoqKtLq1as1YMAAbd68WTNnzlR5eXlnTA8A6AYMw1B8fHxQm9frVUxM6x9t39/u5Xzf8z9SfvdaXq9XkoKu1atXLxUVFal///566KGHNGPGDNlsthbXbm3spT6uhISEkMaakdm2kGgPWTuHVbJaJafVWOl5DTWrGYtdAAAAAAAAAKA1YS8QOXnypDZt2qQ777xTf/3rXyVJGzZsUHJysgYMGCBJysvL06OPPqrKykoNHTo0pHn8fr8aGxvDFRvdjNn2vQ91z3uz4XkNv2h6Tv1+f1h/REtPT9epU6eC2hoaGpSWltaib0ZGhurr64OyNDY2Ki0tTRkZGZKk+vr6QJFIQ0ODHA6HEhMTW1wrPz9fV1xxhb755hsNHjxYDocj6NoNDQ2S1GoOAAAAAAAAAAAAIFLCWiDi9/u1fPlyPfroo/qXf/mXQHtFRYVcLlfg2G63y+Vyafv27SEXiHi9Xu3du7ejkdFNmW3f+47seW8mPK/hF23P6Xe3g+moUaNGyTAMud1uZWZmSjqXb/To0S365uXlBW0BcPDgQfl8Po0cOVKJiYlKTU3V3//+90BRh9vt1siRIwMrhHzfD37wAw0cOFCSNGbMmKBru91uJSYm6rrrrgvbYwUAAAAAAAAAAAA6KqwFIqtXr9b//J//s8Ud19XV1YHVQ85zOp2qqakJeS673a6srKyQx6N7M9tS4KHueW82PK/hF03P6XeLKMIhKSlJ48eP15YtWzRlyhR5PB5VVlbq1Vdflcfj0YoVKzRp0iSlpKTo3nvv1X333afTp08rPj5eW7du1cSJE5WSkiJJKiws1JYtWzR69Gj5fD5VVFToiSeekCR99dVX+s///E/dfvvtstls+sMf/qBf/vKXcjqdkqTJkydr3rx5euSRR2QYhrZs2aKpU6eGvMUMAAAAAAAAAAAA0BnC9uvVxx9/rB/84Ae6/vrrW5wzDENxcXFBbV6vV3a7PeT5DMNQQkJCyOMBMzHTFiLRhOc1/DrynHZGscvs2bNVWlqqsrIyHTt2TIsWLVLfvn1VXV2tjRs3Kj8/XykpKXK5XFq4cKFKSkqUlpYmj8ejOXPmBK5TVFSkpUuXatmyZWpoaNC0adMCK1wdO3ZMpaWlWrFihYYMGaLbb79dY8aMCYwdOnSopkyZooULF6pnz55KTU1VUVFR2B8rAAAAAAAAAAAA0BFhKxBZuXKlPvvsMy1evFiSdObMGTU3N2v48OHKzs5WfX19UP+GhobAUv4AAIQiPj5e8+bNa9GekZGhioqKoLbc3Fzl5ua2eh2bzaaZM2e2eu5HP/qRPvzww3Zz3HHHHbrjjjsuMjUAAAAAAAAAAADQ9cJWILJkyRKdOXMmcLx27Vrt3r1bzz33nP7yl7/o7bffDpxramrS4cOH2/yhDgAAAAAAAAAAAAAAAOFjC9eFUlNT1a9fv8D/LrvsMsXFxalfv3666667VF1draNHj0qStm3bptzcXOXk5IRregAAAAAAAAAAAAAAALQhbCuItKdnz55auXKlli5dKpfLpRMnTui5557riqkBAAAAAAAAAAAAAAC6vU4rEJk+fXrQcU5Ojp555pnOmg4AAAAAAAAAAAAAAABtCNsWMwAAAAAAAAAAAAAAADAnCkQAAAAAAAAAWFJzc7P+9Kc/6Y477tChQ4cuety7776rn/zkJ52YDAAAAADMhwIRAAAAAAAAAJZ04MAB1dfX64svvrikMStWrOjEVAAAAABgThSIAAAAAAAAALCkK6+8Uj/+8Y8vuv+ZM2dUXl6uX/7yl52YCgAAAADMKSbSAQAAAAAAAAAgVIZhXHTf5cuXa+rUqfrrX/8alrn9fr8aGxvDci3AzAzDkMPhiHSMAI/HI7/fH+kYsCC/339JnxsAAEQbCkQAC/D7fDJskV/wxyw5AAAAEJrm5mYtW7ZMhmHo2LFjuvvuuzVixIg2+xcWFmrHjh2SpNTUVG3dulWxsbFdFRcA0M2Vlpa2u3XMjBkzNHjw4Iu+3qZNmzRs2DC5XK6wFYh4vV7t3bs3LNcCzMzhcCg7OzvSMQLcbrc8Hk+kY8Ci+JsGANCdUSACWIBhs2n/X/6sxrqTEcuQ0CtZg27Oj9j8AAAA6LglS5YoISFB06dP15kzZ1RQUKDVq1fL5XK16Ltz507deOONmjp1qiSpd+/efJEKAOhSs2bNCtu1vvzyS/3Xf/2XfvOb34TtmpJkt9uVlZUV1msCZmS2FRcyMzNZQQQhqaqqinQEAAAiigIRwCIa607q2xPHIx0DAAAAFlVbW6t169Zp48aNkqS4uDgNGzZM5eXlWrBgQYv+L774ooqKivTDH/5QMTH86QgAsLbVq1frvffe0yuvvCLp3MofZ86c0fDhw/Xiiy9q+PDhIV3XMAwlJCSEMyqAi2Cm7W5gLWYrdgIAoKvxLR8AAAAAdAMfffSRvF5v0GohAwcO1Pr161v03bdvnyorK3X//fcrOTlZTzzxhMaNG9eh+f1+vxobG0Maa6U978kaumjIaracknWyRsPrL1kr64X4/f6o+hGtuLhY06ZNCxy///77Wrt2rV577TWlpqZGMBkAAAAAdB0KRAAAAACgG6iurpbT6QzaJsbpdKqmpqZF32uuuUY7d+7UwYMH9eKLL+qRRx5Rc3OzCgoKQp7f6/Vq7969IY210p73ZA1dNGQ1W07JOlmj4fWXrJX1Ylhla7HzRTDfL4aprKwMKnhMTk4OnEtKSlJMTIz69evXpVkBAAAAIJIoEAEAAACAbsAwDMXHxwe1eb3edrePcblcKikpkcPh0AsvvNChAhG73a6srKyQxprtDvb29rwna+iiIavZckrWyRoNr79krawXUlVVFeY0neObb77Rm2++KUl644039POf/1x9+vSRJO3evVubNm3S/fffH8GEAAAAAGAeFIgAAAAAQDeQnp6uU6dOBbU1NDQoLS3tgmMfeOABvf766x2a3zAMJSQkdOgaZmGm7SMuhKydg6zhZ5WcUvfJarZil7b07dtXM2bM0IwZM1qcKywsVGFhYavj7rnnHt1zzz2dHQ8AAAAATMUW6QAAAAAAgM43atQoGYYht9sdaHO73Ro9evQFx/bo0UODBw/uzHgAAAAAAAAAOhkFIgAAAADQDSQlJWn8+PHasmWLJMnj8aiyslKTJk2Sx+PRkiVLdOLECUnShx9+qE8++UTSuW1oysrK9NRTT0UqOgAAAAAAAIAwYIsZAAAAAOgmZs+erdLSUpWVlenYsWNatGiR+vbtq+rqam3cuFH5+flKSUnR/v37VVZWpquuukpXX321Jk2apMzMzEjHBwAAAAAAANABFIgAAAAAQDcRHx+vefPmtWjPyMhQRUVF4Hjy5MmaPHlyV0YDAAAAAAAA0MnYYgYAAAAAAAAAAAAAACDKsYIIAAAAAAAAAAAAYCLNzc1atmyZDMPQsWPHdPfdd2vEiBFt9i8sLNSOHTskSampqdq6datiY2O7Ki4AwCIoELEKX7Nk6xHpFObJAQAAAAAAAAAAEKWWLFmihIQETZ8+XWfOnFFBQYFWr14tl8vVou/OnTt14403aurUqZKk3r17UxwCAGhVty4Qafb51cNmRDrGxeWw9ZDemCId3981oVpz+SBp/MuRmx8AAAAAAAAAACDK1dbWat26ddq4caMkKS4uTsOGDVN5ebkWLFjQov+LL76ooqIi/fCHP1RMTMd/+vP7/WpsbAx5vGEYcjgcHc4RLh6PR36/v0W72XJK1snaVk6JrB0RDVnNllOyTtb2Xv8L8fv9MoyLq3vo1gUiPWyGZvxhl6qONkQsQ1bvnlo+8YaL63x8v1S9u3MDdZDP75PNsEU6hiRzZQEAAAAAAAAAALgYH330kbxeb9BqIQMHDtT69etb9N23b58qKyt1//33Kzk5WU888YTGjRvXofm9Xq/27t0b8niHw6Hs7OwOZQgnt9stj8fTot1sOSXrZG0rp0TWjoiGrGbLKVkna3uv/8W42JWjunWBiCRVHW3QZ4dPRTpG1LAZNm059rbqvCcimqOXPUU/Tb09ohkAAAAAAAAAAAAuVXV1tZxOZ9CPfU6nUzU1NS36XnPNNdq5c6cOHjyoF198UY888oiam5tVUFAQ8vx2u11ZWVkhj7/Yu9i7SmZmZpurB5iNVbK2lVMia0dEQ1az5ZSsk7W91/9CqqqqLrpvty8QQfjVeU/oeNPRSMcAAAAAAAAAAACwHMMwFB8fH9Tm9Xrb3T7G5XKppKREDodDL7zwQocKRAzDUEJCQsjjzcZMW0hciFWyWiWnRNbOQtbw60jOSyl2Yf8NAAAAAAAAAAAAwCTS09N16lTw6vcNDQ1KS0u74NgHHnhA33zzTWdFAwBYHAUiAAAAAAAAAAAAgEmMGjVKhmHI7XYH2txut0aPHn3BsT169NDgwYM7Mx4AwMIoEAEAAAAAAAAAAABMIikpSePHj9eWLVskSR6PR5WVlZo0aZI8Ho+WLFmiEydOSJI+/PBDffLJJ5LObUNTVlamp556KlLRAQAm1/ZmZQAAAAAkST6fTzZb5GurzZIDAAAAAAB0rtmzZ6u0tFRlZWU6duyYFi1apL59+6q6ulobN25Ufn6+UlJStH//fpWVlemqq67S1VdfrUmTJikzMzPS8QEAJkWBCAAAAHABNptNb7zxho4fPx6xDJdffrnGjx8fsfkBAAAAAEDXiY+P17x581q0Z2RkqKKiInA8efJkTZ48uSujAQAsjAIRAAAA4CIcP35c1dXVkY4BAAAAAAAAAEBIWJ8aAAAAAAAAAAAAAAAgylEgAqBb8vt9kY4QYKYsAAAAAAAAAAAAAKITW8wA6JYMw6aamnfk9Z6MaA67PVnp6eMimgEAAAAAAAAAAABA9KNABEC35fWe1JkzRyMdAwAAAAAAAAAAAAA6HVvMAAAAAAAAAAAAAAAARDkKRAAAAAAAAAAAAAAAAKIcBSIAAAAAAAAAAAAAAABRjgIRAAAAAAAAAAAAAACAKBcT6QAAAISqublZy5Ytk2EYOnbsmO6++26NGDGi1b579uzR+vXrlZSUpKamJhUXFys2NjZw/uWXX9bJkyfV0NCgUaNGaezYsYFz7777rpYtW6Zjx45p+PDhevLJJ9W3b9+gHOPGjdPXX38tScrJydGbb77ZOQ8aAAAAAAAAAAAACAEFIgAAy1qyZIkSEhI0ffp0nTlzRgUFBVq9erVcLldQvyNHjujhhx/WW2+9pcTERK1du1YlJSWaP3++JOn3v/+9qqqqtGjRIvn9fk2YMEHp6em64YYbtH//fm3evFmLFy/WyZMntWDBAj300EN66623AtfftGmT7rvvPmVmZkpSi/kBAAAAAAAAAACASAvrFjP79u3TxIkTdcMNN+juu+/Wxx9/HDh34MABzZ07V0uXLtW8efN06tSpcE4NAOhmamtrtW7dOo0bN06SFBcXp2HDhqm8vLxF3zVr1mjIkCFKTEyUJOXn5+v111/XkSNH1NzcrLKyssB1DMPQzTffrLKyMknS0aNHtXjxYg0ePFg333yznnjiCX3++ec6efKkpHOrh6xfv17XX3+9Ro4cqZtuuokCEQAAAAAAAAAAAJhO2FYQaWpq0vLlyzV9+nT17NlTS5cu1YMPPqj33ntPP/jBD1RUVKTVq1drwIAB2rx5s2bOnNnqj3gAAFyMjz76SF6vN6gYY+DAgVq/fn2LvhUVFcrPzw8c9+nTR7GxsdqxY4cyMzNVW1ur/v37B13npZde0tmzZzV69Oiga/Xv31+XXXaZLrvsMknSBx98oH379mnChAnq27evSkpKNGrUqJAfl9/vV2NjY8jjDcOQw+EIeXy4eTwe+f3+Vs+RNXRtZTVbTsk6WaPh9Zesk9VsOaX2n9f2+P1+GYbRCYkAAAAAAAAAILzCViBy4MABPfnkk0pPT5ckLV++XDfddJN27dqlo0ePKjk5WQMGDJAk5eXl6dFHH1VlZaWGDh0arggAgG6kurpaTqdTsbGxgTan06mamppW+/bq1Suozel06siRI4qLi5OkoPNOp1NnzpxRbW2tUlNTg8b99a9/VWFhoWJizn2E3nLLLdq1a5f279+vpUuXavLkyVq3bp2GDx8e0uPyer3au3dvSGMlyeFwKDs7O+Tx4eZ2u+XxeFo9R9bQtZXVbDkl62SNhtdfsk5Ws+WU2n9eL+S7n0UAAAAAAAAAYFZhKxC56qqrgo4TExOVmJiofv366Y033gi6w9tut8vlcmn79u0hF4hwh3XkRMOdoGbLGg2vv2SdrGbLKVkna6h3V0vhv8PaMAzFx8cHtXm93kDhxvedLwT5ft/zmb57La/XK0ktrtXY2Kjt27dr8eLFLa4/aNAgrVy5Ug899JBWrVoVcoGI3W5XVlZWSGMlme4u9szMzHb/P8NMoiGr2XJK1skaDa+/ZJ2sZssptf+8tqeqqqoT0gAAAAAAAABA+IWtQOT7vvrqK2VlZem6665TdXV1YPWQ89q6y/ticYd15ETDnaBmyxoNr79knaxmyylZJ2tH7q6WwnuHdXp6uk6dOhXU1tDQoLS0tBZ9MzIyVF9fHzg+X2SYlpamjIwMSVJ9fX2gSKShoUEOh0OJiYlB1ykrK9M///M/tyg2Oc8wDBUVFenxxx8P+XEZhqGEhISQx5uNmQqcLoSsncMqWa2SUyJrZwk1qxmLXQAAAAAAAACgNZ1WILJmzRotWLBA0rkvTVu7c9tut4d8fe6wjpxouBPUbFmj4fWXrJPVbDkl62QN9e5qKfx3WI8aNUqGYcjtdiszM1PSuQKW0aNHt+ibl5cXNP/Bgwfl8/k0cuRIJSYmKjU1VX//+98DxSVut1sjR46UzWYLjCkvL9f48eNbLUD5rh49emjIkCHheIgAAAAAAAAAAABA2HRKgciGDRs0duxYXXHFFZLO3eX93Tu3pbbv8r5Y3GEdOWQNP6vklMjaWayStSM5w13skpSUpPHjx2vLli2aMmWKPB6PKisr9eqrr8rj8WjFihWaNGmSUlJSdO+99+q+++7T6dOnFR8fr61bt2rixIlKSUmRJBUWFmrLli0aPXq0fD6fKioq9MQTTwTmWr58ua6//nrFxcXp0KFDOnr0qPbv36+JEyfq3Xff1YABA5Sdna1vv/1Wr732mmbNmhXWxwoAAAAAAAAAAAB0VNgLRLZu3arExETl5uYG2vLy8vT2228HjpuamnT48OGgPgAAXKrZs2ertLRUZWVlOnbsmBYtWqS+ffuqurqFbxJ7AAANdUlEQVRaGzduVH5+vlJSUuRyubRw4UKVlJQoLS1NHo9Hc+bMCVynqKhIS5cu1bJly9TQ0KBp06Zp6NChkqQnn3xSf/jDH1rMvX79eknSrl27NHfuXF1//fUaNGiQZs6cGSg8AQAAAAB0rubmZr333ntatWqVVq5cqX79+l1wzMmTJ7Vhwwb17t1bffr00YgRI7ogKQAAAABEXlgLRN5//30dPHhQP/vZz3To0CE1Njbqz3/+swoLC1VeXq6jR4+qd+/e2rZtm3Jzc5WTkxPO6QEA3Ux8fLzmzZvXoj0jI0MVFRVBbbm5uW0WJtpsNs2cObPVc/Pnz9f8+fPbzDB37lzNnTv3ElIDAAAAAMLlwIEDqq+v1xdffHFR/fft26fFixertLSU4n4AAAAA3U7YCkQ2bdqkxx57TM3NzVq8eHGgfebMmerZs6dWrlyppUuXyuVy6cSJE3ruuefCNTUAAAAAAACAbujKK6+U3W6/qL7Hjh3T9OnTtW7dOopDAAAAAHRLYSsQKSgoUEFBQZvnc3Jy9Mwzz4RrOgAAAAAAAACQYRgX1e+5555T//79tWHDBu3cuVM5OTmaMWOGYmNjQ57b7/ersbEx5PGAVRiGIYfDEekYAR6PR36/P9IxYEF+v/+iPzcAAIhGYd1iBgAAAAAAAADCobS0tN2tY2bMmKHBgwdf1LUaGxv1zjvv6JFHHtH999+vmpoa3XPPPaqrq1NJSUnIGb1er/bu3RvyeMAqHA6HsrOzIx0jwO12y+PxRDoGLKojhYEAAFgdBSIAAAAAAAAATGfWrFlhu9bXX3+t06dP66abbpIkpaena8KECXr55Zf11FNPXfQ2Nd9nt9uVlZUVtpyAWZltxYXMzExWEEFIqqqqIh0BAICIokAEAAAAAAAAQFQ7e/asJKm5uTnQdu211+rs2bP67//+byUnJ4d0XcMwlJCQEJaMAC6emba7gbWYrdgJAICuZot0AAAAAAAAAADoTAMHDlRcXJzcbnegLSYmRqmpqSEXhwAAAACA1VAgAgAAAAAAAMCyzm8z8f3tJiorK/Xqq69KkpxOp371q19p06ZNgfM7d+7UlClTuiwnAAAAAEQaBSIAAAAAAAAALOmbb77Rm2++KUl64403dPjw4cC53bt3BxWEPProo7riiitUWlqq8vJy9ezZU4WFhV2eGQAAAAAiJSbSAQAAAAAAAAAgFH379tWMGTM0Y8aMFucKCwuDCkB69Oihxx57rCvjAQAAAICpsIIIAAAAAAAAAAAAAABAlKNABAAAAAAAAAAAAAAAIMpRIAIAAAAAAAAAAAAAABDlKBABAAAAAAAAAAAAAACIchSIAAAAAAAAAAAAAAAARDkKRAAAAAAAAAAAAAAAAKIcBSIAAAAAAAAAAAAAAABRjgIRAAAAAAAAAAAAAACAKEeBCAAAAAAAAAAAAAAAQJSjQAQAAAAAAAAAAAAAACDKUSACAAAAAAAAAAAAAAAQ5SgQAQAAAAAAAAAAAAAAiHIUiAAAAAAAAAAAAAAAAEQ5CkQAAAAA/P/t3c9rnHUeB/D3pJ22ybibRrdN4qprKbuIKKFUbANSFemhgoqIIl5ED40HKz3oH+BBkVLRQ/BgVBC9FBHFkwj1B6XqSTSBIljIQTEJxaaW2DSNyexBGsx2t8m0M9OZZ1+v28w3z/f5PDMh73xmPswAAAAAAFBwBkQAAAAAAAAAAArOgAgAAAAAAAAAQMGtvdIFAAAAAAAAra9r49X/1+cHAGh3BkQAAAAAAICLqi4u5l937b7SZaS6uJhShw9HBwC4FP6LAgAAAAAALqpVhjJapQ4AgHbkPykAAAAAAAAAgIIzIAIAAAAAAAAAUHAGRAAAAAAAAAAACs6ACAAAAAAAAABAwRkQAQAAAAAAAAAoOAMiAAAAAAAAAAAFZ0AEAAAAAAAAAKDgDIgAAAAAAAAAABScAREAAAAAAAAAgIIzIAIAAAAAAAAAUHAGRAAAAAAAAAAACs6ACAAAAAAAAABAwRkQAQAAAAAAAAAoOAMiAAAAAAAAAAAFt7aZJ1tYWMirr76aUqmUEydO5MEHH8ztt9/ezBIAKJBacmV0dDSHDh1KT09Pzp07l2effTbr1q1bWn/jjTdy8uTJzMzMZHBwMHv27FlaO3nyZF5++eVs2rQpU1NTeeaZZ9Lf37+0/sUXX+Tw4cOpVCrp7OzMvn37UiqVGnfhAHCJ6pmdAAAANI7+DYBGaOqAyMGDB9PV1ZV9+/Zlbm4u9913X958881cf/31zSwDgIJYba5MTU1l//79+eCDD9Ld3Z233347L7zwQp5//vkkybvvvpvjx4/npZdeSrVazcMPP5y+vr5s27Yti4uL2bt3b5577rns2LEj33//fYaGhvL++++nXC7n2LFjOXDgQD788MOUy+W8+OKLGRkZyd69e6/EQwIAF1Wv7AQAAKCx9G8ANELTvmJmeno677zzTu69994kyfr167N9+/aMjIw0qwQACqSWXHnrrbcyMDCQ7u7uJMnu3bvz3nvvZWpqKgsLCxkeHl7ap1Qq5a677srw8HCS5LPPPsvk5GR27NiRJLnpppvy22+/5ZNPPkmSvPbaa7n77rtTLpeX9h4ZGcnc3FxjHwAAqFG9shMAAIDG0r8B0ChN+wSRr776KvPz88smG7du3ZpDhw7VvNf8/Hyq1WpGR0cvq6ZSqZTnbu/K/MKGy9rncpTXdGRsbCzVavWiP1cqlZKB55Nb55tU2X/RUU5WqLVUKuX6hZvy9/yriYVdqCMdGftl5Vq33L02/1j4axMrW66jhud/8e9bsqH/H02q7EKLHauvdWBgILfeemuTKrtQxypqLZVKWVj4Z6rVrU2s7EIzMx05dWrlWvsXFtO7wmPfSB2/lTL26+SKz//FzM/P1/VrV2rJlSNHjmT37t1Lt6+99tqsW7cuX3/9dbZs2ZLp6enccMMNy/Z5/fXX8/vvv+fIkSO57rrrlu23devWfPnll9mzZ0+OHj2aXbt2LVs7ffp0xsbGctttt9V0TfLtCllFviWtkXHtkm/J6jJOvtVmNfmWtEbGtUu+JZefcfXOt0aqV3Y+8MADNZ+7Hhkn32pUsHxLWiPj2iXfktVlnHyrzWryLWmNjGvFHq6Izufb2NjYlS4FgFU6d+5cW+Rbu/dvSfv0cC3RvyVt8x5c0fq3pDV6uKK9RtkK/VvSPq9RNrt/a9qAyMTERCqVyrLvPKtUKpmcnKx5r/MXV48Qv6bSGt/Btqprqfyt8YWswkq1dq7palIlK1ux1r+Um1TJxa3m+S9v6GxCJStbTa2VSqUJlaxspVrXtNHv6oY1TfvAp4u6nL+7pVKprs1XLbkyMTGRjRs3LruvUqlkamoq69evT5Jl65VKJXNzc5menv6fx05OTubUqVM5c+bMBccmuaQJffl2Za2m1lbJuHbJt2TlWuVb7VZTa6tkXLvkW3Lpf3vrnW+NVK/svBT1yjj5Vrsi5VvSOhnXLvmWrFyrfKvdamptlYxrpR6uiDw+AO2nXfKtCP1b0kY9XIv0b0n7vAdXpP4taZ0erkivUbZK/5a0z2uUzerfmjYgUiqVsmHD8inB+fn5rF1bewnbtm2rV1kAtKlac+X8IMh//uz5wPzzXvPzf0yLn1//b+cpl8tLx/557z8fWyv5BkAj1Ss7L4WMA6CI5BsAjaJ/A6BRmjYO09fXl9OnTy+7b2ZmJr29vc0qAYACqSVX+vv78+uvvy7drlarOXPmTHp7e9Pf358ky9ZnZmbS2dmZ7u7u9PX1LVs7v7558+b09PSks7PzgmOTyDcAWk69shMAAIDG0r8B0ChNGxAZHBxMqVTK+Pj40n3j4+O54447mlUCAAVSS67ceeedOX78+NLtH3/8MYuLi9m5c2duvvnmbNq0KT/88MOyfXbu3JmOjo4Ljv3P8+zatWvZ+vj4eLq7u3PLLbfU7VoBoB7qlZ0AAAA0lv4NgEZp2oBIT09PHnrooRw+fDhJMjs7m2+//TZPPPFEs0oAoEAuliuzs7M5ePBgfvnllyTJY489lm+++SZnz55Nknz66ad59NFHc80112Tt2rV5/PHHl/ZZXFzMkSNH8tRTTyX5o8HauHFjRkdHkyTHjh1Ld3d37rnnniTJk08+mc8//zzVajVJcvjw4QwNDV3yRzgCQKPUKzsBAABoLP0bAI1Sqp5/R6sJzp49mwMHDuTqq6/OiRMncv/992f79u3NOj0ABfO/cmViYiKPPPJIhoeHMzAwkCQ5evRoPv744/T29mZ2djb79+9PuVxO8sdQyCuvvJI1a9ZkZmYmg4ODSwMgSfLTTz9leHg4N954Y37++ec8/fTT2bx589L6Rx99lO+++y5XXXVVurq6MjQ01NwHAgBWqV7ZCQAAQGPp3wBohKYOiAAAAAAAAAAA0HxN+4oZAAAAAAAAAACuDAMiAAAAAAAAAAAFZ0AEAAAAAAAAAKDgDIgAAAAAAAAAABScAREAAAAAAAAAgIIzIAIAAAAAAAAAUHAGRAAAAAAAAAAACs6ACAAAAAAAAABAwRkQAQAAAAAAAAAoOAMiAAAAAAAAAAAFZ0AEAAAAAAAAAKDg/g0p3C/WUvZKywAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 2200x1000 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_name_map = {\n",
    "    'DeepHermes-LLaMA-3B': 'dh.3b-llama.fp32.json',\n",
    "    'DeepHermes-3B-8bit': 'dh.3b.bnb8bit.fp32.json',\n",
    "    'DeepHermes-3B-4bit': 'ldh.3b.bnb4bit.fp32.json',\n",
    "    'DeepHermes-3B-1.58bit': 'dh.3b.ptsq.fp32.json',\n",
    "    'LLaMA-Instruct-8B': 'llama.8b-instruct.fp32.json',\n",
    "    'LLaMA-Instruct-8B-8bit': 'llama.8b-bnb8bit.fp32.json',\n",
    "    'LLaMA-Instruct-8B-4bit': 'llama.8b-bnb4bit.fp32.json',\n",
    "    'HF1BitLLM': 'llama.8b-1.58.fp32.json',\n",
    "}\n",
    "\n",
    "\"\"\"model_name_map_fixed = {\n",
    "    key: val.replace(\".json\", \"\") for key, val in model_name_map.items()\n",
    "}\"\"\"\n",
    "\n",
    "metrics = [\n",
    "    'Perplexity',\n",
    "    'CPU Usage (%)',\n",
    "    'RAM Usage (%)',\n",
    "    'GPU Memory (MB)',\n",
    "    'Activation Similarity',\n",
    "    'Latency (s)',\n",
    "    \"Last Layer Mean Activation\",\n",
    "    \"Last Layer Activation Std\",\n",
    "    \"Mean Logits\",\n",
    "    \"Logit Std\",\n",
    "]\n",
    "\n",
    "plot_flat_metrics_by_model(\n",
    "    df,\n",
    "    metrics=metrics,\n",
    "    model_col=\"Model\",\n",
    "    model_name_map=model_name_map,\n",
    "    title=\"Flat Metrics Across Models (GSM8K)\",\n",
    "    log_metrics=[\"Perplexity\"],\n",
    "    save_path='GSM8K_QA'\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = [\n",
    "    'Perplexity',\n",
    "    'CPU Usage (%)',\n",
    "    'RAM Usage (%)',\n",
    "    'GPU Memory (MB)',\n",
    "    'Activation Similarity',\n",
    "    'Latency (s)',\n",
    "    \"Last Layer Mean Activation\",\n",
    "    \"Last Layer Activation Std\",\n",
    "    \"Mean Logits\",\n",
    "    \"Logit Std\",\n",
    "]\n",
    "\n",
    "# Generate a color map\n",
    "models = df['Model'].tolist()\n",
    "num_models = len(models)\n",
    "colors = cm.get_cmap('coolwarm', num_models)\n",
    "model_colors = {model: colors(i) for i, model in enumerate(models)}\n",
    "\n",
    "# Prepare subplot grid dynamically\n",
    "n_metrics = len(metrics)\n",
    "ncols = 5\n",
    "nrows = int(np.ceil(n_metrics / ncols))\n",
    "\n",
    "plt.rcParams['font.family'] = 'Times New Roman'\n",
    "fig, axes = plt.subplots(nrows=nrows, ncols=ncols, figsize=(20, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, metric in enumerate(metrics):\n",
    "    ax = axes[i]\n",
    "    if metric in df.columns:\n",
    "        for j, model in enumerate(models):\n",
    "            ax.bar(model, df.loc[j, metric], color=model_colors[model])\n",
    "        ax.set_title(metric, fontsize=12)\n",
    "        ax.set_xticks(range(len(models)))\n",
    "        ax.set_xticklabels(models, rotation=45, ha='right', fontsize=10)\n",
    "        ax.grid(True)\n",
    "\n",
    "# Hide unused axes\n",
    "for j in range(i + 1, len(axes)):\n",
    "    fig.delaxes(axes[j])\n",
    "\n",
    "# Legend\n",
    "##handles = [plt.Rectangle((0, 0), 1, 1, color=model_colors[model]) for model in models]\n",
    "#fig.legend(handles, models, loc='upper center', ncol=min(num_models, 5))\n",
    "\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.93])\n",
    "#plt.suptitle(\"Deep Hermes LLaMA 3B & LLaMA Instruct 8B GSM8K (n=10)\", fontsize=12)\n",
    "plt.suptitle(\"LLaMA 8B Instruct GSM8K (n=10)\", fontsize=14)\n",
    "plt.savefig('Outputs/Report/gsm8k_qa/llama8b_subplots.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = [\n",
    "    'Perplexity',\n",
    "    'CPU Usage (%)',\n",
    "    'RAM Usage (%)',\n",
    "    'GPU Memory (MB)',\n",
    "    'Activation Similarity',\n",
    "    'Latency (s)',\n",
    "    \"Last Layer Mean Activation\",\n",
    "    \"Last Layer Activation Std\",\n",
    "    \"Mean Logits\",\n",
    "    \"Logit Std\",\n",
    "]\n",
    "\n",
    "# Generate a color map\n",
    "models = df['Model'].tolist()\n",
    "num_models = len(models)\n",
    "colors = cm.get_cmap('coolwarm', num_models)\n",
    "model_colors = {model: colors(i) for i, model in enumerate(models)}\n",
    "\n",
    "# Prepare subplot grid dynamically\n",
    "n_metrics = len(metrics)\n",
    "ncols = 5\n",
    "nrows = int(np.ceil(n_metrics / ncols))\n",
    "\n",
    "plt.rcParams['font.family'] = 'Times New Roman'\n",
    "fig, axes = plt.subplots(nrows=nrows, ncols=ncols, figsize=(20, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, metric in enumerate(metrics):\n",
    "    ax = axes[i]\n",
    "    if metric in df.columns:\n",
    "        values = df[metric].copy()\n",
    "\n",
    "        # Normalize perplexity via log-scale\n",
    "        if metric == \"Perplexity\":\n",
    "            values = np.log1p(values)  # log1p handles 0 gracefully\n",
    "\n",
    "        for j, model in enumerate(models):\n",
    "            ax.bar(model, values[j], color=model_colors[model])\n",
    "        ax.set_title(metric + (\" (log)\" if metric == \"Perplexity\" else \"\"), fontsize=12)\n",
    "        ax.set_xticks(range(len(models)))\n",
    "        ax.set_xticklabels(models, rotation=45, ha='right', fontsize=10)\n",
    "        ax.grid(True)\n",
    "\n",
    "# Hide unused axes\n",
    "for j in range(i + 1, len(axes)):\n",
    "    fig.delaxes(axes[j])\n",
    "\n",
    "# Legend\n",
    "##handles = [plt.Rectangle((0, 0), 1, 1, color=model_colors[model]) for model in models]\n",
    "#fig.legend(handles, models, loc='upper center', ncol=min(num_models, 5))\n",
    "\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.93])\n",
    "plt.suptitle(\"Deep Hermes 3B LLaMA & LLaMA 8B Instruct GSM8K (n=10)\", fontsize=14)\n",
    "plt.savefig('Outputs/Report/gsm8k_qa/llamadh3b_subplots_normalized_perplexity.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metrics to include (excluding 'Activation Similarity')\n",
    "metrics_for_corr = [\n",
    "    'Perplexity',\n",
    "    'CPU Usage (%)',\n",
    "    'RAM Usage (%)',\n",
    "    'GPU Memory (MB)',\n",
    "    'Latency (s)',\n",
    "    \"Last Layer Mean Activation\",\n",
    "    \"Last Layer Activation Std\",\n",
    "    \"Mean Logits\",\n",
    "    \"Logit Std\",\n",
    "]\n",
    "\n",
    "# Compute correlation\n",
    "corr_matrix = df[metrics_for_corr].corr()\n",
    "\n",
    "# Plot\n",
    "plt.rcParams['font.family'] = 'Times New Roman'\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(\n",
    "    corr_matrix, \n",
    "    annot=True, \n",
    "    cmap='coolwarm', \n",
    "    fmt=\".2f\", \n",
    "    linewidths=0.5, \n",
    "    square=True,\n",
    "    cbar_kws={\"shrink\": 0.75}\n",
    ")\n",
    "plt.title(\"LLaMA 8B Instruct GSM8K (n=10)\", fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.savefig('Outputs/Report/gsm8k_qa/llama8b_corr_heatmap.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "original = torch.randn(512) * 0.5  # Original activations\n",
    "\n",
    "def quantize_dequantize(tensor, scale_value):\n",
    "    scale = max(scale_value, 1e-8)\n",
    "    qmin, qmax = -127, 127\n",
    "    tensor_int = (tensor / scale).round().clamp(qmin, qmax).to(torch.int8)\n",
    "    tensor_dequant = tensor_int.float() * scale\n",
    "    return tensor_int, tensor_dequant\n",
    "\n",
    "# Quantize with different scales\n",
    "_, dequant_1e2 = quantize_dequantize(original, 1e-2)\n",
    "_, dequant_1e5 = quantize_dequantize(original, 1e-5)\n",
    "\n",
    "# L2 distance\n",
    "print(\"L2 Distance (scale=1e-2):\", torch.norm(original - dequant_1e2).item())\n",
    "print(\"L2 Distance (scale=1e-5):\", torch.norm(original - dequant_1e5).item())\n",
    "\n",
    "# Plot histograms + KDEs\n",
    "plt.rcParams['font.family'] = 'Times New Roman'\n",
    "plt.figure(figsize=(14, 6))\n",
    "sns.histplot(original.numpy(),bel='Original', kde=True, stat=\"count\", bins=50, color='black', alpha=0.5)\n",
    "sns.histplot(dequant_1e2.numpy(), label='Dequant (scale=1e-2)', kde=True, stat=\"count\", bins=50, color='red', alpha=0.5)\n",
    "sns.histplot(dequant_1e5.numpy(), label='Dequant (scale=1e-5)', kde=True, stat=\"count\", bins=50, color='blue', alpha=0.5)\n",
    "\n",
    "plt.title(\"Histogram (Count) + KDE of Quantized vs Original Activations\")\n",
    "plt.xlabel(\"Activation Value\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.ylim(0, 40)  \n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "datasets = {\n",
    "    \"GSM8K Query\": gsm8k_questions,  # 20 samples\n",
    "    \"GSM8K Answer\": gsm8k_answers,\n",
    "    \"Natural Questions Query\": nq_queries,\n",
    "    \"Natural Questions Answer\": nq_answers,\n",
    "}\n",
    "\n",
    "\n",
    "length_data = []\n",
    "for name, samples in datasets.items():\n",
    "    for sample in samples:\n",
    "        length_data.append({\n",
    "            \"Dataset\": name,\n",
    "            \"Length\": len(sample) \n",
    "        })\n",
    "\n",
    "\n",
    "df = pd.DataFrame(length_data)\n",
    "\n",
    "sns.set(style=\"whitegrid\")\n",
    "plt.rcParams['font.family'] = 'Times New Roman'\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.boxplot(data=df, x=\"Dataset\", y=\"Length\", palette=\"tab20\")\n",
    "sns.stripplot(data=df, x=\"Dataset\", y=\"Length\", color=\"black\", alpha=0.5, jitter=True)\n",
    "\n",
    "#plt.title(\"Sequence Length Distribution by Dataset\")\n",
    "plt.legend(df['Dataset'].unique(), fontsize=20)\n",
    "plt.ylabel(\"Length\")\n",
    "plt.grid(True, linestyle=\"--\", alpha=0.4)\n",
    "plt.tight_layout()\n",
    "#plt.savefig('datasets_lengths.jpg')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = {\n",
    "    \"GSM8K Query\": gsm8k_questions,  # 20 samples\n",
    "    \"GSM8K Answer\": gsm8k_answers,\n",
    "    \"Natural Questions Query\": nq_queries,\n",
    "    \"Natural Questions Answer\": nq_answers,\n",
    "}\n",
    "\n",
    "\n",
    "length_data = []\n",
    "for name, samples in datasets.items():\n",
    "    for sample in samples:\n",
    "        length_data.append({\n",
    "            \"Dataset\": name,\n",
    "            \"Length\": len(sample) \n",
    "        })\n",
    "\n",
    "\n",
    "df = pd.DataFrame(length_data)\n",
    "\n",
    "\n",
    "fig = px.box(df, x=\"Dataset\", y=\"Length\", points=\"all\", color=\"Dataset\",\n",
    "             color_discrete_sequence=px.colors.diverging.Spectral)\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"Sequence Length Distribution by Dataset\",\n",
    "    yaxis_title=\"Length\",\n",
    "    font_family=\"DejaVu Sans\",\n",
    "    showlegend=False,\n",
    "    plot_bgcolor='white'\n",
    ")\n",
    "\n",
    "fig.update_yaxes(gridcolor='lightgray')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute correlation matrix\n",
    "corr_matrix = df[metrics_for_corr].corr().round(2)\n",
    "\n",
    "# Plot heatmap\n",
    "fig = go.Figure(data=go.Heatmap(\n",
    "    z=corr_matrix.values,\n",
    "    x=corr_matrix.columns,\n",
    "    y=corr_matrix.index,\n",
    "    colorscale='RdBu',\n",
    "    zmin=-1, zmax=1,\n",
    "    colorbar=dict(title='Correlation'),\n",
    "    hovertemplate='Correlation: %{z}<extra></extra>'\n",
    "))\n",
    "\n",
    "# Add text annotations\n",
    "for i in range(len(corr_matrix)):\n",
    "    for j in range(len(corr_matrix)):\n",
    "        fig.add_annotation(dict(\n",
    "            x=corr_matrix.columns[j],\n",
    "            y=corr_matrix.index[i],\n",
    "            text=str(corr_matrix.values[i][j]),\n",
    "            showarrow=False,\n",
    "            font=dict(color=\"black\" if abs(corr_matrix.values[i][j]) < 0.7 else \"white\")\n",
    "        ))\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"LLaMA 8B Instruct GSM8K (n=10)\",\n",
    "    font_family=\"Times New Roman\",\n",
    "    height=700,\n",
    "    width=800,\n",
    "    plot_bgcolor=\"white\"\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# Define models and metrics\n",
    "models = df['Model'].tolist()\n",
    "n_models = len(models)\n",
    "n_metrics = len(metrics)\n",
    "ncols = 5\n",
    "nrows = int(np.ceil(n_metrics / ncols))\n",
    "\n",
    "# Create subplot grid\n",
    "fig = make_subplots(rows=nrows, cols=ncols, subplot_titles=metrics)\n",
    "\n",
    "# Assign each metric to subplot\n",
    "for idx, metric in enumerate(metrics):\n",
    "    row = idx // ncols + 1\n",
    "    col = idx % ncols + 1\n",
    "\n",
    "    if metric in df.columns:\n",
    "        values = df[metric].copy()\n",
    "\n",
    "        # Normalize perplexity via log-scale\n",
    "        if metric == \"Perplexity\":\n",
    "            values = np.log1p(values)\n",
    "\n",
    "        fig.add_trace(go.Bar(\n",
    "            x=models,\n",
    "            y=values,\n",
    "            marker_color='indianred',\n",
    "            text=values.round(2),\n",
    "            textposition='outside',\n",
    "            name=metric\n",
    "        ), row=row, col=col)\n",
    "\n",
    "# Layout aesthetics\n",
    "fig.update_layout(\n",
    "    height=1000,\n",
    "    width=1500,\n",
    "    title_text=\"Deep Hermes 3B LLaMA & LLaMA 8B Instruct GSM8K (n=10)\",\n",
    "    showlegend=False,\n",
    "    font=dict(family=\"Times New Roman\")\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MechInterp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
