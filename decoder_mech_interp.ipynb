{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "from typing import Tuple, List, Dict, Optional, Any\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "\"\"\"import torch.nn as nn\n",
    "import torch.nn.functional as F\"\"\"\n",
    "from pathlib import Path\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from datasets import load_dataset, DownloadMode\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import tqdm\n",
    "\n",
    "from helper_utils.enum_keys import (\n",
    "    FPKey,\n",
    "    DirPath,\n",
    "    QuantStyle,\n",
    "    MiscPrompts,\n",
    "    Contexts,\n",
    "    Texts,\n",
    "    ModelKey\n",
    ")\n",
    "\n",
    "from PTQ.apply_ptq import applyPTQ\n",
    "from PTQ.olmo_act_fns import patch_olmo_mlp\n",
    "import helper_utils.utils as utils\n",
    "\n",
    "from mech_interp_utils.utils_main.src.transformer_utils import (\n",
    "    logit_lens,\n",
    "    activation_lens,\n",
    "    dictionary_learning,\n",
    "    chatbot_analysis\n",
    ")\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"import torch._dynamo\n",
    "torch._dynamo.config.suppress_errors = True\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LL_DIR = 'Outputs/Report/LogitLens'\n",
    "SAE_DIR = 'Outputs/SAE'\n",
    "MISC_DIR = 'Outputs/Report/Misc'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = r'D:\\ThesisData\\wikitext'\n",
    "\n",
    "destination_path = str(Path(filepath))\n",
    "dataset = load_dataset(\n",
    "    'wikitext', 'wikitext-103-raw-v1',\n",
    "    split={\n",
    "        'train': 'train[:30%]',\n",
    "        'validation': 'validation[:10%]',\n",
    "        'test': 'test[:10%]',\n",
    "    },\n",
    "    cache_dir=destination_path,\n",
    "    download_mode=DownloadMode.REUSE_DATASET_IF_EXISTS,\n",
    "    keep_in_memory=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_texts = dataset['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calibration_texts = [t for t in dataset['train'][\"text\"] if isinstance(t, str) and t.strip()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_txts = train_texts.take(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calibration_texts = [t for t in sub_txts[\"text\"] if isinstance(t, str) and t.strip()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PARAMS:Dict = {\n",
    "    'context': Contexts.C1.value,\n",
    "    'prompt': MiscPrompts.Q11.value,\n",
    "    'max_new_tokens': 250,\n",
    "    'temperature': 0.8,\n",
    "    'repetition_penalty': 1.1,\n",
    "    'sample': True,\n",
    "    'device': None\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models and Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load fp models func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_test_model(model_path:str, dtype=torch.dtype) -> AutoModelForCausalLM:\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_path,\n",
    "        return_dict=True,\n",
    "        output_hidden_states=True,\n",
    "        torch_dtype=dtype,\n",
    "        low_cpu_mem_usage=True,\n",
    "        local_files_only=True,\n",
    "        use_safetensors=True\n",
    "    )\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "olmo_tokenizer = AutoTokenizer.from_pretrained(FPKey.OLMO1B_TOKENIZER.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "olmo_fp16 = load_test_model(FPKey.OLMO1B_FP.value, dtype=torch.float16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "olmo_fp16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "olmo_fp16.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "olmo1b_bitnet_fp32 = applyPTQ(\n",
    "    load_test_model(FPKey.OLMO1B_FP.value, dtype=torch.float32),\n",
    "    tokenizer=olmo_tokenizer,\n",
    "    #calibration_input=sub_txts['text'],\n",
    "    calibration_input=PARAMS.get('prompt'),\n",
    "    mode='1.58bit',\n",
    "    safer_quant=True,\n",
    "    model_half=False,\n",
    "    quant_half=False,\n",
    "    layers_to_quant_weights=QuantStyle.BITNET.value,\n",
    "    layers_to_quant_activations=QuantStyle.BITNET.value,\n",
    "    fragile_layers=False,\n",
    "    act_quant=True,\n",
    "    act_bits=8,\n",
    "    debugging=True,\n",
    "    plot_debugging=False,\n",
    "    plot_quantization=False,\n",
    "    freeze_modules=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "olmo1b_bitnet_fp32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FP model and tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deep3b_tokenizer = AutoTokenizer.from_pretrained(FPKey.TOKENIZER_3B.value, local_files_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deep3b_fp32 = load_test_model(FPKey.FP16_3B.value, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deep3b_fp32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deep3b_fp16 = load_test_model(FPKey.FP16_3B.value, dtype=torch.float16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, v in deep3b_fp32.named_parameters():\n",
    "    print(v.data.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quantized Deep Hermes 3B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deep3b_fp32_158bit_ptsq = applyPTQ(\n",
    "    load_test_model(FPKey.FP16_3B.value, dtype=torch.float32),\n",
    "    tokenizer=deep3b_tokenizer,\n",
    "    calibration_input=\"The quick brown fox jumps over the  lazy dog\",\n",
    "    mode='1.58bit',\n",
    "    qkv_safety=False,\n",
    "    safer_quant=False,\n",
    "    ffq=False,\n",
    "    model_half=False,\n",
    "    quant_half=True,\n",
    "    layers_to_quant=QuantStyle.BITNET.value,\n",
    "    act_quant=True,\n",
    "    act_bits=8,\n",
    "    dropout_prob=0.0,\n",
    "    redundancy=0,\n",
    "    frame_dropout_prob=0.0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chatbot Analysis (text template only)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3B Version Dicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"dict_3b_fp16_sym = {\n",
    "    #'3b-fp16': deep3b_fp16,\n",
    "    #'3b-fp16-1.58bit-ptsq': deep3b_fp16_158bit_ptsq,\n",
    "    #'3b-fp16-2bit-ptsq-sym': deep3b_fp16_2bit_ptsq_sym,\n",
    "    #'3b-fp16-2bit-ffsq-sym': deep3b_fp16_2bit_ffsq_sym,\n",
    "}\"\"\"\n",
    "\n",
    "dict_3b_fp32_sym = {\n",
    "    #'3b-fp32': deep3b_fp32,\n",
    "    '3b-fp32-1.58bit-ptsq': deep3b_fp32_158bit_ptsq,\n",
    "    #'3b-fp32-2bit-ffsq-sym': deep3b_fp32_2bit_ffsq_sym,\n",
    "    #'3b-fp32-2bit-ptsq-sym': deep3b_fp32_2bit_ptsq_sym\n",
    "}\n",
    "\"\"\"\n",
    "dict_3b_fp16_asym = {\n",
    "    '3b-fp16': deep3b_fp16,\n",
    "    '3b-fp16-1.58bit-ptsq': deep3b_fp16_158bit_ptsq,\n",
    "    '3b-fp16-2bit-ffsq-asym': deep3b_fp16_2bit_ffsq_asym,\n",
    "    '3b-fp16-2bit-ptsq-asym': deep3b_fp16_2bit_ptsq_asym\n",
    "}\n",
    "\n",
    "dict_3b_fp32_asym = {\n",
    "    '3b-fp32': deep3b_fp32,\n",
    "    '3b-fp32-1.58bit-ptsq': deep3b_fp32_158bit_ptsq,\n",
    "    '3b-fp32-2bit-ffsq-asym': deep3b_fp32_2bit_ffsq_asym,\n",
    "    '3b-fp32-2bit-ptsq-asym': deep3b_fp32_2bit_ptsq_asym\n",
    "}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chatbot_analysis.run_chatbot_analysis(\n",
    "    models=dict_3b_fp32_sym,\n",
    "    tokenizer=deep3b_tokenizer,\n",
    "    #device='cpu',\n",
    "    full_path='logs/chatbot_logs/DeepHermes3B/SymPTQ'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chatbot_analysis.plot_chatbot_analysis(\n",
    "    json_logs='logs/chatbot_logs/DeepHermes3B/SymPTQ',\n",
    "    parallel_plot=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activation Lens and Logit Lens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logit_lens.plot_logit_lens(\n",
    "    model=olmo_fp16,\n",
    "    tokenizer=olmo_tokenizer,\n",
    "    input_ids=MiscPrompts.Q11.value,\n",
    "    start_ix=0, end_ix=15,\n",
    "    #save_fig_path=None,\n",
    "    save_fig_path='Outputs/LogitLens/logits_allenai-olmo1b-fp16.jpg',\n",
    "    #entropy=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logit_lens.plot_logit_lens(\n",
    "    model=olmo1b_bitnet_fp32,\n",
    "    tokenizer=olmo_tokenizer,\n",
    "    input_ids=MiscPrompts.Q11.value,\n",
    "    start_ix=0, end_ix=15,\n",
    "    #save_fig_path=None,\n",
    "    save_fig_path='Outputs/LogitLens/logits_allenai-olmo1b-singlecalibration.jpg',\n",
    "    #entropy=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logit_lens.plot_comparing_lens(\n",
    "    models=(olmo_fp16, olmo1b_bitnet_fp32),\n",
    "    tokenizer=olmo_tokenizer,\n",
    "    input_ids=PARAMS.get('prompt'),\n",
    "    start_ix=0, end_ix=15,\n",
    "    save_fig_path='Outputs/LogitLens/nwd_allenai-olmo1bfp16-ptsq-singlecalibration.jpg',\n",
    "    #save_fig_path=None,\n",
    "    wasserstein=True,\n",
    "    #top_down=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "olmo1b_bitnet_fp32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logit_lens.plot_topk_lens(\n",
    "    model=olmo_fp16,\n",
    "    tokenizer=olmo_tokenizer,\n",
    "    input_ids=PARAMS.get('prompt'),\n",
    "    start_ix=0, end_ix=15,\n",
    "    topk_n=5,\n",
    "    save_fig_path='Outputs/LogitLens/tokk5-logits_allenai-olmo1b-fp16.jpg'\n",
    "    #save_fig_path=None,\n",
    "    #entropy=True,\n",
    "    #top_down=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logit_lens.plot_topk_lens(\n",
    "    model=olmo1b_bitnet_fp32,\n",
    "    tokenizer=olmo_tokenizer,\n",
    "    input_ids=PARAMS.get('prompt'),\n",
    "    start_ix=0, end_ix=15,\n",
    "    topk_n=5,\n",
    "    save_fig_path='Outputs/LogitLens/topk5-logits_allenai-olmo1b-ptdq.jpg'\n",
    "    #save_fig_path=None,\n",
    "    #entropy=True,\n",
    "    #top_down=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activation_lens.plot_activation_lens(\n",
    "    model=olmo_fp16,\n",
    "    tokenizer=olmo_tokenizer,\n",
    "    input_ids=PARAMS.get('prompt'),\n",
    "    start_ix=0, end_ix=15,\n",
    "    metric='norm',\n",
    "    save_fig_path='Outputs/LogitLens/actnorm_allenai-olmo1b-fp16.jpg'\n",
    "    #save_fig_path=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activation_lens.plot_activation_lens(\n",
    "    model=olmo1b_bitnet_fp32,\n",
    "    tokenizer=olmo_tokenizer,\n",
    "    input_ids=PARAMS.get('prompt'),\n",
    "    start_ix=0, end_ix=15,\n",
    "    metric='norm',\n",
    "    save_fig_path='Outputs/LogitLens/actnorm_allenai-olmo1b-ptsdq.jpg'\n",
    "    #save_fig_path=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activation_lens.plot_comparing_act_lens(\n",
    "    models=(olmo_fp16, olmo1b_bitnet_fp32),\n",
    "    tokenizer=olmo_tokenizer,\n",
    "    input_ids=PARAMS.get('prompt'),\n",
    "    start_ix=0, end_ix=15,\n",
    "    metric='norm',\n",
    "    metric_name='l2',\n",
    "    save_fig_path='Outputs/LogitLens/actnorml2_allenai-olmo1bfp16-ptdq.jpg'\n",
    "    #save_fig_path=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dictionary Learning: SAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary_learning.plot_sae_tokens(\n",
    "    model=olmo1b_bitnet_fp32,\n",
    "    tokenizer=olmo_tokenizer,\n",
    "    inputs=PARAMS.get('prompt'),\n",
    "    multi_tokens=False,\n",
    "    do_log=False,\n",
    "    target_layers=[5],\n",
    "    vis_projection=None,\n",
    "    log_path=None,\n",
    "    log_name=None,\n",
    "    fig_path=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary_learning.plot_sae_tokens(\n",
    "    model=olmo1b_bitnet_fp32,\n",
    "    tokenizer=olmo_tokenizer,\n",
    "    inputs=PARAMS.get('prompt'),\n",
    "    multi_tokens=True,\n",
    "    do_log=False,\n",
    "    target_layers=[5],\n",
    "    vis_projection=None,\n",
    "    log_path=None,\n",
    "    log_name=None,\n",
    "    fig_path=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary_learning.plot_sae_tokens(\n",
    "    model=olmo_fp16,\n",
    "    tokenizer=olmo_tokenizer,\n",
    "    inputs=PARAMS.get('prompt'),\n",
    "    multi_tokens=True,\n",
    "    do_log=False,\n",
    "    target_layers=[5],\n",
    "    vis_projection=None,\n",
    "    log_path=None,\n",
    "    log_name=None,\n",
    "    fig_path=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary_learning.plot_sae_heatmap(\n",
    "    model=olmo_fp16,\n",
    "    tokenizer=olmo_tokenizer,\n",
    "    inputs=Texts.T1.value,\n",
    "    do_log=False,\n",
    "    top_k=5,\n",
    "    tokens_per_row=30,\n",
    "    target_layers=[5],\n",
    "    log_path=None,\n",
    "    log_name=None,\n",
    "    fig_path=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary_learning.plot_sae_heatmap(\n",
    "    model=olmo1b_bitnet_fp32,\n",
    "    tokenizer=olmo_tokenizer,\n",
    "    inputs=Texts.T1.value,\n",
    "    do_log=False,\n",
    "    top_k=5,\n",
    "    tokens_per_row=30,\n",
    "    target_layers=[5],\n",
    "    log_path=None,\n",
    "    log_name=None,\n",
    "    fig_path=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary_learning.plot_comparing_heatmap(\n",
    "    models=(olmo_fp16, olmo1b_bitnet_fp32),\n",
    "    tokenizer=olmo_tokenizer,\n",
    "    inputs=Texts.T1.value,\n",
    "    top_k=5,\n",
    "    tokens_per_row=30,\n",
    "    target_layers=[5],\n",
    "    #fig_path='Outputs/PTQ/sae5_deep3b_fp32_158bit_sym-math.png',\n",
    "    fig_path=None\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MechInterp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
