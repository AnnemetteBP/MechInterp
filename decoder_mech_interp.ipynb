{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ampir\\anaconda3\\envs\\MechInterp\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from __future__ import annotations\n",
    "from typing import Tuple, List, Dict, Optional, Any\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from pathlib import Path\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from datasets import load_dataset, DownloadMode\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import tqdm\n",
    "\n",
    "from helper_utils.enum_keys import (\n",
    "    FPKey,\n",
    "    ModelKey,\n",
    "    QuantStyle,\n",
    "    MiscPrompts,\n",
    "    Contexts,\n",
    "    Texts\n",
    ")\n",
    "\n",
    "from PTQ.bitlinear_wrapper_class import BitLinear\n",
    "from PTQ.apply_ptq import applyPTQ\n",
    "from PTQ.olmo_act_fns import patch_olmo_mlp\n",
    "import helper_utils.utils as utils\n",
    "from helper_utils.models_loader import load_4bit_auto, load_8bit_auto\n",
    "from mech_interp_utils.utils_main.src.transformer_utils import (\n",
    "    logit_lens,\n",
    "    activation_lens,\n",
    "    dictionary_learning,\n",
    "    chatbot_analysis\n",
    ")\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "original = torch.randn(512) * 0.5  # Original activations\n",
    "\n",
    "def quantize_dequantize(tensor, scale_value):\n",
    "    scale = max(scale_value, 1e-8)\n",
    "    qmin, qmax = -127, 127\n",
    "    tensor_int = (tensor / scale).round().clamp(qmin, qmax).to(torch.int8)\n",
    "    tensor_dequant = tensor_int.float() * scale\n",
    "    return tensor_int, tensor_dequant\n",
    "\n",
    "# Quantize with different scales\n",
    "_, dequant_1e2 = quantize_dequantize(original, 1e-2)\n",
    "_, dequant_1e5 = quantize_dequantize(original, 1e-5)\n",
    "\n",
    "# L2 distance\n",
    "print(\"L2 Distance (scale=1e-2):\", torch.norm(original - dequant_1e2).item())\n",
    "print(\"L2 Distance (scale=1e-5):\", torch.norm(original - dequant_1e5).item())\n",
    "\n",
    "# Plot histograms + KDEs\n",
    "plt.figure(figsize=(14, 6))\n",
    "sns.histplot(original.numpy(),bel='Original', kde=True, stat=\"count\", bins=50, color='black', alpha=0.5)\n",
    "sns.histplot(dequant_1e2.numpy(), label='Dequant (scale=1e-2)', kde=True, stat=\"count\", bins=50, color='red', alpha=0.5)\n",
    "sns.histplot(dequant_1e5.numpy(), label='Dequant (scale=1e-5)', kde=True, stat=\"count\", bins=50, color='blue', alpha=0.5)\n",
    "\n",
    "plt.title(\"Histogram (Count) + KDE of Quantized vs Original Activations\")\n",
    "plt.xlabel(\"Activation Value\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.ylim(0, 40)  \n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch._dynamo\n",
    "torch._dynamo.config.suppress_errors = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datasets for calibrating activations and analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = r'D:\\ThesisData\\wikitext'\n",
    "\n",
    "destination_path = str(Path(filepath))\n",
    "dataset = load_dataset(\n",
    "    'wikitext', 'wikitext-103-raw-v1',\n",
    "    split={\n",
    "        'train': 'train[:200]',\n",
    "    },\n",
    "    cache_dir=destination_path,\n",
    "    download_mode=DownloadMode.REUSE_DATASET_IF_EXISTS,\n",
    "    keep_in_memory=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_texts = dataset['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calibration_texts = [t for t in dataset['train'][\"text\"] if isinstance(t, str) and t.strip()]\n",
    "#calibration_texts = [t for t in sub_txts[\"text\"] if isinstance(t, str) and t.strip()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_txts = train_texts.take(200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GSM8K (Math) \"gsm8k\"\n",
    "#### LogiQA (Logic & Reasoning): \"logiq\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = r'D:\\ThesisData\\nq'\n",
    "\n",
    "destination_path = str(Path(filepath))\n",
    "nq_dataset = load_dataset(\n",
    "    'sentence-transformers/natural-questions',\n",
    "    split={\n",
    "        'train': 'train[:20]'\n",
    "    },\n",
    "    cache_dir=destination_path,\n",
    "    download_mode=DownloadMode.REUSE_DATASET_IF_EXISTS,\n",
    "    keep_in_memory=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nq_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nq_queries= nq_dataset['train']['query']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nq_queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "nq_answers = nq_dataset['train']['answer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"Richmond Football Club Richmond began 2017 with 5 straight wins, a feat it had not achieved since 1995. A series of close losses hampered the Tigers throughout the middle of the season, including a 5-point loss to the Western Bulldogs, 2-point loss to Fremantle, and a 3-point loss to the Giants. Richmond ended the season strongly with convincing victories over Fremantle and St Kilda in the final two rounds, elevating the club to 3rd on the ladder. Richmond's first final of the season against the Cats at the MCG attracted a record qualifying final crowd of 95,028; the Tigers won by 51 points. Having advanced to the first preliminary finals for the first time since 2001, Richmond defeated Greater Western Sydney by 36 points in front of a crowd of 94,258 to progress to the Grand Final against Adelaide, their first Grand Final appearance since 1982. The attendance was 100,021, the largest crowd to a grand final since 1986. The Crows led at quarter time and led by as many as 13, but the Tigers took over the game as it progressed and scored seven straight goals at one point. They eventually would win by 48 points – 16.12 (108) to Adelaide's 8.12 (60) – to end their 37-year flag drought.[22] Dustin Martin also became the first player to win a Premiership medal, the Brownlow Medal and the Norm Smith Medal in the same season, while Damien Hardwick was named AFL Coaches Association Coach of the Year. Richmond's jump from 13th to premiers also marked the biggest jump from one AFL season to the next.\",\n",
       " 'Jack Scott (singer) At the beginning of 1960, Scott again changed record labels, this time to Top Rank Records.[1] He then recorded four Billboard Hot 100 hits\\xa0– \"What in the World\\'s Come Over You\" (#5), \"Burning Bridges\" (#3) b/w \"Oh Little One\" (#34), and \"It Only Happened Yesterday\" (#38).[1] \"What in the World\\'s Come Over You\" was Scott\\'s second gold disc winner.[6] Scott continued to record and perform during the 1960s and 1970s.[1] His song \"You\\'re Just Gettin\\' Better\" reached the country charts in 1974.[1] In May 1977, Scott recorded a Peel session for BBC Radio\\xa01 disc jockey, John Peel.',\n",
       " 'Wool Global wool production is about 2 million tonnes per year, of which 60% goes into apparel. Wool comprises ca 3% of the global textile market, but its value is higher owing to dying and other modifications of the material.[1] Australia is a leading producer of wool which is mostly from Merino sheep but has been eclipsed by China in terms of total weight.[30] New Zealand (2016) is the third-largest producer of wool, and the largest producer of crossbred wool. Breeds such as Lincoln, Romney, Drysdale, and Elliotdale produce coarser fibers, and wool from these sheep is usually used for making carpets.',\n",
       " 'Alaska: The Last Frontier Alaska: The Last Frontier is an American reality cable television series on the Discovery Channel, currently in its 7th season of broadcast. The show documents the extended Kilcher family, descendants of Swiss immigrants and Alaskan pioneers, Yule and Ruth Kilcher, at their homestead 11 miles outside of Homer.[1] By living without plumbing or modern heating, the clan chooses to subsist by farming, hunting and preparing for the long winters.[2] The Kilcher family are relatives of the singer Jewel,[1][3] who has appeared on the show.[4]',\n",
       " 'All I Want (A Day to Remember song) The music video for the song, which was filmed in October 2010,[4] was released on January 6, 2011.[5] It features cameos of numerous popular bands and musicians. The cameos are: Tom Denney (A Day to Remember\\'s former guitarist), Pete Wentz, Winston McCall of Parkway Drive, The Devil Wears Prada, Bring Me the Horizon, Sam Carter of Architects, Tim Lambesis of As I Lay Dying, Silverstein, Andrew WK, August Burns Red, Seventh Star, Matt Heafy of Trivium, Vic Fuentes of Pierce the Veil, Mike Herrera of MxPx, and Set Your Goals.[5] Rock Sound called the video \"quite excellent\".[5]',\n",
       " 'Flag of the United States The flag of the United States of America, often referred to as the American flag, is the national flag of the United States. It consists of thirteen equal horizontal stripes of red (top and bottom) alternating with white, with a blue rectangle in the canton (referred to specifically as the \"union\") bearing fifty small, white, five-pointed stars arranged in nine offset horizontal rows, where rows of six stars (top and bottom) alternate with rows of five stars. The 50 stars on the flag represent the 50 states of the United States of America, and the 13 stripes represent the thirteen British colonies that declared independence from the Kingdom of Great Britain, and became the first states in the U.S.[1] Nicknames for the flag include The Stars and Stripes,[2] Old Glory,[3] and The Star-Spangled Banner.',\n",
       " 'Diary of a Wimpy Kid (film) Filming of Diary of a Wimpy Kid was in Vancouver and wrapped up on October 16, 2009.',\n",
       " 'Beasts of the Southern Wild The film\\'s fictional setting, \"Isle de Charles Doucet\", known to its residents as the Bathtub, was inspired by several isolated and independent fishing communities threatened by erosion, hurricanes and rising sea levels in Louisiana\\'s Terrebonne Parish, most notably the rapidly eroding Isle de Jean Charles. It was filmed in Terrebonne Parish town Montegut.[5]',\n",
       " \"Mollisol Mollisols occur in savannahs and mountain valleys (such as Central Asia, or the North American Great Plains). These environments have historically been strongly influenced by fire and abundant pedoturbation from organisms such as ants and earthworms. It was estimated that in 2003, only 14 to 26 percent of grassland ecosystems still remained in a relatively natural state (that is, they were not used for agriculture due to the fertility of the A horizon). Globally, they represent ~7% of ice-free land area. As the world's most agriculturally productive soil order, the Mollisols represent one of the more economically important soil orders.\",\n",
       " \"Foster's Home for Imaginary Friends McCracken conceived the series after adopting two dogs from an animal shelter and applying the concept to imaginary friends. The show first premiered on Cartoon Network on August 13, 2004, as a 90-minute television film. On August 20, it began its normal run of twenty-to-thirty-minute episodes on Fridays, at 7 pm. The series finished its run on May 3, 2009, with a total of six seasons and seventy-nine episodes. McCracken left Cartoon Network shortly after the series ended. Reruns have aired on Boomerang from August 11, 2012 to November 3, 2013 and again from June 1, 2014 to April 3, 2017.\",\n",
       " 'Role of the United States in the Vietnam War The role of the United States in the Vietnam War began after World War II and escalated into full commitment during the Vietnam War from 1955 to 1975.',\n",
       " 'Declaration of the Rights of Man and of the Citizen The concepts in the Declaration come from the philosophical and political duties of the Enlightenment, such as individualism, the social contract as theorized by the Genevan philosopher Rousseau, and the separation of powers espoused by the Baron de Montesquieu. As can be seen in the texts, the French declaration was heavily influenced by the political philosophy of the Enlightenment and principles of human rights as was the U.S. Declaration of Independence which preceded it (4 July 1776).',\n",
       " 'Kashmir conflict The Kashmir conflict is a territorial conflict primarily between India and Pakistan, having started just after the partition of India in 1947. China has at times played a minor role.[2] India and Pakistan have fought three wars over Kashmir, including the Indo-Pakistani Wars of 1947 and 1965, as well as the Kargil War of 1999. The two countries have also been involved in several skirmishes over control of the Siachen Glacier.',\n",
       " '16th Summit of the Non-Aligned Movement The summit consisted of two preceding events: a \"Senior Officials Meeting\" on 26 and 27 August 2012, and a \"Ministerial Meeting\" on 28 and 29 August 2012. The leaders summit took place on 30 and 31 August.[5][7] Egyptian President Mohammad Morsi officially handed the presidency of the Non-Aligned Movement (NAM) to Iranian President Mahmoud Ahmadinejad, during the inaugural ceremony of Leaders\\' Meeting.[8] Iran will hold the NAM presidency for four years until the 17th summit in Venezuela in 2016.',\n",
       " \"Jack and the Beanstalk According to researchers at the universities in Durham and Lisbon, the story originated more than 5,000 years ago, based on a widespread archaic story form which is now classified by folklorists as ATU 328 The Boy Who Stole Ogre's Treasure.[7]\",\n",
       " \"Doctor of Medicine Historically, Australian medical schools have followed the British tradition by conferring the degrees of Bachelor of Medicine and Bachelor of Surgery (MBBS) to its graduates whilst reserving the title of Doctor of Medicine (MD) for their research training degree, analogous to the PhD, or for their honorary doctorates. Although the majority of Australian MBBS degrees have been graduate programs since the 1990s, under the previous Australian Qualifications Framework (AQF) they remained categorized as Level 7 Bachelor's degrees together with other undergraduate programs.\",\n",
       " \"Prince Hamlet Prince Hamlet is the title character and protagonist of William Shakespeare's tragedy Hamlet. He is the Prince of Denmark, nephew to the usurping Claudius, and son of King Hamlet, the previous King of Denmark. At the beginning of the play, he struggles with whether, and how, to avenge the murder of his father, and struggles with his own sanity along the way. By the end of the tragedy, Hamlet has caused the deaths of Polonius, Laertes, Claudius, and two acquaintances of his from the University of Wittenberg Rosencrantz and Guildenstern. He is also indirectly involved in the deaths of his love Ophelia (drowning) and of his mother Gertrude (poisoned by Claudius by mistake).\",\n",
       " 'Achy Breaky Heart \"Achy Breaky Heart\" is a country song written by Don Von Tress. Originally titled \"Don\\'t Tell My Heart\" and performed by The Marcy Brothers in 1991, its name was later changed to \"Achy Breaky Heart\" and performed by Billy Ray Cyrus on his 1992 album Some Gave All. The song is Cyrus\\' debut single and signature song, it made him famous and has been his most successful song. It became the first single ever to achieve triple Platinum status in Australia[1] and also 1992\\'s best-selling single in the same country.[2][3] In the United States it became a crossover hit on pop and country radio, peaking at number 4 on the Billboard Hot 100 and topping the Hot Country Songs chart, becoming the first country single to be certified Platinum since Kenny Rogers and Dolly Parton\\'s \"Islands in the Stream\" in 1983.[4] The single topped in several countries, and after being featured on Top of the Pops in the United Kingdom, peaked at number 3 on the UK Singles Chart. It remains Cyrus\\'s biggest hit single in the U.S. to date, and his only one to reach the top 10 of the Billboard Hot 100. Thanks to the video of this hit, there was the explosion of the line dance into the mainstream, becoming a craze.[5][6][7][8] The song is considered by some as one of the worst songs of all time, featuring at number two in VH1 and Blender\\'s list of the \"50 Most Awesomely Bad Songs Ever.\"[9]However it is recognized as a transitional period in country music where Cyrus brought renewed interest in a dying breed of music amongst younger listeners.',\n",
       " \"The Assassination of Gianni Versace: American Crime Story The Assassination of Gianni Versace: American Crime Story is the second season of the FX true crime anthology television series American Crime Story. The season premiered on January 17, 2018,[1][2] and concluded on March 21, 2018. It consists of a total of 9 episodes,[3] and explores the murder of designer Gianni Versace by serial killer Andrew Cunanan, based on Maureen Orth's book Vulgar Favors: Andrew Cunanan, Gianni Versace, and the Largest Failed Manhunt in U.S. History.[2][4]\",\n",
       " \"Paint Your Wagon (film) Paint Your Wagon was shot near Baker City, Oregon with filming beginning in May 1968 and ending in October.[2] Other locations include Big Bear Lake, California and San Bernardino National Forest; the interiors were filmed at Paramount Studios with Joshua Logan directing. The film's initial budget was $10\\xa0million, before it eventually doubled to $20\\xa0million. A daily expense of $80,000 was incurred to transport cast and crew to the filming location, as the closest hotel was nearly 60 miles away. The elaborate camp used in the film cost $2.4\\xa0million to build.[2]\"]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nq_answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = r'D:\\ThesisData\\logiqa'\n",
    "\n",
    "destination_path = str(Path(filepath))\n",
    "logiqa_dataset = load_dataset(\n",
    "    'lucasmccabe/logiqa',\n",
    "    split={\n",
    "        'train': 'train[:20]'\n",
    "    },\n",
    "    cache_dir=destination_path,\n",
    "    download_mode=DownloadMode.REUSE_DATASET_IF_EXISTS,\n",
    "    keep_in_memory=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logiqa_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = r'D:\\ThesisData\\gsm8k'\n",
    "\n",
    "destination_path = str(Path(filepath))\n",
    "gsm8k_dataset = load_dataset(\n",
    "    'gsm8k', 'main',\n",
    "    split={\n",
    "        'train': 'train[:20]'\n",
    "    },\n",
    "    cache_dir=destination_path,\n",
    "    download_mode=DownloadMode.REUSE_DATASET_IF_EXISTS,\n",
    "    keep_in_memory=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gsm8k_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gsm8k_questions = gsm8k_dataset['train']['question']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gsm8k_answers = gsm8k_dataset['train']['answer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gsm8k_answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gsm8k_questions_sae = \"\"\"\n",
    "Natalia sold clips to 48 of her friends in April, and then she sold half as many clips in May. How many clips did Natalia sell altogether in April and May?\n",
    "Weng earns $12 an hour for babysitting. Yesterday, she just did 50 minutes of babysitting. How much did she earn?\n",
    "Betty is saving money for a new wallet which costs $100. Betty has only half of the money she needs. Her parents decided to give her $15 for that purpose, and her grandparents twice as much as her parents. How much more money does Betty need to buy the wallet?\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models and Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_test_model(model_path:str, dtype=torch.dtype) -> AutoModelForCausalLM:\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_path,\n",
    "        return_dict=True,\n",
    "        output_hidden_states=True,\n",
    "        torch_dtype=dtype,\n",
    "        low_cpu_mem_usage=True,\n",
    "        local_files_only=True,\n",
    "        use_safetensors=True,\n",
    "        #trust_remote_code=True\n",
    "    )\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hfbit1_tokenizer = AutoTokenizer.from_pretrained(FPKey.HFBIT1_TOKENIZER.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hfbit1_fp32 = load_test_model(FPKey.HFBIT1_8B.value, dtype=torch.float32) # https://huggingface.co/HF1BitLLM/Llama3-8B-1.58-100B-tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print model layers to inspect their names\n",
    "for name, module in hfbit1_fp32.named_modules():\n",
    "    print(f\"Layer name: {name}, Module: {module}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "llama8b_tokenizer = AutoTokenizer.from_pretrained(FPKey.LINSTRUCT_TOKENIZER.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llama8b_fp32 = load_test_model(FPKey.LINSTRUCT_8B.value, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n",
      "`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:28<00:00,  7.04s/it]\n"
     ]
    }
   ],
   "source": [
    "# Load the model with 8-bit quantization using BNB\n",
    "llama8b_bnb8_float32 = AutoModelForCausalLM.from_pretrained(\n",
    "    ModelKey.LLINSTRUCT8B.value,           # Replace with your actual model\n",
    "    torch_dtype=torch.float32,     # Specify the dtype (for 8-bit, use uint8)\n",
    "    #device_map=\"cpu\",           # Automatically map the model to available devices (GPU/CPU)\n",
    "    load_in_8bit=True,\n",
    "    return_dict=True,\n",
    "    output_hidden_states=True,            # Set to True for 8-bit quantization\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llama8b_bnb8_float32.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n",
      "`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:16<00:00,  4.09s/it]\n"
     ]
    }
   ],
   "source": [
    "# Load the model with 4-bit quantization using BNB\n",
    "llama8b_bnb4_float32 = AutoModelForCausalLM.from_pretrained(\n",
    "    ModelKey.LLINSTRUCT8B.value,           # Replace with your actual model\n",
    "    torch_dtype=torch.float32,     # This would still use uint8 or another type based on quantization method\n",
    "    #device_map=\"auto\",           # Automatically map the model to available devices (GPU/CPU)\n",
    "    load_in_4bit=True,            # Set to True for 4-bit quantization (if available)\n",
    "    return_dict=True,\n",
    "    output_hidden_states=True, \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llama8b_ptsq_float32 = applyPTQ(\n",
    "    load_test_model(ModelKey.LLINSTRUCT8B.value, dtype=torch.float32),\n",
    "    tokenizer=llama8b_tokenizer,\n",
    "    #calibration_input=None,\n",
    "    #calibration_input=sub_txts['text'],\n",
    "    calibration_input=Texts.T1.value,\n",
    "    mode='1.58bit',\n",
    "    safer_quant=True,\n",
    "    q_lmhead=True,\n",
    "    model_half=False,\n",
    "    quant_half=False,\n",
    "    layers_to_quant_weights=QuantStyle.BITNET.value,\n",
    "    layers_to_quant_activations=QuantStyle.BITNET.value,\n",
    "    fragile_layers=False,\n",
    "    act_quant=True,\n",
    "    act_bits=8,\n",
    "    torch_backends=False,\n",
    "    debugging=True,\n",
    "    plot_debugging=False,\n",
    "    plot_quantization=False,\n",
    "    freeze_modules=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hfbit1_tokenizer.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llama8b_tokenizer.vocab_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### allenai/OLMo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "olmo1b_tokenizer = AutoTokenizer.from_pretrained(FPKey.OLMO1B_TOKENIZER.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "olmo2t_tokenizer = AutoTokenizer.from_pretrained(FPKey.OLMO7B2T_TOKENIZER.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "olmo1b_fp32 = load_test_model(FPKey.OLMO1B_FP.value, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "olmo2t_fp32 = load_test_model(FPKey.OLMO7B2T_FP.value, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "olmo1b_bitnet_fp32_ptsq = applyPTQ(\n",
    "    load_test_model(FPKey.OLMO1B_FP.value, dtype=torch.float32),\n",
    "    tokenizer=olmo1b_tokenizer,\n",
    "    #calibration_input=None,\n",
    "    #calibration_input=sub_txts['text'],\n",
    "    calibration_input=Texts.T1.value,\n",
    "    mode='1.58bit',\n",
    "    safer_quant=True,\n",
    "    q_lmhead=True,\n",
    "    model_half=False,\n",
    "    quant_half=False,\n",
    "    layers_to_quant_weights=QuantStyle.BITNET.value,\n",
    "    layers_to_quant_activations=QuantStyle.BITNET.value,\n",
    "    fragile_layers=False,\n",
    "    act_quant=True,\n",
    "    act_bits=8,\n",
    "    torch_backends=True,\n",
    "    debugging=True,\n",
    "    plot_debugging=False,\n",
    "    plot_quantization=False,\n",
    "    freeze_modules=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "olmo2t_bitnet_fp32_ptsq = applyPTQ(\n",
    "    load_test_model(FPKey.OLMO7B2T_FP.value, dtype=torch.float32),\n",
    "    tokenizer=olmo2t_tokenizer,\n",
    "    #calibration_input=None,\n",
    "    #calibration_input=sub_txts['text'],\n",
    "    calibration_input=Texts.T1.value,\n",
    "    mode='1.58bit',\n",
    "    safer_quant=True,\n",
    "    q_lmhead=True,\n",
    "    model_half=False,\n",
    "    quant_half=False,\n",
    "    layers_to_quant_weights=QuantStyle.BITNET.value,\n",
    "    layers_to_quant_activations=QuantStyle.BITNET.value,\n",
    "    fragile_layers=False,\n",
    "    act_quant=True,\n",
    "    act_bits=8,\n",
    "    torch_backends=False,\n",
    "    debugging=True,\n",
    "    plot_debugging=False,\n",
    "    plot_quantization=False,\n",
    "    freeze_modules=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model with 8-bit quantization using BNB\n",
    "olmo1b_bnb8_float32 = AutoModelForCausalLM.from_pretrained(\n",
    "    FPKey.OLMO1B_FP.value,           # Replace with your actual model\n",
    "    torch_dtype=torch.float32,     # Specify the dtype (for 8-bit, use uint8)\n",
    "    #device_map=\"cpu\",           # Automatically map the model to available devices (GPU/CPU)\n",
    "    load_in_8bit=True,\n",
    "    return_dict=True,\n",
    "    output_hidden_states=True,            # Set to True for 8-bit quantization\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model with 4-bit quantization using BNB\n",
    "olmo1b_bnb4_float32 = AutoModelForCausalLM.from_pretrained(\n",
    "    FPKey.OLMO1B_FP.value,           # Replace with your actual model\n",
    "    torch_dtype=torch.float32,     # This would still use uint8 or another type based on quantization method\n",
    "    #device_map=\"auto\",           # Automatically map the model to available devices (GPU/CPU)\n",
    "    load_in_4bit=True,            # Set to True for 4-bit quantization (if available)\n",
    "    return_dict=True,\n",
    "    output_hidden_states=True, \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NousResearch/DeepHermes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dh3b_tokenizer = AutoTokenizer.from_pretrained(FPKey.TOKENIZER_3B.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dh8b_tokenizer = AutoTokenizer.from_pretrained(FPKey.TOKENIZER_8B.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dh3b_fp32 = load_test_model(FPKey.FP_3B.value, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dh8b_fp32 = load_test_model(FPKey.FP_8B.value, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dh3b_bitnet_fp32_ptsq = applyPTQ(\n",
    "    load_test_model(FPKey.FP_3B.value, dtype=torch.float32),\n",
    "    tokenizer=dh3b_tokenizer,\n",
    "    #calibration_input=None,\n",
    "    #calibration_input=sub_txts['text'],\n",
    "    calibration_input=Texts.T1.value,\n",
    "    mode='1.58bit',\n",
    "    safer_quant=True,\n",
    "    q_lmhead=True,\n",
    "    model_half=False,\n",
    "    quant_half=False,\n",
    "    layers_to_quant_weights=QuantStyle.BITNET.value,\n",
    "    layers_to_quant_activations=QuantStyle.BITNET.value,\n",
    "    fragile_layers=False,\n",
    "    act_quant=True,\n",
    "    act_bits=8,\n",
    "    torch_backends=False,\n",
    "    debugging=True,\n",
    "    plot_debugging=False,\n",
    "    plot_quantization=False,\n",
    "    freeze_modules=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model with 8-bit quantization using BNB\n",
    "dh3b_bnb8_float32 = AutoModelForCausalLM.from_pretrained(\n",
    "    FPKey.FP_3B.value,           # Replace with your actual model\n",
    "    torch_dtype=torch.float32,     # Specify the dtype (for 8-bit, use uint8)\n",
    "    #device_map=\"cpu\",           # Automatically map the model to available devices (GPU/CPU)\n",
    "    load_in_8bit=True,\n",
    "    return_dict=True,\n",
    "    output_hidden_states=True,            # Set to True for 8-bit quantization\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model with 4-bit quantization using BNB\n",
    "dh3b_bnb4_float32 = AutoModelForCausalLM.from_pretrained(\n",
    "    FPKey.FP_3B.value,           # Replace with your actual model\n",
    "    torch_dtype=torch.float32,     # This would still use uint8 or another type based on quantization method\n",
    "    #device_map=\"auto\",           # Automatically map the model to available devices (GPU/CPU)\n",
    "    load_in_4bit=True,            # Set to True for 4-bit quantization (if available)\n",
    "    return_dict=True,\n",
    "    output_hidden_states=True, \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model with 8-bit quantization using BNB\n",
    "dh8b_bnb8_float32 = AutoModelForCausalLM.from_pretrained(\n",
    "    FPKey.FP_8B.value,           # Replace with your actual model\n",
    "    torch_dtype=torch.float32,     # Specify the dtype (for 8-bit, use uint8)\n",
    "    #device_map=\"cpu\",           # Automatically map the model to available devices (GPU/CPU)\n",
    "    load_in_8bit=True,\n",
    "    return_dict=True,\n",
    "    output_hidden_states=True,            # Set to True for 8-bit quantization\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model with 4-bit quantization using BNB\n",
    "dh8b_bnb4_float32 = AutoModelForCausalLM.from_pretrained(\n",
    "    FPKey.FP_8B.value,           # Replace with your actual model\n",
    "    torch_dtype=torch.float32,     # This would still use uint8 or another type based on quantization method\n",
    "    #device_map=\"auto\",           # Automatically map the model to available devices (GPU/CPU)\n",
    "    load_in_4bit=True,            # Set to True for 4-bit quantization (if available)\n",
    "    return_dict=True,\n",
    "    output_hidden_states=True, \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dh8b_bnb4_float32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activation Lens and Logit Lens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_inputs = [\n",
    "    # Language understanding\n",
    "    \"The quick brown fox jumps over the lazy dog.\",\n",
    "    \"Despite the rain, the event continued as planned.\",\n",
    "    \n",
    "    # Logic/reasoning\n",
    "    \"If all humans are mortal and Socrates is a human, then Socrates is mortal.\",\n",
    "    \"Either the lights are off or the power is out. The lights are on, so the power must be out.\",\n",
    "\n",
    "    # Math/numerical\n",
    "    \"The derivative of sin(x) with respect to x is cos(x).\",\n",
    "    \"What is the sum of the first 100 natural numbers?\",\n",
    "\n",
    "    # Programming\n",
    "    \"In Python, list comprehensions provide a concise way to create lists.\",\n",
    "    \"To define a function in JavaScript, use the 'function' keyword.\",\n",
    "\n",
    "    # Commonsense knowledge\n",
    "    \"You should refrigerate milk after opening it to keep it fresh.\",\n",
    "    \"People usually eat breakfast in the morning before starting their day.\",\n",
    "\n",
    "    # Scientific knowledge\n",
    "    \"Water boils at 100 degrees Celsius under standard atmospheric pressure.\",\n",
    "    \"Photosynthesis is the process by which plants convert sunlight into chemical energy.\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gsm8k_questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gsm8k_answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nq_queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [\"The quick brown fox jumps over the lazy dog.\", \"Despite the rain, the event continued as planned.\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch._dynamo\n",
    "torch._dynamo.config.suppress_errors = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Debug] Layer names being passed: ['model.layers.0', 'model.layers.1', 'model.layers.2', 'model.layers.3', 'model.layers.4', 'model.layers.5', 'model.layers.6', 'model.layers.7', 'model.layers.8', 'model.layers.9', 'model.layers.10', 'model.layers.11', 'model.layers.12', 'model.layers.13', 'model.layers.14', 'model.layers.15', 'model.layers.16', 'model.layers.17', 'model.layers.18', 'model.layers.19', 'model.layers.20', 'model.layers.21', 'model.layers.22', 'model.layers.23', 'model.layers.24', 'model.layers.25', 'model.layers.26', 'model.layers.27', 'model.layers.28', 'model.layers.29', 'model.layers.30', 'model.layers.31', 'model.embed_tokens']\n",
      "[Debug] Trying to access layer: model.layers.0\n",
      "[Debug] Successfully found layer: model.layers.0\n",
      "[Debug] Trying to access layer: model.layers.1\n",
      "[Debug] Successfully found layer: model.layers.1\n",
      "[Debug] Trying to access layer: model.layers.2\n",
      "[Debug] Successfully found layer: model.layers.2\n",
      "[Debug] Trying to access layer: model.layers.3\n",
      "[Debug] Successfully found layer: model.layers.3\n",
      "[Debug] Trying to access layer: model.layers.4\n",
      "[Debug] Successfully found layer: model.layers.4\n",
      "[Debug] Trying to access layer: model.layers.5\n",
      "[Debug] Successfully found layer: model.layers.5\n",
      "[Debug] Trying to access layer: model.layers.6\n",
      "[Debug] Successfully found layer: model.layers.6\n",
      "[Debug] Trying to access layer: model.layers.7\n",
      "[Debug] Successfully found layer: model.layers.7\n",
      "[Debug] Trying to access layer: model.layers.8\n",
      "[Debug] Successfully found layer: model.layers.8\n",
      "[Debug] Trying to access layer: model.layers.9\n",
      "[Debug] Successfully found layer: model.layers.9\n",
      "[Debug] Trying to access layer: model.layers.10\n",
      "[Debug] Successfully found layer: model.layers.10\n",
      "[Debug] Trying to access layer: model.layers.11\n",
      "[Debug] Successfully found layer: model.layers.11\n",
      "[Debug] Trying to access layer: model.layers.12\n",
      "[Debug] Successfully found layer: model.layers.12\n",
      "[Debug] Trying to access layer: model.layers.13\n",
      "[Debug] Successfully found layer: model.layers.13\n",
      "[Debug] Trying to access layer: model.layers.14\n",
      "[Debug] Successfully found layer: model.layers.14\n",
      "[Debug] Trying to access layer: model.layers.15\n",
      "[Debug] Successfully found layer: model.layers.15\n",
      "[Debug] Trying to access layer: model.layers.16\n",
      "[Debug] Successfully found layer: model.layers.16\n",
      "[Debug] Trying to access layer: model.layers.17\n",
      "[Debug] Successfully found layer: model.layers.17\n",
      "[Debug] Trying to access layer: model.layers.18\n",
      "[Debug] Successfully found layer: model.layers.18\n",
      "[Debug] Trying to access layer: model.layers.19\n",
      "[Debug] Successfully found layer: model.layers.19\n",
      "[Debug] Trying to access layer: model.layers.20\n",
      "[Debug] Successfully found layer: model.layers.20\n",
      "[Debug] Trying to access layer: model.layers.21\n",
      "[Debug] Successfully found layer: model.layers.21\n",
      "[Debug] Trying to access layer: model.layers.22\n",
      "[Debug] Successfully found layer: model.layers.22\n",
      "[Debug] Trying to access layer: model.layers.23\n",
      "[Debug] Successfully found layer: model.layers.23\n",
      "[Debug] Trying to access layer: model.layers.24\n",
      "[Debug] Successfully found layer: model.layers.24\n",
      "[Debug] Trying to access layer: model.layers.25\n",
      "[Debug] Successfully found layer: model.layers.25\n",
      "[Debug] Trying to access layer: model.layers.26\n",
      "[Debug] Successfully found layer: model.layers.26\n",
      "[Debug] Trying to access layer: model.layers.27\n",
      "[Debug] Successfully found layer: model.layers.27\n",
      "[Debug] Trying to access layer: model.layers.28\n",
      "[Debug] Successfully found layer: model.layers.28\n",
      "[Debug] Trying to access layer: model.layers.29\n",
      "[Debug] Successfully found layer: model.layers.29\n",
      "[Debug] Trying to access layer: model.layers.30\n",
      "[Debug] Successfully found layer: model.layers.30\n",
      "[Debug] Trying to access layer: model.layers.31\n",
      "[Debug] Successfully found layer: model.layers.31\n",
      "[Debug] Trying to access layer: model.embed_tokens\n",
      "[Debug] Successfully found layer: model.embed_tokens\n",
      "[Hook] Layer Embedding(128256, 4096) received input (tensor([[128000,  28276,  12669,  21424,  10349,  35348,   6137,    220,    679,\n",
      "             22,    449,    220,     20,   7833,  15160,     11,    264,  12627,\n",
      "            433,   1047,    539,  17427,   2533,    220,   2550,     20,     13,\n",
      "            362,   4101,    315,   3345,  18151,  13824,  43868,    279,  40816,\n",
      "           6957,    279,   6278,    315,    279,   3280,     11,   2737,    264,\n",
      "            220,     20,  16983,   4814,    311,    279,  11104,  76564,     11,\n",
      "            220,     17,  16983,   4814,    311,  68310,    519,    273,     11,\n",
      "            323,    264,    220,     18,  16983,   4814,    311,    279,  30835,\n",
      "             13,  35348,   9670,    279,   3280,  16917,    449,  40661,  46146,\n",
      "            927,  68310,    519,    273,    323,    800,    735,  56261,    304,\n",
      "            279,   1620,   1403,  20101,     11,  12231,   1113,    279,   6469,\n",
      "            311,    220,     18,   6634,    389,    279,  36865,     13,  35348,\n",
      "            596,   1176,   1620,    315,    279,   3280,   2403,    279,  51849,\n",
      "            520,    279,    386,   8974,  29123,    264,   3335,  37214,   1620,\n",
      "          13734,    315,    220,   2721,     11,  22000,     26,    279,  40816,\n",
      "           2834,    555,    220,   3971,   3585,     13,  20636,  11084,    311,\n",
      "            279,   1176,  33269,  41402,    369,    279,   1176,    892,   2533,\n",
      "            220,   1049,     16,     11,  35348,  24164,  33381,  11104,  21972,\n",
      "            555,    220,   1927,   3585,    304,   4156,    315,    264,  13734,\n",
      "            315,    220,   6281,     11,  15966,    311,   5208,    311,    279,\n",
      "          10517,  13321,   2403,  50301,     11,    872,   1176,  10517,  13321,\n",
      "          11341,   2533,    220,   3753,     17,     13,    578,  28116,    574,\n",
      "            220,   1041,     11,  11592,     11,    279,   7928,  13734,    311,\n",
      "            264,   6800,   1620,   2533,    220,   3753,     21,     13,    578,\n",
      "            356,   1849,   6197,    520,   8502,    892,    323,   6197,    555,\n",
      "            439,   1690,    439,    220,   1032,     11,    719,    279,  40816,\n",
      "           3952,    927,    279,   1847,    439,    433,  62916,    323,  16957,\n",
      "           8254,   7833,   9021,    520,    832,   1486,     13,   2435,   9778,\n",
      "           1053,   3243,    555,    220,   2166,   3585,   1389,    220,    845,\n",
      "             13,    717,    320,   6640,      8,    311,  50301,    596,    220,\n",
      "             23,     13,    717,    320,   1399,      8,   1389,    311,    842,\n",
      "            872,    220,   1806,   4771,   5292,  37846,   8032,   1313,     60,\n",
      "          79418,  11826,   1101,   6244,    279,   1176,   2851,    311,   3243,\n",
      "            264,  97988,  37712,     11,    279,  10690,  10516,  17867,    323,\n",
      "            279,  20935,   9259,  17867,    304,    279,   1890,   3280,     11,\n",
      "           1418,  90012,  11481,  21878,    574,   7086,  58018,   3623,  14576,\n",
      "          10229,  28275,    315,    279,   9941,     13,  35348,    596,   7940,\n",
      "            505,    220,   1032,    339,    311,   6954,   4918,   1101,  13160,\n",
      "            279,   8706,   7940,    505,    832,  58018,   3280,    311,    279,\n",
      "           1828,     13]], device='cuda:0'),) and output tensor([[[ 2.6512e-04, -4.9973e-04, -5.8365e-04,  ...,  3.8147e-03,\n",
      "           6.3419e-05,  1.1902e-03],\n",
      "         [-1.1063e-03, -2.1973e-02, -9.1553e-03,  ...,  1.0071e-03,\n",
      "           7.3242e-03,  1.0864e-02],\n",
      "         [ 8.5449e-03, -2.2583e-03, -1.1841e-02,  ..., -9.6893e-04,\n",
      "          -8.8501e-03,  1.9287e-02],\n",
      "         ...,\n",
      "         [ 1.2283e-03, -3.0060e-03, -4.1771e-04,  ...,  2.7008e-03,\n",
      "          -6.4697e-03,  2.9755e-03],\n",
      "         [-1.4221e-02, -2.4414e-02, -1.0132e-02,  ..., -1.2146e-02,\n",
      "          -1.8845e-03,  1.7853e-03],\n",
      "         [-6.7520e-04, -5.7602e-04,  4.7684e-04,  ...,  3.3264e-03,\n",
      "           3.9101e-04,  4.7493e-04]]], device='cuda:0')\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[ 2.6512e-04, -4.9973e-04, -5.8365e-04,  ...,  3.8147e-03,\n",
      "           6.3419e-05,  1.1902e-03],\n",
      "         [-1.1063e-03, -2.1973e-02, -9.1553e-03,  ...,  1.0071e-03,\n",
      "           7.3242e-03,  1.0864e-02],\n",
      "         [ 8.5449e-03, -2.2583e-03, -1.1841e-02,  ..., -9.6893e-04,\n",
      "          -8.8501e-03,  1.9287e-02],\n",
      "         ...,\n",
      "         [ 1.2283e-03, -3.0060e-03, -4.1771e-04,  ...,  2.7008e-03,\n",
      "          -6.4697e-03,  2.9755e-03],\n",
      "         [-1.4221e-02, -2.4414e-02, -1.0132e-02,  ..., -1.2146e-02,\n",
      "          -1.8845e-03,  1.7853e-03],\n",
      "         [-6.7520e-04, -5.7602e-04,  4.7684e-04,  ...,  3.3264e-03,\n",
      "           3.9101e-04,  4.7493e-04]]], device='cuda:0'),) and output (tensor([[[-0.0004,  0.0146, -0.0020,  ...,  0.0066, -0.0194,  0.0020],\n",
      "         [ 0.0241, -0.0079, -0.0178,  ..., -0.0242, -0.0138, -0.0032],\n",
      "         [-0.0124,  0.0154, -0.0188,  ..., -0.0415, -0.0249,  0.0213],\n",
      "         ...,\n",
      "         [-0.0095, -0.0072,  0.0077,  ..., -0.0040, -0.0039,  0.0039],\n",
      "         [-0.0181, -0.0193, -0.0032,  ..., -0.0332,  0.0032, -0.0006],\n",
      "         [ 0.0047, -0.0039,  0.0089,  ...,  0.0071,  0.0037,  0.0005]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.0004,  0.0146, -0.0020,  ...,  0.0066, -0.0194,  0.0020],\n",
      "         [ 0.0241, -0.0079, -0.0178,  ..., -0.0242, -0.0138, -0.0032],\n",
      "         [-0.0124,  0.0154, -0.0188,  ..., -0.0415, -0.0249,  0.0213],\n",
      "         ...,\n",
      "         [-0.0095, -0.0072,  0.0077,  ..., -0.0040, -0.0039,  0.0039],\n",
      "         [-0.0181, -0.0193, -0.0032,  ..., -0.0332,  0.0032, -0.0006],\n",
      "         [ 0.0047, -0.0039,  0.0089,  ...,  0.0071,  0.0037,  0.0005]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.0972,  0.0787, -0.0419,  ...,  0.0071,  0.0869,  0.0218],\n",
      "         [ 0.0238, -0.0297, -0.0086,  ..., -0.0402, -0.0133, -0.0226],\n",
      "         [ 0.0099,  0.0117,  0.0076,  ..., -0.0963, -0.0303,  0.0338],\n",
      "         ...,\n",
      "         [-0.0137, -0.0002,  0.0259,  ...,  0.0310, -0.0032, -0.0189],\n",
      "         [-0.0137, -0.0161, -0.0008,  ..., -0.0181,  0.0140,  0.0070],\n",
      "         [ 0.0008, -0.0047, -0.0004,  ...,  0.0053, -0.0104, -0.0015]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.0972,  0.0787, -0.0419,  ...,  0.0071,  0.0869,  0.0218],\n",
      "         [ 0.0238, -0.0297, -0.0086,  ..., -0.0402, -0.0133, -0.0226],\n",
      "         [ 0.0099,  0.0117,  0.0076,  ..., -0.0963, -0.0303,  0.0338],\n",
      "         ...,\n",
      "         [-0.0137, -0.0002,  0.0259,  ...,  0.0310, -0.0032, -0.0189],\n",
      "         [-0.0137, -0.0161, -0.0008,  ..., -0.0181,  0.0140,  0.0070],\n",
      "         [ 0.0008, -0.0047, -0.0004,  ...,  0.0053, -0.0104, -0.0015]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.1040,  0.0875, -0.0360,  ...,  0.0205,  0.0997,  0.0184],\n",
      "         [ 0.0401, -0.0338, -0.0030,  ...,  0.0008, -0.0622, -0.0298],\n",
      "         [ 0.0117,  0.0191, -0.0039,  ..., -0.0989, -0.0513,  0.0102],\n",
      "         ...,\n",
      "         [-0.0091, -0.0183,  0.0321,  ..., -0.0168,  0.0192,  0.0031],\n",
      "         [-0.0454, -0.0042,  0.0281,  ..., -0.1064,  0.0481, -0.0012],\n",
      "         [-0.0315, -0.0010,  0.0080,  ...,  0.0168, -0.0215,  0.0019]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.1040,  0.0875, -0.0360,  ...,  0.0205,  0.0997,  0.0184],\n",
      "         [ 0.0401, -0.0338, -0.0030,  ...,  0.0008, -0.0622, -0.0298],\n",
      "         [ 0.0117,  0.0191, -0.0039,  ..., -0.0989, -0.0513,  0.0102],\n",
      "         ...,\n",
      "         [-0.0091, -0.0183,  0.0321,  ..., -0.0168,  0.0192,  0.0031],\n",
      "         [-0.0454, -0.0042,  0.0281,  ..., -0.1064,  0.0481, -0.0012],\n",
      "         [-0.0315, -0.0010,  0.0080,  ...,  0.0168, -0.0215,  0.0019]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.0870,  0.1049, -0.0173,  ...,  0.0700,  0.0989,  0.0181],\n",
      "         [ 0.0293,  0.0086,  0.0128,  ...,  0.0004, -0.0467, -0.0304],\n",
      "         [-0.0071,  0.0472,  0.0273,  ..., -0.0485,  0.0143,  0.0269],\n",
      "         ...,\n",
      "         [-0.0317, -0.0543,  0.0107,  ..., -0.0350,  0.0807, -0.0068],\n",
      "         [-0.0432, -0.0413, -0.0280,  ..., -0.1323,  0.0415,  0.0409],\n",
      "         [-0.0488, -0.0040,  0.0054,  ...,  0.0178, -0.0276,  0.0134]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.0870,  0.1049, -0.0173,  ...,  0.0700,  0.0989,  0.0181],\n",
      "         [ 0.0293,  0.0086,  0.0128,  ...,  0.0004, -0.0467, -0.0304],\n",
      "         [-0.0071,  0.0472,  0.0273,  ..., -0.0485,  0.0143,  0.0269],\n",
      "         ...,\n",
      "         [-0.0317, -0.0543,  0.0107,  ..., -0.0350,  0.0807, -0.0068],\n",
      "         [-0.0432, -0.0413, -0.0280,  ..., -0.1323,  0.0415,  0.0409],\n",
      "         [-0.0488, -0.0040,  0.0054,  ...,  0.0178, -0.0276,  0.0134]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.1113,  0.1022, -0.0102,  ...,  0.0436,  0.1283,  0.0321],\n",
      "         [ 0.0109, -0.0066,  0.0546,  ...,  0.0646, -0.0991, -0.0238],\n",
      "         [-0.0054,  0.0048, -0.0107,  ..., -0.0347,  0.0310,  0.0415],\n",
      "         ...,\n",
      "         [-0.0085, -0.0046,  0.0156,  ..., -0.0182,  0.0951, -0.0006],\n",
      "         [-0.0281, -0.0445, -0.0677,  ..., -0.1021,  0.0331, -0.0158],\n",
      "         [-0.0550, -0.0091,  0.0327,  ...,  0.0949, -0.0642, -0.0112]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.1113,  0.1022, -0.0102,  ...,  0.0436,  0.1283,  0.0321],\n",
      "         [ 0.0109, -0.0066,  0.0546,  ...,  0.0646, -0.0991, -0.0238],\n",
      "         [-0.0054,  0.0048, -0.0107,  ..., -0.0347,  0.0310,  0.0415],\n",
      "         ...,\n",
      "         [-0.0085, -0.0046,  0.0156,  ..., -0.0182,  0.0951, -0.0006],\n",
      "         [-0.0281, -0.0445, -0.0677,  ..., -0.1021,  0.0331, -0.0158],\n",
      "         [-0.0550, -0.0091,  0.0327,  ...,  0.0949, -0.0642, -0.0112]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.1034,  0.1132,  0.0096,  ...,  0.0453,  0.1477,  0.0301],\n",
      "         [ 0.0801,  0.0085,  0.0218,  ...,  0.0030, -0.0365,  0.0561],\n",
      "         [ 0.0484,  0.0302, -0.0291,  ..., -0.1751,  0.0242,  0.0290],\n",
      "         ...,\n",
      "         [-0.0217, -0.0113, -0.0392,  ...,  0.1150,  0.0657, -0.0870],\n",
      "         [ 0.0795, -0.0058, -0.0717,  ..., -0.0634,  0.0060, -0.0338],\n",
      "         [-0.0462, -0.0477, -0.0047,  ...,  0.0917, -0.0734, -0.0196]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.1034,  0.1132,  0.0096,  ...,  0.0453,  0.1477,  0.0301],\n",
      "         [ 0.0801,  0.0085,  0.0218,  ...,  0.0030, -0.0365,  0.0561],\n",
      "         [ 0.0484,  0.0302, -0.0291,  ..., -0.1751,  0.0242,  0.0290],\n",
      "         ...,\n",
      "         [-0.0217, -0.0113, -0.0392,  ...,  0.1150,  0.0657, -0.0870],\n",
      "         [ 0.0795, -0.0058, -0.0717,  ..., -0.0634,  0.0060, -0.0338],\n",
      "         [-0.0462, -0.0477, -0.0047,  ...,  0.0917, -0.0734, -0.0196]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.1071,  0.1253,  0.0211,  ...,  0.0126,  0.1586,  0.0278],\n",
      "         [ 0.0824, -0.0013,  0.0047,  ...,  0.0573,  0.0338,  0.0423],\n",
      "         [-0.0338,  0.0216, -0.1469,  ..., -0.1305,  0.0323,  0.0222],\n",
      "         ...,\n",
      "         [-0.0403,  0.1071,  0.0121,  ...,  0.0562,  0.0272, -0.0256],\n",
      "         [ 0.0130, -0.1115, -0.0604,  ..., -0.0469, -0.0118, -0.0342],\n",
      "         [-0.0314, -0.0019, -0.0116,  ...,  0.1397, -0.0083,  0.0112]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.1071,  0.1253,  0.0211,  ...,  0.0126,  0.1586,  0.0278],\n",
      "         [ 0.0824, -0.0013,  0.0047,  ...,  0.0573,  0.0338,  0.0423],\n",
      "         [-0.0338,  0.0216, -0.1469,  ..., -0.1305,  0.0323,  0.0222],\n",
      "         ...,\n",
      "         [-0.0403,  0.1071,  0.0121,  ...,  0.0562,  0.0272, -0.0256],\n",
      "         [ 0.0130, -0.1115, -0.0604,  ..., -0.0469, -0.0118, -0.0342],\n",
      "         [-0.0314, -0.0019, -0.0116,  ...,  0.1397, -0.0083,  0.0112]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.0945,  0.1848,  0.0036,  ..., -0.0473,  0.1489,  0.0535],\n",
      "         [ 0.1426,  0.0234,  0.0191,  ..., -0.0536, -0.1273, -0.0037],\n",
      "         [-0.0757,  0.0279, -0.1420,  ..., -0.1951,  0.0500, -0.0184],\n",
      "         ...,\n",
      "         [ 0.0114,  0.0163, -0.0250,  ..., -0.0030,  0.0488, -0.0456],\n",
      "         [ 0.0215, -0.0898, -0.0141,  ...,  0.0224,  0.0128,  0.0039],\n",
      "         [-0.0457, -0.0682, -0.0936,  ...,  0.0738, -0.0374, -0.0429]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.0945,  0.1848,  0.0036,  ..., -0.0473,  0.1489,  0.0535],\n",
      "         [ 0.1426,  0.0234,  0.0191,  ..., -0.0536, -0.1273, -0.0037],\n",
      "         [-0.0757,  0.0279, -0.1420,  ..., -0.1951,  0.0500, -0.0184],\n",
      "         ...,\n",
      "         [ 0.0114,  0.0163, -0.0250,  ..., -0.0030,  0.0488, -0.0456],\n",
      "         [ 0.0215, -0.0898, -0.0141,  ...,  0.0224,  0.0128,  0.0039],\n",
      "         [-0.0457, -0.0682, -0.0936,  ...,  0.0738, -0.0374, -0.0429]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.1060,  0.2113,  0.0270,  ..., -0.1141,  0.1610,  0.0528],\n",
      "         [ 0.1410,  0.1097,  0.0086,  ..., -0.0326, -0.1383, -0.0158],\n",
      "         [-0.1021,  0.0066, -0.1391,  ..., -0.1800,  0.0036, -0.0910],\n",
      "         ...,\n",
      "         [ 0.0173,  0.0192,  0.0202,  ..., -0.0295,  0.0909, -0.0872],\n",
      "         [-0.0154, -0.1547, -0.0708,  ...,  0.0391,  0.1096, -0.0612],\n",
      "         [-0.0239,  0.0155, -0.0809,  ...,  0.0677, -0.0007, -0.0960]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.1060,  0.2113,  0.0270,  ..., -0.1141,  0.1610,  0.0528],\n",
      "         [ 0.1410,  0.1097,  0.0086,  ..., -0.0326, -0.1383, -0.0158],\n",
      "         [-0.1021,  0.0066, -0.1391,  ..., -0.1800,  0.0036, -0.0910],\n",
      "         ...,\n",
      "         [ 0.0173,  0.0192,  0.0202,  ..., -0.0295,  0.0909, -0.0872],\n",
      "         [-0.0154, -0.1547, -0.0708,  ...,  0.0391,  0.1096, -0.0612],\n",
      "         [-0.0239,  0.0155, -0.0809,  ...,  0.0677, -0.0007, -0.0960]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.0954,  0.2493,  0.0073,  ..., -0.1568,  0.1818,  0.0855],\n",
      "         [ 0.0958,  0.0800,  0.0146,  ..., -0.0163, -0.0689, -0.0583],\n",
      "         [-0.0519, -0.1134, -0.1781,  ..., -0.1144,  0.0454, -0.0909],\n",
      "         ...,\n",
      "         [ 0.0477,  0.0420, -0.0928,  ..., -0.0041,  0.0626, -0.0356],\n",
      "         [-0.0390, -0.1656,  0.0277,  ...,  0.0006,  0.0721, -0.0606],\n",
      "         [ 0.0073, -0.0492, -0.0258,  ...,  0.0703,  0.1135, -0.0579]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.0954,  0.2493,  0.0073,  ..., -0.1568,  0.1818,  0.0855],\n",
      "         [ 0.0958,  0.0800,  0.0146,  ..., -0.0163, -0.0689, -0.0583],\n",
      "         [-0.0519, -0.1134, -0.1781,  ..., -0.1144,  0.0454, -0.0909],\n",
      "         ...,\n",
      "         [ 0.0477,  0.0420, -0.0928,  ..., -0.0041,  0.0626, -0.0356],\n",
      "         [-0.0390, -0.1656,  0.0277,  ...,  0.0006,  0.0721, -0.0606],\n",
      "         [ 0.0073, -0.0492, -0.0258,  ...,  0.0703,  0.1135, -0.0579]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.0152,  0.3340,  0.0011,  ..., -0.2750,  0.1933,  0.1034],\n",
      "         [ 0.1368,  0.0541,  0.0278,  ..., -0.0495, -0.0859, -0.0800],\n",
      "         [-0.0306, -0.1874, -0.1756,  ..., -0.2165,  0.0281, -0.0715],\n",
      "         ...,\n",
      "         [-0.0697,  0.0813, -0.1322,  ..., -0.0778,  0.0333, -0.0811],\n",
      "         [-0.0230, -0.1482, -0.0636,  ..., -0.0525,  0.0227, -0.0232],\n",
      "         [-0.0656, -0.1209, -0.0176,  ...,  0.1221,  0.0187, -0.0048]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.0152,  0.3340,  0.0011,  ..., -0.2750,  0.1933,  0.1034],\n",
      "         [ 0.1368,  0.0541,  0.0278,  ..., -0.0495, -0.0859, -0.0800],\n",
      "         [-0.0306, -0.1874, -0.1756,  ..., -0.2165,  0.0281, -0.0715],\n",
      "         ...,\n",
      "         [-0.0697,  0.0813, -0.1322,  ..., -0.0778,  0.0333, -0.0811],\n",
      "         [-0.0230, -0.1482, -0.0636,  ..., -0.0525,  0.0227, -0.0232],\n",
      "         [-0.0656, -0.1209, -0.0176,  ...,  0.1221,  0.0187, -0.0048]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.0356,  0.3483, -0.0366,  ..., -0.2962,  0.2088,  0.1065],\n",
      "         [ 0.1225, -0.0173,  0.0028,  ..., -0.0840,  0.0275, -0.0576],\n",
      "         [-0.0561, -0.1716, -0.1226,  ..., -0.2074,  0.1013, -0.0882],\n",
      "         ...,\n",
      "         [-0.0140,  0.0853, -0.1342,  ...,  0.0771,  0.0806, -0.1593],\n",
      "         [ 0.0324, -0.1988, -0.1528,  ...,  0.0766,  0.0656, -0.0803],\n",
      "         [-0.0761, -0.0336, -0.0232,  ...,  0.1292,  0.0367, -0.0667]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.0356,  0.3483, -0.0366,  ..., -0.2962,  0.2088,  0.1065],\n",
      "         [ 0.1225, -0.0173,  0.0028,  ..., -0.0840,  0.0275, -0.0576],\n",
      "         [-0.0561, -0.1716, -0.1226,  ..., -0.2074,  0.1013, -0.0882],\n",
      "         ...,\n",
      "         [-0.0140,  0.0853, -0.1342,  ...,  0.0771,  0.0806, -0.1593],\n",
      "         [ 0.0324, -0.1988, -0.1528,  ...,  0.0766,  0.0656, -0.0803],\n",
      "         [-0.0761, -0.0336, -0.0232,  ...,  0.1292,  0.0367, -0.0667]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.0127,  0.3661, -0.0316,  ..., -0.3678,  0.1973,  0.1019],\n",
      "         [ 0.1129, -0.0486,  0.0404,  ..., -0.0710,  0.0136, -0.0032],\n",
      "         [-0.0946, -0.2703, -0.0468,  ..., -0.2063,  0.1234, -0.1251],\n",
      "         ...,\n",
      "         [-0.1154,  0.1396, -0.1729,  ..., -0.0143, -0.0597, -0.1335],\n",
      "         [-0.0280, -0.1451, -0.0601,  ...,  0.0041, -0.0308, -0.0890],\n",
      "         [-0.1646, -0.0160, -0.0113,  ..., -0.0235,  0.0589, -0.1593]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.0127,  0.3661, -0.0316,  ..., -0.3678,  0.1973,  0.1019],\n",
      "         [ 0.1129, -0.0486,  0.0404,  ..., -0.0710,  0.0136, -0.0032],\n",
      "         [-0.0946, -0.2703, -0.0468,  ..., -0.2063,  0.1234, -0.1251],\n",
      "         ...,\n",
      "         [-0.1154,  0.1396, -0.1729,  ..., -0.0143, -0.0597, -0.1335],\n",
      "         [-0.0280, -0.1451, -0.0601,  ...,  0.0041, -0.0308, -0.0890],\n",
      "         [-0.1646, -0.0160, -0.0113,  ..., -0.0235,  0.0589, -0.1593]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-8.9477e-03,  3.2332e-01, -7.7907e-02,  ..., -4.4913e-01,\n",
      "           1.8449e-01,  9.5069e-02],\n",
      "         [ 6.7886e-02, -7.3661e-02,  2.3261e-02,  ..., -1.0160e-01,\n",
      "           1.1484e-02, -5.7892e-02],\n",
      "         [-1.0502e-01, -2.4437e-01, -1.0500e-01,  ..., -1.9553e-01,\n",
      "           1.8563e-01, -1.3612e-01],\n",
      "         ...,\n",
      "         [ 5.5423e-02,  1.5857e-01,  1.2010e-02,  ...,  8.7481e-03,\n",
      "          -9.4188e-02, -4.4105e-02],\n",
      "         [ 1.0655e-02, -2.4255e-01, -3.1963e-02,  ...,  8.1488e-02,\n",
      "          -3.1176e-02,  4.7247e-03],\n",
      "         [-2.3194e-01, -4.1960e-02,  1.6011e-01,  ..., -4.9917e-02,\n",
      "          -5.5525e-02,  1.4968e-04]]], device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-8.9477e-03,  3.2332e-01, -7.7907e-02,  ..., -4.4913e-01,\n",
      "           1.8449e-01,  9.5069e-02],\n",
      "         [ 6.7886e-02, -7.3661e-02,  2.3261e-02,  ..., -1.0160e-01,\n",
      "           1.1484e-02, -5.7892e-02],\n",
      "         [-1.0502e-01, -2.4437e-01, -1.0500e-01,  ..., -1.9553e-01,\n",
      "           1.8563e-01, -1.3612e-01],\n",
      "         ...,\n",
      "         [ 5.5423e-02,  1.5857e-01,  1.2010e-02,  ...,  8.7481e-03,\n",
      "          -9.4188e-02, -4.4105e-02],\n",
      "         [ 1.0655e-02, -2.4255e-01, -3.1963e-02,  ...,  8.1488e-02,\n",
      "          -3.1176e-02,  4.7247e-03],\n",
      "         [-2.3194e-01, -4.1960e-02,  1.6011e-01,  ..., -4.9917e-02,\n",
      "          -5.5525e-02,  1.4968e-04]]], device='cuda:0'),) and output (tensor([[[-0.0244,  0.2828, -0.0532,  ..., -0.4592,  0.2300,  0.0142],\n",
      "         [ 0.0443, -0.0744, -0.0349,  ..., -0.0517, -0.0934, -0.0537],\n",
      "         [-0.2744, -0.2577, -0.1036,  ..., -0.2667,  0.1295, -0.0727],\n",
      "         ...,\n",
      "         [ 0.0472, -0.0199,  0.0523,  ..., -0.0158,  0.0419, -0.1064],\n",
      "         [ 0.0196, -0.1804, -0.1510,  ..., -0.0465,  0.0010,  0.0307],\n",
      "         [-0.2680, -0.0092,  0.0730,  ..., -0.0212, -0.1876,  0.0069]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.0244,  0.2828, -0.0532,  ..., -0.4592,  0.2300,  0.0142],\n",
      "         [ 0.0443, -0.0744, -0.0349,  ..., -0.0517, -0.0934, -0.0537],\n",
      "         [-0.2744, -0.2577, -0.1036,  ..., -0.2667,  0.1295, -0.0727],\n",
      "         ...,\n",
      "         [ 0.0472, -0.0199,  0.0523,  ..., -0.0158,  0.0419, -0.1064],\n",
      "         [ 0.0196, -0.1804, -0.1510,  ..., -0.0465,  0.0010,  0.0307],\n",
      "         [-0.2680, -0.0092,  0.0730,  ..., -0.0212, -0.1876,  0.0069]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.0928,  0.1485,  0.0158,  ..., -0.5380,  0.2664, -0.0333],\n",
      "         [-0.0757, -0.1047,  0.0093,  ..., -0.1339,  0.0033, -0.0424],\n",
      "         [-0.3205, -0.2577, -0.0463,  ..., -0.3494,  0.1327, -0.0205],\n",
      "         ...,\n",
      "         [ 0.0353,  0.0299,  0.0769,  ...,  0.0964,  0.1141, -0.0651],\n",
      "         [ 0.0479, -0.0878, -0.2129,  ..., -0.1081, -0.0996,  0.0912],\n",
      "         [-0.1360, -0.0416,  0.0209,  ..., -0.0201, -0.2882, -0.0078]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.0928,  0.1485,  0.0158,  ..., -0.5380,  0.2664, -0.0333],\n",
      "         [-0.0757, -0.1047,  0.0093,  ..., -0.1339,  0.0033, -0.0424],\n",
      "         [-0.3205, -0.2577, -0.0463,  ..., -0.3494,  0.1327, -0.0205],\n",
      "         ...,\n",
      "         [ 0.0353,  0.0299,  0.0769,  ...,  0.0964,  0.1141, -0.0651],\n",
      "         [ 0.0479, -0.0878, -0.2129,  ..., -0.1081, -0.0996,  0.0912],\n",
      "         [-0.1360, -0.0416,  0.0209,  ..., -0.0201, -0.2882, -0.0078]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.1000,  0.1323,  0.0241,  ..., -0.5470,  0.2844, -0.0457],\n",
      "         [ 0.0097, -0.2019, -0.0470,  ..., -0.1526, -0.2608, -0.0794],\n",
      "         [-0.1596, -0.2769,  0.0455,  ..., -0.2037, -0.1032,  0.0982],\n",
      "         ...,\n",
      "         [-0.0323,  0.0723,  0.1326,  ..., -0.1580,  0.1566,  0.0051],\n",
      "         [ 0.0231, -0.2603, -0.2716,  ..., -0.1126, -0.0999, -0.0574],\n",
      "         [-0.1076,  0.0761,  0.0902,  ..., -0.0430, -0.2598, -0.0669]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.1000,  0.1323,  0.0241,  ..., -0.5470,  0.2844, -0.0457],\n",
      "         [ 0.0097, -0.2019, -0.0470,  ..., -0.1526, -0.2608, -0.0794],\n",
      "         [-0.1596, -0.2769,  0.0455,  ..., -0.2037, -0.1032,  0.0982],\n",
      "         ...,\n",
      "         [-0.0323,  0.0723,  0.1326,  ..., -0.1580,  0.1566,  0.0051],\n",
      "         [ 0.0231, -0.2603, -0.2716,  ..., -0.1126, -0.0999, -0.0574],\n",
      "         [-0.1076,  0.0761,  0.0902,  ..., -0.0430, -0.2598, -0.0669]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-1.2863e-01,  8.3288e-02,  2.3926e-02,  ..., -5.6571e-01,\n",
      "           3.6938e-01, -7.2483e-02],\n",
      "         [-4.6083e-02, -1.3947e-01, -5.3288e-02,  ..., -2.8542e-01,\n",
      "          -2.7233e-01, -6.3701e-02],\n",
      "         [-1.9245e-01, -1.1941e-01,  1.7361e-02,  ..., -2.4347e-01,\n",
      "           3.0807e-02,  1.5414e-01],\n",
      "         ...,\n",
      "         [ 2.2622e-03,  2.4077e-01,  5.1533e-02,  ..., -4.8721e-02,\n",
      "           2.1158e-01, -2.0246e-04],\n",
      "         [-1.8584e-02, -2.9799e-01, -2.2277e-01,  ...,  3.8698e-02,\n",
      "          -3.4156e-02, -1.0076e-01],\n",
      "         [ 2.5548e-02, -1.8128e-03, -3.2142e-02,  ..., -5.4541e-02,\n",
      "          -3.5051e-01, -1.0678e-01]]], device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-1.2863e-01,  8.3288e-02,  2.3926e-02,  ..., -5.6571e-01,\n",
      "           3.6938e-01, -7.2483e-02],\n",
      "         [-4.6083e-02, -1.3947e-01, -5.3288e-02,  ..., -2.8542e-01,\n",
      "          -2.7233e-01, -6.3701e-02],\n",
      "         [-1.9245e-01, -1.1941e-01,  1.7361e-02,  ..., -2.4347e-01,\n",
      "           3.0807e-02,  1.5414e-01],\n",
      "         ...,\n",
      "         [ 2.2622e-03,  2.4077e-01,  5.1533e-02,  ..., -4.8721e-02,\n",
      "           2.1158e-01, -2.0246e-04],\n",
      "         [-1.8584e-02, -2.9799e-01, -2.2277e-01,  ...,  3.8698e-02,\n",
      "          -3.4156e-02, -1.0076e-01],\n",
      "         [ 2.5548e-02, -1.8128e-03, -3.2142e-02,  ..., -5.4541e-02,\n",
      "          -3.5051e-01, -1.0678e-01]]], device='cuda:0'),) and output (tensor([[[-0.1417,  0.0703,  0.0343,  ..., -0.5549,  0.3175, -0.0847],\n",
      "         [-0.1243, -0.0623, -0.1808,  ..., -0.3169, -0.2098, -0.1342],\n",
      "         [-0.2967, -0.1681, -0.0613,  ..., -0.3756,  0.1089,  0.2178],\n",
      "         ...,\n",
      "         [-0.0010,  0.2033, -0.0190,  ..., -0.1335,  0.2663,  0.0370],\n",
      "         [ 0.0359, -0.2372, -0.3346,  ..., -0.1129,  0.0655,  0.0266],\n",
      "         [-0.0870,  0.0324, -0.0198,  ..., -0.0554, -0.3614, -0.0695]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.1417,  0.0703,  0.0343,  ..., -0.5549,  0.3175, -0.0847],\n",
      "         [-0.1243, -0.0623, -0.1808,  ..., -0.3169, -0.2098, -0.1342],\n",
      "         [-0.2967, -0.1681, -0.0613,  ..., -0.3756,  0.1089,  0.2178],\n",
      "         ...,\n",
      "         [-0.0010,  0.2033, -0.0190,  ..., -0.1335,  0.2663,  0.0370],\n",
      "         [ 0.0359, -0.2372, -0.3346,  ..., -0.1129,  0.0655,  0.0266],\n",
      "         [-0.0870,  0.0324, -0.0198,  ..., -0.0554, -0.3614, -0.0695]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-1.5285e-01,  7.2501e-02,  6.0947e-02,  ..., -5.6241e-01,\n",
      "           3.1026e-01, -2.9341e-02],\n",
      "         [-5.1543e-02, -3.7645e-02, -2.8013e-01,  ..., -2.3277e-01,\n",
      "          -1.2671e-01, -9.7367e-02],\n",
      "         [-3.8323e-01, -1.7149e-01, -1.4104e-01,  ..., -3.0396e-01,\n",
      "           1.2889e-01,  2.8563e-01],\n",
      "         ...,\n",
      "         [-5.6358e-02,  1.3874e-01, -1.4715e-04,  ..., -5.5409e-02,\n",
      "           2.0027e-01,  2.4069e-02],\n",
      "         [ 1.6746e-01, -2.8382e-01, -3.7172e-01,  ..., -3.6223e-01,\n",
      "           1.8211e-01,  9.4576e-02],\n",
      "         [-1.4367e-01, -7.4845e-02, -3.7492e-03,  ..., -1.4292e-01,\n",
      "          -3.8379e-01, -1.3379e-01]]], device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-1.5285e-01,  7.2501e-02,  6.0947e-02,  ..., -5.6241e-01,\n",
      "           3.1026e-01, -2.9341e-02],\n",
      "         [-5.1543e-02, -3.7645e-02, -2.8013e-01,  ..., -2.3277e-01,\n",
      "          -1.2671e-01, -9.7367e-02],\n",
      "         [-3.8323e-01, -1.7149e-01, -1.4104e-01,  ..., -3.0396e-01,\n",
      "           1.2889e-01,  2.8563e-01],\n",
      "         ...,\n",
      "         [-5.6358e-02,  1.3874e-01, -1.4715e-04,  ..., -5.5409e-02,\n",
      "           2.0027e-01,  2.4069e-02],\n",
      "         [ 1.6746e-01, -2.8382e-01, -3.7172e-01,  ..., -3.6223e-01,\n",
      "           1.8211e-01,  9.4576e-02],\n",
      "         [-1.4367e-01, -7.4845e-02, -3.7492e-03,  ..., -1.4292e-01,\n",
      "          -3.8379e-01, -1.3379e-01]]], device='cuda:0'),) and output (tensor([[[-0.1559,  0.0520,  0.0566,  ..., -0.5805,  0.2614, -0.0032],\n",
      "         [-0.1045,  0.0393, -0.3064,  ..., -0.2956, -0.1827, -0.1318],\n",
      "         [-0.3716, -0.0973, -0.1125,  ..., -0.3716,  0.1216,  0.3044],\n",
      "         ...,\n",
      "         [-0.2247,  0.3792,  0.1301,  ..., -0.0885,  0.2349, -0.0678],\n",
      "         [ 0.0880, -0.3845, -0.1972,  ..., -0.3722,  0.0873,  0.0147],\n",
      "         [ 0.0280, -0.0925, -0.0283,  ..., -0.1830, -0.4395, -0.0924]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.1559,  0.0520,  0.0566,  ..., -0.5805,  0.2614, -0.0032],\n",
      "         [-0.1045,  0.0393, -0.3064,  ..., -0.2956, -0.1827, -0.1318],\n",
      "         [-0.3716, -0.0973, -0.1125,  ..., -0.3716,  0.1216,  0.3044],\n",
      "         ...,\n",
      "         [-0.2247,  0.3792,  0.1301,  ..., -0.0885,  0.2349, -0.0678],\n",
      "         [ 0.0880, -0.3845, -0.1972,  ..., -0.3722,  0.0873,  0.0147],\n",
      "         [ 0.0280, -0.0925, -0.0283,  ..., -0.1830, -0.4395, -0.0924]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.1488,  0.0445,  0.0765,  ..., -0.5456,  0.2702,  0.0098],\n",
      "         [-0.0488,  0.1464, -0.2852,  ..., -0.3167, -0.2157, -0.1749],\n",
      "         [-0.4504, -0.0078, -0.0903,  ..., -0.4469,  0.2030,  0.3495],\n",
      "         ...,\n",
      "         [-0.1218,  0.5426,  0.0982,  ..., -0.1227,  0.2407,  0.0192],\n",
      "         [ 0.1155, -0.3203, -0.0583,  ..., -0.4448,  0.1996,  0.0908],\n",
      "         [ 0.0196, -0.1477, -0.0557,  ..., -0.1976, -0.4251, -0.1541]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.1488,  0.0445,  0.0765,  ..., -0.5456,  0.2702,  0.0098],\n",
      "         [-0.0488,  0.1464, -0.2852,  ..., -0.3167, -0.2157, -0.1749],\n",
      "         [-0.4504, -0.0078, -0.0903,  ..., -0.4469,  0.2030,  0.3495],\n",
      "         ...,\n",
      "         [-0.1218,  0.5426,  0.0982,  ..., -0.1227,  0.2407,  0.0192],\n",
      "         [ 0.1155, -0.3203, -0.0583,  ..., -0.4448,  0.1996,  0.0908],\n",
      "         [ 0.0196, -0.1477, -0.0557,  ..., -0.1976, -0.4251, -0.1541]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-1.2677e-01,  3.3336e-02,  9.3478e-02,  ..., -5.3204e-01,\n",
      "           2.7953e-01,  5.9780e-04],\n",
      "         [ 9.8610e-02,  2.7694e-01, -2.6097e-01,  ..., -2.9726e-01,\n",
      "          -2.7287e-01, -1.4989e-01],\n",
      "         [-5.6114e-01,  6.2954e-02, -9.7599e-02,  ..., -5.5544e-01,\n",
      "           2.8999e-01,  3.0415e-01],\n",
      "         ...,\n",
      "         [-9.5203e-02,  6.5255e-01,  2.9972e-01,  ...,  3.4108e-03,\n",
      "           3.4395e-01,  9.5308e-02],\n",
      "         [ 4.9997e-02, -5.3527e-01, -3.4385e-02,  ..., -1.4303e-01,\n",
      "           2.4181e-01,  2.3667e-01],\n",
      "         [-3.2699e-02, -7.2602e-02,  9.6885e-02,  ..., -1.8872e-01,\n",
      "          -3.6562e-01, -7.6016e-02]]], device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-1.2677e-01,  3.3336e-02,  9.3478e-02,  ..., -5.3204e-01,\n",
      "           2.7953e-01,  5.9780e-04],\n",
      "         [ 9.8610e-02,  2.7694e-01, -2.6097e-01,  ..., -2.9726e-01,\n",
      "          -2.7287e-01, -1.4989e-01],\n",
      "         [-5.6114e-01,  6.2954e-02, -9.7599e-02,  ..., -5.5544e-01,\n",
      "           2.8999e-01,  3.0415e-01],\n",
      "         ...,\n",
      "         [-9.5203e-02,  6.5255e-01,  2.9972e-01,  ...,  3.4108e-03,\n",
      "           3.4395e-01,  9.5308e-02],\n",
      "         [ 4.9997e-02, -5.3527e-01, -3.4385e-02,  ..., -1.4303e-01,\n",
      "           2.4181e-01,  2.3667e-01],\n",
      "         [-3.2699e-02, -7.2602e-02,  9.6885e-02,  ..., -1.8872e-01,\n",
      "          -3.6562e-01, -7.6016e-02]]], device='cuda:0'),) and output (tensor([[[-0.1355,  0.0227,  0.0554,  ..., -0.5473,  0.2470, -0.0229],\n",
      "         [ 0.0304,  0.2657, -0.2057,  ..., -0.3178, -0.3632, -0.1384],\n",
      "         [-0.6007,  0.0194, -0.1221,  ..., -0.5328,  0.2678,  0.2678],\n",
      "         ...,\n",
      "         [-0.0911,  0.4808,  0.2206,  ...,  0.1208,  0.3632,  0.2282],\n",
      "         [-0.0145, -0.2744,  0.0339,  ..., -0.0827,  0.2545,  0.3300],\n",
      "         [-0.0865, -0.0904,  0.0378,  ..., -0.1105, -0.2355, -0.0799]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.1355,  0.0227,  0.0554,  ..., -0.5473,  0.2470, -0.0229],\n",
      "         [ 0.0304,  0.2657, -0.2057,  ..., -0.3178, -0.3632, -0.1384],\n",
      "         [-0.6007,  0.0194, -0.1221,  ..., -0.5328,  0.2678,  0.2678],\n",
      "         ...,\n",
      "         [-0.0911,  0.4808,  0.2206,  ...,  0.1208,  0.3632,  0.2282],\n",
      "         [-0.0145, -0.2744,  0.0339,  ..., -0.0827,  0.2545,  0.3300],\n",
      "         [-0.0865, -0.0904,  0.0378,  ..., -0.1105, -0.2355, -0.0799]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.1350,  0.0163,  0.0576,  ..., -0.5555,  0.2435,  0.0051],\n",
      "         [ 0.0132,  0.3334, -0.1294,  ..., -0.4147, -0.3102, -0.0913],\n",
      "         [-0.6788,  0.1138, -0.1086,  ..., -0.4677,  0.2515,  0.2810],\n",
      "         ...,\n",
      "         [-0.0122,  0.5592, -0.0882,  ..., -0.0975,  0.0292,  0.2985],\n",
      "         [ 0.0238, -0.2012, -0.1908,  ..., -0.1807,  0.3656,  0.5161],\n",
      "         [ 0.0196, -0.2301, -0.0682,  ..., -0.1462, -0.1642, -0.0342]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.1350,  0.0163,  0.0576,  ..., -0.5555,  0.2435,  0.0051],\n",
      "         [ 0.0132,  0.3334, -0.1294,  ..., -0.4147, -0.3102, -0.0913],\n",
      "         [-0.6788,  0.1138, -0.1086,  ..., -0.4677,  0.2515,  0.2810],\n",
      "         ...,\n",
      "         [-0.0122,  0.5592, -0.0882,  ..., -0.0975,  0.0292,  0.2985],\n",
      "         [ 0.0238, -0.2012, -0.1908,  ..., -0.1807,  0.3656,  0.5161],\n",
      "         [ 0.0196, -0.2301, -0.0682,  ..., -0.1462, -0.1642, -0.0342]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.1301, -0.0029,  0.0279,  ..., -0.5537,  0.2277,  0.0164],\n",
      "         [-0.0130,  0.2905,  0.0064,  ..., -0.6867,  0.0153, -0.1184],\n",
      "         [-0.6621,  0.0370, -0.1071,  ..., -0.4338,  0.2871,  0.2114],\n",
      "         ...,\n",
      "         [-0.0217,  0.7425,  0.0256,  ..., -0.0731,  0.1595,  0.1564],\n",
      "         [ 0.0642, -0.1307, -0.1720,  ..., -0.2900,  0.2895,  0.4539],\n",
      "         [-0.0681, -0.1221,  0.0897,  ...,  0.0103, -0.0560, -0.0408]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.1301, -0.0029,  0.0279,  ..., -0.5537,  0.2277,  0.0164],\n",
      "         [-0.0130,  0.2905,  0.0064,  ..., -0.6867,  0.0153, -0.1184],\n",
      "         [-0.6621,  0.0370, -0.1071,  ..., -0.4338,  0.2871,  0.2114],\n",
      "         ...,\n",
      "         [-0.0217,  0.7425,  0.0256,  ..., -0.0731,  0.1595,  0.1564],\n",
      "         [ 0.0642, -0.1307, -0.1720,  ..., -0.2900,  0.2895,  0.4539],\n",
      "         [-0.0681, -0.1221,  0.0897,  ...,  0.0103, -0.0560, -0.0408]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.1219,  0.0061, -0.0021,  ..., -0.5922,  0.2788, -0.0020],\n",
      "         [ 0.0448,  0.3538,  0.0334,  ..., -0.5458,  0.0561, -0.0758],\n",
      "         [-0.8455, -0.1339, -0.1432,  ..., -0.3807,  0.3277,  0.2763],\n",
      "         ...,\n",
      "         [-0.3054,  0.7361,  0.2526,  ..., -0.2998,  0.4266,  0.1145],\n",
      "         [ 0.2369,  0.0012,  0.0575,  ..., -0.2826,  0.4956,  0.5935],\n",
      "         [-0.0609, -0.1248,  0.1023,  ...,  0.1253,  0.0939,  0.1215]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.1219,  0.0061, -0.0021,  ..., -0.5922,  0.2788, -0.0020],\n",
      "         [ 0.0448,  0.3538,  0.0334,  ..., -0.5458,  0.0561, -0.0758],\n",
      "         [-0.8455, -0.1339, -0.1432,  ..., -0.3807,  0.3277,  0.2763],\n",
      "         ...,\n",
      "         [-0.3054,  0.7361,  0.2526,  ..., -0.2998,  0.4266,  0.1145],\n",
      "         [ 0.2369,  0.0012,  0.0575,  ..., -0.2826,  0.4956,  0.5935],\n",
      "         [-0.0609, -0.1248,  0.1023,  ...,  0.1253,  0.0939,  0.1215]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.1067,  0.0272,  0.0495,  ..., -0.5550,  0.3449,  0.0266],\n",
      "         [ 0.0806,  0.3846, -0.0111,  ..., -0.5944, -0.1360,  0.0451],\n",
      "         [-0.7293,  0.0086, -0.1650,  ..., -0.6146,  0.3336,  0.3467],\n",
      "         ...,\n",
      "         [-0.4185,  0.7513,  0.2993,  ..., -0.2174,  0.4282,  0.4307],\n",
      "         [ 0.1569,  0.1012, -0.0586,  ..., -0.3924,  0.4863,  0.6977],\n",
      "         [-0.2662,  0.2544, -0.0386,  ...,  0.0249, -0.1241,  0.0490]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.1067,  0.0272,  0.0495,  ..., -0.5550,  0.3449,  0.0266],\n",
      "         [ 0.0806,  0.3846, -0.0111,  ..., -0.5944, -0.1360,  0.0451],\n",
      "         [-0.7293,  0.0086, -0.1650,  ..., -0.6146,  0.3336,  0.3467],\n",
      "         ...,\n",
      "         [-0.4185,  0.7513,  0.2993,  ..., -0.2174,  0.4282,  0.4307],\n",
      "         [ 0.1569,  0.1012, -0.0586,  ..., -0.3924,  0.4863,  0.6977],\n",
      "         [-0.2662,  0.2544, -0.0386,  ...,  0.0249, -0.1241,  0.0490]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.0592,  0.0731,  0.0500,  ..., -0.5517,  0.3541,  0.0595],\n",
      "         [ 0.0288,  0.3952,  0.0412,  ..., -0.6782, -0.0273, -0.0857],\n",
      "         [-0.7804, -0.2529, -0.0609,  ..., -0.7065,  0.3261,  0.3665],\n",
      "         ...,\n",
      "         [-0.6776,  0.4386,  0.6394,  ..., -0.2871,  0.2421,  0.6800],\n",
      "         [ 0.0642, -0.2940,  0.1667,  ..., -0.5032,  0.4910,  0.8150],\n",
      "         [-0.3971, -0.0957,  0.2206,  ..., -0.1596, -0.1503, -0.1547]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.0592,  0.0731,  0.0500,  ..., -0.5517,  0.3541,  0.0595],\n",
      "         [ 0.0288,  0.3952,  0.0412,  ..., -0.6782, -0.0273, -0.0857],\n",
      "         [-0.7804, -0.2529, -0.0609,  ..., -0.7065,  0.3261,  0.3665],\n",
      "         ...,\n",
      "         [-0.6776,  0.4386,  0.6394,  ..., -0.2871,  0.2421,  0.6800],\n",
      "         [ 0.0642, -0.2940,  0.1667,  ..., -0.5032,  0.4910,  0.8150],\n",
      "         [-0.3971, -0.0957,  0.2206,  ..., -0.1596, -0.1503, -0.1547]]],\n",
      "       device='cuda:0'),) and output (tensor([[[ 0.1143,  0.2237,  0.3292,  ..., -0.7706,  0.2608,  0.0269],\n",
      "         [ 0.0805,  0.2283,  0.0503,  ..., -0.9871, -0.0694, -0.2603],\n",
      "         [-0.8194, -0.3083, -0.1084,  ..., -0.6613,  0.2271,  0.0474],\n",
      "         ...,\n",
      "         [-0.8523,  0.2628,  0.5947,  ..., -0.5826, -0.0267,  0.7019],\n",
      "         [ 0.3775, -0.4550, -0.5521,  ..., -0.4318,  0.3591,  1.2096],\n",
      "         [-0.2398, -0.5223, -0.0507,  ..., -0.1526, -0.1260, -0.3648]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[ 0.1143,  0.2237,  0.3292,  ..., -0.7706,  0.2608,  0.0269],\n",
      "         [ 0.0805,  0.2283,  0.0503,  ..., -0.9871, -0.0694, -0.2603],\n",
      "         [-0.8194, -0.3083, -0.1084,  ..., -0.6613,  0.2271,  0.0474],\n",
      "         ...,\n",
      "         [-0.8523,  0.2628,  0.5947,  ..., -0.5826, -0.0267,  0.7019],\n",
      "         [ 0.3775, -0.4550, -0.5521,  ..., -0.4318,  0.3591,  1.2096],\n",
      "         [-0.2398, -0.5223, -0.0507,  ..., -0.1526, -0.1260, -0.3648]]],\n",
      "       device='cuda:0'),) and output (tensor([[[ 0.3711,  0.5489,  0.6100,  ..., -0.7499,  0.2080,  0.4195],\n",
      "         [ 0.0975, -0.0220, -0.2125,  ..., -1.2747, -0.2876, -0.4182],\n",
      "         [-0.9059, -0.6552, -0.3840,  ..., -0.6643, -0.0620, -0.4225],\n",
      "         ...,\n",
      "         [-0.2331, -0.1477,  0.1142,  ..., -0.8822,  0.0334,  0.4309],\n",
      "         [ 0.4296, -0.6062, -0.5326,  ..., -0.4869,  0.1479,  0.8580],\n",
      "         [-0.6095, -1.0981, -0.0283,  ..., -0.4680, -0.5162, -1.0368]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[ 0.3711,  0.5489,  0.6100,  ..., -0.7499,  0.2080,  0.4195],\n",
      "         [ 0.0975, -0.0220, -0.2125,  ..., -1.2747, -0.2876, -0.4182],\n",
      "         [-0.9059, -0.6552, -0.3840,  ..., -0.6643, -0.0620, -0.4225],\n",
      "         ...,\n",
      "         [-0.2331, -0.1477,  0.1142,  ..., -0.8822,  0.0334,  0.4309],\n",
      "         [ 0.4296, -0.6062, -0.5326,  ..., -0.4869,  0.1479,  0.8580],\n",
      "         [-0.6095, -1.0981, -0.0283,  ..., -0.4680, -0.5162, -1.0368]]],\n",
      "       device='cuda:0'),) and output (tensor([[[ 1.1544,  3.2965,  1.6534,  ..., -2.7009,  2.6553,  2.2700],\n",
      "         [ 0.3826,  0.6421,  0.0382,  ..., -1.2800, -0.8118, -0.2006],\n",
      "         [-0.6015, -0.6714, -0.1594,  ..., -0.5900, -0.2341, -0.7064],\n",
      "         ...,\n",
      "         [-0.0924, -0.9105,  0.1935,  ..., -0.2548, -0.6773,  1.0303],\n",
      "         [ 0.3177, -0.9689, -0.4310,  ...,  0.5802, -0.0360,  1.3873],\n",
      "         [-0.4973, -2.0786, -0.4256,  ...,  0.2074, -0.9977, -0.9261]]],\n",
      "       device='cuda:0'),)\n",
      "[Debug] Layer names being passed: ['model.layers.0', 'model.layers.1', 'model.layers.2', 'model.layers.3', 'model.layers.4', 'model.layers.5', 'model.layers.6', 'model.layers.7', 'model.layers.8', 'model.layers.9', 'model.layers.10', 'model.layers.11', 'model.layers.12', 'model.layers.13', 'model.layers.14', 'model.layers.15', 'model.layers.16', 'model.layers.17', 'model.layers.18', 'model.layers.19', 'model.layers.20', 'model.layers.21', 'model.layers.22', 'model.layers.23', 'model.layers.24', 'model.layers.25', 'model.layers.26', 'model.layers.27', 'model.layers.28', 'model.layers.29', 'model.layers.30', 'model.layers.31', 'model.embed_tokens']\n",
      "[Debug] Trying to access layer: model.layers.0\n",
      "[Debug] Successfully found layer: model.layers.0\n",
      "[Debug] Trying to access layer: model.layers.1\n",
      "[Debug] Successfully found layer: model.layers.1\n",
      "[Debug] Trying to access layer: model.layers.2\n",
      "[Debug] Successfully found layer: model.layers.2\n",
      "[Debug] Trying to access layer: model.layers.3\n",
      "[Debug] Successfully found layer: model.layers.3\n",
      "[Debug] Trying to access layer: model.layers.4\n",
      "[Debug] Successfully found layer: model.layers.4\n",
      "[Debug] Trying to access layer: model.layers.5\n",
      "[Debug] Successfully found layer: model.layers.5\n",
      "[Debug] Trying to access layer: model.layers.6\n",
      "[Debug] Successfully found layer: model.layers.6\n",
      "[Debug] Trying to access layer: model.layers.7\n",
      "[Debug] Successfully found layer: model.layers.7\n",
      "[Debug] Trying to access layer: model.layers.8\n",
      "[Debug] Successfully found layer: model.layers.8\n",
      "[Debug] Trying to access layer: model.layers.9\n",
      "[Debug] Successfully found layer: model.layers.9\n",
      "[Debug] Trying to access layer: model.layers.10\n",
      "[Debug] Successfully found layer: model.layers.10\n",
      "[Debug] Trying to access layer: model.layers.11\n",
      "[Debug] Successfully found layer: model.layers.11\n",
      "[Debug] Trying to access layer: model.layers.12\n",
      "[Debug] Successfully found layer: model.layers.12\n",
      "[Debug] Trying to access layer: model.layers.13\n",
      "[Debug] Successfully found layer: model.layers.13\n",
      "[Debug] Trying to access layer: model.layers.14\n",
      "[Debug] Successfully found layer: model.layers.14\n",
      "[Debug] Trying to access layer: model.layers.15\n",
      "[Debug] Successfully found layer: model.layers.15\n",
      "[Debug] Trying to access layer: model.layers.16\n",
      "[Debug] Successfully found layer: model.layers.16\n",
      "[Debug] Trying to access layer: model.layers.17\n",
      "[Debug] Successfully found layer: model.layers.17\n",
      "[Debug] Trying to access layer: model.layers.18\n",
      "[Debug] Successfully found layer: model.layers.18\n",
      "[Debug] Trying to access layer: model.layers.19\n",
      "[Debug] Successfully found layer: model.layers.19\n",
      "[Debug] Trying to access layer: model.layers.20\n",
      "[Debug] Successfully found layer: model.layers.20\n",
      "[Debug] Trying to access layer: model.layers.21\n",
      "[Debug] Successfully found layer: model.layers.21\n",
      "[Debug] Trying to access layer: model.layers.22\n",
      "[Debug] Successfully found layer: model.layers.22\n",
      "[Debug] Trying to access layer: model.layers.23\n",
      "[Debug] Successfully found layer: model.layers.23\n",
      "[Debug] Trying to access layer: model.layers.24\n",
      "[Debug] Successfully found layer: model.layers.24\n",
      "[Debug] Trying to access layer: model.layers.25\n",
      "[Debug] Successfully found layer: model.layers.25\n",
      "[Debug] Trying to access layer: model.layers.26\n",
      "[Debug] Successfully found layer: model.layers.26\n",
      "[Debug] Trying to access layer: model.layers.27\n",
      "[Debug] Successfully found layer: model.layers.27\n",
      "[Debug] Trying to access layer: model.layers.28\n",
      "[Debug] Successfully found layer: model.layers.28\n",
      "[Debug] Trying to access layer: model.layers.29\n",
      "[Debug] Successfully found layer: model.layers.29\n",
      "[Debug] Trying to access layer: model.layers.30\n",
      "[Debug] Successfully found layer: model.layers.30\n",
      "[Debug] Trying to access layer: model.layers.31\n",
      "[Debug] Successfully found layer: model.layers.31\n",
      "[Debug] Trying to access layer: model.embed_tokens\n",
      "[Debug] Successfully found layer: model.embed_tokens\n",
      "[Hook] Layer Embedding(128256, 4096) received input (tensor([[128000,  33731,  10016,    320,     82,   5248,      8,   2468,    279,\n",
      "           7314,    315,    220,   5162,     15,     11,  10016,   1578,   5614,\n",
      "           3335,   9382,     11,    420,    892,    311,   7054,  19856,  22293,\n",
      "           8032,     16,     60,   1283,   1243,  12715,   3116,  67293,   8166,\n",
      "            220,   1041,  13280,   4194,   4235,    330,   3923,    304,    279,\n",
      "           4435,    596,  15936,   6193,   1472,      1,  30183,     20,    705,\n",
      "            330,     33,  54444,  77339,      1,  30183,     18,      8,    293,\n",
      "           6458,    330,  12174,  15013,   3861,      1,  30183,   1958,    705,\n",
      "            323,    330,   2181,   8442,  35800,   6901,  61133,      1,  30183,\n",
      "           1987,  94638,     16,     60,    330,   3923,    304,    279,   4435,\n",
      "            596,  15936,   6193,   1472,      1,    574,  10016,    596,   2132,\n",
      "           6761,   2624,  13946,   8032,     21,     60,  10016,   8738,    311,\n",
      "           3335,    323,   2804,   2391,    279,    220,   5162,     15,     82,\n",
      "            323,    220,   4468,     15,     82,   8032,     16,     60,   5414,\n",
      "           5609,    330,   2675,   2351,   4702,   2175,  57071,      6,  24327,\n",
      "              1,   8813,    279,   3224,  27223,    304,    220,   4468,     19,\n",
      "           8032,     16,     60,    763,   3297,    220,   4468,     22,     11,\n",
      "          10016,  12715,    264,  89694,   3882,    369,  18588,  13792,   4194,\n",
      "             16,   2624,    503,  18369,     11,   3842,  89694,     13]],\n",
      "       device='cuda:0'),) and output tensor([[[ 2.6512e-04, -4.9973e-04, -5.8365e-04,  ...,  3.8147e-03,\n",
      "           6.3419e-05,  1.1902e-03],\n",
      "         [-6.4392e-03,  2.1820e-03,  5.8899e-03,  ..., -3.5400e-03,\n",
      "          -4.8828e-03,  1.5335e-03],\n",
      "         [-4.1962e-04,  9.3994e-03, -6.3324e-04,  ..., -1.1230e-02,\n",
      "          -8.6670e-03,  1.1353e-02],\n",
      "         ...,\n",
      "         [-1.0010e-02,  5.2795e-03, -2.3193e-03,  ..., -3.8910e-03,\n",
      "          -9.2163e-03, -4.2534e-04],\n",
      "         [-4.1504e-03,  2.8229e-03,  7.5684e-03,  ...,  4.7913e-03,\n",
      "          -2.8229e-03, -6.4373e-06],\n",
      "         [-6.7520e-04, -5.7602e-04,  4.7684e-04,  ...,  3.3264e-03,\n",
      "           3.9101e-04,  4.7493e-04]]], device='cuda:0')\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[ 2.6512e-04, -4.9973e-04, -5.8365e-04,  ...,  3.8147e-03,\n",
      "           6.3419e-05,  1.1902e-03],\n",
      "         [-6.4392e-03,  2.1820e-03,  5.8899e-03,  ..., -3.5400e-03,\n",
      "          -4.8828e-03,  1.5335e-03],\n",
      "         [-4.1962e-04,  9.3994e-03, -6.3324e-04,  ..., -1.1230e-02,\n",
      "          -8.6670e-03,  1.1353e-02],\n",
      "         ...,\n",
      "         [-1.0010e-02,  5.2795e-03, -2.3193e-03,  ..., -3.8910e-03,\n",
      "          -9.2163e-03, -4.2534e-04],\n",
      "         [-4.1504e-03,  2.8229e-03,  7.5684e-03,  ...,  4.7913e-03,\n",
      "          -2.8229e-03, -6.4373e-06],\n",
      "         [-6.7520e-04, -5.7602e-04,  4.7684e-04,  ...,  3.3264e-03,\n",
      "           3.9101e-04,  4.7493e-04]]], device='cuda:0'),) and output (tensor([[[-0.0004,  0.0146, -0.0020,  ...,  0.0066, -0.0194,  0.0020],\n",
      "         [-0.0114,  0.0114,  0.0048,  ..., -0.0059, -0.0299, -0.0098],\n",
      "         [-0.0038,  0.0056,  0.0002,  ..., -0.0154, -0.0148,  0.0104],\n",
      "         ...,\n",
      "         [-0.0065,  0.0103,  0.0025,  ..., -0.0121, -0.0037,  0.0071],\n",
      "         [-0.0060,  0.0284,  0.0102,  ..., -0.0136, -0.0105, -0.0169],\n",
      "         [ 0.0049,  0.0007,  0.0038,  ...,  0.0057,  0.0014, -0.0010]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.0004,  0.0146, -0.0020,  ...,  0.0066, -0.0194,  0.0020],\n",
      "         [-0.0114,  0.0114,  0.0048,  ..., -0.0059, -0.0299, -0.0098],\n",
      "         [-0.0038,  0.0056,  0.0002,  ..., -0.0154, -0.0148,  0.0104],\n",
      "         ...,\n",
      "         [-0.0065,  0.0103,  0.0025,  ..., -0.0121, -0.0037,  0.0071],\n",
      "         [-0.0060,  0.0284,  0.0102,  ..., -0.0136, -0.0105, -0.0169],\n",
      "         [ 0.0049,  0.0007,  0.0038,  ...,  0.0057,  0.0014, -0.0010]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-9.7233e-02,  7.8729e-02, -4.1909e-02,  ...,  7.0540e-03,\n",
      "           8.6910e-02,  2.1819e-02],\n",
      "         [-5.6585e-03,  1.4769e-02, -1.1904e-02,  ..., -3.3010e-02,\n",
      "          -3.2423e-02, -1.5660e-02],\n",
      "         [-1.1491e-02, -2.2439e-02, -3.2647e-02,  ..., -7.0174e-02,\n",
      "          -1.4547e-02, -3.2723e-02],\n",
      "         ...,\n",
      "         [-3.5777e-03,  1.7955e-02, -4.7945e-03,  ..., -3.5094e-02,\n",
      "          -1.4551e-02, -2.1614e-03],\n",
      "         [-1.6814e-02,  2.5624e-02, -2.4341e-02,  ..., -6.1496e-02,\n",
      "          -1.2847e-02, -9.9324e-03],\n",
      "         [ 1.6279e-02, -2.5872e-03, -1.5863e-02,  ...,  3.6652e-05,\n",
      "          -8.0469e-03, -1.3246e-02]]], device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-9.7233e-02,  7.8729e-02, -4.1909e-02,  ...,  7.0540e-03,\n",
      "           8.6910e-02,  2.1819e-02],\n",
      "         [-5.6585e-03,  1.4769e-02, -1.1904e-02,  ..., -3.3010e-02,\n",
      "          -3.2423e-02, -1.5660e-02],\n",
      "         [-1.1491e-02, -2.2439e-02, -3.2647e-02,  ..., -7.0174e-02,\n",
      "          -1.4547e-02, -3.2723e-02],\n",
      "         ...,\n",
      "         [-3.5777e-03,  1.7955e-02, -4.7945e-03,  ..., -3.5094e-02,\n",
      "          -1.4551e-02, -2.1614e-03],\n",
      "         [-1.6814e-02,  2.5624e-02, -2.4341e-02,  ..., -6.1496e-02,\n",
      "          -1.2847e-02, -9.9324e-03],\n",
      "         [ 1.6279e-02, -2.5872e-03, -1.5863e-02,  ...,  3.6652e-05,\n",
      "          -8.0469e-03, -1.3246e-02]]], device='cuda:0'),) and output (tensor([[[-0.1040,  0.0875, -0.0360,  ...,  0.0205,  0.0997,  0.0184],\n",
      "         [ 0.0325,  0.0205, -0.0052,  ..., -0.0066, -0.0628, -0.0052],\n",
      "         [-0.0120, -0.0158, -0.0840,  ..., -0.0697, -0.0200, -0.0428],\n",
      "         ...,\n",
      "         [ 0.0288,  0.0068, -0.0004,  ..., -0.0532, -0.0251,  0.0011],\n",
      "         [-0.0186,  0.0195, -0.0229,  ..., -0.0154, -0.0179, -0.0423],\n",
      "         [-0.0207, -0.0013,  0.0024,  ...,  0.0264, -0.0113, -0.0085]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.1040,  0.0875, -0.0360,  ...,  0.0205,  0.0997,  0.0184],\n",
      "         [ 0.0325,  0.0205, -0.0052,  ..., -0.0066, -0.0628, -0.0052],\n",
      "         [-0.0120, -0.0158, -0.0840,  ..., -0.0697, -0.0200, -0.0428],\n",
      "         ...,\n",
      "         [ 0.0288,  0.0068, -0.0004,  ..., -0.0532, -0.0251,  0.0011],\n",
      "         [-0.0186,  0.0195, -0.0229,  ..., -0.0154, -0.0179, -0.0423],\n",
      "         [-0.0207, -0.0013,  0.0024,  ...,  0.0264, -0.0113, -0.0085]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.0870,  0.1049, -0.0173,  ...,  0.0700,  0.0989,  0.0181],\n",
      "         [ 0.0028,  0.0630, -0.0347,  ..., -0.0333, -0.0941, -0.0442],\n",
      "         [-0.0024, -0.0261, -0.0654,  ..., -0.0467,  0.0204, -0.0289],\n",
      "         ...,\n",
      "         [ 0.0193,  0.0039,  0.0060,  ..., -0.0681, -0.0186, -0.0209],\n",
      "         [-0.0553,  0.0091, -0.0432,  ...,  0.0036, -0.0214, -0.0533],\n",
      "         [-0.0232, -0.0274, -0.0047,  ...,  0.0488, -0.0231, -0.0264]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.0870,  0.1049, -0.0173,  ...,  0.0700,  0.0989,  0.0181],\n",
      "         [ 0.0028,  0.0630, -0.0347,  ..., -0.0333, -0.0941, -0.0442],\n",
      "         [-0.0024, -0.0261, -0.0654,  ..., -0.0467,  0.0204, -0.0289],\n",
      "         ...,\n",
      "         [ 0.0193,  0.0039,  0.0060,  ..., -0.0681, -0.0186, -0.0209],\n",
      "         [-0.0553,  0.0091, -0.0432,  ...,  0.0036, -0.0214, -0.0533],\n",
      "         [-0.0232, -0.0274, -0.0047,  ...,  0.0488, -0.0231, -0.0264]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.1113,  0.1022, -0.0102,  ...,  0.0436,  0.1283,  0.0321],\n",
      "         [-0.0439,  0.0840,  0.0259,  ..., -0.0137, -0.1078, -0.0067],\n",
      "         [-0.0439, -0.0184, -0.0414,  ..., -0.0373,  0.0136,  0.0236],\n",
      "         ...,\n",
      "         [-0.0037,  0.0104,  0.0087,  ..., -0.0577, -0.0855, -0.0418],\n",
      "         [-0.0780,  0.0029, -0.0062,  ..., -0.0205, -0.0787, -0.0135],\n",
      "         [-0.0050, -0.0170, -0.0332,  ...,  0.0601, -0.0237, -0.0191]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.1113,  0.1022, -0.0102,  ...,  0.0436,  0.1283,  0.0321],\n",
      "         [-0.0439,  0.0840,  0.0259,  ..., -0.0137, -0.1078, -0.0067],\n",
      "         [-0.0439, -0.0184, -0.0414,  ..., -0.0373,  0.0136,  0.0236],\n",
      "         ...,\n",
      "         [-0.0037,  0.0104,  0.0087,  ..., -0.0577, -0.0855, -0.0418],\n",
      "         [-0.0780,  0.0029, -0.0062,  ..., -0.0205, -0.0787, -0.0135],\n",
      "         [-0.0050, -0.0170, -0.0332,  ...,  0.0601, -0.0237, -0.0191]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.1034,  0.1132,  0.0096,  ...,  0.0453,  0.1477,  0.0301],\n",
      "         [ 0.0286,  0.0389,  0.0113,  ..., -0.0610, -0.0768,  0.0174],\n",
      "         [-0.0267,  0.0421, -0.0808,  ..., -0.0521, -0.0420,  0.0100],\n",
      "         ...,\n",
      "         [ 0.0682,  0.0226, -0.0200,  ..., -0.0747, -0.0381,  0.0225],\n",
      "         [-0.0632,  0.0678,  0.0129,  ..., -0.1566, -0.0060,  0.0227],\n",
      "         [ 0.0047, -0.0140,  0.0513,  ...,  0.1193, -0.0675,  0.0686]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.1034,  0.1132,  0.0096,  ...,  0.0453,  0.1477,  0.0301],\n",
      "         [ 0.0286,  0.0389,  0.0113,  ..., -0.0610, -0.0768,  0.0174],\n",
      "         [-0.0267,  0.0421, -0.0808,  ..., -0.0521, -0.0420,  0.0100],\n",
      "         ...,\n",
      "         [ 0.0682,  0.0226, -0.0200,  ..., -0.0747, -0.0381,  0.0225],\n",
      "         [-0.0632,  0.0678,  0.0129,  ..., -0.1566, -0.0060,  0.0227],\n",
      "         [ 0.0047, -0.0140,  0.0513,  ...,  0.1193, -0.0675,  0.0686]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.1071,  0.1253,  0.0211,  ...,  0.0126,  0.1586,  0.0278],\n",
      "         [ 0.0254,  0.0138, -0.0056,  ..., -0.0460, -0.0385, -0.0067],\n",
      "         [-0.0725,  0.0493, -0.1062,  ...,  0.0147,  0.0470,  0.0636],\n",
      "         ...,\n",
      "         [ 0.0515, -0.0036,  0.0037,  ..., -0.0251, -0.0920,  0.0593],\n",
      "         [ 0.0487,  0.1189, -0.0741,  ..., -0.1122, -0.0185,  0.1065],\n",
      "         [ 0.0280, -0.0539,  0.0559,  ...,  0.1710, -0.0796,  0.0899]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.1071,  0.1253,  0.0211,  ...,  0.0126,  0.1586,  0.0278],\n",
      "         [ 0.0254,  0.0138, -0.0056,  ..., -0.0460, -0.0385, -0.0067],\n",
      "         [-0.0725,  0.0493, -0.1062,  ...,  0.0147,  0.0470,  0.0636],\n",
      "         ...,\n",
      "         [ 0.0515, -0.0036,  0.0037,  ..., -0.0251, -0.0920,  0.0593],\n",
      "         [ 0.0487,  0.1189, -0.0741,  ..., -0.1122, -0.0185,  0.1065],\n",
      "         [ 0.0280, -0.0539,  0.0559,  ...,  0.1710, -0.0796,  0.0899]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.0945,  0.1848,  0.0036,  ..., -0.0473,  0.1489,  0.0536],\n",
      "         [ 0.0708,  0.0055, -0.0248,  ..., -0.1380, -0.1420,  0.0132],\n",
      "         [-0.0885,  0.0002, -0.1301,  ..., -0.0043,  0.0094,  0.0479],\n",
      "         ...,\n",
      "         [ 0.0205, -0.0419, -0.0779,  ..., -0.0282, -0.0153,  0.0605],\n",
      "         [ 0.1364, -0.0403, -0.1037,  ..., -0.0716, -0.0033,  0.1689],\n",
      "         [ 0.0398, -0.1343,  0.0184,  ...,  0.1543, -0.0893, -0.0013]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.0945,  0.1848,  0.0036,  ..., -0.0473,  0.1489,  0.0536],\n",
      "         [ 0.0708,  0.0055, -0.0248,  ..., -0.1380, -0.1420,  0.0132],\n",
      "         [-0.0885,  0.0002, -0.1301,  ..., -0.0043,  0.0094,  0.0479],\n",
      "         ...,\n",
      "         [ 0.0205, -0.0419, -0.0779,  ..., -0.0282, -0.0153,  0.0605],\n",
      "         [ 0.1364, -0.0403, -0.1037,  ..., -0.0716, -0.0033,  0.1689],\n",
      "         [ 0.0398, -0.1343,  0.0184,  ...,  0.1543, -0.0893, -0.0013]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.1060,  0.2113,  0.0270,  ..., -0.1141,  0.1610,  0.0528],\n",
      "         [ 0.0891,  0.0464,  0.0150,  ..., -0.1351, -0.1431, -0.0111],\n",
      "         [-0.1186,  0.0203, -0.0747,  ...,  0.0187,  0.0518, -0.0269],\n",
      "         ...,\n",
      "         [ 0.0676, -0.0044, -0.1408,  ..., -0.0021, -0.0075,  0.0722],\n",
      "         [ 0.2438, -0.0502, -0.0689,  ...,  0.0307, -0.0701,  0.1275],\n",
      "         [ 0.0234, -0.1817, -0.0593,  ...,  0.0979, -0.1048,  0.0844]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.1060,  0.2113,  0.0270,  ..., -0.1141,  0.1610,  0.0528],\n",
      "         [ 0.0891,  0.0464,  0.0150,  ..., -0.1351, -0.1431, -0.0111],\n",
      "         [-0.1186,  0.0203, -0.0747,  ...,  0.0187,  0.0518, -0.0269],\n",
      "         ...,\n",
      "         [ 0.0676, -0.0044, -0.1408,  ..., -0.0021, -0.0075,  0.0722],\n",
      "         [ 0.2438, -0.0502, -0.0689,  ...,  0.0307, -0.0701,  0.1275],\n",
      "         [ 0.0234, -0.1817, -0.0593,  ...,  0.0979, -0.1048,  0.0844]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.0954,  0.2493,  0.0073,  ..., -0.1568,  0.1818,  0.0855],\n",
      "         [ 0.0247,  0.0353,  0.0174,  ..., -0.1185, -0.1029, -0.0125],\n",
      "         [-0.1189,  0.0464, -0.1282,  ..., -0.0012,  0.1187,  0.0251],\n",
      "         ...,\n",
      "         [ 0.0547,  0.0124, -0.1577,  ..., -0.0486, -0.0530,  0.0246],\n",
      "         [ 0.2236, -0.0961, -0.1163,  ...,  0.0402, -0.0885,  0.1684],\n",
      "         [ 0.0864, -0.0735,  0.0480,  ...,  0.0917, -0.0574,  0.0982]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.0954,  0.2493,  0.0073,  ..., -0.1568,  0.1818,  0.0855],\n",
      "         [ 0.0247,  0.0353,  0.0174,  ..., -0.1185, -0.1029, -0.0125],\n",
      "         [-0.1189,  0.0464, -0.1282,  ..., -0.0012,  0.1187,  0.0251],\n",
      "         ...,\n",
      "         [ 0.0547,  0.0124, -0.1577,  ..., -0.0486, -0.0530,  0.0246],\n",
      "         [ 0.2236, -0.0961, -0.1163,  ...,  0.0402, -0.0885,  0.1684],\n",
      "         [ 0.0864, -0.0735,  0.0480,  ...,  0.0917, -0.0574,  0.0982]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.0152,  0.3340,  0.0011,  ..., -0.2750,  0.1933,  0.1034],\n",
      "         [ 0.1112,  0.0284,  0.0888,  ..., -0.1266, -0.1327,  0.0091],\n",
      "         [-0.1653,  0.0843, -0.1003,  ..., -0.0103,  0.0637, -0.0262],\n",
      "         ...,\n",
      "         [ 0.0326, -0.0129, -0.0163,  ..., -0.0737, -0.1444,  0.0194],\n",
      "         [ 0.1577, -0.1211, -0.0551,  ..., -0.0130, -0.1055,  0.2409],\n",
      "         [ 0.0475, -0.0591, -0.0268,  ...,  0.1012, -0.0668,  0.1292]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.0152,  0.3340,  0.0011,  ..., -0.2750,  0.1933,  0.1034],\n",
      "         [ 0.1112,  0.0284,  0.0888,  ..., -0.1266, -0.1327,  0.0091],\n",
      "         [-0.1653,  0.0843, -0.1003,  ..., -0.0103,  0.0637, -0.0262],\n",
      "         ...,\n",
      "         [ 0.0326, -0.0129, -0.0163,  ..., -0.0737, -0.1444,  0.0194],\n",
      "         [ 0.1577, -0.1211, -0.0551,  ..., -0.0130, -0.1055,  0.2409],\n",
      "         [ 0.0475, -0.0591, -0.0268,  ...,  0.1012, -0.0668,  0.1292]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.0356,  0.3483, -0.0366,  ..., -0.2962,  0.2088,  0.1065],\n",
      "         [ 0.0570, -0.0095,  0.0588,  ..., -0.1291, -0.0475,  0.0199],\n",
      "         [-0.1888,  0.0561, -0.2317,  ...,  0.0224,  0.0880, -0.0699],\n",
      "         ...,\n",
      "         [ 0.0334,  0.0206, -0.0295,  ..., -0.0183, -0.0501, -0.0120],\n",
      "         [ 0.1263, -0.0313, -0.0066,  ..., -0.0658, -0.0623,  0.2589],\n",
      "         [-0.0218, -0.0591,  0.0674,  ...,  0.0207, -0.0245,  0.0875]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.0356,  0.3483, -0.0366,  ..., -0.2962,  0.2088,  0.1065],\n",
      "         [ 0.0570, -0.0095,  0.0588,  ..., -0.1291, -0.0475,  0.0199],\n",
      "         [-0.1888,  0.0561, -0.2317,  ...,  0.0224,  0.0880, -0.0699],\n",
      "         ...,\n",
      "         [ 0.0334,  0.0206, -0.0295,  ..., -0.0183, -0.0501, -0.0120],\n",
      "         [ 0.1263, -0.0313, -0.0066,  ..., -0.0658, -0.0623,  0.2589],\n",
      "         [-0.0218, -0.0591,  0.0674,  ...,  0.0207, -0.0245,  0.0875]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.0127,  0.3661, -0.0316,  ..., -0.3678,  0.1973,  0.1019],\n",
      "         [ 0.0589, -0.0548,  0.1048,  ..., -0.1313, -0.0421,  0.0594],\n",
      "         [-0.1531,  0.0481, -0.1638,  ..., -0.0581,  0.1726, -0.0939],\n",
      "         ...,\n",
      "         [-0.0521, -0.0680, -0.0547,  ...,  0.0502,  0.0415,  0.0672],\n",
      "         [ 0.0550, -0.0818, -0.0551,  ...,  0.0320, -0.0534,  0.3440],\n",
      "         [-0.1131, -0.0338,  0.0410,  ...,  0.0413, -0.0032,  0.0537]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.0127,  0.3661, -0.0316,  ..., -0.3678,  0.1973,  0.1019],\n",
      "         [ 0.0589, -0.0548,  0.1048,  ..., -0.1313, -0.0421,  0.0594],\n",
      "         [-0.1531,  0.0481, -0.1638,  ..., -0.0581,  0.1726, -0.0939],\n",
      "         ...,\n",
      "         [-0.0521, -0.0680, -0.0547,  ...,  0.0502,  0.0415,  0.0672],\n",
      "         [ 0.0550, -0.0818, -0.0551,  ...,  0.0320, -0.0534,  0.3440],\n",
      "         [-0.1131, -0.0338,  0.0410,  ...,  0.0413, -0.0032,  0.0537]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.0089,  0.3233, -0.0779,  ..., -0.4491,  0.1845,  0.0951],\n",
      "         [ 0.0398, -0.0933,  0.0544,  ..., -0.1363,  0.0038,  0.0334],\n",
      "         [-0.1918,  0.0412, -0.1267,  ..., -0.0713,  0.2042, -0.0886],\n",
      "         ...,\n",
      "         [ 0.0219, -0.0659, -0.0347,  ...,  0.0369, -0.0176, -0.0150],\n",
      "         [-0.0036, -0.1399,  0.0049,  ..., -0.0413, -0.1747,  0.2164],\n",
      "         [-0.1696,  0.0317,  0.0649,  ..., -0.1466, -0.0224,  0.1316]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.0089,  0.3233, -0.0779,  ..., -0.4491,  0.1845,  0.0951],\n",
      "         [ 0.0398, -0.0933,  0.0544,  ..., -0.1363,  0.0038,  0.0334],\n",
      "         [-0.1918,  0.0412, -0.1267,  ..., -0.0713,  0.2042, -0.0886],\n",
      "         ...,\n",
      "         [ 0.0219, -0.0659, -0.0347,  ...,  0.0369, -0.0176, -0.0150],\n",
      "         [-0.0036, -0.1399,  0.0049,  ..., -0.0413, -0.1747,  0.2164],\n",
      "         [-0.1696,  0.0317,  0.0649,  ..., -0.1466, -0.0224,  0.1316]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.0244,  0.2828, -0.0532,  ..., -0.4592,  0.2300,  0.0142],\n",
      "         [ 0.0482, -0.1366, -0.0849,  ..., -0.0976, -0.0585,  0.0084],\n",
      "         [-0.1271,  0.1184, -0.1328,  ..., -0.0904,  0.1190, -0.0516],\n",
      "         ...,\n",
      "         [ 0.0756, -0.0403, -0.0931,  ...,  0.0360, -0.0083,  0.0225],\n",
      "         [ 0.0263, -0.0898, -0.0338,  ..., -0.0231, -0.0679,  0.2986],\n",
      "         [-0.1766,  0.0666, -0.1188,  ..., -0.1172, -0.1211,  0.0675]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.0244,  0.2828, -0.0532,  ..., -0.4592,  0.2300,  0.0142],\n",
      "         [ 0.0482, -0.1366, -0.0849,  ..., -0.0976, -0.0585,  0.0084],\n",
      "         [-0.1271,  0.1184, -0.1328,  ..., -0.0904,  0.1190, -0.0516],\n",
      "         ...,\n",
      "         [ 0.0756, -0.0403, -0.0931,  ...,  0.0360, -0.0083,  0.0225],\n",
      "         [ 0.0263, -0.0898, -0.0338,  ..., -0.0231, -0.0679,  0.2986],\n",
      "         [-0.1766,  0.0666, -0.1188,  ..., -0.1172, -0.1211,  0.0675]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.0928,  0.1485,  0.0158,  ..., -0.5380,  0.2664, -0.0333],\n",
      "         [-0.0566, -0.2087, -0.0116,  ..., -0.0875,  0.0538,  0.0152],\n",
      "         [-0.1207,  0.0571, -0.1515,  ..., -0.0479,  0.0742, -0.0149],\n",
      "         ...,\n",
      "         [ 0.0059, -0.0869, -0.1102,  ...,  0.0879, -0.0310,  0.0273],\n",
      "         [ 0.0522, -0.0844, -0.0950,  ...,  0.0494, -0.1573,  0.2041],\n",
      "         [-0.0522,  0.0473,  0.0367,  ..., -0.0094, -0.2520, -0.0750]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.0928,  0.1485,  0.0158,  ..., -0.5380,  0.2664, -0.0333],\n",
      "         [-0.0566, -0.2087, -0.0116,  ..., -0.0875,  0.0538,  0.0152],\n",
      "         [-0.1207,  0.0571, -0.1515,  ..., -0.0479,  0.0742, -0.0149],\n",
      "         ...,\n",
      "         [ 0.0059, -0.0869, -0.1102,  ...,  0.0879, -0.0310,  0.0273],\n",
      "         [ 0.0522, -0.0844, -0.0950,  ...,  0.0494, -0.1573,  0.2041],\n",
      "         [-0.0522,  0.0473,  0.0367,  ..., -0.0094, -0.2520, -0.0750]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.1000,  0.1323,  0.0241,  ..., -0.5470,  0.2844, -0.0457],\n",
      "         [-0.1309, -0.2241, -0.0490,  ...,  0.0622, -0.0486,  0.0798],\n",
      "         [-0.1844,  0.0109, -0.0420,  ...,  0.1513, -0.0039, -0.0294],\n",
      "         ...,\n",
      "         [ 0.0944, -0.0838, -0.0443,  ...,  0.0893, -0.0366,  0.0864],\n",
      "         [ 0.1122, -0.1630, -0.1321,  ...,  0.0673, -0.0957,  0.3244],\n",
      "         [-0.0671,  0.1170,  0.0549,  ...,  0.0466, -0.2616,  0.0200]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.1000,  0.1323,  0.0241,  ..., -0.5470,  0.2844, -0.0457],\n",
      "         [-0.1309, -0.2241, -0.0490,  ...,  0.0622, -0.0486,  0.0798],\n",
      "         [-0.1844,  0.0109, -0.0420,  ...,  0.1513, -0.0039, -0.0294],\n",
      "         ...,\n",
      "         [ 0.0944, -0.0838, -0.0443,  ...,  0.0893, -0.0366,  0.0864],\n",
      "         [ 0.1122, -0.1630, -0.1321,  ...,  0.0673, -0.0957,  0.3244],\n",
      "         [-0.0671,  0.1170,  0.0549,  ...,  0.0466, -0.2616,  0.0200]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.1286,  0.0833,  0.0239,  ..., -0.5657,  0.3694, -0.0725],\n",
      "         [-0.2528, -0.0341,  0.0986,  ..., -0.0898, -0.0587,  0.0939],\n",
      "         [-0.1440,  0.0469, -0.1200,  ...,  0.1806,  0.0746, -0.1096],\n",
      "         ...,\n",
      "         [ 0.2922, -0.1113, -0.0451,  ...,  0.1168, -0.0880,  0.0308],\n",
      "         [ 0.0418, -0.2008,  0.0009,  ...,  0.1151, -0.1247,  0.3107],\n",
      "         [-0.0915,  0.0891, -0.0299,  ...,  0.0710, -0.3996, -0.1268]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.1286,  0.0833,  0.0239,  ..., -0.5657,  0.3694, -0.0725],\n",
      "         [-0.2528, -0.0341,  0.0986,  ..., -0.0898, -0.0587,  0.0939],\n",
      "         [-0.1440,  0.0469, -0.1200,  ...,  0.1806,  0.0746, -0.1096],\n",
      "         ...,\n",
      "         [ 0.2922, -0.1113, -0.0451,  ...,  0.1168, -0.0880,  0.0308],\n",
      "         [ 0.0418, -0.2008,  0.0009,  ...,  0.1151, -0.1247,  0.3107],\n",
      "         [-0.0915,  0.0891, -0.0299,  ...,  0.0710, -0.3996, -0.1268]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.1417,  0.0703,  0.0343,  ..., -0.5549,  0.3175, -0.0847],\n",
      "         [-0.3476, -0.0937,  0.1124,  ..., -0.0447, -0.1078,  0.1821],\n",
      "         [-0.2412,  0.0704, -0.0434,  ...,  0.1855,  0.0505, -0.1668],\n",
      "         ...,\n",
      "         [ 0.4836, -0.2135, -0.0819,  ...,  0.2061, -0.0716,  0.0859],\n",
      "         [ 0.0379, -0.2848,  0.0400,  ...,  0.1789, -0.1095,  0.2678],\n",
      "         [-0.1623, -0.0260,  0.0180,  ...,  0.1799, -0.3248, -0.0595]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.1417,  0.0703,  0.0343,  ..., -0.5549,  0.3175, -0.0847],\n",
      "         [-0.3476, -0.0937,  0.1124,  ..., -0.0447, -0.1078,  0.1821],\n",
      "         [-0.2412,  0.0704, -0.0434,  ...,  0.1855,  0.0505, -0.1668],\n",
      "         ...,\n",
      "         [ 0.4836, -0.2135, -0.0819,  ...,  0.2061, -0.0716,  0.0859],\n",
      "         [ 0.0379, -0.2848,  0.0400,  ...,  0.1789, -0.1095,  0.2678],\n",
      "         [-0.1623, -0.0260,  0.0180,  ...,  0.1799, -0.3248, -0.0595]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.1528,  0.0725,  0.0609,  ..., -0.5624,  0.3103, -0.0293],\n",
      "         [-0.3468, -0.1552,  0.2314,  ..., -0.0407, -0.0664,  0.3190],\n",
      "         [-0.2351,  0.1050,  0.0245,  ...,  0.2240,  0.1402, -0.1482],\n",
      "         ...,\n",
      "         [ 0.5813, -0.1658, -0.1481,  ...,  0.1943, -0.0924,  0.0314],\n",
      "         [ 0.0010, -0.4502,  0.0164,  ...,  0.2709, -0.1429,  0.2168],\n",
      "         [-0.1297, -0.1102, -0.0122,  ...,  0.1949, -0.3611, -0.1702]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.1528,  0.0725,  0.0609,  ..., -0.5624,  0.3103, -0.0293],\n",
      "         [-0.3468, -0.1552,  0.2314,  ..., -0.0407, -0.0664,  0.3190],\n",
      "         [-0.2351,  0.1050,  0.0245,  ...,  0.2240,  0.1402, -0.1482],\n",
      "         ...,\n",
      "         [ 0.5813, -0.1658, -0.1481,  ...,  0.1943, -0.0924,  0.0314],\n",
      "         [ 0.0010, -0.4502,  0.0164,  ...,  0.2709, -0.1429,  0.2168],\n",
      "         [-0.1297, -0.1102, -0.0122,  ...,  0.1949, -0.3611, -0.1702]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.1559,  0.0520,  0.0566,  ..., -0.5805,  0.2614, -0.0032],\n",
      "         [-0.3409, -0.1812,  0.2162,  ..., -0.0630, -0.0938,  0.4737],\n",
      "         [-0.2003,  0.0577, -0.0385,  ...,  0.2068,  0.1210, -0.1325],\n",
      "         ...,\n",
      "         [ 0.5419, -0.1353, -0.3270,  ...,  0.1145, -0.0509, -0.1870],\n",
      "         [ 0.0050, -0.4178, -0.0576,  ...,  0.3430, -0.2054,  0.1357],\n",
      "         [-0.0763, -0.2359, -0.0382,  ...,  0.2908, -0.4592, -0.1985]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.1559,  0.0520,  0.0566,  ..., -0.5805,  0.2614, -0.0032],\n",
      "         [-0.3409, -0.1812,  0.2162,  ..., -0.0630, -0.0938,  0.4737],\n",
      "         [-0.2003,  0.0577, -0.0385,  ...,  0.2068,  0.1210, -0.1325],\n",
      "         ...,\n",
      "         [ 0.5419, -0.1353, -0.3270,  ...,  0.1145, -0.0509, -0.1870],\n",
      "         [ 0.0050, -0.4178, -0.0576,  ...,  0.3430, -0.2054,  0.1357],\n",
      "         [-0.0763, -0.2359, -0.0382,  ...,  0.2908, -0.4592, -0.1985]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.1488,  0.0445,  0.0765,  ..., -0.5456,  0.2702,  0.0098],\n",
      "         [-0.3340, -0.1390,  0.2591,  ..., -0.0094, -0.2378,  0.6264],\n",
      "         [-0.1210, -0.0595,  0.0405,  ...,  0.2131,  0.1394, -0.0737],\n",
      "         ...,\n",
      "         [ 0.7279, -0.0172, -0.4659,  ...,  0.2827, -0.2305, -0.1720],\n",
      "         [-0.0240, -0.3846,  0.0579,  ...,  0.3565, -0.1856,  0.1467],\n",
      "         [-0.1255, -0.2112, -0.0431,  ...,  0.3120, -0.4698, -0.2204]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.1488,  0.0445,  0.0765,  ..., -0.5456,  0.2702,  0.0098],\n",
      "         [-0.3340, -0.1390,  0.2591,  ..., -0.0094, -0.2378,  0.6264],\n",
      "         [-0.1210, -0.0595,  0.0405,  ...,  0.2131,  0.1394, -0.0737],\n",
      "         ...,\n",
      "         [ 0.7279, -0.0172, -0.4659,  ...,  0.2827, -0.2305, -0.1720],\n",
      "         [-0.0240, -0.3846,  0.0579,  ...,  0.3565, -0.1856,  0.1467],\n",
      "         [-0.1255, -0.2112, -0.0431,  ...,  0.3120, -0.4698, -0.2204]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-1.2677e-01,  3.3335e-02,  9.3478e-02,  ..., -5.3204e-01,\n",
      "           2.7953e-01,  5.9818e-04],\n",
      "         [-3.8235e-01, -1.4862e-01,  2.7782e-01,  ..., -3.6674e-02,\n",
      "          -3.2798e-01,  6.4951e-01],\n",
      "         [-2.0899e-01, -3.7166e-02,  7.0932e-02,  ...,  2.0182e-01,\n",
      "           1.8469e-01, -3.9982e-02],\n",
      "         ...,\n",
      "         [ 8.2684e-01,  2.3298e-01, -4.2942e-01,  ...,  3.8100e-01,\n",
      "          -3.8534e-01, -9.4766e-02],\n",
      "         [-7.2406e-02, -3.7812e-01,  1.3713e-01,  ...,  3.8133e-01,\n",
      "          -2.2418e-01,  2.2052e-01],\n",
      "         [-1.1716e-01, -1.3050e-01,  3.8509e-02,  ...,  3.9373e-01,\n",
      "          -4.5872e-01, -1.1123e-01]]], device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-1.2677e-01,  3.3335e-02,  9.3478e-02,  ..., -5.3204e-01,\n",
      "           2.7953e-01,  5.9818e-04],\n",
      "         [-3.8235e-01, -1.4862e-01,  2.7782e-01,  ..., -3.6674e-02,\n",
      "          -3.2798e-01,  6.4951e-01],\n",
      "         [-2.0899e-01, -3.7166e-02,  7.0932e-02,  ...,  2.0182e-01,\n",
      "           1.8469e-01, -3.9982e-02],\n",
      "         ...,\n",
      "         [ 8.2684e-01,  2.3298e-01, -4.2942e-01,  ...,  3.8100e-01,\n",
      "          -3.8534e-01, -9.4766e-02],\n",
      "         [-7.2406e-02, -3.7812e-01,  1.3713e-01,  ...,  3.8133e-01,\n",
      "          -2.2418e-01,  2.2052e-01],\n",
      "         [-1.1716e-01, -1.3050e-01,  3.8509e-02,  ...,  3.9373e-01,\n",
      "          -4.5872e-01, -1.1123e-01]]], device='cuda:0'),) and output (tensor([[[-0.1355,  0.0227,  0.0554,  ..., -0.5473,  0.2470, -0.0229],\n",
      "         [-0.3956, -0.1122,  0.3374,  ..., -0.1388, -0.2907,  0.5349],\n",
      "         [-0.2091, -0.0414,  0.1750,  ...,  0.2454,  0.1852, -0.1023],\n",
      "         ...,\n",
      "         [ 0.9022,  0.1513, -0.2102,  ...,  0.3191, -0.4278, -0.1721],\n",
      "         [-0.0573, -0.4670,  0.3161,  ...,  0.3617, -0.2895,  0.3044],\n",
      "         [-0.0556, -0.1836,  0.0512,  ...,  0.4304, -0.4969, -0.1003]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.1355,  0.0227,  0.0554,  ..., -0.5473,  0.2470, -0.0229],\n",
      "         [-0.3956, -0.1122,  0.3374,  ..., -0.1388, -0.2907,  0.5349],\n",
      "         [-0.2091, -0.0414,  0.1750,  ...,  0.2454,  0.1852, -0.1023],\n",
      "         ...,\n",
      "         [ 0.9022,  0.1513, -0.2102,  ...,  0.3191, -0.4278, -0.1721],\n",
      "         [-0.0573, -0.4670,  0.3161,  ...,  0.3617, -0.2895,  0.3044],\n",
      "         [-0.0556, -0.1836,  0.0512,  ...,  0.4304, -0.4969, -0.1003]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.1350,  0.0163,  0.0576,  ..., -0.5555,  0.2435,  0.0051],\n",
      "         [-0.3819, -0.0926,  0.2660,  ..., -0.1231, -0.2469,  0.5790],\n",
      "         [-0.2839,  0.1047,  0.3056,  ...,  0.1931,  0.1934, -0.0787],\n",
      "         ...,\n",
      "         [ 0.9035,  0.1129, -0.2082,  ...,  0.0171, -0.5728,  0.1660],\n",
      "         [-0.0338, -0.5094,  0.3920,  ...,  0.3540, -0.2097,  0.3646],\n",
      "         [-0.0859, -0.2587,  0.0311,  ...,  0.4686, -0.4875,  0.0540]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.1350,  0.0163,  0.0576,  ..., -0.5555,  0.2435,  0.0051],\n",
      "         [-0.3819, -0.0926,  0.2660,  ..., -0.1231, -0.2469,  0.5790],\n",
      "         [-0.2839,  0.1047,  0.3056,  ...,  0.1931,  0.1934, -0.0787],\n",
      "         ...,\n",
      "         [ 0.9035,  0.1129, -0.2082,  ...,  0.0171, -0.5728,  0.1660],\n",
      "         [-0.0338, -0.5094,  0.3920,  ...,  0.3540, -0.2097,  0.3646],\n",
      "         [-0.0859, -0.2587,  0.0311,  ...,  0.4686, -0.4875,  0.0540]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-1.3006e-01, -2.8984e-03,  2.7912e-02,  ..., -5.5374e-01,\n",
      "           2.2766e-01,  1.6436e-02],\n",
      "         [-3.4164e-01, -1.9288e-01,  2.2457e-01,  ..., -9.4415e-02,\n",
      "          -1.7347e-01,  3.6338e-01],\n",
      "         [-3.5085e-01,  1.5368e-01,  3.3340e-01,  ...,  2.8183e-01,\n",
      "           1.7143e-01, -3.2053e-02],\n",
      "         ...,\n",
      "         [ 8.1974e-01,  9.4559e-02, -1.5751e-01,  ..., -2.0970e-02,\n",
      "          -4.6383e-01, -1.2477e-01],\n",
      "         [ 1.9414e-04, -6.1240e-01,  2.8261e-01,  ...,  5.3479e-01,\n",
      "          -2.0119e-01,  3.0068e-01],\n",
      "         [-1.1261e-01, -1.7401e-01,  1.9789e-01,  ...,  3.9898e-01,\n",
      "          -4.8694e-01,  3.8944e-03]]], device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-1.3006e-01, -2.8984e-03,  2.7912e-02,  ..., -5.5374e-01,\n",
      "           2.2766e-01,  1.6436e-02],\n",
      "         [-3.4164e-01, -1.9288e-01,  2.2457e-01,  ..., -9.4415e-02,\n",
      "          -1.7347e-01,  3.6338e-01],\n",
      "         [-3.5085e-01,  1.5368e-01,  3.3340e-01,  ...,  2.8183e-01,\n",
      "           1.7143e-01, -3.2053e-02],\n",
      "         ...,\n",
      "         [ 8.1974e-01,  9.4559e-02, -1.5751e-01,  ..., -2.0970e-02,\n",
      "          -4.6383e-01, -1.2477e-01],\n",
      "         [ 1.9414e-04, -6.1240e-01,  2.8261e-01,  ...,  5.3479e-01,\n",
      "          -2.0119e-01,  3.0068e-01],\n",
      "         [-1.1261e-01, -1.7401e-01,  1.9789e-01,  ...,  3.9898e-01,\n",
      "          -4.8694e-01,  3.8944e-03]]], device='cuda:0'),) and output (tensor([[[-1.2185e-01,  6.1148e-03, -2.0994e-03,  ..., -5.9223e-01,\n",
      "           2.7877e-01, -1.9512e-03],\n",
      "         [-3.2836e-01, -6.6102e-05,  1.6677e-01,  ...,  7.2438e-02,\n",
      "          -1.0604e-01,  4.2332e-01],\n",
      "         [-3.8904e-01,  2.2533e-01,  3.3234e-01,  ...,  2.1182e-01,\n",
      "           1.6837e-01,  4.7129e-02],\n",
      "         ...,\n",
      "         [ 6.5870e-01,  2.3505e-02, -2.2797e-01,  ..., -1.9466e-01,\n",
      "          -1.7239e-01, -2.2450e-01],\n",
      "         [ 1.3311e-01, -5.3505e-01,  5.1019e-01,  ...,  6.0082e-01,\n",
      "          -5.1215e-02,  4.8094e-01],\n",
      "         [-1.9971e-01, -3.0655e-01,  2.7682e-01,  ...,  3.5980e-01,\n",
      "          -5.1372e-01,  7.0222e-02]]], device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-1.2185e-01,  6.1148e-03, -2.0994e-03,  ..., -5.9223e-01,\n",
      "           2.7877e-01, -1.9512e-03],\n",
      "         [-3.2836e-01, -6.6102e-05,  1.6677e-01,  ...,  7.2438e-02,\n",
      "          -1.0604e-01,  4.2332e-01],\n",
      "         [-3.8904e-01,  2.2533e-01,  3.3234e-01,  ...,  2.1182e-01,\n",
      "           1.6837e-01,  4.7129e-02],\n",
      "         ...,\n",
      "         [ 6.5870e-01,  2.3505e-02, -2.2797e-01,  ..., -1.9466e-01,\n",
      "          -1.7239e-01, -2.2450e-01],\n",
      "         [ 1.3311e-01, -5.3505e-01,  5.1019e-01,  ...,  6.0082e-01,\n",
      "          -5.1215e-02,  4.8094e-01],\n",
      "         [-1.9971e-01, -3.0655e-01,  2.7682e-01,  ...,  3.5980e-01,\n",
      "          -5.1372e-01,  7.0222e-02]]], device='cuda:0'),) and output (tensor([[[-1.0667e-01,  2.7209e-02,  4.9523e-02,  ..., -5.5496e-01,\n",
      "           3.4494e-01,  2.6647e-02],\n",
      "         [-2.2630e-01, -9.0985e-02,  2.3431e-01,  ...,  1.6737e-01,\n",
      "           2.7912e-02,  2.3807e-01],\n",
      "         [-3.9984e-01,  2.3161e-01,  3.8343e-01,  ...,  2.1342e-01,\n",
      "           6.4422e-02,  9.1275e-03],\n",
      "         ...,\n",
      "         [ 9.6201e-01,  1.3824e-01, -9.1347e-02,  ..., -8.5355e-04,\n",
      "           8.4996e-02, -1.2975e-02],\n",
      "         [-2.3809e-02, -2.9946e-01,  6.6934e-01,  ...,  5.8564e-01,\n",
      "          -9.0383e-02,  6.6543e-01],\n",
      "         [-4.3774e-01, -3.4476e-01,  2.0946e-01,  ...,  5.5759e-01,\n",
      "          -6.2901e-01,  1.5884e-01]]], device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-1.0667e-01,  2.7209e-02,  4.9523e-02,  ..., -5.5496e-01,\n",
      "           3.4494e-01,  2.6647e-02],\n",
      "         [-2.2630e-01, -9.0985e-02,  2.3431e-01,  ...,  1.6737e-01,\n",
      "           2.7912e-02,  2.3807e-01],\n",
      "         [-3.9984e-01,  2.3161e-01,  3.8343e-01,  ...,  2.1342e-01,\n",
      "           6.4422e-02,  9.1275e-03],\n",
      "         ...,\n",
      "         [ 9.6201e-01,  1.3824e-01, -9.1347e-02,  ..., -8.5355e-04,\n",
      "           8.4996e-02, -1.2975e-02],\n",
      "         [-2.3809e-02, -2.9946e-01,  6.6934e-01,  ...,  5.8564e-01,\n",
      "          -9.0383e-02,  6.6543e-01],\n",
      "         [-4.3774e-01, -3.4476e-01,  2.0946e-01,  ...,  5.5759e-01,\n",
      "          -6.2901e-01,  1.5884e-01]]], device='cuda:0'),) and output (tensor([[[-0.0592,  0.0731,  0.0500,  ..., -0.5517,  0.3541,  0.0595],\n",
      "         [-0.5974,  0.0706,  0.3566,  ...,  0.0952,  0.0330,  0.1969],\n",
      "         [-0.4897, -0.0110,  0.3705,  ...,  0.2358,  0.0026,  0.0077],\n",
      "         ...,\n",
      "         [ 0.4914,  0.2757, -0.1401,  ...,  0.3107,  0.0861,  0.3933],\n",
      "         [-0.1184, -0.8919,  0.7666,  ...,  0.4666,  0.0456,  0.4703],\n",
      "         [-0.4048, -0.7124,  0.3977,  ...,  0.3957, -0.5998, -0.0720]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.0592,  0.0731,  0.0500,  ..., -0.5517,  0.3541,  0.0595],\n",
      "         [-0.5974,  0.0706,  0.3566,  ...,  0.0952,  0.0330,  0.1969],\n",
      "         [-0.4897, -0.0110,  0.3705,  ...,  0.2358,  0.0026,  0.0077],\n",
      "         ...,\n",
      "         [ 0.4914,  0.2757, -0.1401,  ...,  0.3107,  0.0861,  0.3933],\n",
      "         [-0.1184, -0.8919,  0.7666,  ...,  0.4666,  0.0456,  0.4703],\n",
      "         [-0.4048, -0.7124,  0.3977,  ...,  0.3957, -0.5998, -0.0720]]],\n",
      "       device='cuda:0'),) and output (tensor([[[ 0.1143,  0.2237,  0.3292,  ..., -0.7706,  0.2608,  0.0269],\n",
      "         [-0.4173, -0.0953,  0.2047,  ..., -0.0104, -0.0170,  0.0384],\n",
      "         [-0.5555,  0.0182,  0.4674,  ...,  0.2667, -0.1717,  0.0280],\n",
      "         ...,\n",
      "         [ 0.6047, -0.2012, -0.3669,  ..., -0.1599, -0.2601,  0.1462],\n",
      "         [ 0.0817, -1.3672,  0.5983,  ...,  0.7146, -0.1119,  0.4321],\n",
      "         [-0.3124, -0.9676,  0.3510,  ...,  0.5772, -0.9084,  0.2252]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[ 0.1143,  0.2237,  0.3292,  ..., -0.7706,  0.2608,  0.0269],\n",
      "         [-0.4173, -0.0953,  0.2047,  ..., -0.0104, -0.0170,  0.0384],\n",
      "         [-0.5555,  0.0182,  0.4674,  ...,  0.2667, -0.1717,  0.0280],\n",
      "         ...,\n",
      "         [ 0.6047, -0.2012, -0.3669,  ..., -0.1599, -0.2601,  0.1462],\n",
      "         [ 0.0817, -1.3672,  0.5983,  ...,  0.7146, -0.1119,  0.4321],\n",
      "         [-0.3124, -0.9676,  0.3510,  ...,  0.5772, -0.9084,  0.2252]]],\n",
      "       device='cuda:0'),) and output (tensor([[[ 0.3711,  0.5489,  0.6100,  ..., -0.7499,  0.2080,  0.4195],\n",
      "         [-0.2543,  0.1464, -0.0604,  ..., -0.2533,  0.0780, -0.3696],\n",
      "         [-0.5582, -0.1240,  0.4592,  ...,  0.3804, -0.3473, -0.3471],\n",
      "         ...,\n",
      "         [ 0.4754, -0.2655, -0.5838,  ..., -0.6328, -1.0259, -0.0170],\n",
      "         [ 0.3754, -1.5523,  0.3944,  ...,  0.4802, -0.3195, -0.1737],\n",
      "         [-1.0225, -1.2698, -0.1392,  ...,  0.2209, -1.1925, -0.0456]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[ 0.3711,  0.5489,  0.6100,  ..., -0.7499,  0.2080,  0.4195],\n",
      "         [-0.2543,  0.1464, -0.0604,  ..., -0.2533,  0.0780, -0.3696],\n",
      "         [-0.5582, -0.1240,  0.4592,  ...,  0.3804, -0.3473, -0.3471],\n",
      "         ...,\n",
      "         [ 0.4754, -0.2655, -0.5838,  ..., -0.6328, -1.0259, -0.0170],\n",
      "         [ 0.3754, -1.5523,  0.3944,  ...,  0.4802, -0.3195, -0.1737],\n",
      "         [-1.0225, -1.2698, -0.1392,  ...,  0.2209, -1.1925, -0.0456]]],\n",
      "       device='cuda:0'),) and output (tensor([[[ 1.1544,  3.2965,  1.6534,  ..., -2.7009,  2.6553,  2.2700],\n",
      "         [-0.4047,  1.1024,  0.2272,  ..., -0.3268, -0.2346, -0.1542],\n",
      "         [-0.4378,  0.2979,  1.2761,  ...,  0.0853, -0.7684, -0.3182],\n",
      "         ...,\n",
      "         [ 0.3587,  0.3224, -0.5377,  ..., -0.9553, -0.3855,  0.0626],\n",
      "         [ 0.1909, -1.8010,  1.3129,  ...,  2.0953, -0.5007,  0.8008],\n",
      "         [-0.9246, -2.5051,  0.6439,  ...,  1.0771, -1.2464,  0.7315]]],\n",
      "       device='cuda:0'),)\n",
      "[Debug] Layer names being passed: ['model.layers.0', 'model.layers.1', 'model.layers.2', 'model.layers.3', 'model.layers.4', 'model.layers.5', 'model.layers.6', 'model.layers.7', 'model.layers.8', 'model.layers.9', 'model.layers.10', 'model.layers.11', 'model.layers.12', 'model.layers.13', 'model.layers.14', 'model.layers.15', 'model.layers.16', 'model.layers.17', 'model.layers.18', 'model.layers.19', 'model.layers.20', 'model.layers.21', 'model.layers.22', 'model.layers.23', 'model.layers.24', 'model.layers.25', 'model.layers.26', 'model.layers.27', 'model.layers.28', 'model.layers.29', 'model.layers.30', 'model.layers.31', 'model.embed_tokens']\n",
      "[Debug] Trying to access layer: model.layers.0\n",
      "[Debug] Successfully found layer: model.layers.0\n",
      "[Debug] Trying to access layer: model.layers.1\n",
      "[Debug] Successfully found layer: model.layers.1\n",
      "[Debug] Trying to access layer: model.layers.2\n",
      "[Debug] Successfully found layer: model.layers.2\n",
      "[Debug] Trying to access layer: model.layers.3\n",
      "[Debug] Successfully found layer: model.layers.3\n",
      "[Debug] Trying to access layer: model.layers.4\n",
      "[Debug] Successfully found layer: model.layers.4\n",
      "[Debug] Trying to access layer: model.layers.5\n",
      "[Debug] Successfully found layer: model.layers.5\n",
      "[Debug] Trying to access layer: model.layers.6\n",
      "[Debug] Successfully found layer: model.layers.6\n",
      "[Debug] Trying to access layer: model.layers.7\n",
      "[Debug] Successfully found layer: model.layers.7\n",
      "[Debug] Trying to access layer: model.layers.8\n",
      "[Debug] Successfully found layer: model.layers.8\n",
      "[Debug] Trying to access layer: model.layers.9\n",
      "[Debug] Successfully found layer: model.layers.9\n",
      "[Debug] Trying to access layer: model.layers.10\n",
      "[Debug] Successfully found layer: model.layers.10\n",
      "[Debug] Trying to access layer: model.layers.11\n",
      "[Debug] Successfully found layer: model.layers.11\n",
      "[Debug] Trying to access layer: model.layers.12\n",
      "[Debug] Successfully found layer: model.layers.12\n",
      "[Debug] Trying to access layer: model.layers.13\n",
      "[Debug] Successfully found layer: model.layers.13\n",
      "[Debug] Trying to access layer: model.layers.14\n",
      "[Debug] Successfully found layer: model.layers.14\n",
      "[Debug] Trying to access layer: model.layers.15\n",
      "[Debug] Successfully found layer: model.layers.15\n",
      "[Debug] Trying to access layer: model.layers.16\n",
      "[Debug] Successfully found layer: model.layers.16\n",
      "[Debug] Trying to access layer: model.layers.17\n",
      "[Debug] Successfully found layer: model.layers.17\n",
      "[Debug] Trying to access layer: model.layers.18\n",
      "[Debug] Successfully found layer: model.layers.18\n",
      "[Debug] Trying to access layer: model.layers.19\n",
      "[Debug] Successfully found layer: model.layers.19\n",
      "[Debug] Trying to access layer: model.layers.20\n",
      "[Debug] Successfully found layer: model.layers.20\n",
      "[Debug] Trying to access layer: model.layers.21\n",
      "[Debug] Successfully found layer: model.layers.21\n",
      "[Debug] Trying to access layer: model.layers.22\n",
      "[Debug] Successfully found layer: model.layers.22\n",
      "[Debug] Trying to access layer: model.layers.23\n",
      "[Debug] Successfully found layer: model.layers.23\n",
      "[Debug] Trying to access layer: model.layers.24\n",
      "[Debug] Successfully found layer: model.layers.24\n",
      "[Debug] Trying to access layer: model.layers.25\n",
      "[Debug] Successfully found layer: model.layers.25\n",
      "[Debug] Trying to access layer: model.layers.26\n",
      "[Debug] Successfully found layer: model.layers.26\n",
      "[Debug] Trying to access layer: model.layers.27\n",
      "[Debug] Successfully found layer: model.layers.27\n",
      "[Debug] Trying to access layer: model.layers.28\n",
      "[Debug] Successfully found layer: model.layers.28\n",
      "[Debug] Trying to access layer: model.layers.29\n",
      "[Debug] Successfully found layer: model.layers.29\n",
      "[Debug] Trying to access layer: model.layers.30\n",
      "[Debug] Successfully found layer: model.layers.30\n",
      "[Debug] Trying to access layer: model.layers.31\n",
      "[Debug] Successfully found layer: model.layers.31\n",
      "[Debug] Trying to access layer: model.embed_tokens\n",
      "[Debug] Successfully found layer: model.embed_tokens\n",
      "[Hook] Layer Embedding(128256, 4096) received input (tensor([[128000,     54,   1786,   8121,  39640,   5788,    374,    922,    220,\n",
      "             17,   3610,  52021,    824,   1060,     11,    315,    902,    220,\n",
      "           1399,      4,   5900,   1139,  55425,     13,  47400,  41095,   2211,\n",
      "            220,     18,      4,    315,    279,   3728,  66638,   3157,     11,\n",
      "            719,   1202,    907,    374,   5190,  56612,    311,  23069,    323,\n",
      "           1023,  29882,    315,    279,   3769,   8032,     16,     60,   8494,\n",
      "            374,    264,   6522,  17276,    315,  39640,    902,    374,  10213,\n",
      "            505,   8930,   3394,  33012,    719,    706,   1027,  93534,    291,\n",
      "            555,   5734,    304,   3878,    315,   2860,   4785,   8032,    966,\n",
      "             60,   1561,  17340,    320,    679,     21,      8,    374,    279,\n",
      "           4948,  68067,  17276,    315,  39640,     11,    323,    279,   7928,\n",
      "          17276,    315,   5425,  91842,  39640,     13,  11681,   6910,   1778,\n",
      "            439,  25379,     11,  26386,     11,  31941,  84782,     11,    323,\n",
      "          69518,  35283,   8356,   1080,  10642,  49774,     11,    323,  39640,\n",
      "            505,   1521,  33012,    374,   6118,   1511,    369,   3339,  89341,\n",
      "             13]], device='cuda:0'),) and output tensor([[[ 2.6512e-04, -4.9973e-04, -5.8365e-04,  ...,  3.8147e-03,\n",
      "           6.3419e-05,  1.1902e-03],\n",
      "         [-1.2512e-02, -1.0742e-02,  1.1368e-03,  ...,  1.2756e-02,\n",
      "           1.6968e-02, -8.9111e-03],\n",
      "         [-1.0376e-02,  1.0986e-02,  9.3384e-03,  ..., -1.4038e-02,\n",
      "          -2.1729e-02,  4.8523e-03],\n",
      "         ...,\n",
      "         [-3.4637e-03, -5.5847e-03,  1.4114e-03,  ..., -9.9182e-04,\n",
      "           1.0498e-02, -6.9580e-03],\n",
      "         [ 1.4191e-03, -2.1484e-02,  2.0905e-03,  ...,  5.2185e-03,\n",
      "          -6.5918e-03,  1.5259e-02],\n",
      "         [-6.7520e-04, -5.7602e-04,  4.7684e-04,  ...,  3.3264e-03,\n",
      "           3.9101e-04,  4.7493e-04]]], device='cuda:0')\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[ 2.6512e-04, -4.9973e-04, -5.8365e-04,  ...,  3.8147e-03,\n",
      "           6.3419e-05,  1.1902e-03],\n",
      "         [-1.2512e-02, -1.0742e-02,  1.1368e-03,  ...,  1.2756e-02,\n",
      "           1.6968e-02, -8.9111e-03],\n",
      "         [-1.0376e-02,  1.0986e-02,  9.3384e-03,  ..., -1.4038e-02,\n",
      "          -2.1729e-02,  4.8523e-03],\n",
      "         ...,\n",
      "         [-3.4637e-03, -5.5847e-03,  1.4114e-03,  ..., -9.9182e-04,\n",
      "           1.0498e-02, -6.9580e-03],\n",
      "         [ 1.4191e-03, -2.1484e-02,  2.0905e-03,  ...,  5.2185e-03,\n",
      "          -6.5918e-03,  1.5259e-02],\n",
      "         [-6.7520e-04, -5.7602e-04,  4.7684e-04,  ...,  3.3264e-03,\n",
      "           3.9101e-04,  4.7493e-04]]], device='cuda:0'),) and output (tensor([[[-0.0004,  0.0146, -0.0020,  ...,  0.0066, -0.0194,  0.0020],\n",
      "         [-0.0105, -0.0014, -0.0082,  ...,  0.0059,  0.0045, -0.0133],\n",
      "         [-0.0140, -0.0082,  0.0235,  ..., -0.0255, -0.0164,  0.0268],\n",
      "         ...,\n",
      "         [-0.0154, -0.0203,  0.0194,  ...,  0.0070,  0.0200, -0.0101],\n",
      "         [-0.0153, -0.0215,  0.0180,  ...,  0.0042, -0.0259,  0.0205],\n",
      "         [ 0.0059,  0.0004,  0.0095,  ...,  0.0070,  0.0072, -0.0019]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.0004,  0.0146, -0.0020,  ...,  0.0066, -0.0194,  0.0020],\n",
      "         [-0.0105, -0.0014, -0.0082,  ...,  0.0059,  0.0045, -0.0133],\n",
      "         [-0.0140, -0.0082,  0.0235,  ..., -0.0255, -0.0164,  0.0268],\n",
      "         ...,\n",
      "         [-0.0154, -0.0203,  0.0194,  ...,  0.0070,  0.0200, -0.0101],\n",
      "         [-0.0153, -0.0215,  0.0180,  ...,  0.0042, -0.0259,  0.0205],\n",
      "         [ 0.0059,  0.0004,  0.0095,  ...,  0.0070,  0.0072, -0.0019]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.0972,  0.0787, -0.0419,  ...,  0.0071,  0.0869,  0.0218],\n",
      "         [-0.0045,  0.0052, -0.0116,  ..., -0.0109, -0.0203, -0.0088],\n",
      "         [-0.0070,  0.0141,  0.0668,  ..., -0.1118, -0.0558,  0.0353],\n",
      "         ...,\n",
      "         [ 0.0026, -0.0167,  0.0044,  ...,  0.0567,  0.0316, -0.0024],\n",
      "         [-0.0169, -0.0297,  0.0146,  ...,  0.0017, -0.0312,  0.0294],\n",
      "         [ 0.0036,  0.0067, -0.0031,  ...,  0.0047, -0.0056, -0.0008]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.0972,  0.0787, -0.0419,  ...,  0.0071,  0.0869,  0.0218],\n",
      "         [-0.0045,  0.0052, -0.0116,  ..., -0.0109, -0.0203, -0.0088],\n",
      "         [-0.0070,  0.0141,  0.0668,  ..., -0.1118, -0.0558,  0.0353],\n",
      "         ...,\n",
      "         [ 0.0026, -0.0167,  0.0044,  ...,  0.0567,  0.0316, -0.0024],\n",
      "         [-0.0169, -0.0297,  0.0146,  ...,  0.0017, -0.0312,  0.0294],\n",
      "         [ 0.0036,  0.0067, -0.0031,  ...,  0.0047, -0.0056, -0.0008]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.1040,  0.0875, -0.0360,  ...,  0.0205,  0.0997,  0.0184],\n",
      "         [ 0.0052, -0.0227, -0.0164,  ..., -0.0005, -0.0341, -0.0088],\n",
      "         [ 0.0224,  0.0147,  0.0586,  ..., -0.1011, -0.0925,  0.0171],\n",
      "         ...,\n",
      "         [-0.0132, -0.0374,  0.0227,  ...,  0.0623,  0.0716, -0.0113],\n",
      "         [ 0.0105,  0.0134, -0.0017,  ..., -0.0051, -0.0617,  0.0300],\n",
      "         [-0.0073,  0.0040,  0.0067,  ..., -0.0145, -0.0139,  0.0100]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.1040,  0.0875, -0.0360,  ...,  0.0205,  0.0997,  0.0184],\n",
      "         [ 0.0052, -0.0227, -0.0164,  ..., -0.0005, -0.0341, -0.0088],\n",
      "         [ 0.0224,  0.0147,  0.0586,  ..., -0.1011, -0.0925,  0.0171],\n",
      "         ...,\n",
      "         [-0.0132, -0.0374,  0.0227,  ...,  0.0623,  0.0716, -0.0113],\n",
      "         [ 0.0105,  0.0134, -0.0017,  ..., -0.0051, -0.0617,  0.0300],\n",
      "         [-0.0073,  0.0040,  0.0067,  ..., -0.0145, -0.0139,  0.0100]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.0870,  0.1049, -0.0173,  ...,  0.0700,  0.0989,  0.0181],\n",
      "         [-0.0266,  0.0261, -0.0419,  ..., -0.0431, -0.0401, -0.0565],\n",
      "         [-0.0341,  0.0245,  0.0495,  ..., -0.1285, -0.0657, -0.0219],\n",
      "         ...,\n",
      "         [-0.0337, -0.0023,  0.0426,  ..., -0.0376,  0.0409, -0.0160],\n",
      "         [ 0.0475,  0.0281, -0.0234,  ..., -0.0333, -0.0619, -0.0089],\n",
      "         [-0.0273, -0.0051,  0.0103,  ...,  0.0073, -0.0161,  0.0379]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.0870,  0.1049, -0.0173,  ...,  0.0700,  0.0989,  0.0181],\n",
      "         [-0.0266,  0.0261, -0.0419,  ..., -0.0431, -0.0401, -0.0565],\n",
      "         [-0.0341,  0.0245,  0.0495,  ..., -0.1285, -0.0657, -0.0219],\n",
      "         ...,\n",
      "         [-0.0337, -0.0023,  0.0426,  ..., -0.0376,  0.0409, -0.0160],\n",
      "         [ 0.0475,  0.0281, -0.0234,  ..., -0.0333, -0.0619, -0.0089],\n",
      "         [-0.0273, -0.0051,  0.0103,  ...,  0.0073, -0.0161,  0.0379]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.1113,  0.1022, -0.0102,  ...,  0.0436,  0.1283,  0.0321],\n",
      "         [-0.0496,  0.0087, -0.0209,  ..., -0.0092, -0.0417, -0.0247],\n",
      "         [-0.0170,  0.0176,  0.0906,  ..., -0.0628, -0.1022,  0.0739],\n",
      "         ...,\n",
      "         [-0.0264,  0.0004,  0.0065,  ..., -0.0045,  0.0285,  0.0400],\n",
      "         [ 0.0441,  0.0958,  0.0211,  ..., -0.0581, -0.0224, -0.0734],\n",
      "         [-0.0220, -0.0006,  0.0282,  ...,  0.0212, -0.0273, -0.0119]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.1113,  0.1022, -0.0102,  ...,  0.0436,  0.1283,  0.0321],\n",
      "         [-0.0496,  0.0087, -0.0209,  ..., -0.0092, -0.0417, -0.0247],\n",
      "         [-0.0170,  0.0176,  0.0906,  ..., -0.0628, -0.1022,  0.0739],\n",
      "         ...,\n",
      "         [-0.0264,  0.0004,  0.0065,  ..., -0.0045,  0.0285,  0.0400],\n",
      "         [ 0.0441,  0.0958,  0.0211,  ..., -0.0581, -0.0224, -0.0734],\n",
      "         [-0.0220, -0.0006,  0.0282,  ...,  0.0212, -0.0273, -0.0119]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.1034,  0.1132,  0.0096,  ...,  0.0453,  0.1477,  0.0301],\n",
      "         [-0.0309,  0.0115, -0.0282,  ..., -0.0522, -0.0520, -0.0058],\n",
      "         [ 0.0101,  0.0656,  0.0817,  ..., -0.0824, -0.0505,  0.0932],\n",
      "         ...,\n",
      "         [-0.0257,  0.1602, -0.0332,  ..., -0.1062,  0.0342,  0.0346],\n",
      "         [ 0.0791,  0.1707,  0.0265,  ..., -0.0737, -0.0718, -0.0095],\n",
      "         [-0.0089, -0.0066,  0.0205,  ...,  0.0782, -0.0460,  0.0002]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.1034,  0.1132,  0.0096,  ...,  0.0453,  0.1477,  0.0301],\n",
      "         [-0.0309,  0.0115, -0.0282,  ..., -0.0522, -0.0520, -0.0058],\n",
      "         [ 0.0101,  0.0656,  0.0817,  ..., -0.0824, -0.0505,  0.0932],\n",
      "         ...,\n",
      "         [-0.0257,  0.1602, -0.0332,  ..., -0.1062,  0.0342,  0.0346],\n",
      "         [ 0.0791,  0.1707,  0.0265,  ..., -0.0737, -0.0718, -0.0095],\n",
      "         [-0.0089, -0.0066,  0.0205,  ...,  0.0782, -0.0460,  0.0002]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.1071,  0.1253,  0.0211,  ...,  0.0126,  0.1586,  0.0278],\n",
      "         [-0.0167,  0.0318, -0.0399,  ..., -0.0149,  0.0060,  0.0116],\n",
      "         [ 0.0690,  0.0742,  0.0607,  ..., -0.0291, -0.0408,  0.1071],\n",
      "         ...,\n",
      "         [-0.0148,  0.1852, -0.0032,  ..., -0.0449,  0.0443,  0.0345],\n",
      "         [ 0.1079,  0.2149,  0.0059,  ..., -0.0684, -0.0070, -0.0290],\n",
      "         [-0.0065,  0.0166,  0.0177,  ...,  0.0806, -0.0628, -0.0198]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.1071,  0.1253,  0.0211,  ...,  0.0126,  0.1586,  0.0278],\n",
      "         [-0.0167,  0.0318, -0.0399,  ..., -0.0149,  0.0060,  0.0116],\n",
      "         [ 0.0690,  0.0742,  0.0607,  ..., -0.0291, -0.0408,  0.1071],\n",
      "         ...,\n",
      "         [-0.0148,  0.1852, -0.0032,  ..., -0.0449,  0.0443,  0.0345],\n",
      "         [ 0.1079,  0.2149,  0.0059,  ..., -0.0684, -0.0070, -0.0290],\n",
      "         [-0.0065,  0.0166,  0.0177,  ...,  0.0806, -0.0628, -0.0198]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.0945,  0.1848,  0.0036,  ..., -0.0473,  0.1489,  0.0536],\n",
      "         [ 0.0605, -0.0084, -0.0522,  ..., -0.1065, -0.1073, -0.0340],\n",
      "         [ 0.0966,  0.1204,  0.0582,  ..., -0.1483, -0.1229,  0.0593],\n",
      "         ...,\n",
      "         [ 0.0147,  0.1756,  0.0536,  ...,  0.0142,  0.0771,  0.0375],\n",
      "         [ 0.1335,  0.1771,  0.0216,  ..., -0.0220,  0.0362, -0.0241],\n",
      "         [-0.1286, -0.0626, -0.0104,  ...,  0.0701, -0.0886, -0.0599]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.0945,  0.1848,  0.0036,  ..., -0.0473,  0.1489,  0.0536],\n",
      "         [ 0.0605, -0.0084, -0.0522,  ..., -0.1065, -0.1073, -0.0340],\n",
      "         [ 0.0966,  0.1204,  0.0582,  ..., -0.1483, -0.1229,  0.0593],\n",
      "         ...,\n",
      "         [ 0.0147,  0.1756,  0.0536,  ...,  0.0142,  0.0771,  0.0375],\n",
      "         [ 0.1335,  0.1771,  0.0216,  ..., -0.0220,  0.0362, -0.0241],\n",
      "         [-0.1286, -0.0626, -0.0104,  ...,  0.0701, -0.0886, -0.0599]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.1060,  0.2113,  0.0270,  ..., -0.1141,  0.1610,  0.0528],\n",
      "         [ 0.0854,  0.0744, -0.0597,  ..., -0.0837, -0.1492, -0.0563],\n",
      "         [ 0.1022,  0.0711,  0.0552,  ..., -0.1706, -0.0483, -0.0150],\n",
      "         ...,\n",
      "         [ 0.0452,  0.1188,  0.0382,  ..., -0.0093,  0.0379, -0.0370],\n",
      "         [ 0.1525,  0.2076,  0.0544,  ...,  0.0147,  0.0799, -0.0316],\n",
      "         [-0.1421, -0.1194, -0.0805,  ...,  0.0577, -0.0142, -0.0681]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.1060,  0.2113,  0.0270,  ..., -0.1141,  0.1610,  0.0528],\n",
      "         [ 0.0854,  0.0744, -0.0597,  ..., -0.0837, -0.1492, -0.0563],\n",
      "         [ 0.1022,  0.0711,  0.0552,  ..., -0.1706, -0.0483, -0.0150],\n",
      "         ...,\n",
      "         [ 0.0452,  0.1188,  0.0382,  ..., -0.0093,  0.0379, -0.0370],\n",
      "         [ 0.1525,  0.2076,  0.0544,  ...,  0.0147,  0.0799, -0.0316],\n",
      "         [-0.1421, -0.1194, -0.0805,  ...,  0.0577, -0.0142, -0.0681]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.0954,  0.2493,  0.0073,  ..., -0.1568,  0.1818,  0.0855],\n",
      "         [ 0.0049,  0.0583,  0.0005,  ..., -0.0601, -0.0846, -0.0510],\n",
      "         [ 0.0725,  0.0510,  0.0796,  ..., -0.2210, -0.0069,  0.0099],\n",
      "         ...,\n",
      "         [ 0.0188,  0.0889,  0.0337,  ...,  0.0028,  0.0281, -0.0130],\n",
      "         [ 0.0535,  0.1162,  0.0375,  ...,  0.0263,  0.0978,  0.0268],\n",
      "         [-0.1020, -0.0697, -0.1192,  ...,  0.0892,  0.0119, -0.0275]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.0954,  0.2493,  0.0073,  ..., -0.1568,  0.1818,  0.0855],\n",
      "         [ 0.0049,  0.0583,  0.0005,  ..., -0.0601, -0.0846, -0.0510],\n",
      "         [ 0.0725,  0.0510,  0.0796,  ..., -0.2210, -0.0069,  0.0099],\n",
      "         ...,\n",
      "         [ 0.0188,  0.0889,  0.0337,  ...,  0.0028,  0.0281, -0.0130],\n",
      "         [ 0.0535,  0.1162,  0.0375,  ...,  0.0263,  0.0978,  0.0268],\n",
      "         [-0.1020, -0.0697, -0.1192,  ...,  0.0892,  0.0119, -0.0275]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.0152,  0.3340,  0.0011,  ..., -0.2750,  0.1933,  0.1034],\n",
      "         [ 0.0818,  0.0419,  0.0324,  ..., -0.0428, -0.0824, -0.0346],\n",
      "         [ 0.1036,  0.0167,  0.0156,  ..., -0.2514, -0.0019,  0.0271],\n",
      "         ...,\n",
      "         [ 0.0044,  0.1028,  0.0375,  ...,  0.0105, -0.0169, -0.0099],\n",
      "         [ 0.0223,  0.1212,  0.0310,  ..., -0.0772,  0.0858,  0.0537],\n",
      "         [-0.1374, -0.1190, -0.0998,  ...,  0.0416,  0.0804, -0.0248]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.0152,  0.3340,  0.0011,  ..., -0.2750,  0.1933,  0.1034],\n",
      "         [ 0.0818,  0.0419,  0.0324,  ..., -0.0428, -0.0824, -0.0346],\n",
      "         [ 0.1036,  0.0167,  0.0156,  ..., -0.2514, -0.0019,  0.0271],\n",
      "         ...,\n",
      "         [ 0.0044,  0.1028,  0.0375,  ...,  0.0105, -0.0169, -0.0099],\n",
      "         [ 0.0223,  0.1212,  0.0310,  ..., -0.0772,  0.0858,  0.0537],\n",
      "         [-0.1374, -0.1190, -0.0998,  ...,  0.0416,  0.0804, -0.0248]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.0356,  0.3483, -0.0366,  ..., -0.2962,  0.2088,  0.1065],\n",
      "         [ 0.0814,  0.0383,  0.0423,  ..., -0.0507, -0.0606, -0.0323],\n",
      "         [ 0.0868, -0.0028,  0.0590,  ..., -0.2638,  0.0832,  0.0347],\n",
      "         ...,\n",
      "         [ 0.0627,  0.1122, -0.1004,  ...,  0.0159,  0.0918, -0.0035],\n",
      "         [ 0.0763,  0.1190, -0.0486,  ..., -0.1266,  0.2038,  0.0860],\n",
      "         [-0.1019, -0.0324, -0.0944,  ..., -0.0775,  0.0104, -0.1132]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.0356,  0.3483, -0.0366,  ..., -0.2962,  0.2088,  0.1065],\n",
      "         [ 0.0814,  0.0383,  0.0423,  ..., -0.0507, -0.0606, -0.0323],\n",
      "         [ 0.0868, -0.0028,  0.0590,  ..., -0.2638,  0.0832,  0.0347],\n",
      "         ...,\n",
      "         [ 0.0627,  0.1122, -0.1004,  ...,  0.0159,  0.0918, -0.0035],\n",
      "         [ 0.0763,  0.1190, -0.0486,  ..., -0.1266,  0.2038,  0.0860],\n",
      "         [-0.1019, -0.0324, -0.0944,  ..., -0.0775,  0.0104, -0.1132]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.0127,  0.3661, -0.0316,  ..., -0.3678,  0.1973,  0.1019],\n",
      "         [ 0.0669,  0.0091,  0.0319,  ..., -0.0852, -0.1201,  0.0245],\n",
      "         [ 0.1192, -0.0712,  0.0702,  ..., -0.2236,  0.0096,  0.0190],\n",
      "         ...,\n",
      "         [ 0.0450,  0.0475, -0.0310,  ...,  0.0799,  0.0357, -0.0652],\n",
      "         [ 0.0141,  0.0861,  0.0784,  ..., -0.0663,  0.0616,  0.0184],\n",
      "         [-0.0151, -0.0820,  0.0183,  ...,  0.0205, -0.0052, -0.0838]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.0127,  0.3661, -0.0316,  ..., -0.3678,  0.1973,  0.1019],\n",
      "         [ 0.0669,  0.0091,  0.0319,  ..., -0.0852, -0.1201,  0.0245],\n",
      "         [ 0.1192, -0.0712,  0.0702,  ..., -0.2236,  0.0096,  0.0190],\n",
      "         ...,\n",
      "         [ 0.0450,  0.0475, -0.0310,  ...,  0.0799,  0.0357, -0.0652],\n",
      "         [ 0.0141,  0.0861,  0.0784,  ..., -0.0663,  0.0616,  0.0184],\n",
      "         [-0.0151, -0.0820,  0.0183,  ...,  0.0205, -0.0052, -0.0838]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.0089,  0.3233, -0.0779,  ..., -0.4491,  0.1845,  0.0951],\n",
      "         [-0.0829, -0.0099,  0.0305,  ..., -0.1210, -0.1020, -0.0282],\n",
      "         [ 0.0648,  0.0282, -0.0499,  ..., -0.2115,  0.0516,  0.0742],\n",
      "         ...,\n",
      "         [-0.0397,  0.0794,  0.0046,  ...,  0.1480, -0.0136, -0.0829],\n",
      "         [ 0.0708,  0.0608,  0.1685,  ..., -0.0819, -0.0791, -0.0008],\n",
      "         [-0.0810,  0.0036,  0.0558,  ..., -0.0437, -0.0605, -0.0104]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.0089,  0.3233, -0.0779,  ..., -0.4491,  0.1845,  0.0951],\n",
      "         [-0.0829, -0.0099,  0.0305,  ..., -0.1210, -0.1020, -0.0282],\n",
      "         [ 0.0648,  0.0282, -0.0499,  ..., -0.2115,  0.0516,  0.0742],\n",
      "         ...,\n",
      "         [-0.0397,  0.0794,  0.0046,  ...,  0.1480, -0.0136, -0.0829],\n",
      "         [ 0.0708,  0.0608,  0.1685,  ..., -0.0819, -0.0791, -0.0008],\n",
      "         [-0.0810,  0.0036,  0.0558,  ..., -0.0437, -0.0605, -0.0104]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-2.4429e-02,  2.8277e-01, -5.3197e-02,  ..., -4.5921e-01,\n",
      "           2.2998e-01,  1.4184e-02],\n",
      "         [-5.5341e-02, -9.1648e-02, -1.6431e-02,  ..., -1.1585e-01,\n",
      "          -1.0387e-01, -5.0892e-02],\n",
      "         [ 7.3908e-02, -6.1671e-02, -6.6506e-02,  ..., -1.9836e-01,\n",
      "           6.7949e-03,  3.5445e-02],\n",
      "         ...,\n",
      "         [ 3.2474e-02,  1.2468e-01,  3.9879e-02,  ...,  1.2994e-01,\n",
      "          -1.1241e-04, -1.0308e-01],\n",
      "         [ 1.4066e-01,  4.6820e-02,  1.4085e-01,  ..., -8.1765e-02,\n",
      "          -1.0697e-01, -1.0472e-01],\n",
      "         [-1.5355e-01, -2.4951e-02,  6.5466e-02,  ...,  2.2224e-02,\n",
      "           3.1349e-02, -1.7239e-01]]], device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-2.4429e-02,  2.8277e-01, -5.3197e-02,  ..., -4.5921e-01,\n",
      "           2.2998e-01,  1.4184e-02],\n",
      "         [-5.5341e-02, -9.1648e-02, -1.6431e-02,  ..., -1.1585e-01,\n",
      "          -1.0387e-01, -5.0892e-02],\n",
      "         [ 7.3908e-02, -6.1671e-02, -6.6506e-02,  ..., -1.9836e-01,\n",
      "           6.7949e-03,  3.5445e-02],\n",
      "         ...,\n",
      "         [ 3.2474e-02,  1.2468e-01,  3.9879e-02,  ...,  1.2994e-01,\n",
      "          -1.1241e-04, -1.0308e-01],\n",
      "         [ 1.4066e-01,  4.6820e-02,  1.4085e-01,  ..., -8.1765e-02,\n",
      "          -1.0697e-01, -1.0472e-01],\n",
      "         [-1.5355e-01, -2.4951e-02,  6.5466e-02,  ...,  2.2224e-02,\n",
      "           3.1349e-02, -1.7239e-01]]], device='cuda:0'),) and output (tensor([[[-0.0928,  0.1485,  0.0158,  ..., -0.5380,  0.2664, -0.0333],\n",
      "         [-0.1615, -0.2392,  0.0406,  ..., -0.1395, -0.0072, -0.0479],\n",
      "         [-0.0290, -0.0074, -0.1191,  ..., -0.0736,  0.0453, -0.0267],\n",
      "         ...,\n",
      "         [-0.0129, -0.0682,  0.1383,  ...,  0.2196,  0.0655,  0.0655],\n",
      "         [ 0.0492,  0.0245,  0.1366,  ..., -0.0281, -0.0971, -0.0285],\n",
      "         [-0.0482, -0.0223,  0.1322,  ..., -0.0448, -0.0291, -0.0298]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.0928,  0.1485,  0.0158,  ..., -0.5380,  0.2664, -0.0333],\n",
      "         [-0.1615, -0.2392,  0.0406,  ..., -0.1395, -0.0072, -0.0479],\n",
      "         [-0.0290, -0.0074, -0.1191,  ..., -0.0736,  0.0453, -0.0267],\n",
      "         ...,\n",
      "         [-0.0129, -0.0682,  0.1383,  ...,  0.2196,  0.0655,  0.0655],\n",
      "         [ 0.0492,  0.0245,  0.1366,  ..., -0.0281, -0.0971, -0.0285],\n",
      "         [-0.0482, -0.0223,  0.1322,  ..., -0.0448, -0.0291, -0.0298]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.1000,  0.1323,  0.0241,  ..., -0.5470,  0.2844, -0.0457],\n",
      "         [-0.1017, -0.2926,  0.0847,  ..., -0.0941, -0.1163, -0.0301],\n",
      "         [-0.0577, -0.0530, -0.0841,  ..., -0.1085,  0.0513,  0.0466],\n",
      "         ...,\n",
      "         [-0.0217, -0.0600,  0.1166,  ...,  0.2708,  0.1520,  0.1218],\n",
      "         [ 0.1211, -0.0607,  0.1656,  ...,  0.0592, -0.1133, -0.0817],\n",
      "         [-0.0238, -0.0557,  0.2081,  ...,  0.0608, -0.1040, -0.0111]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.1000,  0.1323,  0.0241,  ..., -0.5470,  0.2844, -0.0457],\n",
      "         [-0.1017, -0.2926,  0.0847,  ..., -0.0941, -0.1163, -0.0301],\n",
      "         [-0.0577, -0.0530, -0.0841,  ..., -0.1085,  0.0513,  0.0466],\n",
      "         ...,\n",
      "         [-0.0217, -0.0600,  0.1166,  ...,  0.2708,  0.1520,  0.1218],\n",
      "         [ 0.1211, -0.0607,  0.1656,  ...,  0.0592, -0.1133, -0.0817],\n",
      "         [-0.0238, -0.0557,  0.2081,  ...,  0.0608, -0.1040, -0.0111]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.1286,  0.0833,  0.0239,  ..., -0.5657,  0.3694, -0.0725],\n",
      "         [-0.1188, -0.3334,  0.2561,  ..., -0.1954, -0.0765,  0.0094],\n",
      "         [-0.0827, -0.0580, -0.0853,  ..., -0.0750,  0.0740,  0.1053],\n",
      "         ...,\n",
      "         [-0.1045, -0.2167,  0.0654,  ...,  0.3138,  0.0517, -0.0101],\n",
      "         [ 0.1041, -0.1389,  0.1474,  ...,  0.1710, -0.0296, -0.0401],\n",
      "         [ 0.0543, -0.0766,  0.1228,  ...,  0.0348, -0.2640, -0.0590]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.1286,  0.0833,  0.0239,  ..., -0.5657,  0.3694, -0.0725],\n",
      "         [-0.1188, -0.3334,  0.2561,  ..., -0.1954, -0.0765,  0.0094],\n",
      "         [-0.0827, -0.0580, -0.0853,  ..., -0.0750,  0.0740,  0.1053],\n",
      "         ...,\n",
      "         [-0.1045, -0.2167,  0.0654,  ...,  0.3138,  0.0517, -0.0101],\n",
      "         [ 0.1041, -0.1389,  0.1474,  ...,  0.1710, -0.0296, -0.0401],\n",
      "         [ 0.0543, -0.0766,  0.1228,  ...,  0.0348, -0.2640, -0.0590]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.1417,  0.0703,  0.0343,  ..., -0.5549,  0.3175, -0.0847],\n",
      "         [-0.1234, -0.4250,  0.2229,  ..., -0.1995, -0.0655, -0.0549],\n",
      "         [-0.0946, -0.0069, -0.0875,  ..., -0.0705,  0.1200,  0.0703],\n",
      "         ...,\n",
      "         [-0.1855, -0.2375,  0.2193,  ...,  0.2145,  0.1983,  0.0510],\n",
      "         [ 0.0135, -0.0763,  0.1542,  ...,  0.1435,  0.0263, -0.0667],\n",
      "         [ 0.0306, -0.0741,  0.1743,  ...,  0.0322, -0.2137, -0.0966]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.1417,  0.0703,  0.0343,  ..., -0.5549,  0.3175, -0.0847],\n",
      "         [-0.1234, -0.4250,  0.2229,  ..., -0.1995, -0.0655, -0.0549],\n",
      "         [-0.0946, -0.0069, -0.0875,  ..., -0.0705,  0.1200,  0.0703],\n",
      "         ...,\n",
      "         [-0.1855, -0.2375,  0.2193,  ...,  0.2145,  0.1983,  0.0510],\n",
      "         [ 0.0135, -0.0763,  0.1542,  ...,  0.1435,  0.0263, -0.0667],\n",
      "         [ 0.0306, -0.0741,  0.1743,  ...,  0.0322, -0.2137, -0.0966]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.1528,  0.0725,  0.0609,  ..., -0.5624,  0.3103, -0.0293],\n",
      "         [-0.0763, -0.4019,  0.1633,  ..., -0.2206, -0.0092, -0.0690],\n",
      "         [-0.1130, -0.1476, -0.0184,  ..., -0.0565,  0.0743,  0.2395],\n",
      "         ...,\n",
      "         [-0.0756, -0.2165,  0.0681,  ...,  0.1552,  0.3019,  0.1254],\n",
      "         [ 0.0068, -0.1454,  0.1474,  ...,  0.0319, -0.0269, -0.0531],\n",
      "         [ 0.0584, -0.1044,  0.1300,  ..., -0.1017, -0.1228, -0.1279]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.1528,  0.0725,  0.0609,  ..., -0.5624,  0.3103, -0.0293],\n",
      "         [-0.0763, -0.4019,  0.1633,  ..., -0.2206, -0.0092, -0.0690],\n",
      "         [-0.1130, -0.1476, -0.0184,  ..., -0.0565,  0.0743,  0.2395],\n",
      "         ...,\n",
      "         [-0.0756, -0.2165,  0.0681,  ...,  0.1552,  0.3019,  0.1254],\n",
      "         [ 0.0068, -0.1454,  0.1474,  ...,  0.0319, -0.0269, -0.0531],\n",
      "         [ 0.0584, -0.1044,  0.1300,  ..., -0.1017, -0.1228, -0.1279]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.1559,  0.0520,  0.0566,  ..., -0.5805,  0.2614, -0.0032],\n",
      "         [-0.1503, -0.5701,  0.1419,  ..., -0.2073, -0.1247,  0.0939],\n",
      "         [-0.0900, -0.0828,  0.0179,  ..., -0.2033,  0.0206,  0.2833],\n",
      "         ...,\n",
      "         [-0.1669, -0.1540,  0.1086,  ...,  0.1143,  0.2745,  0.0011],\n",
      "         [ 0.0132, -0.1224,  0.0764,  ...,  0.0466, -0.0741, -0.0940],\n",
      "         [ 0.1941, -0.1765,  0.0512,  ..., -0.1170, -0.0940, -0.0958]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.1559,  0.0520,  0.0566,  ..., -0.5805,  0.2614, -0.0032],\n",
      "         [-0.1503, -0.5701,  0.1419,  ..., -0.2073, -0.1247,  0.0939],\n",
      "         [-0.0900, -0.0828,  0.0179,  ..., -0.2033,  0.0206,  0.2833],\n",
      "         ...,\n",
      "         [-0.1669, -0.1540,  0.1086,  ...,  0.1143,  0.2745,  0.0011],\n",
      "         [ 0.0132, -0.1224,  0.0764,  ...,  0.0466, -0.0741, -0.0940],\n",
      "         [ 0.1941, -0.1765,  0.0512,  ..., -0.1170, -0.0940, -0.0958]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.1488,  0.0445,  0.0765,  ..., -0.5456,  0.2702,  0.0098],\n",
      "         [-0.1163, -0.6801,  0.2604,  ..., -0.2189, -0.0397,  0.0938],\n",
      "         [-0.0232,  0.0058,  0.1746,  ..., -0.2584,  0.1246,  0.2173],\n",
      "         ...,\n",
      "         [-0.1408, -0.0702,  0.1585,  ...,  0.2801,  0.2582, -0.0316],\n",
      "         [ 0.0247,  0.0626,  0.0760,  ..., -0.1505, -0.1777, -0.0303],\n",
      "         [ 0.1443, -0.0873, -0.0352,  ..., -0.1227, -0.1835, -0.0306]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.1488,  0.0445,  0.0765,  ..., -0.5456,  0.2702,  0.0098],\n",
      "         [-0.1163, -0.6801,  0.2604,  ..., -0.2189, -0.0397,  0.0938],\n",
      "         [-0.0232,  0.0058,  0.1746,  ..., -0.2584,  0.1246,  0.2173],\n",
      "         ...,\n",
      "         [-0.1408, -0.0702,  0.1585,  ...,  0.2801,  0.2582, -0.0316],\n",
      "         [ 0.0247,  0.0626,  0.0760,  ..., -0.1505, -0.1777, -0.0303],\n",
      "         [ 0.1443, -0.0873, -0.0352,  ..., -0.1227, -0.1835, -0.0306]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-1.2677e-01,  3.3335e-02,  9.3478e-02,  ..., -5.3204e-01,\n",
      "           2.7953e-01,  5.9812e-04],\n",
      "         [-7.7811e-03, -7.0562e-01,  2.9289e-01,  ..., -1.9120e-01,\n",
      "           1.0342e-01,  2.7175e-02],\n",
      "         [-6.5063e-02,  1.6068e-01,  2.4491e-01,  ..., -4.4499e-01,\n",
      "           2.0737e-01,  1.9197e-01],\n",
      "         ...,\n",
      "         [-1.8905e-01, -9.6742e-02,  3.1825e-01,  ..., -5.8172e-02,\n",
      "           4.3082e-01, -1.0280e-01],\n",
      "         [ 5.6838e-02,  6.8129e-02,  1.4097e-01,  ..., -1.7802e-01,\n",
      "          -1.3405e-01, -2.1894e-01],\n",
      "         [ 1.5529e-01, -7.7063e-02, -3.9978e-02,  ..., -1.7392e-01,\n",
      "          -1.0554e-01,  9.4371e-02]]], device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-1.2677e-01,  3.3335e-02,  9.3478e-02,  ..., -5.3204e-01,\n",
      "           2.7953e-01,  5.9812e-04],\n",
      "         [-7.7811e-03, -7.0562e-01,  2.9289e-01,  ..., -1.9120e-01,\n",
      "           1.0342e-01,  2.7175e-02],\n",
      "         [-6.5063e-02,  1.6068e-01,  2.4491e-01,  ..., -4.4499e-01,\n",
      "           2.0737e-01,  1.9197e-01],\n",
      "         ...,\n",
      "         [-1.8905e-01, -9.6742e-02,  3.1825e-01,  ..., -5.8172e-02,\n",
      "           4.3082e-01, -1.0280e-01],\n",
      "         [ 5.6838e-02,  6.8129e-02,  1.4097e-01,  ..., -1.7802e-01,\n",
      "          -1.3405e-01, -2.1894e-01],\n",
      "         [ 1.5529e-01, -7.7063e-02, -3.9978e-02,  ..., -1.7392e-01,\n",
      "          -1.0554e-01,  9.4371e-02]]], device='cuda:0'),) and output (tensor([[[-0.1355,  0.0227,  0.0554,  ..., -0.5473,  0.2470, -0.0229],\n",
      "         [ 0.0029, -0.8183,  0.2647,  ..., -0.3063,  0.0962,  0.0010],\n",
      "         [ 0.0312,  0.1157,  0.3030,  ..., -0.5482,  0.2167,  0.1238],\n",
      "         ...,\n",
      "         [-0.3480,  0.3912,  0.6527,  ..., -0.1325,  0.3523, -0.0192],\n",
      "         [ 0.0804,  0.0839,  0.3387,  ..., -0.1520, -0.1178, -0.2843],\n",
      "         [-0.0401,  0.0787,  0.0873,  ..., -0.0617, -0.1008,  0.0710]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.1355,  0.0227,  0.0554,  ..., -0.5473,  0.2470, -0.0229],\n",
      "         [ 0.0029, -0.8183,  0.2647,  ..., -0.3063,  0.0962,  0.0010],\n",
      "         [ 0.0312,  0.1157,  0.3030,  ..., -0.5482,  0.2167,  0.1238],\n",
      "         ...,\n",
      "         [-0.3480,  0.3912,  0.6527,  ..., -0.1325,  0.3523, -0.0192],\n",
      "         [ 0.0804,  0.0839,  0.3387,  ..., -0.1520, -0.1178, -0.2843],\n",
      "         [-0.0401,  0.0787,  0.0873,  ..., -0.0617, -0.1008,  0.0710]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.1350,  0.0163,  0.0576,  ..., -0.5555,  0.2435,  0.0051],\n",
      "         [-0.0637, -0.8226,  0.2304,  ..., -0.2985,  0.0672, -0.0231],\n",
      "         [ 0.0313,  0.0968,  0.3946,  ..., -0.4000,  0.2097,  0.0737],\n",
      "         ...,\n",
      "         [-0.0620,  0.6573,  0.7394,  ..., -0.0955,  0.4751, -0.0284],\n",
      "         [ 0.2062,  0.0589,  0.3851,  ..., -0.0359, -0.0717, -0.1819],\n",
      "         [ 0.0622, -0.0392,  0.2068,  ...,  0.0384,  0.0319,  0.1855]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.1350,  0.0163,  0.0576,  ..., -0.5555,  0.2435,  0.0051],\n",
      "         [-0.0637, -0.8226,  0.2304,  ..., -0.2985,  0.0672, -0.0231],\n",
      "         [ 0.0313,  0.0968,  0.3946,  ..., -0.4000,  0.2097,  0.0737],\n",
      "         ...,\n",
      "         [-0.0620,  0.6573,  0.7394,  ..., -0.0955,  0.4751, -0.0284],\n",
      "         [ 0.2062,  0.0589,  0.3851,  ..., -0.0359, -0.0717, -0.1819],\n",
      "         [ 0.0622, -0.0392,  0.2068,  ...,  0.0384,  0.0319,  0.1855]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.1301, -0.0029,  0.0279,  ..., -0.5537,  0.2277,  0.0164],\n",
      "         [-0.1233, -0.9101,  0.1513,  ..., -0.3609,  0.0778, -0.0096],\n",
      "         [ 0.0019, -0.0020,  0.3299,  ..., -0.3823,  0.2339, -0.0465],\n",
      "         ...,\n",
      "         [-0.2814,  0.7530,  0.7833,  ..., -0.2555,  0.5365, -0.0412],\n",
      "         [ 0.1841,  0.1119,  0.3807,  ...,  0.0193, -0.1321, -0.2032],\n",
      "         [-0.0889, -0.0146,  0.2762,  ..., -0.0086,  0.0561,  0.2377]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.1301, -0.0029,  0.0279,  ..., -0.5537,  0.2277,  0.0164],\n",
      "         [-0.1233, -0.9101,  0.1513,  ..., -0.3609,  0.0778, -0.0096],\n",
      "         [ 0.0019, -0.0020,  0.3299,  ..., -0.3823,  0.2339, -0.0465],\n",
      "         ...,\n",
      "         [-0.2814,  0.7530,  0.7833,  ..., -0.2555,  0.5365, -0.0412],\n",
      "         [ 0.1841,  0.1119,  0.3807,  ...,  0.0193, -0.1321, -0.2032],\n",
      "         [-0.0889, -0.0146,  0.2762,  ..., -0.0086,  0.0561,  0.2377]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.1219,  0.0061, -0.0021,  ..., -0.5922,  0.2788, -0.0020],\n",
      "         [-0.1832, -0.8292,  0.1650,  ..., -0.4010,  0.0482, -0.0414],\n",
      "         [ 0.2166,  0.0146,  0.1507,  ..., -0.1609,  0.4041,  0.0591],\n",
      "         ...,\n",
      "         [-0.0146,  0.7829,  0.8553,  ...,  0.1404,  0.7837, -0.0809],\n",
      "         [ 0.1827,  0.2283,  0.4077,  ...,  0.2403,  0.0672, -0.1371],\n",
      "         [ 0.0112, -0.2589,  0.2920,  ...,  0.1147,  0.0440,  0.3660]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.1219,  0.0061, -0.0021,  ..., -0.5922,  0.2788, -0.0020],\n",
      "         [-0.1832, -0.8292,  0.1650,  ..., -0.4010,  0.0482, -0.0414],\n",
      "         [ 0.2166,  0.0146,  0.1507,  ..., -0.1609,  0.4041,  0.0591],\n",
      "         ...,\n",
      "         [-0.0146,  0.7829,  0.8553,  ...,  0.1404,  0.7837, -0.0809],\n",
      "         [ 0.1827,  0.2283,  0.4077,  ...,  0.2403,  0.0672, -0.1371],\n",
      "         [ 0.0112, -0.2589,  0.2920,  ...,  0.1147,  0.0440,  0.3660]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.1067,  0.0272,  0.0495,  ..., -0.5550,  0.3449,  0.0266],\n",
      "         [-0.0491, -1.1620,  0.2403,  ..., -0.3126, -0.0967, -0.0974],\n",
      "         [ 0.2135,  0.2067,  0.2756,  ..., -0.1777,  0.3103,  0.1613],\n",
      "         ...,\n",
      "         [-0.0857,  0.9622,  0.9535,  ..., -0.1508,  0.4804, -0.1841],\n",
      "         [ 0.0658,  0.0982,  0.6347,  ...,  0.0219,  0.0130, -0.0564],\n",
      "         [-0.0073, -0.1647,  0.3921,  ...,  0.1471, -0.1221,  0.3201]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.1067,  0.0272,  0.0495,  ..., -0.5550,  0.3449,  0.0266],\n",
      "         [-0.0491, -1.1620,  0.2403,  ..., -0.3126, -0.0967, -0.0974],\n",
      "         [ 0.2135,  0.2067,  0.2756,  ..., -0.1777,  0.3103,  0.1613],\n",
      "         ...,\n",
      "         [-0.0857,  0.9622,  0.9535,  ..., -0.1508,  0.4804, -0.1841],\n",
      "         [ 0.0658,  0.0982,  0.6347,  ...,  0.0219,  0.0130, -0.0564],\n",
      "         [-0.0073, -0.1647,  0.3921,  ...,  0.1471, -0.1221,  0.3201]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.0592,  0.0731,  0.0500,  ..., -0.5517,  0.3541,  0.0595],\n",
      "         [-0.1335, -1.2266,  0.4808,  ..., -0.1718, -0.1913,  0.0303],\n",
      "         [ 0.2230, -0.2289,  0.4650,  ..., -0.1563,  0.3545,  0.4243],\n",
      "         ...,\n",
      "         [-0.1294,  0.8480,  1.1045,  ...,  0.1628,  0.6028,  0.0022],\n",
      "         [-0.1226, -0.2594,  0.8233,  ...,  0.0133,  0.0195, -0.1155],\n",
      "         [-0.0718, -0.4502,  0.5965,  ...,  0.1184,  0.1135,  0.4866]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.0592,  0.0731,  0.0500,  ..., -0.5517,  0.3541,  0.0595],\n",
      "         [-0.1335, -1.2266,  0.4808,  ..., -0.1718, -0.1913,  0.0303],\n",
      "         [ 0.2230, -0.2289,  0.4650,  ..., -0.1563,  0.3545,  0.4243],\n",
      "         ...,\n",
      "         [-0.1294,  0.8480,  1.1045,  ...,  0.1628,  0.6028,  0.0022],\n",
      "         [-0.1226, -0.2594,  0.8233,  ...,  0.0133,  0.0195, -0.1155],\n",
      "         [-0.0718, -0.4502,  0.5965,  ...,  0.1184,  0.1135,  0.4866]]],\n",
      "       device='cuda:0'),) and output (tensor([[[ 0.1143,  0.2237,  0.3292,  ..., -0.7706,  0.2608,  0.0269],\n",
      "         [-0.2453, -1.5157,  0.3421,  ..., -0.0691, -0.1729, -0.0540],\n",
      "         [ 0.2796, -0.3881,  0.7109,  ..., -0.1956,  0.3004,  0.4284],\n",
      "         ...,\n",
      "         [-0.0151,  0.6436,  1.3582,  ..., -0.0540,  0.7306,  0.4299],\n",
      "         [-0.0123, -0.5337,  0.6488,  ..., -0.1577, -0.2570,  0.2873],\n",
      "         [-0.1082, -0.9062,  0.5413,  ...,  0.3393, -0.1345,  0.6476]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[ 0.1143,  0.2237,  0.3292,  ..., -0.7706,  0.2608,  0.0269],\n",
      "         [-0.2453, -1.5157,  0.3421,  ..., -0.0691, -0.1729, -0.0540],\n",
      "         [ 0.2796, -0.3881,  0.7109,  ..., -0.1956,  0.3004,  0.4284],\n",
      "         ...,\n",
      "         [-0.0151,  0.6436,  1.3582,  ..., -0.0540,  0.7306,  0.4299],\n",
      "         [-0.0123, -0.5337,  0.6488,  ..., -0.1577, -0.2570,  0.2873],\n",
      "         [-0.1082, -0.9062,  0.5413,  ...,  0.3393, -0.1345,  0.6476]]],\n",
      "       device='cuda:0'),) and output (tensor([[[ 0.3711,  0.5489,  0.6100,  ..., -0.7499,  0.2080,  0.4195],\n",
      "         [-0.1534, -2.2961, -0.2115,  ..., -0.3765, -0.3558, -0.7319],\n",
      "         [ 0.4829, -0.5924,  0.7886,  ..., -0.1706,  0.4092, -0.1872],\n",
      "         ...,\n",
      "         [ 0.1047,  0.5371,  1.4386,  ...,  0.0086,  0.7273,  0.0509],\n",
      "         [ 0.2117, -0.4669,  0.7005,  ..., -0.2010, -0.1452, -0.5100],\n",
      "         [-0.2832, -1.2662,  0.2475,  ...,  0.0334, -0.4700,  0.1404]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[ 0.3711,  0.5489,  0.6100,  ..., -0.7499,  0.2080,  0.4195],\n",
      "         [-0.1534, -2.2961, -0.2115,  ..., -0.3765, -0.3558, -0.7319],\n",
      "         [ 0.4829, -0.5924,  0.7886,  ..., -0.1706,  0.4092, -0.1872],\n",
      "         ...,\n",
      "         [ 0.1047,  0.5371,  1.4386,  ...,  0.0086,  0.7273,  0.0509],\n",
      "         [ 0.2117, -0.4669,  0.7005,  ..., -0.2010, -0.1452, -0.5100],\n",
      "         [-0.2832, -1.2662,  0.2475,  ...,  0.0334, -0.4700,  0.1404]]],\n",
      "       device='cuda:0'),) and output (tensor([[[ 1.1544,  3.2965,  1.6534,  ..., -2.7009,  2.6553,  2.2700],\n",
      "         [ 0.3225, -1.2413,  0.0815,  ...,  0.1815, -0.4879, -0.7572],\n",
      "         [ 0.7332, -0.1877,  1.0599,  ..., -0.1442,  0.2849,  0.3483],\n",
      "         ...,\n",
      "         [ 0.4194,  1.0704,  1.5619,  ..., -0.0972,  1.3121,  0.8980],\n",
      "         [-0.2332,  0.1677,  1.4843,  ...,  0.7988, -0.8416,  0.5772],\n",
      "         [-0.5309, -1.8571,  0.3252,  ...,  0.8687, -1.7440,  0.8068]]],\n",
      "       device='cuda:0'),)\n",
      "[Debug] Layer names being passed: ['model.layers.0', 'model.layers.1', 'model.layers.2', 'model.layers.3', 'model.layers.4', 'model.layers.5', 'model.layers.6', 'model.layers.7', 'model.layers.8', 'model.layers.9', 'model.layers.10', 'model.layers.11', 'model.layers.12', 'model.layers.13', 'model.layers.14', 'model.layers.15', 'model.layers.16', 'model.layers.17', 'model.layers.18', 'model.layers.19', 'model.layers.20', 'model.layers.21', 'model.layers.22', 'model.layers.23', 'model.layers.24', 'model.layers.25', 'model.layers.26', 'model.layers.27', 'model.layers.28', 'model.layers.29', 'model.layers.30', 'model.layers.31', 'model.embed_tokens']\n",
      "[Debug] Trying to access layer: model.layers.0\n",
      "[Debug] Successfully found layer: model.layers.0\n",
      "[Debug] Trying to access layer: model.layers.1\n",
      "[Debug] Successfully found layer: model.layers.1\n",
      "[Debug] Trying to access layer: model.layers.2\n",
      "[Debug] Successfully found layer: model.layers.2\n",
      "[Debug] Trying to access layer: model.layers.3\n",
      "[Debug] Successfully found layer: model.layers.3\n",
      "[Debug] Trying to access layer: model.layers.4\n",
      "[Debug] Successfully found layer: model.layers.4\n",
      "[Debug] Trying to access layer: model.layers.5\n",
      "[Debug] Successfully found layer: model.layers.5\n",
      "[Debug] Trying to access layer: model.layers.6\n",
      "[Debug] Successfully found layer: model.layers.6\n",
      "[Debug] Trying to access layer: model.layers.7\n",
      "[Debug] Successfully found layer: model.layers.7\n",
      "[Debug] Trying to access layer: model.layers.8\n",
      "[Debug] Successfully found layer: model.layers.8\n",
      "[Debug] Trying to access layer: model.layers.9\n",
      "[Debug] Successfully found layer: model.layers.9\n",
      "[Debug] Trying to access layer: model.layers.10\n",
      "[Debug] Successfully found layer: model.layers.10\n",
      "[Debug] Trying to access layer: model.layers.11\n",
      "[Debug] Successfully found layer: model.layers.11\n",
      "[Debug] Trying to access layer: model.layers.12\n",
      "[Debug] Successfully found layer: model.layers.12\n",
      "[Debug] Trying to access layer: model.layers.13\n",
      "[Debug] Successfully found layer: model.layers.13\n",
      "[Debug] Trying to access layer: model.layers.14\n",
      "[Debug] Successfully found layer: model.layers.14\n",
      "[Debug] Trying to access layer: model.layers.15\n",
      "[Debug] Successfully found layer: model.layers.15\n",
      "[Debug] Trying to access layer: model.layers.16\n",
      "[Debug] Successfully found layer: model.layers.16\n",
      "[Debug] Trying to access layer: model.layers.17\n",
      "[Debug] Successfully found layer: model.layers.17\n",
      "[Debug] Trying to access layer: model.layers.18\n",
      "[Debug] Successfully found layer: model.layers.18\n",
      "[Debug] Trying to access layer: model.layers.19\n",
      "[Debug] Successfully found layer: model.layers.19\n",
      "[Debug] Trying to access layer: model.layers.20\n",
      "[Debug] Successfully found layer: model.layers.20\n",
      "[Debug] Trying to access layer: model.layers.21\n",
      "[Debug] Successfully found layer: model.layers.21\n",
      "[Debug] Trying to access layer: model.layers.22\n",
      "[Debug] Successfully found layer: model.layers.22\n",
      "[Debug] Trying to access layer: model.layers.23\n",
      "[Debug] Successfully found layer: model.layers.23\n",
      "[Debug] Trying to access layer: model.layers.24\n",
      "[Debug] Successfully found layer: model.layers.24\n",
      "[Debug] Trying to access layer: model.layers.25\n",
      "[Debug] Successfully found layer: model.layers.25\n",
      "[Debug] Trying to access layer: model.layers.26\n",
      "[Debug] Successfully found layer: model.layers.26\n",
      "[Debug] Trying to access layer: model.layers.27\n",
      "[Debug] Successfully found layer: model.layers.27\n",
      "[Debug] Trying to access layer: model.layers.28\n",
      "[Debug] Successfully found layer: model.layers.28\n",
      "[Debug] Trying to access layer: model.layers.29\n",
      "[Debug] Successfully found layer: model.layers.29\n",
      "[Debug] Trying to access layer: model.layers.30\n",
      "[Debug] Successfully found layer: model.layers.30\n",
      "[Debug] Trying to access layer: model.layers.31\n",
      "[Debug] Successfully found layer: model.layers.31\n",
      "[Debug] Trying to access layer: model.embed_tokens\n",
      "[Debug] Successfully found layer: model.embed_tokens\n",
      "[Hook] Layer Embedding(128256, 4096) received input (tensor([[128000,   2149,  17309,     25,    578,   8155,  58418,  28366,     25,\n",
      "            578,   8155,  58418,    374,    459,   3778,   8903,  14994,  12707,\n",
      "           4101,    389,    279,  39193,  13740,     11,   5131,    304,   1202,\n",
      "            220,     22,    339,   3280,    315,  13195,     13,    578,   1501,\n",
      "           9477,    279,  11838,  38988,   9211,   3070,     11,  49446,    315,\n",
      "          30791,  20550,    323,   1708,  79451,  83407,     11,    816,   1130,\n",
      "            323,  42893,  38988,   9211,     11,    520,    872,   5105,  73437,\n",
      "            220,    806,   8931,   4994,    315,  66805,   8032,     16,     60,\n",
      "           3296,   5496,   2085,  44288,    477,   6617,  24494,     11,    279,\n",
      "          39562,  41011,    311,   5258,    380,    555,  33489,     11,  23330,\n",
      "            323,  20646,    369,    279,   1317,  86082,   8032,     17,     60,\n",
      "            578,  38988,   9211,   3070,    527,  29658,    315,    279,  23597,\n",
      "          84304,  17706,     16,   1483,     18,     60,    889,    706,   9922,\n",
      "            389,    279,   1501,   8032,     19,     60]], device='cuda:0'),) and output tensor([[[ 2.6512e-04, -4.9973e-04, -5.8365e-04,  ...,  3.8147e-03,\n",
      "           6.3419e-05,  1.1902e-03],\n",
      "         [ 2.4567e-03,  1.0254e-02, -1.2695e-02,  ...,  5.7068e-03,\n",
      "           2.1362e-03,  1.1963e-02],\n",
      "         [-1.7822e-02, -1.3245e-02,  8.3008e-03,  ..., -3.9062e-03,\n",
      "          -6.5994e-04,  1.5945e-03],\n",
      "         ...,\n",
      "         [ 2.1667e-03,  7.7515e-03,  1.2817e-02,  ...,  9.7656e-03,\n",
      "          -8.9111e-03, -1.9836e-03],\n",
      "         [ 3.5706e-03,  3.3569e-03,  6.8970e-03,  ...,  1.3550e-02,\n",
      "          -4.5471e-03, -4.9744e-03],\n",
      "         [ 3.2616e-04,  7.5531e-04,  3.9368e-03,  ...,  1.0132e-02,\n",
      "           6.0425e-03, -1.3542e-04]]], device='cuda:0')\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[ 2.6512e-04, -4.9973e-04, -5.8365e-04,  ...,  3.8147e-03,\n",
      "           6.3419e-05,  1.1902e-03],\n",
      "         [ 2.4567e-03,  1.0254e-02, -1.2695e-02,  ...,  5.7068e-03,\n",
      "           2.1362e-03,  1.1963e-02],\n",
      "         [-1.7822e-02, -1.3245e-02,  8.3008e-03,  ..., -3.9062e-03,\n",
      "          -6.5994e-04,  1.5945e-03],\n",
      "         ...,\n",
      "         [ 2.1667e-03,  7.7515e-03,  1.2817e-02,  ...,  9.7656e-03,\n",
      "          -8.9111e-03, -1.9836e-03],\n",
      "         [ 3.5706e-03,  3.3569e-03,  6.8970e-03,  ...,  1.3550e-02,\n",
      "          -4.5471e-03, -4.9744e-03],\n",
      "         [ 3.2616e-04,  7.5531e-04,  3.9368e-03,  ...,  1.0132e-02,\n",
      "           6.0425e-03, -1.3542e-04]]], device='cuda:0'),) and output (tensor([[[-4.3565e-04,  1.4559e-02, -2.0488e-03,  ...,  6.5782e-03,\n",
      "          -1.9362e-02,  1.9588e-03],\n",
      "         [ 9.6060e-03,  2.1791e-02, -2.3862e-02,  ..., -1.2838e-02,\n",
      "          -5.7576e-03,  4.7264e-03],\n",
      "         [-2.6526e-02, -9.4509e-03, -4.6778e-04,  ..., -2.8090e-02,\n",
      "          -1.4115e-02, -2.6099e-02],\n",
      "         ...,\n",
      "         [-1.3158e-02, -4.3530e-06,  1.0289e-02,  ...,  3.2024e-02,\n",
      "           7.6566e-03,  2.1468e-04],\n",
      "         [-7.6317e-03, -1.9603e-02, -3.7159e-03,  ...,  2.3010e-02,\n",
      "          -3.7031e-03,  1.1783e-02],\n",
      "         [-1.0925e-02,  5.3335e-03, -6.3019e-03,  ...,  4.5307e-03,\n",
      "          -1.9888e-03,  1.1081e-02]]], device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-4.3565e-04,  1.4559e-02, -2.0488e-03,  ...,  6.5782e-03,\n",
      "          -1.9362e-02,  1.9588e-03],\n",
      "         [ 9.6060e-03,  2.1791e-02, -2.3862e-02,  ..., -1.2838e-02,\n",
      "          -5.7576e-03,  4.7264e-03],\n",
      "         [-2.6526e-02, -9.4509e-03, -4.6778e-04,  ..., -2.8090e-02,\n",
      "          -1.4115e-02, -2.6099e-02],\n",
      "         ...,\n",
      "         [-1.3158e-02, -4.3530e-06,  1.0289e-02,  ...,  3.2024e-02,\n",
      "           7.6566e-03,  2.1468e-04],\n",
      "         [-7.6317e-03, -1.9603e-02, -3.7159e-03,  ...,  2.3010e-02,\n",
      "          -3.7031e-03,  1.1783e-02],\n",
      "         [-1.0925e-02,  5.3335e-03, -6.3019e-03,  ...,  4.5307e-03,\n",
      "          -1.9888e-03,  1.1081e-02]]], device='cuda:0'),) and output (tensor([[[-0.0972,  0.0787, -0.0419,  ...,  0.0071,  0.0869,  0.0218],\n",
      "         [ 0.0012,  0.0117, -0.0237,  ..., -0.0227, -0.0257, -0.0034],\n",
      "         [-0.0072, -0.0118,  0.0319,  ..., -0.0331, -0.0326, -0.0442],\n",
      "         ...,\n",
      "         [-0.0154,  0.0120, -0.0136,  ...,  0.0318, -0.0125, -0.0072],\n",
      "         [-0.0160, -0.0153, -0.0196,  ...,  0.0264, -0.0254,  0.0165],\n",
      "         [-0.0099, -0.0131, -0.0110,  ...,  0.0154, -0.0147,  0.0127]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.0972,  0.0787, -0.0419,  ...,  0.0071,  0.0869,  0.0218],\n",
      "         [ 0.0012,  0.0117, -0.0237,  ..., -0.0227, -0.0257, -0.0034],\n",
      "         [-0.0072, -0.0118,  0.0319,  ..., -0.0331, -0.0326, -0.0442],\n",
      "         ...,\n",
      "         [-0.0154,  0.0120, -0.0136,  ...,  0.0318, -0.0125, -0.0072],\n",
      "         [-0.0160, -0.0153, -0.0196,  ...,  0.0264, -0.0254,  0.0165],\n",
      "         [-0.0099, -0.0131, -0.0110,  ...,  0.0154, -0.0147,  0.0127]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.1040,  0.0875, -0.0360,  ...,  0.0205,  0.0997,  0.0184],\n",
      "         [ 0.0304,  0.0068, -0.0194,  ..., -0.0158, -0.0308, -0.0007],\n",
      "         [-0.0124, -0.0045,  0.0370,  ..., -0.0084, -0.0585, -0.0518],\n",
      "         ...,\n",
      "         [-0.0369,  0.0281,  0.0057,  ...,  0.0176, -0.0291,  0.0271],\n",
      "         [-0.0297, -0.0281, -0.0564,  ..., -0.0226, -0.0486,  0.0390],\n",
      "         [-0.0565, -0.0063,  0.0042,  ...,  0.0103, -0.0365,  0.0088]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.1040,  0.0875, -0.0360,  ...,  0.0205,  0.0997,  0.0184],\n",
      "         [ 0.0304,  0.0068, -0.0194,  ..., -0.0158, -0.0308, -0.0007],\n",
      "         [-0.0124, -0.0045,  0.0370,  ..., -0.0084, -0.0585, -0.0518],\n",
      "         ...,\n",
      "         [-0.0369,  0.0281,  0.0057,  ...,  0.0176, -0.0291,  0.0271],\n",
      "         [-0.0297, -0.0281, -0.0564,  ..., -0.0226, -0.0486,  0.0390],\n",
      "         [-0.0565, -0.0063,  0.0042,  ...,  0.0103, -0.0365,  0.0088]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.0870,  0.1049, -0.0173,  ...,  0.0700,  0.0989,  0.0181],\n",
      "         [ 0.0002,  0.0506, -0.0286,  ..., -0.0473, -0.0033, -0.0467],\n",
      "         [-0.0284,  0.0267,  0.0248,  ..., -0.0153, -0.0257, -0.0896],\n",
      "         ...,\n",
      "         [-0.0266,  0.0392, -0.0058,  ...,  0.0249, -0.0733,  0.0054],\n",
      "         [-0.0384, -0.0753, -0.0476,  ..., -0.0276, -0.0510,  0.0502],\n",
      "         [-0.0686,  0.0033, -0.0418,  ...,  0.0219, -0.0870,  0.0318]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.0870,  0.1049, -0.0173,  ...,  0.0700,  0.0989,  0.0181],\n",
      "         [ 0.0002,  0.0506, -0.0286,  ..., -0.0473, -0.0033, -0.0467],\n",
      "         [-0.0284,  0.0267,  0.0248,  ..., -0.0153, -0.0257, -0.0896],\n",
      "         ...,\n",
      "         [-0.0266,  0.0392, -0.0058,  ...,  0.0249, -0.0733,  0.0054],\n",
      "         [-0.0384, -0.0753, -0.0476,  ..., -0.0276, -0.0510,  0.0502],\n",
      "         [-0.0686,  0.0033, -0.0418,  ...,  0.0219, -0.0870,  0.0318]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.1113,  0.1022, -0.0102,  ...,  0.0436,  0.1283,  0.0321],\n",
      "         [-0.0266,  0.0296,  0.0091,  ..., -0.0268, -0.0199, -0.0102],\n",
      "         [ 0.0333, -0.0316,  0.0567,  ...,  0.0314,  0.0040, -0.0338],\n",
      "         ...,\n",
      "         [-0.0046, -0.0115, -0.0216,  ...,  0.0572, -0.0804,  0.0375],\n",
      "         [-0.0080, -0.0125, -0.0042,  ..., -0.0678, -0.0851,  0.1305],\n",
      "         [-0.0724, -0.0116, -0.0320,  ...,  0.0556, -0.0585,  0.0428]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.1113,  0.1022, -0.0102,  ...,  0.0436,  0.1283,  0.0321],\n",
      "         [-0.0266,  0.0296,  0.0091,  ..., -0.0268, -0.0199, -0.0102],\n",
      "         [ 0.0333, -0.0316,  0.0567,  ...,  0.0314,  0.0040, -0.0338],\n",
      "         ...,\n",
      "         [-0.0046, -0.0115, -0.0216,  ...,  0.0572, -0.0804,  0.0375],\n",
      "         [-0.0080, -0.0125, -0.0042,  ..., -0.0678, -0.0851,  0.1305],\n",
      "         [-0.0724, -0.0116, -0.0320,  ...,  0.0556, -0.0585,  0.0428]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.1034,  0.1132,  0.0096,  ...,  0.0453,  0.1477,  0.0301],\n",
      "         [ 0.0013,  0.0242, -0.0012,  ..., -0.0458, -0.0245,  0.0204],\n",
      "         [ 0.0197, -0.0207,  0.0135,  ..., -0.0519, -0.0059,  0.0239],\n",
      "         ...,\n",
      "         [-0.0289,  0.0291,  0.0894,  ...,  0.0404, -0.0195,  0.0552],\n",
      "         [-0.0369, -0.0783,  0.0539,  ..., -0.0091, -0.0660,  0.1991],\n",
      "         [-0.0733, -0.0491, -0.0047,  ...,  0.0813, -0.0569,  0.0421]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.1034,  0.1132,  0.0096,  ...,  0.0453,  0.1477,  0.0301],\n",
      "         [ 0.0013,  0.0242, -0.0012,  ..., -0.0458, -0.0245,  0.0204],\n",
      "         [ 0.0197, -0.0207,  0.0135,  ..., -0.0519, -0.0059,  0.0239],\n",
      "         ...,\n",
      "         [-0.0289,  0.0291,  0.0894,  ...,  0.0404, -0.0195,  0.0552],\n",
      "         [-0.0369, -0.0783,  0.0539,  ..., -0.0091, -0.0660,  0.1991],\n",
      "         [-0.0733, -0.0491, -0.0047,  ...,  0.0813, -0.0569,  0.0421]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.1071,  0.1253,  0.0211,  ...,  0.0126,  0.1586,  0.0278],\n",
      "         [ 0.0103,  0.0415, -0.0178,  ...,  0.0218,  0.0268,  0.0381],\n",
      "         [-0.0268, -0.0012, -0.0918,  ..., -0.1330, -0.0195,  0.0402],\n",
      "         ...,\n",
      "         [-0.1066, -0.0070,  0.0669,  ...,  0.0799, -0.0006,  0.1158],\n",
      "         [-0.0208, -0.1014,  0.0481,  ..., -0.0124, -0.0940,  0.1671],\n",
      "         [-0.1186, -0.0411, -0.0132,  ...,  0.1060, -0.0515,  0.0818]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.1071,  0.1253,  0.0211,  ...,  0.0126,  0.1586,  0.0278],\n",
      "         [ 0.0103,  0.0415, -0.0178,  ...,  0.0218,  0.0268,  0.0381],\n",
      "         [-0.0268, -0.0012, -0.0918,  ..., -0.1330, -0.0195,  0.0402],\n",
      "         ...,\n",
      "         [-0.1066, -0.0070,  0.0669,  ...,  0.0799, -0.0006,  0.1158],\n",
      "         [-0.0208, -0.1014,  0.0481,  ..., -0.0124, -0.0940,  0.1671],\n",
      "         [-0.1186, -0.0411, -0.0132,  ...,  0.1060, -0.0515,  0.0818]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.0945,  0.1848,  0.0036,  ..., -0.0473,  0.1489,  0.0536],\n",
      "         [ 0.0691,  0.0036, -0.0297,  ..., -0.0674, -0.1036, -0.0129],\n",
      "         [-0.0599, -0.0418, -0.1139,  ..., -0.1620, -0.0501,  0.0104],\n",
      "         ...,\n",
      "         [-0.0487, -0.0284,  0.0398,  ...,  0.0952, -0.0293,  0.1615],\n",
      "         [-0.0526, -0.0337,  0.0403,  ...,  0.0930, -0.0931,  0.1834],\n",
      "         [-0.1008, -0.0241, -0.0221,  ...,  0.1294, -0.1431,  0.0784]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.0945,  0.1848,  0.0036,  ..., -0.0473,  0.1489,  0.0536],\n",
      "         [ 0.0691,  0.0036, -0.0297,  ..., -0.0674, -0.1036, -0.0129],\n",
      "         [-0.0599, -0.0418, -0.1139,  ..., -0.1620, -0.0501,  0.0104],\n",
      "         ...,\n",
      "         [-0.0487, -0.0284,  0.0398,  ...,  0.0952, -0.0293,  0.1615],\n",
      "         [-0.0526, -0.0337,  0.0403,  ...,  0.0930, -0.0931,  0.1834],\n",
      "         [-0.1008, -0.0241, -0.0221,  ...,  0.1294, -0.1431,  0.0784]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.1060,  0.2113,  0.0270,  ..., -0.1141,  0.1610,  0.0528],\n",
      "         [ 0.0852,  0.0852, -0.0168,  ..., -0.0277, -0.1314, -0.0322],\n",
      "         [-0.0286, -0.0705, -0.1147,  ..., -0.1546, -0.0657, -0.0735],\n",
      "         ...,\n",
      "         [-0.1663, -0.1181,  0.0312,  ...,  0.0513,  0.0233,  0.1413],\n",
      "         [-0.1593, -0.0986,  0.0606,  ...,  0.1276, -0.0919,  0.1700],\n",
      "         [-0.1066, -0.0963, -0.0304,  ...,  0.0010, -0.1544,  0.0896]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.1060,  0.2113,  0.0270,  ..., -0.1141,  0.1610,  0.0528],\n",
      "         [ 0.0852,  0.0852, -0.0168,  ..., -0.0277, -0.1314, -0.0322],\n",
      "         [-0.0286, -0.0705, -0.1147,  ..., -0.1546, -0.0657, -0.0735],\n",
      "         ...,\n",
      "         [-0.1663, -0.1181,  0.0312,  ...,  0.0513,  0.0233,  0.1413],\n",
      "         [-0.1593, -0.0986,  0.0606,  ...,  0.1276, -0.0919,  0.1700],\n",
      "         [-0.1066, -0.0963, -0.0304,  ...,  0.0010, -0.1544,  0.0896]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.0954,  0.2493,  0.0073,  ..., -0.1568,  0.1818,  0.0855],\n",
      "         [-0.0042,  0.0650,  0.0136,  ..., -0.0168, -0.0782, -0.0481],\n",
      "         [ 0.0203, -0.1875, -0.0974,  ..., -0.1818, -0.0429, -0.1230],\n",
      "         ...,\n",
      "         [-0.0997, -0.0585,  0.0153,  ...,  0.0875,  0.0605,  0.1245],\n",
      "         [-0.1497, -0.0791,  0.0448,  ...,  0.1470,  0.0789,  0.1326],\n",
      "         [-0.0681, -0.0579, -0.0466,  ..., -0.0466, -0.0904,  0.1344]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.0954,  0.2493,  0.0073,  ..., -0.1568,  0.1818,  0.0855],\n",
      "         [-0.0042,  0.0650,  0.0136,  ..., -0.0168, -0.0782, -0.0481],\n",
      "         [ 0.0203, -0.1875, -0.0974,  ..., -0.1818, -0.0429, -0.1230],\n",
      "         ...,\n",
      "         [-0.0997, -0.0585,  0.0153,  ...,  0.0875,  0.0605,  0.1245],\n",
      "         [-0.1497, -0.0791,  0.0448,  ...,  0.1470,  0.0789,  0.1326],\n",
      "         [-0.0681, -0.0579, -0.0466,  ..., -0.0466, -0.0904,  0.1344]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.0152,  0.3340,  0.0011,  ..., -0.2750,  0.1933,  0.1034],\n",
      "         [ 0.0748,  0.0498,  0.0236,  ..., -0.0186, -0.0850, -0.0203],\n",
      "         [ 0.0152, -0.1831, -0.1036,  ..., -0.2173, -0.0826, -0.1018],\n",
      "         ...,\n",
      "         [-0.1198, -0.0862, -0.0133,  ..., -0.0186,  0.1238,  0.1627],\n",
      "         [-0.1646, -0.1341, -0.0164,  ...,  0.0361,  0.0053,  0.1829],\n",
      "         [-0.0861, -0.1059, -0.0024,  ..., -0.0053, -0.1273,  0.2654]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.0152,  0.3340,  0.0011,  ..., -0.2750,  0.1933,  0.1034],\n",
      "         [ 0.0748,  0.0498,  0.0236,  ..., -0.0186, -0.0850, -0.0203],\n",
      "         [ 0.0152, -0.1831, -0.1036,  ..., -0.2173, -0.0826, -0.1018],\n",
      "         ...,\n",
      "         [-0.1198, -0.0862, -0.0133,  ..., -0.0186,  0.1238,  0.1627],\n",
      "         [-0.1646, -0.1341, -0.0164,  ...,  0.0361,  0.0053,  0.1829],\n",
      "         [-0.0861, -0.1059, -0.0024,  ..., -0.0053, -0.1273,  0.2654]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.0356,  0.3483, -0.0366,  ..., -0.2962,  0.2088,  0.1065],\n",
      "         [ 0.0512,  0.0492,  0.0434,  ..., -0.0295, -0.0289, -0.0020],\n",
      "         [ 0.0253, -0.1721, -0.0779,  ..., -0.2210, -0.0346, -0.0586],\n",
      "         ...,\n",
      "         [-0.0850, -0.1029, -0.0255,  ..., -0.1116,  0.1928,  0.1470],\n",
      "         [-0.1826, -0.1393,  0.0013,  ..., -0.1197,  0.1084, -0.0557],\n",
      "         [-0.0740, -0.1091, -0.0201,  ..., -0.0953,  0.0119,  0.1433]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.0356,  0.3483, -0.0366,  ..., -0.2962,  0.2088,  0.1065],\n",
      "         [ 0.0512,  0.0492,  0.0434,  ..., -0.0295, -0.0289, -0.0020],\n",
      "         [ 0.0253, -0.1721, -0.0779,  ..., -0.2210, -0.0346, -0.0586],\n",
      "         ...,\n",
      "         [-0.0850, -0.1029, -0.0255,  ..., -0.1116,  0.1928,  0.1470],\n",
      "         [-0.1826, -0.1393,  0.0013,  ..., -0.1197,  0.1084, -0.0557],\n",
      "         [-0.0740, -0.1091, -0.0201,  ..., -0.0953,  0.0119,  0.1433]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.0127,  0.3661, -0.0316,  ..., -0.3678,  0.1973,  0.1019],\n",
      "         [ 0.0142,  0.0169,  0.0424,  ..., -0.0825, -0.0826,  0.0456],\n",
      "         [ 0.0544, -0.1960, -0.0389,  ..., -0.1819, -0.0065, -0.0739],\n",
      "         ...,\n",
      "         [-0.1111, -0.0744,  0.0651,  ..., -0.0422,  0.0673,  0.0459],\n",
      "         [-0.1852, -0.1823,  0.0131,  ..., -0.0884,  0.0317, -0.0776],\n",
      "         [-0.0999, -0.0347, -0.0017,  ..., -0.0155, -0.0859,  0.0790]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.0127,  0.3661, -0.0316,  ..., -0.3678,  0.1973,  0.1019],\n",
      "         [ 0.0142,  0.0169,  0.0424,  ..., -0.0825, -0.0826,  0.0456],\n",
      "         [ 0.0544, -0.1960, -0.0389,  ..., -0.1819, -0.0065, -0.0739],\n",
      "         ...,\n",
      "         [-0.1111, -0.0744,  0.0651,  ..., -0.0422,  0.0673,  0.0459],\n",
      "         [-0.1852, -0.1823,  0.0131,  ..., -0.0884,  0.0317, -0.0776],\n",
      "         [-0.0999, -0.0347, -0.0017,  ..., -0.0155, -0.0859,  0.0790]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.0089,  0.3233, -0.0779,  ..., -0.4491,  0.1845,  0.0951],\n",
      "         [-0.0841,  0.0100,  0.0382,  ..., -0.0684, -0.1056,  0.0324],\n",
      "         [-0.0088, -0.1545, -0.0888,  ..., -0.1115,  0.0427, -0.0712],\n",
      "         ...,\n",
      "         [-0.2091, -0.1683,  0.0431,  ...,  0.0183,  0.0053,  0.1014],\n",
      "         [-0.3158, -0.0707,  0.0345,  ..., -0.1290, -0.0323,  0.0269],\n",
      "         [-0.1039,  0.0223,  0.0494,  ..., -0.0856, -0.1078,  0.1332]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.0089,  0.3233, -0.0779,  ..., -0.4491,  0.1845,  0.0951],\n",
      "         [-0.0841,  0.0100,  0.0382,  ..., -0.0684, -0.1056,  0.0324],\n",
      "         [-0.0088, -0.1545, -0.0888,  ..., -0.1115,  0.0427, -0.0712],\n",
      "         ...,\n",
      "         [-0.2091, -0.1683,  0.0431,  ...,  0.0183,  0.0053,  0.1014],\n",
      "         [-0.3158, -0.0707,  0.0345,  ..., -0.1290, -0.0323,  0.0269],\n",
      "         [-0.1039,  0.0223,  0.0494,  ..., -0.0856, -0.1078,  0.1332]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.0244,  0.2828, -0.0532,  ..., -0.4592,  0.2300,  0.0142],\n",
      "         [-0.0379, -0.0144,  0.0530,  ..., -0.0479, -0.1224, -0.0643],\n",
      "         [-0.0289, -0.1798, -0.0065,  ..., -0.1549,  0.0589, -0.0165],\n",
      "         ...,\n",
      "         [-0.2006, -0.1458,  0.0731,  ..., -0.0587,  0.0556,  0.0272],\n",
      "         [-0.2779, -0.0294,  0.0479,  ..., -0.1315, -0.0648, -0.1282],\n",
      "         [-0.0263,  0.0344, -0.0794,  ..., -0.0054, -0.2175,  0.0603]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.0244,  0.2828, -0.0532,  ..., -0.4592,  0.2300,  0.0142],\n",
      "         [-0.0379, -0.0144,  0.0530,  ..., -0.0479, -0.1224, -0.0643],\n",
      "         [-0.0289, -0.1798, -0.0065,  ..., -0.1549,  0.0589, -0.0165],\n",
      "         ...,\n",
      "         [-0.2006, -0.1458,  0.0731,  ..., -0.0587,  0.0556,  0.0272],\n",
      "         [-0.2779, -0.0294,  0.0479,  ..., -0.1315, -0.0648, -0.1282],\n",
      "         [-0.0263,  0.0344, -0.0794,  ..., -0.0054, -0.2175,  0.0603]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.0928,  0.1485,  0.0158,  ..., -0.5380,  0.2664, -0.0333],\n",
      "         [-0.1107, -0.0978,  0.0040,  ..., -0.0543, -0.0907, -0.1168],\n",
      "         [ 0.0043, -0.1457,  0.0238,  ..., -0.2032,  0.0231,  0.0247],\n",
      "         ...,\n",
      "         [-0.1348, -0.1424,  0.0234,  ..., -0.0076, -0.0138,  0.0371],\n",
      "         [-0.2161, -0.0980,  0.0974,  ..., -0.2378, -0.1543, -0.2166],\n",
      "         [ 0.0832, -0.0607, -0.1535,  ..., -0.0148, -0.2705,  0.1375]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.0928,  0.1485,  0.0158,  ..., -0.5380,  0.2664, -0.0333],\n",
      "         [-0.1107, -0.0978,  0.0040,  ..., -0.0543, -0.0907, -0.1168],\n",
      "         [ 0.0043, -0.1457,  0.0238,  ..., -0.2032,  0.0231,  0.0247],\n",
      "         ...,\n",
      "         [-0.1348, -0.1424,  0.0234,  ..., -0.0076, -0.0138,  0.0371],\n",
      "         [-0.2161, -0.0980,  0.0974,  ..., -0.2378, -0.1543, -0.2166],\n",
      "         [ 0.0832, -0.0607, -0.1535,  ..., -0.0148, -0.2705,  0.1375]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.1000,  0.1323,  0.0241,  ..., -0.5470,  0.2844, -0.0457],\n",
      "         [ 0.0921, -0.0759,  0.0336,  ..., -0.0162, -0.2380, -0.0634],\n",
      "         [-0.0455, -0.1427,  0.0711,  ..., -0.1920, -0.1404,  0.0677],\n",
      "         ...,\n",
      "         [-0.0810, -0.1118, -0.1159,  ..., -0.0550, -0.0034,  0.2817],\n",
      "         [-0.2055, -0.0876,  0.0335,  ..., -0.2463, -0.0955, -0.1416],\n",
      "         [ 0.0887,  0.0711, -0.1342,  ...,  0.0543, -0.3723,  0.1100]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.1000,  0.1323,  0.0241,  ..., -0.5470,  0.2844, -0.0457],\n",
      "         [ 0.0921, -0.0759,  0.0336,  ..., -0.0162, -0.2380, -0.0634],\n",
      "         [-0.0455, -0.1427,  0.0711,  ..., -0.1920, -0.1404,  0.0677],\n",
      "         ...,\n",
      "         [-0.0810, -0.1118, -0.1159,  ..., -0.0550, -0.0034,  0.2817],\n",
      "         [-0.2055, -0.0876,  0.0335,  ..., -0.2463, -0.0955, -0.1416],\n",
      "         [ 0.0887,  0.0711, -0.1342,  ...,  0.0543, -0.3723,  0.1100]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.1286,  0.0833,  0.0239,  ..., -0.5657,  0.3694, -0.0725],\n",
      "         [ 0.1912, -0.0257,  0.1215,  ..., -0.0530, -0.2901, -0.0216],\n",
      "         [ 0.0042, -0.1482,  0.1018,  ..., -0.3273, -0.1808,  0.0305],\n",
      "         ...,\n",
      "         [ 0.0611, -0.1089, -0.0720,  ..., -0.1710, -0.0510,  0.2586],\n",
      "         [-0.1306, -0.1711, -0.0203,  ..., -0.2584, -0.2871, -0.1922],\n",
      "         [ 0.0620, -0.0155, -0.0818,  ...,  0.0581, -0.5653,  0.0163]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.1286,  0.0833,  0.0239,  ..., -0.5657,  0.3694, -0.0725],\n",
      "         [ 0.1912, -0.0257,  0.1215,  ..., -0.0530, -0.2901, -0.0216],\n",
      "         [ 0.0042, -0.1482,  0.1018,  ..., -0.3273, -0.1808,  0.0305],\n",
      "         ...,\n",
      "         [ 0.0611, -0.1089, -0.0720,  ..., -0.1710, -0.0510,  0.2586],\n",
      "         [-0.1306, -0.1711, -0.0203,  ..., -0.2584, -0.2871, -0.1922],\n",
      "         [ 0.0620, -0.0155, -0.0818,  ...,  0.0581, -0.5653,  0.0163]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-1.4171e-01,  7.0286e-02,  3.4280e-02,  ..., -5.5491e-01,\n",
      "           3.1749e-01, -8.4654e-02],\n",
      "         [ 9.0403e-02, -1.1492e-01,  8.6984e-02,  ...,  5.4988e-02,\n",
      "          -2.1391e-01, -2.2289e-01],\n",
      "         [ 7.8999e-02, -3.5413e-01,  2.2676e-02,  ..., -4.0437e-01,\n",
      "          -1.1825e-01,  9.1876e-03],\n",
      "         ...,\n",
      "         [-1.3268e-03, -1.1186e-01, -1.6210e-01,  ..., -3.6162e-01,\n",
      "          -8.4224e-02,  1.6501e-01],\n",
      "         [-2.5835e-01, -1.7502e-01, -1.3406e-01,  ..., -3.9808e-01,\n",
      "          -3.0417e-01, -1.8217e-01],\n",
      "         [ 5.9465e-03, -2.6727e-02, -3.6284e-02,  ..., -5.5599e-04,\n",
      "          -5.9881e-01, -8.5799e-02]]], device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-1.4171e-01,  7.0286e-02,  3.4280e-02,  ..., -5.5491e-01,\n",
      "           3.1749e-01, -8.4654e-02],\n",
      "         [ 9.0403e-02, -1.1492e-01,  8.6984e-02,  ...,  5.4988e-02,\n",
      "          -2.1391e-01, -2.2289e-01],\n",
      "         [ 7.8999e-02, -3.5413e-01,  2.2676e-02,  ..., -4.0437e-01,\n",
      "          -1.1825e-01,  9.1876e-03],\n",
      "         ...,\n",
      "         [-1.3268e-03, -1.1186e-01, -1.6210e-01,  ..., -3.6162e-01,\n",
      "          -8.4224e-02,  1.6501e-01],\n",
      "         [-2.5835e-01, -1.7502e-01, -1.3406e-01,  ..., -3.9808e-01,\n",
      "          -3.0417e-01, -1.8217e-01],\n",
      "         [ 5.9465e-03, -2.6727e-02, -3.6284e-02,  ..., -5.5599e-04,\n",
      "          -5.9881e-01, -8.5799e-02]]], device='cuda:0'),) and output (tensor([[[-0.1528,  0.0725,  0.0609,  ..., -0.5624,  0.3103, -0.0293],\n",
      "         [ 0.0472, -0.0958,  0.0518,  ...,  0.0159, -0.0735, -0.1731],\n",
      "         [ 0.1321, -0.4644,  0.0808,  ..., -0.5367, -0.1624,  0.0594],\n",
      "         ...,\n",
      "         [-0.0461, -0.1437, -0.1661,  ..., -0.3562, -0.1086,  0.0734],\n",
      "         [-0.2866, -0.2441, -0.1405,  ..., -0.4007, -0.2495, -0.1577],\n",
      "         [-0.0409, -0.0289, -0.0229,  ..., -0.0054, -0.4635,  0.0148]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.1528,  0.0725,  0.0609,  ..., -0.5624,  0.3103, -0.0293],\n",
      "         [ 0.0472, -0.0958,  0.0518,  ...,  0.0159, -0.0735, -0.1731],\n",
      "         [ 0.1321, -0.4644,  0.0808,  ..., -0.5367, -0.1624,  0.0594],\n",
      "         ...,\n",
      "         [-0.0461, -0.1437, -0.1661,  ..., -0.3562, -0.1086,  0.0734],\n",
      "         [-0.2866, -0.2441, -0.1405,  ..., -0.4007, -0.2495, -0.1577],\n",
      "         [-0.0409, -0.0289, -0.0229,  ..., -0.0054, -0.4635,  0.0148]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-1.5591e-01,  5.2011e-02,  5.6647e-02,  ..., -5.8049e-01,\n",
      "           2.6142e-01, -3.2377e-03],\n",
      "         [ 8.0086e-02, -1.2986e-01,  5.4941e-02,  ...,  4.1270e-02,\n",
      "          -6.3362e-02, -3.1112e-02],\n",
      "         [ 2.3400e-01, -4.5655e-01, -3.7198e-02,  ..., -5.1471e-01,\n",
      "          -2.6052e-01,  5.7943e-05],\n",
      "         ...,\n",
      "         [-4.6313e-02, -1.4795e-01, -3.5150e-01,  ..., -5.1206e-02,\n",
      "           3.9087e-02,  1.7922e-01],\n",
      "         [-2.6286e-01, -1.8339e-01, -3.5810e-01,  ..., -2.4701e-01,\n",
      "          -2.2042e-01, -1.6700e-01],\n",
      "         [-9.8698e-03, -1.0006e-01, -1.0551e-01,  ...,  6.5870e-02,\n",
      "          -4.9067e-01,  9.9519e-02]]], device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-1.5591e-01,  5.2011e-02,  5.6647e-02,  ..., -5.8049e-01,\n",
      "           2.6142e-01, -3.2377e-03],\n",
      "         [ 8.0086e-02, -1.2986e-01,  5.4941e-02,  ...,  4.1270e-02,\n",
      "          -6.3362e-02, -3.1112e-02],\n",
      "         [ 2.3400e-01, -4.5655e-01, -3.7198e-02,  ..., -5.1471e-01,\n",
      "          -2.6052e-01,  5.7943e-05],\n",
      "         ...,\n",
      "         [-4.6313e-02, -1.4795e-01, -3.5150e-01,  ..., -5.1206e-02,\n",
      "           3.9087e-02,  1.7922e-01],\n",
      "         [-2.6286e-01, -1.8339e-01, -3.5810e-01,  ..., -2.4701e-01,\n",
      "          -2.2042e-01, -1.6700e-01],\n",
      "         [-9.8698e-03, -1.0006e-01, -1.0551e-01,  ...,  6.5870e-02,\n",
      "          -4.9067e-01,  9.9519e-02]]], device='cuda:0'),) and output (tensor([[[-0.1488,  0.0445,  0.0765,  ..., -0.5456,  0.2702,  0.0098],\n",
      "         [ 0.1841, -0.1772,  0.0807,  ...,  0.0126, -0.1188, -0.0777],\n",
      "         [ 0.1646, -0.4784, -0.0161,  ..., -0.4733, -0.0905,  0.1745],\n",
      "         ...,\n",
      "         [-0.1134, -0.1745, -0.3535,  ..., -0.0341,  0.1097,  0.1208],\n",
      "         [-0.2089, -0.1102, -0.3397,  ..., -0.3086, -0.1374, -0.1083],\n",
      "         [-0.0444,  0.0022, -0.1243,  ...,  0.0414, -0.5175,  0.0558]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.1488,  0.0445,  0.0765,  ..., -0.5456,  0.2702,  0.0098],\n",
      "         [ 0.1841, -0.1772,  0.0807,  ...,  0.0126, -0.1188, -0.0777],\n",
      "         [ 0.1646, -0.4784, -0.0161,  ..., -0.4733, -0.0905,  0.1745],\n",
      "         ...,\n",
      "         [-0.1134, -0.1745, -0.3535,  ..., -0.0341,  0.1097,  0.1208],\n",
      "         [-0.2089, -0.1102, -0.3397,  ..., -0.3086, -0.1374, -0.1083],\n",
      "         [-0.0444,  0.0022, -0.1243,  ...,  0.0414, -0.5175,  0.0558]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-1.2677e-01,  3.3335e-02,  9.3479e-02,  ..., -5.3204e-01,\n",
      "           2.7953e-01,  5.9814e-04],\n",
      "         [ 1.3144e-01, -2.5511e-02,  1.8736e-01,  ...,  4.7536e-02,\n",
      "          -2.3346e-01, -1.4217e-01],\n",
      "         [ 4.3738e-02, -4.0037e-01,  6.5129e-02,  ..., -6.9702e-01,\n",
      "           7.0879e-02,  1.4432e-01],\n",
      "         ...,\n",
      "         [ 8.2823e-02, -1.9821e-01, -4.2381e-01,  ...,  7.4114e-02,\n",
      "           1.1447e-01,  9.6538e-02],\n",
      "         [-1.5909e-01, -8.8059e-02, -3.9285e-01,  ..., -3.5184e-01,\n",
      "          -5.4500e-02, -8.1703e-02],\n",
      "         [ 6.9558e-02,  7.2340e-02,  5.9552e-02,  ...,  2.1793e-01,\n",
      "          -5.1970e-01,  5.8128e-02]]], device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-1.2677e-01,  3.3335e-02,  9.3479e-02,  ..., -5.3204e-01,\n",
      "           2.7953e-01,  5.9814e-04],\n",
      "         [ 1.3144e-01, -2.5511e-02,  1.8736e-01,  ...,  4.7536e-02,\n",
      "          -2.3346e-01, -1.4217e-01],\n",
      "         [ 4.3738e-02, -4.0037e-01,  6.5129e-02,  ..., -6.9702e-01,\n",
      "           7.0879e-02,  1.4432e-01],\n",
      "         ...,\n",
      "         [ 8.2823e-02, -1.9821e-01, -4.2381e-01,  ...,  7.4114e-02,\n",
      "           1.1447e-01,  9.6538e-02],\n",
      "         [-1.5909e-01, -8.8059e-02, -3.9285e-01,  ..., -3.5184e-01,\n",
      "          -5.4500e-02, -8.1703e-02],\n",
      "         [ 6.9558e-02,  7.2340e-02,  5.9552e-02,  ...,  2.1793e-01,\n",
      "          -5.1970e-01,  5.8128e-02]]], device='cuda:0'),) and output (tensor([[[-0.1355,  0.0227,  0.0554,  ..., -0.5473,  0.2470, -0.0229],\n",
      "         [ 0.0074,  0.0447,  0.2460,  ...,  0.1256, -0.2658, -0.2095],\n",
      "         [ 0.1075, -0.5686, -0.1428,  ..., -0.6440,  0.0043,  0.2025],\n",
      "         ...,\n",
      "         [ 0.1478, -0.3076, -0.4426,  ..., -0.0711,  0.0654, -0.0187],\n",
      "         [-0.2054, -0.1269, -0.4131,  ..., -0.4409, -0.0482, -0.1440],\n",
      "         [ 0.0644,  0.0952,  0.0668,  ...,  0.2897, -0.4854,  0.0171]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.1355,  0.0227,  0.0554,  ..., -0.5473,  0.2470, -0.0229],\n",
      "         [ 0.0074,  0.0447,  0.2460,  ...,  0.1256, -0.2658, -0.2095],\n",
      "         [ 0.1075, -0.5686, -0.1428,  ..., -0.6440,  0.0043,  0.2025],\n",
      "         ...,\n",
      "         [ 0.1478, -0.3076, -0.4426,  ..., -0.0711,  0.0654, -0.0187],\n",
      "         [-0.2054, -0.1269, -0.4131,  ..., -0.4409, -0.0482, -0.1440],\n",
      "         [ 0.0644,  0.0952,  0.0668,  ...,  0.2897, -0.4854,  0.0171]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.1350,  0.0163,  0.0576,  ..., -0.5555,  0.2435,  0.0051],\n",
      "         [ 0.0045,  0.1343,  0.3041,  ...,  0.1135, -0.3017, -0.1634],\n",
      "         [ 0.1458, -0.4253, -0.0808,  ..., -0.7591,  0.3062,  0.4114],\n",
      "         ...,\n",
      "         [ 0.0951, -0.4782, -0.4537,  ..., -0.1528,  0.2128,  0.1238],\n",
      "         [-0.1172, -0.0601, -0.3980,  ..., -0.4486, -0.0495, -0.0703],\n",
      "         [ 0.0562, -0.0353,  0.1493,  ...,  0.3865, -0.3299,  0.0815]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.1350,  0.0163,  0.0576,  ..., -0.5555,  0.2435,  0.0051],\n",
      "         [ 0.0045,  0.1343,  0.3041,  ...,  0.1135, -0.3017, -0.1634],\n",
      "         [ 0.1458, -0.4253, -0.0808,  ..., -0.7591,  0.3062,  0.4114],\n",
      "         ...,\n",
      "         [ 0.0951, -0.4782, -0.4537,  ..., -0.1528,  0.2128,  0.1238],\n",
      "         [-0.1172, -0.0601, -0.3980,  ..., -0.4486, -0.0495, -0.0703],\n",
      "         [ 0.0562, -0.0353,  0.1493,  ...,  0.3865, -0.3299,  0.0815]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.1301, -0.0029,  0.0279,  ..., -0.5537,  0.2277,  0.0164],\n",
      "         [ 0.1932,  0.2115,  0.2158,  ...,  0.2230, -0.1352, -0.0926],\n",
      "         [ 0.2112, -0.5057, -0.1779,  ..., -0.5900,  0.4423,  0.5544],\n",
      "         ...,\n",
      "         [ 0.0457, -0.4782, -0.3286,  ..., -0.1290,  0.2331,  0.2513],\n",
      "         [-0.2314, -0.1345, -0.4307,  ..., -0.6998,  0.1091, -0.1396],\n",
      "         [-0.0617,  0.0469,  0.3572,  ...,  0.4796, -0.2734,  0.0827]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.1301, -0.0029,  0.0279,  ..., -0.5537,  0.2277,  0.0164],\n",
      "         [ 0.1932,  0.2115,  0.2158,  ...,  0.2230, -0.1352, -0.0926],\n",
      "         [ 0.2112, -0.5057, -0.1779,  ..., -0.5900,  0.4423,  0.5544],\n",
      "         ...,\n",
      "         [ 0.0457, -0.4782, -0.3286,  ..., -0.1290,  0.2331,  0.2513],\n",
      "         [-0.2314, -0.1345, -0.4307,  ..., -0.6998,  0.1091, -0.1396],\n",
      "         [-0.0617,  0.0469,  0.3572,  ...,  0.4796, -0.2734,  0.0827]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.1219,  0.0061, -0.0021,  ..., -0.5922,  0.2788, -0.0020],\n",
      "         [ 0.1408,  0.4741,  0.1994,  ...,  0.1736, -0.1669, -0.0657],\n",
      "         [ 0.0675, -0.5802, -0.2206,  ..., -0.2797,  0.5405,  0.5461],\n",
      "         ...,\n",
      "         [ 0.1173, -0.3642, -0.2526,  ..., -0.0105,  0.3099,  0.1698],\n",
      "         [-0.2228, -0.2130, -0.3106,  ..., -0.7480,  0.2199, -0.1024],\n",
      "         [-0.0228, -0.1039,  0.5663,  ...,  0.6041, -0.1969,  0.2619]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.1219,  0.0061, -0.0021,  ..., -0.5922,  0.2788, -0.0020],\n",
      "         [ 0.1408,  0.4741,  0.1994,  ...,  0.1736, -0.1669, -0.0657],\n",
      "         [ 0.0675, -0.5802, -0.2206,  ..., -0.2797,  0.5405,  0.5461],\n",
      "         ...,\n",
      "         [ 0.1173, -0.3642, -0.2526,  ..., -0.0105,  0.3099,  0.1698],\n",
      "         [-0.2228, -0.2130, -0.3106,  ..., -0.7480,  0.2199, -0.1024],\n",
      "         [-0.0228, -0.1039,  0.5663,  ...,  0.6041, -0.1969,  0.2619]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.1067,  0.0272,  0.0495,  ..., -0.5550,  0.3449,  0.0266],\n",
      "         [ 0.0907,  0.2616,  0.1038,  ...,  0.3818, -0.3617, -0.1181],\n",
      "         [-0.0048, -0.6436, -0.0299,  ..., -0.4320,  0.5980,  0.5759],\n",
      "         ...,\n",
      "         [-0.0512, -0.0910, -0.2197,  ...,  0.0992,  0.1580,  0.4197],\n",
      "         [-0.3224, -0.0124, -0.0413,  ..., -0.8041,  0.2570,  0.0217],\n",
      "         [ 0.0142, -0.3231,  0.7857,  ...,  0.5767, -0.1332,  0.2598]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.1067,  0.0272,  0.0495,  ..., -0.5550,  0.3449,  0.0266],\n",
      "         [ 0.0907,  0.2616,  0.1038,  ...,  0.3818, -0.3617, -0.1181],\n",
      "         [-0.0048, -0.6436, -0.0299,  ..., -0.4320,  0.5980,  0.5759],\n",
      "         ...,\n",
      "         [-0.0512, -0.0910, -0.2197,  ...,  0.0992,  0.1580,  0.4197],\n",
      "         [-0.3224, -0.0124, -0.0413,  ..., -0.8041,  0.2570,  0.0217],\n",
      "         [ 0.0142, -0.3231,  0.7857,  ...,  0.5767, -0.1332,  0.2598]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.0592,  0.0731,  0.0500,  ..., -0.5517,  0.3541,  0.0595],\n",
      "         [-0.0961,  0.3234, -0.1794,  ...,  0.5932, -0.3765, -0.0416],\n",
      "         [-0.3532, -0.6554, -0.0882,  ..., -0.2317,  0.5868,  0.4198],\n",
      "         ...,\n",
      "         [-0.0709, -0.2652,  0.1107,  ..., -0.0156,  0.1960,  0.3693],\n",
      "         [-0.4671, -0.0681,  0.1268,  ..., -1.0645,  0.3378, -0.1752],\n",
      "         [-0.0310, -0.4432,  1.0558,  ...,  0.3428, -0.1362,  0.2165]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.0592,  0.0731,  0.0500,  ..., -0.5517,  0.3541,  0.0595],\n",
      "         [-0.0961,  0.3234, -0.1794,  ...,  0.5932, -0.3765, -0.0416],\n",
      "         [-0.3532, -0.6554, -0.0882,  ..., -0.2317,  0.5868,  0.4198],\n",
      "         ...,\n",
      "         [-0.0709, -0.2652,  0.1107,  ..., -0.0156,  0.1960,  0.3693],\n",
      "         [-0.4671, -0.0681,  0.1268,  ..., -1.0645,  0.3378, -0.1752],\n",
      "         [-0.0310, -0.4432,  1.0558,  ...,  0.3428, -0.1362,  0.2165]]],\n",
      "       device='cuda:0'),) and output (tensor([[[ 0.1143,  0.2237,  0.3292,  ..., -0.7706,  0.2608,  0.0269],\n",
      "         [-0.1867,  0.5093, -0.0295,  ...,  1.0077, -0.3938, -0.3475],\n",
      "         [-0.1324, -0.9602, -0.0075,  ..., -0.3636,  0.3921,  0.4154],\n",
      "         ...,\n",
      "         [ 0.0149, -0.8346, -0.0687,  ..., -0.2570,  0.1477,  0.6085],\n",
      "         [-0.3451, -0.2973, -0.1625,  ..., -1.0418,  0.1361, -0.1221],\n",
      "         [ 0.2172, -0.6925,  1.1672,  ...,  0.4927, -0.0767,  0.4421]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[ 0.1143,  0.2237,  0.3292,  ..., -0.7706,  0.2608,  0.0269],\n",
      "         [-0.1867,  0.5093, -0.0295,  ...,  1.0077, -0.3938, -0.3475],\n",
      "         [-0.1324, -0.9602, -0.0075,  ..., -0.3636,  0.3921,  0.4154],\n",
      "         ...,\n",
      "         [ 0.0149, -0.8346, -0.0687,  ..., -0.2570,  0.1477,  0.6085],\n",
      "         [-0.3451, -0.2973, -0.1625,  ..., -1.0418,  0.1361, -0.1221],\n",
      "         [ 0.2172, -0.6925,  1.1672,  ...,  0.4927, -0.0767,  0.4421]]],\n",
      "       device='cuda:0'),) and output (tensor([[[ 0.3711,  0.5489,  0.6100,  ..., -0.7499,  0.2080,  0.4195],\n",
      "         [-0.3386, -0.1421,  0.1944,  ...,  1.0779, -0.6320, -0.9413],\n",
      "         [-0.0882, -0.9632,  0.0918,  ..., -0.2650,  0.3702, -0.0403],\n",
      "         ...,\n",
      "         [-0.3798, -1.0941, -0.6596,  ..., -0.1608, -0.4701,  0.3099],\n",
      "         [-0.7914, -0.7042, -0.5507,  ..., -0.9062, -0.1162, -0.7706],\n",
      "         [ 0.1238, -1.1100,  1.2006,  ...,  0.5474, -0.2407, -0.1750]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[ 0.3711,  0.5489,  0.6100,  ..., -0.7499,  0.2080,  0.4195],\n",
      "         [-0.3386, -0.1421,  0.1944,  ...,  1.0779, -0.6320, -0.9413],\n",
      "         [-0.0882, -0.9632,  0.0918,  ..., -0.2650,  0.3702, -0.0403],\n",
      "         ...,\n",
      "         [-0.3798, -1.0941, -0.6596,  ..., -0.1608, -0.4701,  0.3099],\n",
      "         [-0.7914, -0.7042, -0.5507,  ..., -0.9062, -0.1162, -0.7706],\n",
      "         [ 0.1238, -1.1100,  1.2006,  ...,  0.5474, -0.2407, -0.1750]]],\n",
      "       device='cuda:0'),) and output (tensor([[[ 1.1544e+00,  3.2965e+00,  1.6534e+00,  ..., -2.7009e+00,\n",
      "           2.6553e+00,  2.2700e+00],\n",
      "         [-1.6535e+00,  1.1175e+00, -1.8364e-01,  ...,  1.5848e+00,\n",
      "          -1.2503e+00, -4.7381e-01],\n",
      "         [-1.5600e-01, -7.5578e-01, -2.1322e-01,  ..., -9.1811e-01,\n",
      "          -6.2605e-02, -9.9508e-02],\n",
      "         ...,\n",
      "         [-4.5516e-01, -2.0654e+00, -1.2219e-02,  ...,  6.7486e-01,\n",
      "          -8.9359e-01,  8.8563e-01],\n",
      "         [-9.3020e-01, -1.7633e+00, -6.4853e-01,  ..., -2.6816e-01,\n",
      "          -9.8335e-01,  2.3458e-03],\n",
      "         [-1.7930e-02, -1.7753e+00,  1.4759e+00,  ...,  1.6371e+00,\n",
      "          -3.3722e-02,  8.7000e-01]]], device='cuda:0'),)\n",
      "[Debug] Layer names being passed: ['model.layers.0', 'model.layers.1', 'model.layers.2', 'model.layers.3', 'model.layers.4', 'model.layers.5', 'model.layers.6', 'model.layers.7', 'model.layers.8', 'model.layers.9', 'model.layers.10', 'model.layers.11', 'model.layers.12', 'model.layers.13', 'model.layers.14', 'model.layers.15', 'model.layers.16', 'model.layers.17', 'model.layers.18', 'model.layers.19', 'model.layers.20', 'model.layers.21', 'model.layers.22', 'model.layers.23', 'model.layers.24', 'model.layers.25', 'model.layers.26', 'model.layers.27', 'model.layers.28', 'model.layers.29', 'model.layers.30', 'model.layers.31', 'model.embed_tokens']\n",
      "[Debug] Trying to access layer: model.layers.0\n",
      "[Debug] Successfully found layer: model.layers.0\n",
      "[Debug] Trying to access layer: model.layers.1\n",
      "[Debug] Successfully found layer: model.layers.1\n",
      "[Debug] Trying to access layer: model.layers.2\n",
      "[Debug] Successfully found layer: model.layers.2\n",
      "[Debug] Trying to access layer: model.layers.3\n",
      "[Debug] Successfully found layer: model.layers.3\n",
      "[Debug] Trying to access layer: model.layers.4\n",
      "[Debug] Successfully found layer: model.layers.4\n",
      "[Debug] Trying to access layer: model.layers.5\n",
      "[Debug] Successfully found layer: model.layers.5\n",
      "[Debug] Trying to access layer: model.layers.6\n",
      "[Debug] Successfully found layer: model.layers.6\n",
      "[Debug] Trying to access layer: model.layers.7\n",
      "[Debug] Successfully found layer: model.layers.7\n",
      "[Debug] Trying to access layer: model.layers.8\n",
      "[Debug] Successfully found layer: model.layers.8\n",
      "[Debug] Trying to access layer: model.layers.9\n",
      "[Debug] Successfully found layer: model.layers.9\n",
      "[Debug] Trying to access layer: model.layers.10\n",
      "[Debug] Successfully found layer: model.layers.10\n",
      "[Debug] Trying to access layer: model.layers.11\n",
      "[Debug] Successfully found layer: model.layers.11\n",
      "[Debug] Trying to access layer: model.layers.12\n",
      "[Debug] Successfully found layer: model.layers.12\n",
      "[Debug] Trying to access layer: model.layers.13\n",
      "[Debug] Successfully found layer: model.layers.13\n",
      "[Debug] Trying to access layer: model.layers.14\n",
      "[Debug] Successfully found layer: model.layers.14\n",
      "[Debug] Trying to access layer: model.layers.15\n",
      "[Debug] Successfully found layer: model.layers.15\n",
      "[Debug] Trying to access layer: model.layers.16\n",
      "[Debug] Successfully found layer: model.layers.16\n",
      "[Debug] Trying to access layer: model.layers.17\n",
      "[Debug] Successfully found layer: model.layers.17\n",
      "[Debug] Trying to access layer: model.layers.18\n",
      "[Debug] Successfully found layer: model.layers.18\n",
      "[Debug] Trying to access layer: model.layers.19\n",
      "[Debug] Successfully found layer: model.layers.19\n",
      "[Debug] Trying to access layer: model.layers.20\n",
      "[Debug] Successfully found layer: model.layers.20\n",
      "[Debug] Trying to access layer: model.layers.21\n",
      "[Debug] Successfully found layer: model.layers.21\n",
      "[Debug] Trying to access layer: model.layers.22\n",
      "[Debug] Successfully found layer: model.layers.22\n",
      "[Debug] Trying to access layer: model.layers.23\n",
      "[Debug] Successfully found layer: model.layers.23\n",
      "[Debug] Trying to access layer: model.layers.24\n",
      "[Debug] Successfully found layer: model.layers.24\n",
      "[Debug] Trying to access layer: model.layers.25\n",
      "[Debug] Successfully found layer: model.layers.25\n",
      "[Debug] Trying to access layer: model.layers.26\n",
      "[Debug] Successfully found layer: model.layers.26\n",
      "[Debug] Trying to access layer: model.layers.27\n",
      "[Debug] Successfully found layer: model.layers.27\n",
      "[Debug] Trying to access layer: model.layers.28\n",
      "[Debug] Successfully found layer: model.layers.28\n",
      "[Debug] Trying to access layer: model.layers.29\n",
      "[Debug] Successfully found layer: model.layers.29\n",
      "[Debug] Trying to access layer: model.layers.30\n",
      "[Debug] Successfully found layer: model.layers.30\n",
      "[Debug] Trying to access layer: model.layers.31\n",
      "[Debug] Successfully found layer: model.layers.31\n",
      "[Debug] Trying to access layer: model.embed_tokens\n",
      "[Debug] Successfully found layer: model.embed_tokens\n",
      "[Hook] Layer Embedding(128256, 4096) received input (tensor([[128000,   2460,    358,  24133,    320,     32,   6187,    311,  20474,\n",
      "           5609,      8,    578,   4731,   2835,    369,    279,   5609,     11,\n",
      "            902,    574,  42508,    304,   6664,    220,    679,     15,  17706,\n",
      "             19,     60,    574,   6004,    389,   6186,    220,     21,     11,\n",
      "            220,    679,     16,   8032,     20,     60,   1102,   4519,   3782,\n",
      "            437,    315,  12387,   5526,  21562,    323,  32629,     13,    578,\n",
      "           3782,    437,    527,     25,   8529,   9973,   3520,    320,     32,\n",
      "           6187,    311,  20474,    596,   4846,  62740,    705,  37373,  54859,\n",
      "             89,     11,  48208,   4584,   7368,    315,  68253,  16542,     11,\n",
      "            578,  40301,    468,   7596,   2394,   2649,     11,  40224,   2206,\n",
      "            279,  56551,     11,   8388,  25581,    315,  78113,     11,   9538,\n",
      "          32402,  14093,    315,   1666,    358,  45024,    423,   7169,     11,\n",
      "          15347,  12711,     11,  13929,  68450,     11,   6287,  51016,   3816,\n",
      "             11,  75725,   7834,     11,  13678,   1283,   2642,     88,    315,\n",
      "           1183,    344,   2411,     11,  44847,    435,  69837,    315,  50930,\n",
      "            279,  23404,    321,     11,  11519,  97108,    315,    386,     87,\n",
      "          62077,     11,    323,   2638,   4718,  55293,   8032,     20,     60,\n",
      "           9305,  14936,   2663,    279,   2835,    330,  84270,   9250,   3343,\n",
      "             58,     20,     60]], device='cuda:0'),) and output tensor([[[ 2.6512e-04, -4.9973e-04, -5.8365e-04,  ...,  3.8147e-03,\n",
      "           6.3419e-05,  1.1902e-03],\n",
      "         [-3.0899e-04, -3.2654e-03, -8.9111e-03,  ...,  1.6113e-02,\n",
      "           1.6098e-03, -4.8523e-03],\n",
      "         [ 2.7618e-03, -2.8839e-03, -6.7749e-03,  ..., -1.4160e-02,\n",
      "          -1.9684e-03, -7.2021e-03],\n",
      "         ...,\n",
      "         [ 1.6785e-03,  1.3123e-03,  6.0730e-03,  ...,  1.7738e-04,\n",
      "           4.1199e-03, -1.1230e-02],\n",
      "         [ 3.3722e-03,  7.2021e-03,  1.4420e-03,  ...,  1.4038e-02,\n",
      "          -3.0670e-03, -7.4463e-03],\n",
      "         [ 3.2616e-04,  7.5531e-04,  3.9368e-03,  ...,  1.0132e-02,\n",
      "           6.0425e-03, -1.3542e-04]]], device='cuda:0')\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[ 2.6512e-04, -4.9973e-04, -5.8365e-04,  ...,  3.8147e-03,\n",
      "           6.3419e-05,  1.1902e-03],\n",
      "         [-3.0899e-04, -3.2654e-03, -8.9111e-03,  ...,  1.6113e-02,\n",
      "           1.6098e-03, -4.8523e-03],\n",
      "         [ 2.7618e-03, -2.8839e-03, -6.7749e-03,  ..., -1.4160e-02,\n",
      "          -1.9684e-03, -7.2021e-03],\n",
      "         ...,\n",
      "         [ 1.6785e-03,  1.3123e-03,  6.0730e-03,  ...,  1.7738e-04,\n",
      "           4.1199e-03, -1.1230e-02],\n",
      "         [ 3.3722e-03,  7.2021e-03,  1.4420e-03,  ...,  1.4038e-02,\n",
      "          -3.0670e-03, -7.4463e-03],\n",
      "         [ 3.2616e-04,  7.5531e-04,  3.9368e-03,  ...,  1.0132e-02,\n",
      "           6.0425e-03, -1.3542e-04]]], device='cuda:0'),) and output (tensor([[[-0.0004,  0.0146, -0.0020,  ...,  0.0066, -0.0194,  0.0020],\n",
      "         [-0.0061,  0.0112, -0.0210,  ..., -0.0219,  0.0027, -0.0200],\n",
      "         [ 0.0025,  0.0101,  0.0304,  ..., -0.0433,  0.0083, -0.0147],\n",
      "         ...,\n",
      "         [-0.0048, -0.0111,  0.0074,  ...,  0.0048,  0.0102, -0.0013],\n",
      "         [-0.0005, -0.0119,  0.0045,  ...,  0.0149, -0.0106, -0.0026],\n",
      "         [-0.0088,  0.0077, -0.0085,  ...,  0.0050,  0.0017,  0.0086]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.0004,  0.0146, -0.0020,  ...,  0.0066, -0.0194,  0.0020],\n",
      "         [-0.0061,  0.0112, -0.0210,  ..., -0.0219,  0.0027, -0.0200],\n",
      "         [ 0.0025,  0.0101,  0.0304,  ..., -0.0433,  0.0083, -0.0147],\n",
      "         ...,\n",
      "         [-0.0048, -0.0111,  0.0074,  ...,  0.0048,  0.0102, -0.0013],\n",
      "         [-0.0005, -0.0119,  0.0045,  ...,  0.0149, -0.0106, -0.0026],\n",
      "         [-0.0088,  0.0077, -0.0085,  ...,  0.0050,  0.0017,  0.0086]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.0972,  0.0787, -0.0419,  ...,  0.0071,  0.0869,  0.0218],\n",
      "         [-0.0155,  0.0172, -0.0324,  ..., -0.0399, -0.0222, -0.0221],\n",
      "         [ 0.0343,  0.0430,  0.0420,  ..., -0.0849,  0.0212, -0.0035],\n",
      "         ...,\n",
      "         [-0.0109, -0.0065, -0.0135,  ...,  0.0108,  0.0034,  0.0105],\n",
      "         [ 0.0021, -0.0020, -0.0260,  ..., -0.0098, -0.0334,  0.0073],\n",
      "         [-0.0328, -0.0209, -0.0334,  ..., -0.0028, -0.0032,  0.0108]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.0972,  0.0787, -0.0419,  ...,  0.0071,  0.0869,  0.0218],\n",
      "         [-0.0155,  0.0172, -0.0324,  ..., -0.0399, -0.0222, -0.0221],\n",
      "         [ 0.0343,  0.0430,  0.0420,  ..., -0.0849,  0.0212, -0.0035],\n",
      "         ...,\n",
      "         [-0.0109, -0.0065, -0.0135,  ...,  0.0108,  0.0034,  0.0105],\n",
      "         [ 0.0021, -0.0020, -0.0260,  ..., -0.0098, -0.0334,  0.0073],\n",
      "         [-0.0328, -0.0209, -0.0334,  ..., -0.0028, -0.0032,  0.0108]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.1040,  0.0875, -0.0360,  ...,  0.0205,  0.0997,  0.0184],\n",
      "         [-0.0148,  0.0234, -0.0295,  ..., -0.0118, -0.0050, -0.0018],\n",
      "         [ 0.0515,  0.0420,  0.0295,  ..., -0.1506, -0.0311, -0.0264],\n",
      "         ...,\n",
      "         [-0.0097,  0.0125, -0.0004,  ...,  0.0215,  0.0038,  0.0435],\n",
      "         [-0.0033, -0.0331, -0.0426,  ...,  0.0139, -0.0249, -0.0004],\n",
      "         [-0.0571, -0.0140,  0.0004,  ..., -0.0288, -0.0309,  0.0057]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.1040,  0.0875, -0.0360,  ...,  0.0205,  0.0997,  0.0184],\n",
      "         [-0.0148,  0.0234, -0.0295,  ..., -0.0118, -0.0050, -0.0018],\n",
      "         [ 0.0515,  0.0420,  0.0295,  ..., -0.1506, -0.0311, -0.0264],\n",
      "         ...,\n",
      "         [-0.0097,  0.0125, -0.0004,  ...,  0.0215,  0.0038,  0.0435],\n",
      "         [-0.0033, -0.0331, -0.0426,  ...,  0.0139, -0.0249, -0.0004],\n",
      "         [-0.0571, -0.0140,  0.0004,  ..., -0.0288, -0.0309,  0.0057]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-8.6989e-02,  1.0490e-01, -1.7346e-02,  ...,  6.9981e-02,\n",
      "           9.8908e-02,  1.8052e-02],\n",
      "         [-2.2166e-02,  8.2290e-02, -2.2922e-02,  ..., -2.4800e-02,\n",
      "           1.0506e-04, -2.4096e-02],\n",
      "         [ 4.6732e-02,  3.9405e-02,  8.6469e-03,  ..., -1.2245e-01,\n",
      "          -6.1291e-02, -1.8148e-02],\n",
      "         ...,\n",
      "         [-1.7604e-02, -1.0589e-02, -4.2463e-03,  ...,  4.2283e-02,\n",
      "          -4.0875e-02,  4.8140e-02],\n",
      "         [-2.6702e-02, -5.6678e-03, -3.1399e-02,  ..., -1.4055e-02,\n",
      "          -5.5700e-02,  5.2217e-02],\n",
      "         [-9.5430e-02,  5.5035e-03, -4.4827e-02,  ..., -9.3729e-03,\n",
      "          -7.3558e-02,  3.1992e-02]]], device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-8.6989e-02,  1.0490e-01, -1.7346e-02,  ...,  6.9981e-02,\n",
      "           9.8908e-02,  1.8052e-02],\n",
      "         [-2.2166e-02,  8.2290e-02, -2.2922e-02,  ..., -2.4800e-02,\n",
      "           1.0506e-04, -2.4096e-02],\n",
      "         [ 4.6732e-02,  3.9405e-02,  8.6469e-03,  ..., -1.2245e-01,\n",
      "          -6.1291e-02, -1.8148e-02],\n",
      "         ...,\n",
      "         [-1.7604e-02, -1.0589e-02, -4.2463e-03,  ...,  4.2283e-02,\n",
      "          -4.0875e-02,  4.8140e-02],\n",
      "         [-2.6702e-02, -5.6678e-03, -3.1399e-02,  ..., -1.4055e-02,\n",
      "          -5.5700e-02,  5.2217e-02],\n",
      "         [-9.5430e-02,  5.5035e-03, -4.4827e-02,  ..., -9.3729e-03,\n",
      "          -7.3558e-02,  3.1992e-02]]], device='cuda:0'),) and output (tensor([[[-0.1113,  0.1022, -0.0102,  ...,  0.0436,  0.1283,  0.0321],\n",
      "         [-0.0483,  0.0857, -0.0033,  ...,  0.0306, -0.0392,  0.0077],\n",
      "         [ 0.0269,  0.0286,  0.0296,  ..., -0.1817, -0.0252, -0.0089],\n",
      "         ...,\n",
      "         [-0.0119, -0.0523, -0.0709,  ..., -0.0012, -0.1030,  0.1583],\n",
      "         [-0.0158,  0.0426, -0.0141,  ..., -0.0020, -0.0678,  0.1520],\n",
      "         [-0.0732, -0.0311, -0.0313,  ..., -0.0022, -0.0659,  0.0825]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.1113,  0.1022, -0.0102,  ...,  0.0436,  0.1283,  0.0321],\n",
      "         [-0.0483,  0.0857, -0.0033,  ...,  0.0306, -0.0392,  0.0077],\n",
      "         [ 0.0269,  0.0286,  0.0296,  ..., -0.1817, -0.0252, -0.0089],\n",
      "         ...,\n",
      "         [-0.0119, -0.0523, -0.0709,  ..., -0.0012, -0.1030,  0.1583],\n",
      "         [-0.0158,  0.0426, -0.0141,  ..., -0.0020, -0.0678,  0.1520],\n",
      "         [-0.0732, -0.0311, -0.0313,  ..., -0.0022, -0.0659,  0.0825]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-1.0335e-01,  1.1320e-01,  9.5766e-03,  ...,  4.5299e-02,\n",
      "           1.4774e-01,  3.0101e-02],\n",
      "         [-1.5002e-02,  8.0839e-02, -2.6444e-02,  ...,  2.3524e-02,\n",
      "          -1.7776e-02,  1.0267e-02],\n",
      "         [ 8.3544e-02,  6.1926e-02,  1.4688e-04,  ..., -8.1310e-02,\n",
      "           5.9234e-02,  3.0695e-02],\n",
      "         ...,\n",
      "         [-6.4400e-02, -1.1687e-01,  8.2659e-02,  ..., -2.1323e-02,\n",
      "          -3.2510e-02,  2.3900e-01],\n",
      "         [-5.0533e-02, -1.8095e-02,  4.3371e-02,  ...,  1.6497e-02,\n",
      "          -1.0887e-02,  1.6386e-01],\n",
      "         [-5.3561e-02, -5.2412e-02,  9.8605e-03,  ...,  8.6089e-03,\n",
      "          -3.4880e-02,  7.7339e-02]]], device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-1.0335e-01,  1.1320e-01,  9.5766e-03,  ...,  4.5299e-02,\n",
      "           1.4774e-01,  3.0101e-02],\n",
      "         [-1.5002e-02,  8.0839e-02, -2.6444e-02,  ...,  2.3524e-02,\n",
      "          -1.7776e-02,  1.0267e-02],\n",
      "         [ 8.3544e-02,  6.1926e-02,  1.4688e-04,  ..., -8.1310e-02,\n",
      "           5.9234e-02,  3.0695e-02],\n",
      "         ...,\n",
      "         [-6.4400e-02, -1.1687e-01,  8.2659e-02,  ..., -2.1323e-02,\n",
      "          -3.2510e-02,  2.3900e-01],\n",
      "         [-5.0533e-02, -1.8095e-02,  4.3371e-02,  ...,  1.6497e-02,\n",
      "          -1.0887e-02,  1.6386e-01],\n",
      "         [-5.3561e-02, -5.2412e-02,  9.8605e-03,  ...,  8.6089e-03,\n",
      "          -3.4880e-02,  7.7339e-02]]], device='cuda:0'),) and output (tensor([[[-0.1071,  0.1253,  0.0211,  ...,  0.0126,  0.1586,  0.0278],\n",
      "         [-0.0377,  0.1051, -0.0685,  ...,  0.0943, -0.0104, -0.0002],\n",
      "         [ 0.1252,  0.1047,  0.0616,  ..., -0.0572,  0.0259,  0.0589],\n",
      "         ...,\n",
      "         [-0.0415, -0.0933,  0.0864,  ..., -0.0056, -0.0070,  0.1632],\n",
      "         [-0.0405,  0.0295,  0.0318,  ...,  0.0225,  0.0088,  0.1489],\n",
      "         [-0.0740,  0.0236, -0.0171,  ...,  0.0382, -0.0169,  0.0400]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.1071,  0.1253,  0.0211,  ...,  0.0126,  0.1586,  0.0278],\n",
      "         [-0.0377,  0.1051, -0.0685,  ...,  0.0943, -0.0104, -0.0002],\n",
      "         [ 0.1252,  0.1047,  0.0616,  ..., -0.0572,  0.0259,  0.0589],\n",
      "         ...,\n",
      "         [-0.0415, -0.0933,  0.0864,  ..., -0.0056, -0.0070,  0.1632],\n",
      "         [-0.0405,  0.0295,  0.0318,  ...,  0.0225,  0.0088,  0.1489],\n",
      "         [-0.0740,  0.0236, -0.0171,  ...,  0.0382, -0.0169,  0.0400]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.0945,  0.1848,  0.0036,  ..., -0.0473,  0.1489,  0.0536],\n",
      "         [ 0.0660,  0.0743, -0.1188,  ...,  0.0491, -0.1443, -0.0557],\n",
      "         [ 0.1482,  0.0562,  0.0183,  ..., -0.2110,  0.0415,  0.1385],\n",
      "         ...,\n",
      "         [-0.0408, -0.1314,  0.0040,  ...,  0.0090, -0.0527,  0.1366],\n",
      "         [-0.0742,  0.0115, -0.0310,  ...,  0.0632, -0.0227,  0.1545],\n",
      "         [-0.1189,  0.0616, -0.0977,  ...,  0.0904, -0.0921,  0.0283]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.0945,  0.1848,  0.0036,  ..., -0.0473,  0.1489,  0.0536],\n",
      "         [ 0.0660,  0.0743, -0.1188,  ...,  0.0491, -0.1443, -0.0557],\n",
      "         [ 0.1482,  0.0562,  0.0183,  ..., -0.2110,  0.0415,  0.1385],\n",
      "         ...,\n",
      "         [-0.0408, -0.1314,  0.0040,  ...,  0.0090, -0.0527,  0.1366],\n",
      "         [-0.0742,  0.0115, -0.0310,  ...,  0.0632, -0.0227,  0.1545],\n",
      "         [-0.1189,  0.0616, -0.0977,  ...,  0.0904, -0.0921,  0.0283]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.1060,  0.2113,  0.0270,  ..., -0.1141,  0.1610,  0.0528],\n",
      "         [ 0.0820,  0.1214, -0.0919,  ...,  0.0480, -0.1562, -0.0596],\n",
      "         [ 0.1682,  0.1284,  0.0434,  ..., -0.1965,  0.0343,  0.1320],\n",
      "         ...,\n",
      "         [-0.0434, -0.1775, -0.0125,  ...,  0.0085, -0.0350,  0.1067],\n",
      "         [-0.0323, -0.0149, -0.0548,  ...,  0.0809, -0.0567,  0.1159],\n",
      "         [-0.1183, -0.0346, -0.1024,  ...,  0.0668, -0.0377,  0.0777]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.1060,  0.2113,  0.0270,  ..., -0.1141,  0.1610,  0.0528],\n",
      "         [ 0.0820,  0.1214, -0.0919,  ...,  0.0480, -0.1562, -0.0596],\n",
      "         [ 0.1682,  0.1284,  0.0434,  ..., -0.1965,  0.0343,  0.1320],\n",
      "         ...,\n",
      "         [-0.0434, -0.1775, -0.0125,  ...,  0.0085, -0.0350,  0.1067],\n",
      "         [-0.0323, -0.0149, -0.0548,  ...,  0.0809, -0.0567,  0.1159],\n",
      "         [-0.1183, -0.0346, -0.1024,  ...,  0.0668, -0.0377,  0.0777]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.0954,  0.2493,  0.0073,  ..., -0.1568,  0.1818,  0.0855],\n",
      "         [-0.0166,  0.1696, -0.0803,  ...,  0.0492, -0.1020, -0.0464],\n",
      "         [ 0.1049,  0.0972,  0.1246,  ..., -0.1733,  0.0386,  0.0513],\n",
      "         ...,\n",
      "         [-0.0856, -0.1672, -0.0092,  ...,  0.0530,  0.0457,  0.0940],\n",
      "         [-0.0495, -0.1416, -0.0567,  ...,  0.1197,  0.1346,  0.1240],\n",
      "         [-0.0292, -0.0739, -0.0662,  ...,  0.0087,  0.0992,  0.1151]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.0954,  0.2493,  0.0073,  ..., -0.1568,  0.1818,  0.0855],\n",
      "         [-0.0166,  0.1696, -0.0803,  ...,  0.0492, -0.1020, -0.0464],\n",
      "         [ 0.1049,  0.0972,  0.1246,  ..., -0.1733,  0.0386,  0.0513],\n",
      "         ...,\n",
      "         [-0.0856, -0.1672, -0.0092,  ...,  0.0530,  0.0457,  0.0940],\n",
      "         [-0.0495, -0.1416, -0.0567,  ...,  0.1197,  0.1346,  0.1240],\n",
      "         [-0.0292, -0.0739, -0.0662,  ...,  0.0087,  0.0992,  0.1151]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.0152,  0.3340,  0.0011,  ..., -0.2750,  0.1933,  0.1034],\n",
      "         [ 0.0338,  0.1505, -0.0621,  ...,  0.0410, -0.1111, -0.0239],\n",
      "         [ 0.1530,  0.0480,  0.0861,  ..., -0.2050,  0.0318,  0.1073],\n",
      "         ...,\n",
      "         [-0.0748, -0.1323, -0.0547,  ...,  0.0400,  0.0916,  0.0967],\n",
      "         [-0.0013, -0.0862, -0.1408,  ...,  0.1326,  0.1649,  0.1208],\n",
      "         [-0.1282,  0.0015, -0.0273,  ...,  0.0038,  0.0679,  0.1571]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.0152,  0.3340,  0.0011,  ..., -0.2750,  0.1933,  0.1034],\n",
      "         [ 0.0338,  0.1505, -0.0621,  ...,  0.0410, -0.1111, -0.0239],\n",
      "         [ 0.1530,  0.0480,  0.0861,  ..., -0.2050,  0.0318,  0.1073],\n",
      "         ...,\n",
      "         [-0.0748, -0.1323, -0.0547,  ...,  0.0400,  0.0916,  0.0967],\n",
      "         [-0.0013, -0.0862, -0.1408,  ...,  0.1326,  0.1649,  0.1208],\n",
      "         [-0.1282,  0.0015, -0.0273,  ...,  0.0038,  0.0679,  0.1571]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.0356,  0.3483, -0.0366,  ..., -0.2962,  0.2088,  0.1065],\n",
      "         [-0.0185,  0.1540, -0.0372,  ..., -0.0202, -0.0598,  0.0024],\n",
      "         [ 0.1325,  0.0565,  0.0558,  ..., -0.2450,  0.0422,  0.1038],\n",
      "         ...,\n",
      "         [-0.1442, -0.1235, -0.0206,  ..., -0.0207,  0.1040,  0.0657],\n",
      "         [ 0.0423, -0.0276, -0.1027,  ...,  0.0523,  0.1953, -0.0681],\n",
      "         [-0.1483,  0.0426, -0.0967,  ..., -0.0475,  0.1392,  0.1009]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.0356,  0.3483, -0.0366,  ..., -0.2962,  0.2088,  0.1065],\n",
      "         [-0.0185,  0.1540, -0.0372,  ..., -0.0202, -0.0598,  0.0024],\n",
      "         [ 0.1325,  0.0565,  0.0558,  ..., -0.2450,  0.0422,  0.1038],\n",
      "         ...,\n",
      "         [-0.1442, -0.1235, -0.0206,  ..., -0.0207,  0.1040,  0.0657],\n",
      "         [ 0.0423, -0.0276, -0.1027,  ...,  0.0523,  0.1953, -0.0681],\n",
      "         [-0.1483,  0.0426, -0.0967,  ..., -0.0475,  0.1392,  0.1009]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-1.2733e-02,  3.6610e-01, -3.1563e-02,  ..., -3.6781e-01,\n",
      "           1.9730e-01,  1.0189e-01],\n",
      "         [-1.3118e-04,  1.1749e-01, -1.1900e-02,  ..., -4.6647e-02,\n",
      "          -1.2547e-01,  8.2009e-02],\n",
      "         [ 1.0336e-01,  5.7795e-02,  8.9802e-02,  ..., -1.4067e-01,\n",
      "           2.8913e-02,  1.7030e-01],\n",
      "         ...,\n",
      "         [-1.0709e-01, -2.3507e-01,  7.7463e-02,  ...,  6.5417e-02,\n",
      "           5.2966e-02,  3.9840e-02],\n",
      "         [ 1.3784e-02, -1.3454e-01, -1.0745e-01,  ...,  6.6846e-02,\n",
      "           1.9341e-01,  1.3644e-02],\n",
      "         [-1.6853e-01,  1.5926e-02, -1.5110e-02,  ..., -3.2584e-03,\n",
      "           6.0308e-04,  4.3116e-02]]], device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-1.2733e-02,  3.6610e-01, -3.1563e-02,  ..., -3.6781e-01,\n",
      "           1.9730e-01,  1.0189e-01],\n",
      "         [-1.3118e-04,  1.1749e-01, -1.1900e-02,  ..., -4.6647e-02,\n",
      "          -1.2547e-01,  8.2009e-02],\n",
      "         [ 1.0336e-01,  5.7795e-02,  8.9802e-02,  ..., -1.4067e-01,\n",
      "           2.8913e-02,  1.7030e-01],\n",
      "         ...,\n",
      "         [-1.0709e-01, -2.3507e-01,  7.7463e-02,  ...,  6.5417e-02,\n",
      "           5.2966e-02,  3.9840e-02],\n",
      "         [ 1.3784e-02, -1.3454e-01, -1.0745e-01,  ...,  6.6846e-02,\n",
      "           1.9341e-01,  1.3644e-02],\n",
      "         [-1.6853e-01,  1.5926e-02, -1.5110e-02,  ..., -3.2584e-03,\n",
      "           6.0308e-04,  4.3116e-02]]], device='cuda:0'),) and output (tensor([[[-0.0089,  0.3233, -0.0779,  ..., -0.4491,  0.1845,  0.0951],\n",
      "         [-0.1312,  0.1510, -0.0165,  ..., -0.0671, -0.0908,  0.0134],\n",
      "         [ 0.1348,  0.0959,  0.0709,  ..., -0.1372, -0.0158,  0.0858],\n",
      "         ...,\n",
      "         [-0.1220, -0.0680,  0.1163,  ...,  0.0890, -0.0163,  0.0986],\n",
      "         [-0.2234, -0.0680, -0.0125,  ..., -0.0441,  0.1120,  0.1029],\n",
      "         [-0.0353,  0.1807,  0.0661,  ..., -0.0739, -0.0697,  0.1858]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.0089,  0.3233, -0.0779,  ..., -0.4491,  0.1845,  0.0951],\n",
      "         [-0.1312,  0.1510, -0.0165,  ..., -0.0671, -0.0908,  0.0134],\n",
      "         [ 0.1348,  0.0959,  0.0709,  ..., -0.1372, -0.0158,  0.0858],\n",
      "         ...,\n",
      "         [-0.1220, -0.0680,  0.1163,  ...,  0.0890, -0.0163,  0.0986],\n",
      "         [-0.2234, -0.0680, -0.0125,  ..., -0.0441,  0.1120,  0.1029],\n",
      "         [-0.0353,  0.1807,  0.0661,  ..., -0.0739, -0.0697,  0.1858]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.0244,  0.2828, -0.0532,  ..., -0.4592,  0.2300,  0.0142],\n",
      "         [-0.0826,  0.1353, -0.0737,  ..., -0.0199, -0.0982, -0.0692],\n",
      "         [ 0.1558,  0.1010, -0.0678,  ..., -0.1223, -0.0519,  0.1066],\n",
      "         ...,\n",
      "         [-0.0719, -0.1581,  0.1390,  ...,  0.1066,  0.0952, -0.0482],\n",
      "         [-0.1767, -0.0243, -0.0462,  ..., -0.0098,  0.0182,  0.0224],\n",
      "         [ 0.0271,  0.0931, -0.0360,  ...,  0.0120, -0.2891,  0.2963]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.0244,  0.2828, -0.0532,  ..., -0.4592,  0.2300,  0.0142],\n",
      "         [-0.0826,  0.1353, -0.0737,  ..., -0.0199, -0.0982, -0.0692],\n",
      "         [ 0.1558,  0.1010, -0.0678,  ..., -0.1223, -0.0519,  0.1066],\n",
      "         ...,\n",
      "         [-0.0719, -0.1581,  0.1390,  ...,  0.1066,  0.0952, -0.0482],\n",
      "         [-0.1767, -0.0243, -0.0462,  ..., -0.0098,  0.0182,  0.0224],\n",
      "         [ 0.0271,  0.0931, -0.0360,  ...,  0.0120, -0.2891,  0.2963]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.0928,  0.1485,  0.0158,  ..., -0.5380,  0.2664, -0.0333],\n",
      "         [-0.1863,  0.0883, -0.1268,  ..., -0.1469, -0.0454, -0.0199],\n",
      "         [ 0.0813,  0.0909, -0.1323,  ..., -0.1346, -0.0126,  0.1664],\n",
      "         ...,\n",
      "         [ 0.0224, -0.1271,  0.1269,  ...,  0.0637,  0.1315, -0.0857],\n",
      "         [-0.1193,  0.0795, -0.1431,  ..., -0.0576,  0.0103, -0.0784],\n",
      "         [-0.0145,  0.0304, -0.1911,  ..., -0.0385, -0.3622,  0.3512]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.0928,  0.1485,  0.0158,  ..., -0.5380,  0.2664, -0.0333],\n",
      "         [-0.1863,  0.0883, -0.1268,  ..., -0.1469, -0.0454, -0.0199],\n",
      "         [ 0.0813,  0.0909, -0.1323,  ..., -0.1346, -0.0126,  0.1664],\n",
      "         ...,\n",
      "         [ 0.0224, -0.1271,  0.1269,  ...,  0.0637,  0.1315, -0.0857],\n",
      "         [-0.1193,  0.0795, -0.1431,  ..., -0.0576,  0.0103, -0.0784],\n",
      "         [-0.0145,  0.0304, -0.1911,  ..., -0.0385, -0.3622,  0.3512]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.1000,  0.1323,  0.0241,  ..., -0.5470,  0.2844, -0.0457],\n",
      "         [-0.1217,  0.0515, -0.1265,  ..., -0.1571, -0.1241, -0.0980],\n",
      "         [ 0.0623,  0.2259, -0.1403,  ..., -0.0218,  0.0185,  0.0657],\n",
      "         ...,\n",
      "         [ 0.1048, -0.1425, -0.0158,  ...,  0.1985,  0.1005,  0.2739],\n",
      "         [-0.1377,  0.1062, -0.1165,  ...,  0.0358, -0.0291,  0.0341],\n",
      "         [ 0.0090,  0.0842, -0.2053,  ...,  0.0604, -0.5170,  0.3290]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.1000,  0.1323,  0.0241,  ..., -0.5470,  0.2844, -0.0457],\n",
      "         [-0.1217,  0.0515, -0.1265,  ..., -0.1571, -0.1241, -0.0980],\n",
      "         [ 0.0623,  0.2259, -0.1403,  ..., -0.0218,  0.0185,  0.0657],\n",
      "         ...,\n",
      "         [ 0.1048, -0.1425, -0.0158,  ...,  0.1985,  0.1005,  0.2739],\n",
      "         [-0.1377,  0.1062, -0.1165,  ...,  0.0358, -0.0291,  0.0341],\n",
      "         [ 0.0090,  0.0842, -0.2053,  ...,  0.0604, -0.5170,  0.3290]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.1286,  0.0833,  0.0239,  ..., -0.5657,  0.3694, -0.0725],\n",
      "         [-0.1292,  0.0873, -0.0563,  ..., -0.1268, -0.1544, -0.1779],\n",
      "         [ 0.1025,  0.0686, -0.1941,  ..., -0.1868,  0.0054,  0.0163],\n",
      "         ...,\n",
      "         [ 0.3324, -0.0556,  0.0203,  ...,  0.1682, -0.0051,  0.2694],\n",
      "         [-0.0082,  0.1124, -0.1675,  ...,  0.0905, -0.1677, -0.0152],\n",
      "         [ 0.0240,  0.0532, -0.2791,  ...,  0.1722, -0.6733,  0.2434]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.1286,  0.0833,  0.0239,  ..., -0.5657,  0.3694, -0.0725],\n",
      "         [-0.1292,  0.0873, -0.0563,  ..., -0.1268, -0.1544, -0.1779],\n",
      "         [ 0.1025,  0.0686, -0.1941,  ..., -0.1868,  0.0054,  0.0163],\n",
      "         ...,\n",
      "         [ 0.3324, -0.0556,  0.0203,  ...,  0.1682, -0.0051,  0.2694],\n",
      "         [-0.0082,  0.1124, -0.1675,  ...,  0.0905, -0.1677, -0.0152],\n",
      "         [ 0.0240,  0.0532, -0.2791,  ...,  0.1722, -0.6733,  0.2434]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.1417,  0.0703,  0.0343,  ..., -0.5549,  0.3175, -0.0847],\n",
      "         [-0.0828, -0.0828, -0.1118,  ..., -0.1900, -0.0874, -0.2623],\n",
      "         [ 0.2078,  0.0506, -0.2073,  ..., -0.3060,  0.0056, -0.0653],\n",
      "         ...,\n",
      "         [ 0.2517, -0.1443,  0.0412,  ...,  0.0316, -0.0372,  0.2488],\n",
      "         [-0.0598,  0.0994, -0.2327,  ..., -0.1508, -0.1833, -0.0592],\n",
      "         [-0.0971, -0.0507, -0.1797,  ...,  0.1537, -0.6331,  0.1922]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.1417,  0.0703,  0.0343,  ..., -0.5549,  0.3175, -0.0847],\n",
      "         [-0.0828, -0.0828, -0.1118,  ..., -0.1900, -0.0874, -0.2623],\n",
      "         [ 0.2078,  0.0506, -0.2073,  ..., -0.3060,  0.0056, -0.0653],\n",
      "         ...,\n",
      "         [ 0.2517, -0.1443,  0.0412,  ...,  0.0316, -0.0372,  0.2488],\n",
      "         [-0.0598,  0.0994, -0.2327,  ..., -0.1508, -0.1833, -0.0592],\n",
      "         [-0.0971, -0.0507, -0.1797,  ...,  0.1537, -0.6331,  0.1922]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.1528,  0.0725,  0.0609,  ..., -0.5624,  0.3103, -0.0293],\n",
      "         [-0.1293, -0.0242, -0.0559,  ..., -0.1908, -0.1006, -0.4406],\n",
      "         [ 0.2983, -0.0227, -0.2391,  ..., -0.2525,  0.1464,  0.0043],\n",
      "         ...,\n",
      "         [ 0.3189, -0.0808,  0.0932,  ...,  0.0848,  0.0841,  0.2869],\n",
      "         [-0.1351, -0.0041, -0.2219,  ..., -0.2638, -0.0413, -0.0063],\n",
      "         [-0.1094, -0.0713, -0.0498,  ...,  0.1151, -0.6710,  0.1456]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.1528,  0.0725,  0.0609,  ..., -0.5624,  0.3103, -0.0293],\n",
      "         [-0.1293, -0.0242, -0.0559,  ..., -0.1908, -0.1006, -0.4406],\n",
      "         [ 0.2983, -0.0227, -0.2391,  ..., -0.2525,  0.1464,  0.0043],\n",
      "         ...,\n",
      "         [ 0.3189, -0.0808,  0.0932,  ...,  0.0848,  0.0841,  0.2869],\n",
      "         [-0.1351, -0.0041, -0.2219,  ..., -0.2638, -0.0413, -0.0063],\n",
      "         [-0.1094, -0.0713, -0.0498,  ...,  0.1151, -0.6710,  0.1456]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.1559,  0.0520,  0.0566,  ..., -0.5805,  0.2614, -0.0032],\n",
      "         [-0.1044, -0.1407, -0.1359,  ..., -0.0994, -0.1927, -0.5248],\n",
      "         [ 0.1679,  0.0592, -0.2737,  ..., -0.2206,  0.1616, -0.0214],\n",
      "         ...,\n",
      "         [ 0.3178, -0.0789, -0.0298,  ...,  0.1842,  0.1809,  0.3378],\n",
      "         [-0.1169,  0.0818, -0.3495,  ..., -0.2090, -0.0431, -0.0381],\n",
      "         [-0.1187, -0.0705, -0.0515,  ...,  0.0872, -0.6480,  0.0495]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.1559,  0.0520,  0.0566,  ..., -0.5805,  0.2614, -0.0032],\n",
      "         [-0.1044, -0.1407, -0.1359,  ..., -0.0994, -0.1927, -0.5248],\n",
      "         [ 0.1679,  0.0592, -0.2737,  ..., -0.2206,  0.1616, -0.0214],\n",
      "         ...,\n",
      "         [ 0.3178, -0.0789, -0.0298,  ...,  0.1842,  0.1809,  0.3378],\n",
      "         [-0.1169,  0.0818, -0.3495,  ..., -0.2090, -0.0431, -0.0381],\n",
      "         [-0.1187, -0.0705, -0.0515,  ...,  0.0872, -0.6480,  0.0495]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.1488,  0.0445,  0.0765,  ..., -0.5456,  0.2702,  0.0098],\n",
      "         [-0.2070, -0.2138, -0.1460,  ..., -0.0727, -0.1393, -0.6045],\n",
      "         [ 0.0168, -0.0305, -0.2105,  ..., -0.0757,  0.2305, -0.0797],\n",
      "         ...,\n",
      "         [ 0.2782, -0.0488, -0.0478,  ...,  0.1313,  0.1142,  0.3342],\n",
      "         [-0.0978,  0.3310, -0.3139,  ..., -0.3052, -0.1603,  0.0821],\n",
      "         [-0.1175, -0.0166,  0.0136,  ..., -0.0190, -0.7121, -0.0094]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.1488,  0.0445,  0.0765,  ..., -0.5456,  0.2702,  0.0098],\n",
      "         [-0.2070, -0.2138, -0.1460,  ..., -0.0727, -0.1393, -0.6045],\n",
      "         [ 0.0168, -0.0305, -0.2105,  ..., -0.0757,  0.2305, -0.0797],\n",
      "         ...,\n",
      "         [ 0.2782, -0.0488, -0.0478,  ...,  0.1313,  0.1142,  0.3342],\n",
      "         [-0.0978,  0.3310, -0.3139,  ..., -0.3052, -0.1603,  0.0821],\n",
      "         [-0.1175, -0.0166,  0.0136,  ..., -0.0190, -0.7121, -0.0094]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-1.2677e-01,  3.3335e-02,  9.3478e-02,  ..., -5.3204e-01,\n",
      "           2.7953e-01,  5.9818e-04],\n",
      "         [-2.1252e-01, -4.8948e-02, -2.3323e-01,  ..., -1.3021e-01,\n",
      "          -1.1630e-03, -6.7346e-01],\n",
      "         [ 1.4638e-01,  1.4504e-01, -1.4900e-01,  ..., -5.1954e-02,\n",
      "           4.3287e-01, -1.5953e-01],\n",
      "         ...,\n",
      "         [ 5.1910e-01, -1.3300e-01,  5.2748e-02,  ...,  1.6193e-01,\n",
      "           3.3863e-02,  2.2249e-01],\n",
      "         [-5.3129e-02,  3.0797e-01, -3.5161e-01,  ..., -3.4854e-01,\n",
      "          -1.3944e-01,  7.6668e-02],\n",
      "         [-3.4513e-02,  5.9478e-03,  3.6258e-02,  ...,  1.4833e-01,\n",
      "          -7.0115e-01,  1.1343e-01]]], device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-1.2677e-01,  3.3335e-02,  9.3478e-02,  ..., -5.3204e-01,\n",
      "           2.7953e-01,  5.9818e-04],\n",
      "         [-2.1252e-01, -4.8948e-02, -2.3323e-01,  ..., -1.3021e-01,\n",
      "          -1.1630e-03, -6.7346e-01],\n",
      "         [ 1.4638e-01,  1.4504e-01, -1.4900e-01,  ..., -5.1954e-02,\n",
      "           4.3287e-01, -1.5953e-01],\n",
      "         ...,\n",
      "         [ 5.1910e-01, -1.3300e-01,  5.2748e-02,  ...,  1.6193e-01,\n",
      "           3.3863e-02,  2.2249e-01],\n",
      "         [-5.3129e-02,  3.0797e-01, -3.5161e-01,  ..., -3.4854e-01,\n",
      "          -1.3944e-01,  7.6668e-02],\n",
      "         [-3.4513e-02,  5.9478e-03,  3.6258e-02,  ...,  1.4833e-01,\n",
      "          -7.0115e-01,  1.1343e-01]]], device='cuda:0'),) and output (tensor([[[-0.1355,  0.0227,  0.0554,  ..., -0.5473,  0.2470, -0.0229],\n",
      "         [-0.3248,  0.2576, -0.3447,  ..., -0.2515, -0.1215, -0.8959],\n",
      "         [ 0.2474,  0.1077, -0.1408,  ..., -0.1629,  0.3987, -0.3142],\n",
      "         ...,\n",
      "         [ 0.6502, -0.1444, -0.0896,  ...,  0.0935,  0.0034,  0.1198],\n",
      "         [-0.1546,  0.2529, -0.3618,  ..., -0.3953, -0.0640,  0.0818],\n",
      "         [ 0.0796, -0.2379,  0.0150,  ...,  0.0429, -0.6671,  0.1404]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.1355,  0.0227,  0.0554,  ..., -0.5473,  0.2470, -0.0229],\n",
      "         [-0.3248,  0.2576, -0.3447,  ..., -0.2515, -0.1215, -0.8959],\n",
      "         [ 0.2474,  0.1077, -0.1408,  ..., -0.1629,  0.3987, -0.3142],\n",
      "         ...,\n",
      "         [ 0.6502, -0.1444, -0.0896,  ...,  0.0935,  0.0034,  0.1198],\n",
      "         [-0.1546,  0.2529, -0.3618,  ..., -0.3953, -0.0640,  0.0818],\n",
      "         [ 0.0796, -0.2379,  0.0150,  ...,  0.0429, -0.6671,  0.1404]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.1350,  0.0163,  0.0576,  ..., -0.5555,  0.2435,  0.0051],\n",
      "         [-0.3877,  0.2245, -0.3214,  ..., -0.3201, -0.0318, -0.7949],\n",
      "         [ 0.2817,  0.2023, -0.0559,  ..., -0.1820,  0.3709, -0.2702],\n",
      "         ...,\n",
      "         [ 0.5208, -0.3386, -0.3248,  ...,  0.1055,  0.0708,  0.1077],\n",
      "         [-0.1002,  0.1607, -0.4321,  ..., -0.2204, -0.0127,  0.0071],\n",
      "         [-0.0186, -0.4443,  0.0019,  ...,  0.2125, -0.5113,  0.1383]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.1350,  0.0163,  0.0576,  ..., -0.5555,  0.2435,  0.0051],\n",
      "         [-0.3877,  0.2245, -0.3214,  ..., -0.3201, -0.0318, -0.7949],\n",
      "         [ 0.2817,  0.2023, -0.0559,  ..., -0.1820,  0.3709, -0.2702],\n",
      "         ...,\n",
      "         [ 0.5208, -0.3386, -0.3248,  ...,  0.1055,  0.0708,  0.1077],\n",
      "         [-0.1002,  0.1607, -0.4321,  ..., -0.2204, -0.0127,  0.0071],\n",
      "         [-0.0186, -0.4443,  0.0019,  ...,  0.2125, -0.5113,  0.1383]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.1301, -0.0029,  0.0279,  ..., -0.5537,  0.2277,  0.0164],\n",
      "         [-0.4023,  0.4463, -0.2834,  ..., -0.3658,  0.0355, -1.0415],\n",
      "         [ 0.3479,  0.2837,  0.0129,  ..., -0.2103,  0.4343, -0.4750],\n",
      "         ...,\n",
      "         [ 0.5961, -0.3488, -0.2717,  ...,  0.1505,  0.0587,  0.1161],\n",
      "         [-0.2123,  0.1061, -0.3649,  ..., -0.4358,  0.0228, -0.0820],\n",
      "         [-0.0632, -0.3592,  0.0924,  ...,  0.2786, -0.4854,  0.1377]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.1301, -0.0029,  0.0279,  ..., -0.5537,  0.2277,  0.0164],\n",
      "         [-0.4023,  0.4463, -0.2834,  ..., -0.3658,  0.0355, -1.0415],\n",
      "         [ 0.3479,  0.2837,  0.0129,  ..., -0.2103,  0.4343, -0.4750],\n",
      "         ...,\n",
      "         [ 0.5961, -0.3488, -0.2717,  ...,  0.1505,  0.0587,  0.1161],\n",
      "         [-0.2123,  0.1061, -0.3649,  ..., -0.4358,  0.0228, -0.0820],\n",
      "         [-0.0632, -0.3592,  0.0924,  ...,  0.2786, -0.4854,  0.1377]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.1219,  0.0061, -0.0021,  ..., -0.5922,  0.2788, -0.0020],\n",
      "         [-0.2967,  0.2847, -0.3594,  ..., -0.5524,  0.2540, -0.8973],\n",
      "         [ 0.4522,  0.2169, -0.1300,  ..., -0.3892,  0.5091, -0.4473],\n",
      "         ...,\n",
      "         [ 0.6366, -0.0804, -0.3298,  ...,  0.2911,  0.2080,  0.0676],\n",
      "         [-0.1964,  0.1587, -0.2412,  ..., -0.3615,  0.2213, -0.0016],\n",
      "         [-0.1970, -0.4231,  0.2813,  ...,  0.1961, -0.3503,  0.1656]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.1219,  0.0061, -0.0021,  ..., -0.5922,  0.2788, -0.0020],\n",
      "         [-0.2967,  0.2847, -0.3594,  ..., -0.5524,  0.2540, -0.8973],\n",
      "         [ 0.4522,  0.2169, -0.1300,  ..., -0.3892,  0.5091, -0.4473],\n",
      "         ...,\n",
      "         [ 0.6366, -0.0804, -0.3298,  ...,  0.2911,  0.2080,  0.0676],\n",
      "         [-0.1964,  0.1587, -0.2412,  ..., -0.3615,  0.2213, -0.0016],\n",
      "         [-0.1970, -0.4231,  0.2813,  ...,  0.1961, -0.3503,  0.1656]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.1067,  0.0272,  0.0495,  ..., -0.5550,  0.3449,  0.0266],\n",
      "         [-0.1664,  0.3359, -0.4058,  ..., -0.4030,  0.1850, -0.8981],\n",
      "         [ 0.4092,  0.2537,  0.0471,  ..., -0.5272,  0.5671, -0.5370],\n",
      "         ...,\n",
      "         [ 0.4383,  0.0586, -0.1129,  ...,  0.4893,  0.0314,  0.3484],\n",
      "         [-0.2312,  0.4082, -0.1123,  ..., -0.2585,  0.1751,  0.0408],\n",
      "         [-0.2692, -0.3201,  0.3966,  ...,  0.2719, -0.5228,  0.3554]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.1067,  0.0272,  0.0495,  ..., -0.5550,  0.3449,  0.0266],\n",
      "         [-0.1664,  0.3359, -0.4058,  ..., -0.4030,  0.1850, -0.8981],\n",
      "         [ 0.4092,  0.2537,  0.0471,  ..., -0.5272,  0.5671, -0.5370],\n",
      "         ...,\n",
      "         [ 0.4383,  0.0586, -0.1129,  ...,  0.4893,  0.0314,  0.3484],\n",
      "         [-0.2312,  0.4082, -0.1123,  ..., -0.2585,  0.1751,  0.0408],\n",
      "         [-0.2692, -0.3201,  0.3966,  ...,  0.2719, -0.5228,  0.3554]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.0592,  0.0731,  0.0500,  ..., -0.5517,  0.3541,  0.0595],\n",
      "         [-0.1404,  0.2131, -0.4877,  ..., -0.0881,  0.0688, -0.8005],\n",
      "         [ 0.1322, -0.0813,  0.3731,  ..., -0.7148,  0.6496, -0.5835],\n",
      "         ...,\n",
      "         [ 0.4239,  0.0134,  0.5554,  ...,  0.4325,  0.1950,  0.3097],\n",
      "         [-0.5534,  0.2175,  0.0893,  ..., -0.5050,  0.1568, -0.5302],\n",
      "         [-0.5114, -0.5303,  0.3741,  ...,  0.1824, -0.4384,  0.1656]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.0592,  0.0731,  0.0500,  ..., -0.5517,  0.3541,  0.0595],\n",
      "         [-0.1404,  0.2131, -0.4877,  ..., -0.0881,  0.0688, -0.8005],\n",
      "         [ 0.1322, -0.0813,  0.3731,  ..., -0.7148,  0.6496, -0.5835],\n",
      "         ...,\n",
      "         [ 0.4239,  0.0134,  0.5554,  ...,  0.4325,  0.1950,  0.3097],\n",
      "         [-0.5534,  0.2175,  0.0893,  ..., -0.5050,  0.1568, -0.5302],\n",
      "         [-0.5114, -0.5303,  0.3741,  ...,  0.1824, -0.4384,  0.1656]]],\n",
      "       device='cuda:0'),) and output (tensor([[[ 0.1143,  0.2237,  0.3292,  ..., -0.7706,  0.2608,  0.0269],\n",
      "         [-0.0170,  0.4703, -0.4169,  ..., -0.0465,  0.2577, -0.9986],\n",
      "         [-0.4164, -0.1231,  0.3195,  ..., -0.9336,  0.6465, -0.6216],\n",
      "         ...,\n",
      "         [ 0.6215, -0.4024,  0.2563,  ...,  0.2529, -0.1696,  0.6125],\n",
      "         [-0.5413,  0.1713, -0.3019,  ..., -0.4093, -0.2593, -0.3672],\n",
      "         [-0.4928, -0.5432,  0.3242,  ...,  0.1726, -0.7314,  0.2983]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[ 0.1143,  0.2237,  0.3292,  ..., -0.7706,  0.2608,  0.0269],\n",
      "         [-0.0170,  0.4703, -0.4169,  ..., -0.0465,  0.2577, -0.9986],\n",
      "         [-0.4164, -0.1231,  0.3195,  ..., -0.9336,  0.6465, -0.6216],\n",
      "         ...,\n",
      "         [ 0.6215, -0.4024,  0.2563,  ...,  0.2529, -0.1696,  0.6125],\n",
      "         [-0.5413,  0.1713, -0.3019,  ..., -0.4093, -0.2593, -0.3672],\n",
      "         [-0.4928, -0.5432,  0.3242,  ...,  0.1726, -0.7314,  0.2983]]],\n",
      "       device='cuda:0'),) and output (tensor([[[ 0.3711,  0.5489,  0.6100,  ..., -0.7499,  0.2080,  0.4195],\n",
      "         [ 0.1076,  0.1096, -0.4580,  ...,  0.3490,  0.4190, -1.1567],\n",
      "         [-0.0219, -0.0213,  0.4042,  ..., -0.9188, -0.0480, -1.0828],\n",
      "         ...,\n",
      "         [ 0.4378, -0.4621, -0.3148,  ...,  0.1956, -0.4903,  0.4002],\n",
      "         [-0.7609, -0.4834, -0.7906,  ..., -0.3158, -0.7127, -1.1745],\n",
      "         [-0.5483, -1.0060, -0.1235,  ...,  0.3825, -0.8743, -0.2305]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[ 0.3711,  0.5489,  0.6100,  ..., -0.7499,  0.2080,  0.4195],\n",
      "         [ 0.1076,  0.1096, -0.4580,  ...,  0.3490,  0.4190, -1.1567],\n",
      "         [-0.0219, -0.0213,  0.4042,  ..., -0.9188, -0.0480, -1.0828],\n",
      "         ...,\n",
      "         [ 0.4378, -0.4621, -0.3148,  ...,  0.1956, -0.4903,  0.4002],\n",
      "         [-0.7609, -0.4834, -0.7906,  ..., -0.3158, -0.7127, -1.1745],\n",
      "         [-0.5483, -1.0060, -0.1235,  ...,  0.3825, -0.8743, -0.2305]]],\n",
      "       device='cuda:0'),) and output (tensor([[[ 1.1544,  3.2965,  1.6534,  ..., -2.7009,  2.6553,  2.2700],\n",
      "         [ 0.9211,  0.0815, -0.7268,  ...,  0.0261, -0.5282, -0.7231],\n",
      "         [-0.4005,  0.2414, -0.0113,  ..., -0.7451, -0.5447, -0.5473],\n",
      "         ...,\n",
      "         [-0.3962, -2.0370,  0.9748,  ...,  0.4322, -0.8599,  1.6791],\n",
      "         [-1.0222, -1.6027, -0.7865,  ...,  0.6795, -2.2182, -0.5556],\n",
      "         [-0.4383, -1.9466, -0.1359,  ...,  1.5578, -0.8211,  0.2903]]],\n",
      "       device='cuda:0'),)\n",
      "[Debug] Layer names being passed: ['model.layers.0', 'model.layers.1', 'model.layers.2', 'model.layers.3', 'model.layers.4', 'model.layers.5', 'model.layers.6', 'model.layers.7', 'model.layers.8', 'model.layers.9', 'model.layers.10', 'model.layers.11', 'model.layers.12', 'model.layers.13', 'model.layers.14', 'model.layers.15', 'model.layers.16', 'model.layers.17', 'model.layers.18', 'model.layers.19', 'model.layers.20', 'model.layers.21', 'model.layers.22', 'model.layers.23', 'model.layers.24', 'model.layers.25', 'model.layers.26', 'model.layers.27', 'model.layers.28', 'model.layers.29', 'model.layers.30', 'model.layers.31', 'model.embed_tokens']\n",
      "[Debug] Trying to access layer: model.layers.0\n",
      "[Debug] Successfully found layer: model.layers.0\n",
      "[Debug] Trying to access layer: model.layers.1\n",
      "[Debug] Successfully found layer: model.layers.1\n",
      "[Debug] Trying to access layer: model.layers.2\n",
      "[Debug] Successfully found layer: model.layers.2\n",
      "[Debug] Trying to access layer: model.layers.3\n",
      "[Debug] Successfully found layer: model.layers.3\n",
      "[Debug] Trying to access layer: model.layers.4\n",
      "[Debug] Successfully found layer: model.layers.4\n",
      "[Debug] Trying to access layer: model.layers.5\n",
      "[Debug] Successfully found layer: model.layers.5\n",
      "[Debug] Trying to access layer: model.layers.6\n",
      "[Debug] Successfully found layer: model.layers.6\n",
      "[Debug] Trying to access layer: model.layers.7\n",
      "[Debug] Successfully found layer: model.layers.7\n",
      "[Debug] Trying to access layer: model.layers.8\n",
      "[Debug] Successfully found layer: model.layers.8\n",
      "[Debug] Trying to access layer: model.layers.9\n",
      "[Debug] Successfully found layer: model.layers.9\n",
      "[Debug] Trying to access layer: model.layers.10\n",
      "[Debug] Successfully found layer: model.layers.10\n",
      "[Debug] Trying to access layer: model.layers.11\n",
      "[Debug] Successfully found layer: model.layers.11\n",
      "[Debug] Trying to access layer: model.layers.12\n",
      "[Debug] Successfully found layer: model.layers.12\n",
      "[Debug] Trying to access layer: model.layers.13\n",
      "[Debug] Successfully found layer: model.layers.13\n",
      "[Debug] Trying to access layer: model.layers.14\n",
      "[Debug] Successfully found layer: model.layers.14\n",
      "[Debug] Trying to access layer: model.layers.15\n",
      "[Debug] Successfully found layer: model.layers.15\n",
      "[Debug] Trying to access layer: model.layers.16\n",
      "[Debug] Successfully found layer: model.layers.16\n",
      "[Debug] Trying to access layer: model.layers.17\n",
      "[Debug] Successfully found layer: model.layers.17\n",
      "[Debug] Trying to access layer: model.layers.18\n",
      "[Debug] Successfully found layer: model.layers.18\n",
      "[Debug] Trying to access layer: model.layers.19\n",
      "[Debug] Successfully found layer: model.layers.19\n",
      "[Debug] Trying to access layer: model.layers.20\n",
      "[Debug] Successfully found layer: model.layers.20\n",
      "[Debug] Trying to access layer: model.layers.21\n",
      "[Debug] Successfully found layer: model.layers.21\n",
      "[Debug] Trying to access layer: model.layers.22\n",
      "[Debug] Successfully found layer: model.layers.22\n",
      "[Debug] Trying to access layer: model.layers.23\n",
      "[Debug] Successfully found layer: model.layers.23\n",
      "[Debug] Trying to access layer: model.layers.24\n",
      "[Debug] Successfully found layer: model.layers.24\n",
      "[Debug] Trying to access layer: model.layers.25\n",
      "[Debug] Successfully found layer: model.layers.25\n",
      "[Debug] Trying to access layer: model.layers.26\n",
      "[Debug] Successfully found layer: model.layers.26\n",
      "[Debug] Trying to access layer: model.layers.27\n",
      "[Debug] Successfully found layer: model.layers.27\n",
      "[Debug] Trying to access layer: model.layers.28\n",
      "[Debug] Successfully found layer: model.layers.28\n",
      "[Debug] Trying to access layer: model.layers.29\n",
      "[Debug] Successfully found layer: model.layers.29\n",
      "[Debug] Trying to access layer: model.layers.30\n",
      "[Debug] Successfully found layer: model.layers.30\n",
      "[Debug] Trying to access layer: model.layers.31\n",
      "[Debug] Successfully found layer: model.layers.31\n",
      "[Debug] Trying to access layer: model.embed_tokens\n",
      "[Debug] Successfully found layer: model.embed_tokens\n",
      "[Hook] Layer Embedding(128256, 4096) received input (tensor([[128000,  12409,    315,    279,   3723,   4273,    578,   5292,    315,\n",
      "            279,   3723,   4273,    315,   5270,     11,   3629,  14183,    311,\n",
      "            439,    279,   3778,   5292,     11,    374,    279,   5426,   5292,\n",
      "            315,    279,   3723,   4273,     13,   1102,  17610,    315,  61759,\n",
      "           6273,  16600,  55788,    315,   2579,    320,   3565,    323,   5740,\n",
      "              8,  73462,    449,   4251,     11,    449,    264,   6437,  23596,\n",
      "            304,    279,  16869,    263,    320,    265,   5671,    311,  11951,\n",
      "            439,    279,    330,  16588,    909,  18534,  33517,   2678,     11,\n",
      "           4251,     11,   4330,  16983,    291,   9958,  28902,    304,  11888,\n",
      "           4445,  16600,   7123,     11,   1405,   7123,    315,   4848,   9958,\n",
      "            320,   3565,    323,   5740,      8,  25631,    449,   7123,    315,\n",
      "           4330,   9958,     13,    578,    220,   1135,   9958,    389,    279,\n",
      "           5292,   4097,    279,    220,   1135,   5415,    315,    279,   3723,\n",
      "           4273,    315,   5270,     11,    323,    279,    220,   1032,  55788,\n",
      "           4097,    279,  61759,   8013,  49028,    430,  14610,  24589,    505,\n",
      "            279,  15422,    315,   8681,  13527,     11,    323,   6244,    279,\n",
      "           1176,   5415,    304,    279,    549,    815,   8032,     16,     60,\n",
      "          15341,  11654,    369,    279,   5292,   2997,    578,  25676,    323,\n",
      "           4610,   9100,  17706,     17,     60,  10846,  59261,  17706,     18,\n",
      "             60,    323,    578,   7834,   6354,     79,  40040,  40714,     13]],\n",
      "       device='cuda:0'),) and output tensor([[[ 2.6512e-04, -4.9973e-04, -5.8365e-04,  ...,  3.8147e-03,\n",
      "           6.3419e-05,  1.1902e-03],\n",
      "         [-1.4832e-02, -1.4877e-03, -1.1969e-04,  ...,  5.7983e-03,\n",
      "          -1.0376e-02, -1.2024e-02],\n",
      "         [ 1.2817e-03,  9.1171e-04,  2.0905e-03,  ...,  1.6251e-03,\n",
      "           4.0894e-03, -4.0283e-03],\n",
      "         ...,\n",
      "         [ 1.5869e-02,  1.7700e-02,  3.1586e-03,  ...,  4.3335e-03,\n",
      "          -2.6398e-03, -9.2773e-03],\n",
      "         [-1.5015e-02, -1.3245e-02,  8.1177e-03,  ..., -1.0254e-02,\n",
      "          -1.1658e-02, -5.5847e-03],\n",
      "         [-6.7520e-04, -5.7602e-04,  4.7684e-04,  ...,  3.3264e-03,\n",
      "           3.9101e-04,  4.7493e-04]]], device='cuda:0')\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[ 2.6512e-04, -4.9973e-04, -5.8365e-04,  ...,  3.8147e-03,\n",
      "           6.3419e-05,  1.1902e-03],\n",
      "         [-1.4832e-02, -1.4877e-03, -1.1969e-04,  ...,  5.7983e-03,\n",
      "          -1.0376e-02, -1.2024e-02],\n",
      "         [ 1.2817e-03,  9.1171e-04,  2.0905e-03,  ...,  1.6251e-03,\n",
      "           4.0894e-03, -4.0283e-03],\n",
      "         ...,\n",
      "         [ 1.5869e-02,  1.7700e-02,  3.1586e-03,  ...,  4.3335e-03,\n",
      "          -2.6398e-03, -9.2773e-03],\n",
      "         [-1.5015e-02, -1.3245e-02,  8.1177e-03,  ..., -1.0254e-02,\n",
      "          -1.1658e-02, -5.5847e-03],\n",
      "         [-6.7520e-04, -5.7602e-04,  4.7684e-04,  ...,  3.3264e-03,\n",
      "           3.9101e-04,  4.7493e-04]]], device='cuda:0'),) and output (tensor([[[-0.0004,  0.0146, -0.0020,  ...,  0.0066, -0.0194,  0.0020],\n",
      "         [-0.0074,  0.0358, -0.0233,  ..., -0.0303,  0.0147, -0.0069],\n",
      "         [ 0.0002,  0.0006,  0.0107,  ..., -0.0108,  0.0054,  0.0018],\n",
      "         ...,\n",
      "         [ 0.0065,  0.0130,  0.0177,  ..., -0.0334, -0.0111, -0.0126],\n",
      "         [-0.0211,  0.0087,  0.0169,  ..., -0.0462, -0.0156, -0.0112],\n",
      "         [ 0.0072,  0.0016,  0.0076,  ..., -0.0054,  0.0044,  0.0025]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.0004,  0.0146, -0.0020,  ...,  0.0066, -0.0194,  0.0020],\n",
      "         [-0.0074,  0.0358, -0.0233,  ..., -0.0303,  0.0147, -0.0069],\n",
      "         [ 0.0002,  0.0006,  0.0107,  ..., -0.0108,  0.0054,  0.0018],\n",
      "         ...,\n",
      "         [ 0.0065,  0.0130,  0.0177,  ..., -0.0334, -0.0111, -0.0126],\n",
      "         [-0.0211,  0.0087,  0.0169,  ..., -0.0462, -0.0156, -0.0112],\n",
      "         [ 0.0072,  0.0016,  0.0076,  ..., -0.0054,  0.0044,  0.0025]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.0972,  0.0787, -0.0419,  ...,  0.0071,  0.0869,  0.0218],\n",
      "         [-0.0115,  0.0291, -0.0381,  ..., -0.0664,  0.0048, -0.0223],\n",
      "         [ 0.0130, -0.0072, -0.0291,  ..., -0.0791, -0.0003,  0.0034],\n",
      "         ...,\n",
      "         [ 0.0028,  0.0237, -0.0137,  ..., -0.0406, -0.0478, -0.0120],\n",
      "         [ 0.0086, -0.0162,  0.0017,  ..., -0.0618, -0.0173, -0.0009],\n",
      "         [ 0.0060, -0.0005,  0.0027,  ...,  0.0130, -0.0128,  0.0039]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.0972,  0.0787, -0.0419,  ...,  0.0071,  0.0869,  0.0218],\n",
      "         [-0.0115,  0.0291, -0.0381,  ..., -0.0664,  0.0048, -0.0223],\n",
      "         [ 0.0130, -0.0072, -0.0291,  ..., -0.0791, -0.0003,  0.0034],\n",
      "         ...,\n",
      "         [ 0.0028,  0.0237, -0.0137,  ..., -0.0406, -0.0478, -0.0120],\n",
      "         [ 0.0086, -0.0162,  0.0017,  ..., -0.0618, -0.0173, -0.0009],\n",
      "         [ 0.0060, -0.0005,  0.0027,  ...,  0.0130, -0.0128,  0.0039]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.1040,  0.0875, -0.0360,  ...,  0.0205,  0.0997,  0.0184],\n",
      "         [-0.0111,  0.0314, -0.0177,  ..., -0.0215,  0.0422, -0.0264],\n",
      "         [ 0.0112, -0.0096,  0.0067,  ..., -0.0651,  0.0283,  0.0197],\n",
      "         ...,\n",
      "         [ 0.0178, -0.0221, -0.0319,  ..., -0.0341, -0.0460, -0.0245],\n",
      "         [-0.0057, -0.0181,  0.0041,  ..., -0.0367, -0.0185, -0.0232],\n",
      "         [-0.0091, -0.0025,  0.0273,  ...,  0.0210, -0.0198,  0.0039]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.1040,  0.0875, -0.0360,  ...,  0.0205,  0.0997,  0.0184],\n",
      "         [-0.0111,  0.0314, -0.0177,  ..., -0.0215,  0.0422, -0.0264],\n",
      "         [ 0.0112, -0.0096,  0.0067,  ..., -0.0651,  0.0283,  0.0197],\n",
      "         ...,\n",
      "         [ 0.0178, -0.0221, -0.0319,  ..., -0.0341, -0.0460, -0.0245],\n",
      "         [-0.0057, -0.0181,  0.0041,  ..., -0.0367, -0.0185, -0.0232],\n",
      "         [-0.0091, -0.0025,  0.0273,  ...,  0.0210, -0.0198,  0.0039]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.0870,  0.1049, -0.0173,  ...,  0.0700,  0.0989,  0.0181],\n",
      "         [-0.0276,  0.0503,  0.0057,  ..., -0.0287,  0.0422, -0.0612],\n",
      "         [-0.0054, -0.0050,  0.0134,  ..., -0.1005,  0.0586, -0.0062],\n",
      "         ...,\n",
      "         [ 0.0135, -0.0325, -0.0192,  ..., -0.0394, -0.0217, -0.0836],\n",
      "         [-0.0295, -0.0419, -0.0139,  ..., -0.1073, -0.0064, -0.0528],\n",
      "         [-0.0334, -0.0338,  0.0030,  ..., -0.0118, -0.0672,  0.0015]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.0870,  0.1049, -0.0173,  ...,  0.0700,  0.0989,  0.0181],\n",
      "         [-0.0276,  0.0503,  0.0057,  ..., -0.0287,  0.0422, -0.0612],\n",
      "         [-0.0054, -0.0050,  0.0134,  ..., -0.1005,  0.0586, -0.0062],\n",
      "         ...,\n",
      "         [ 0.0135, -0.0325, -0.0192,  ..., -0.0394, -0.0217, -0.0836],\n",
      "         [-0.0295, -0.0419, -0.0139,  ..., -0.1073, -0.0064, -0.0528],\n",
      "         [-0.0334, -0.0338,  0.0030,  ..., -0.0118, -0.0672,  0.0015]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.1113,  0.1022, -0.0102,  ...,  0.0436,  0.1283,  0.0321],\n",
      "         [ 0.0152,  0.0740,  0.0464,  ...,  0.0360,  0.0458, -0.0619],\n",
      "         [-0.0204,  0.0027, -0.0048,  ..., -0.0391,  0.0286, -0.0145],\n",
      "         ...,\n",
      "         [ 0.0094, -0.0467, -0.0378,  ..., -0.0473, -0.0025, -0.1179],\n",
      "         [-0.0246,  0.0005,  0.0045,  ..., -0.1115,  0.0096, -0.1425],\n",
      "         [-0.0471, -0.0298,  0.0060,  ...,  0.0500, -0.0846,  0.0155]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.1113,  0.1022, -0.0102,  ...,  0.0436,  0.1283,  0.0321],\n",
      "         [ 0.0152,  0.0740,  0.0464,  ...,  0.0360,  0.0458, -0.0619],\n",
      "         [-0.0204,  0.0027, -0.0048,  ..., -0.0391,  0.0286, -0.0145],\n",
      "         ...,\n",
      "         [ 0.0094, -0.0467, -0.0378,  ..., -0.0473, -0.0025, -0.1179],\n",
      "         [-0.0246,  0.0005,  0.0045,  ..., -0.1115,  0.0096, -0.1425],\n",
      "         [-0.0471, -0.0298,  0.0060,  ...,  0.0500, -0.0846,  0.0155]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.1034,  0.1132,  0.0096,  ...,  0.0453,  0.1477,  0.0301],\n",
      "         [ 0.0953,  0.0950, -0.0347,  ..., -0.0092,  0.0317, -0.0297],\n",
      "         [ 0.1059,  0.0248,  0.0189,  ..., -0.0514,  0.0865,  0.0026],\n",
      "         ...,\n",
      "         [ 0.0168, -0.0182, -0.0793,  ..., -0.0746, -0.0105, -0.0867],\n",
      "         [-0.0220,  0.0596, -0.0183,  ..., -0.1988, -0.0239, -0.1560],\n",
      "         [-0.0606, -0.0244, -0.0011,  ...,  0.1157, -0.0998,  0.0428]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.1034,  0.1132,  0.0096,  ...,  0.0453,  0.1477,  0.0301],\n",
      "         [ 0.0953,  0.0950, -0.0347,  ..., -0.0092,  0.0317, -0.0297],\n",
      "         [ 0.1059,  0.0248,  0.0189,  ..., -0.0514,  0.0865,  0.0026],\n",
      "         ...,\n",
      "         [ 0.0168, -0.0182, -0.0793,  ..., -0.0746, -0.0105, -0.0867],\n",
      "         [-0.0220,  0.0596, -0.0183,  ..., -0.1988, -0.0239, -0.1560],\n",
      "         [-0.0606, -0.0244, -0.0011,  ...,  0.1157, -0.0998,  0.0428]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.1071,  0.1253,  0.0211,  ...,  0.0126,  0.1586,  0.0278],\n",
      "         [ 0.0581,  0.1188, -0.0449,  ...,  0.0405,  0.0782, -0.0313],\n",
      "         [ 0.0477,  0.0458, -0.0045,  ..., -0.0506,  0.0756, -0.0060],\n",
      "         ...,\n",
      "         [ 0.0690,  0.0424, -0.0574,  ..., -0.1582, -0.0462, -0.0890],\n",
      "         [-0.0034,  0.0842, -0.0111,  ..., -0.2293, -0.0439, -0.2004],\n",
      "         [-0.0616,  0.0004,  0.0167,  ...,  0.1010, -0.0930,  0.0508]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.1071,  0.1253,  0.0211,  ...,  0.0126,  0.1586,  0.0278],\n",
      "         [ 0.0581,  0.1188, -0.0449,  ...,  0.0405,  0.0782, -0.0313],\n",
      "         [ 0.0477,  0.0458, -0.0045,  ..., -0.0506,  0.0756, -0.0060],\n",
      "         ...,\n",
      "         [ 0.0690,  0.0424, -0.0574,  ..., -0.1582, -0.0462, -0.0890],\n",
      "         [-0.0034,  0.0842, -0.0111,  ..., -0.2293, -0.0439, -0.2004],\n",
      "         [-0.0616,  0.0004,  0.0167,  ...,  0.1010, -0.0930,  0.0508]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.0945,  0.1848,  0.0036,  ..., -0.0473,  0.1489,  0.0536],\n",
      "         [ 0.0870,  0.0575, -0.0740,  ..., -0.0414,  0.0129, -0.0728],\n",
      "         [ 0.0634,  0.1217, -0.0425,  ..., -0.1297,  0.0489, -0.0718],\n",
      "         ...,\n",
      "         [ 0.0707,  0.1231, -0.0723,  ..., -0.2360, -0.0805,  0.0195],\n",
      "         [ 0.0718,  0.1487,  0.1382,  ..., -0.2373,  0.0581, -0.1004],\n",
      "         [-0.0635, -0.0287, -0.0562,  ...,  0.1319, -0.0710,  0.0460]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.0945,  0.1848,  0.0036,  ..., -0.0473,  0.1489,  0.0536],\n",
      "         [ 0.0870,  0.0575, -0.0740,  ..., -0.0414,  0.0129, -0.0728],\n",
      "         [ 0.0634,  0.1217, -0.0425,  ..., -0.1297,  0.0489, -0.0718],\n",
      "         ...,\n",
      "         [ 0.0707,  0.1231, -0.0723,  ..., -0.2360, -0.0805,  0.0195],\n",
      "         [ 0.0718,  0.1487,  0.1382,  ..., -0.2373,  0.0581, -0.1004],\n",
      "         [-0.0635, -0.0287, -0.0562,  ...,  0.1319, -0.0710,  0.0460]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.1060,  0.2113,  0.0270,  ..., -0.1141,  0.1610,  0.0528],\n",
      "         [ 0.1066,  0.0846, -0.0455,  ..., -0.1235,  0.0529, -0.1168],\n",
      "         [ 0.0425,  0.1449,  0.0259,  ..., -0.1767,  0.0329, -0.0988],\n",
      "         ...,\n",
      "         [ 0.0107,  0.0540,  0.0575,  ..., -0.1611, -0.1383, -0.0602],\n",
      "         [ 0.0802,  0.1176,  0.1796,  ..., -0.1850, -0.0035, -0.1043],\n",
      "         [-0.1308, -0.0513,  0.0329,  ...,  0.1592, -0.0083,  0.0835]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.1060,  0.2113,  0.0270,  ..., -0.1141,  0.1610,  0.0528],\n",
      "         [ 0.1066,  0.0846, -0.0455,  ..., -0.1235,  0.0529, -0.1168],\n",
      "         [ 0.0425,  0.1449,  0.0259,  ..., -0.1767,  0.0329, -0.0988],\n",
      "         ...,\n",
      "         [ 0.0107,  0.0540,  0.0575,  ..., -0.1611, -0.1383, -0.0602],\n",
      "         [ 0.0802,  0.1176,  0.1796,  ..., -0.1850, -0.0035, -0.1043],\n",
      "         [-0.1308, -0.0513,  0.0329,  ...,  0.1592, -0.0083,  0.0835]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.0954,  0.2493,  0.0073,  ..., -0.1568,  0.1818,  0.0855],\n",
      "         [ 0.0452,  0.0566, -0.0129,  ..., -0.0843,  0.0883, -0.0774],\n",
      "         [ 0.0032,  0.0188,  0.0341,  ..., -0.1444,  0.0727, -0.1151],\n",
      "         ...,\n",
      "         [-0.0554,  0.0258,  0.1270,  ..., -0.2088, -0.1310,  0.0262],\n",
      "         [ 0.0664,  0.1520,  0.1817,  ..., -0.1066, -0.0032, -0.0752],\n",
      "         [-0.0742, -0.1305, -0.0091,  ...,  0.1936,  0.0791,  0.0421]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.0954,  0.2493,  0.0073,  ..., -0.1568,  0.1818,  0.0855],\n",
      "         [ 0.0452,  0.0566, -0.0129,  ..., -0.0843,  0.0883, -0.0774],\n",
      "         [ 0.0032,  0.0188,  0.0341,  ..., -0.1444,  0.0727, -0.1151],\n",
      "         ...,\n",
      "         [-0.0554,  0.0258,  0.1270,  ..., -0.2088, -0.1310,  0.0262],\n",
      "         [ 0.0664,  0.1520,  0.1817,  ..., -0.1066, -0.0032, -0.0752],\n",
      "         [-0.0742, -0.1305, -0.0091,  ...,  0.1936,  0.0791,  0.0421]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-1.5196e-02,  3.3404e-01,  1.0954e-03,  ..., -2.7502e-01,\n",
      "           1.9330e-01,  1.0341e-01],\n",
      "         [ 9.7553e-02,  6.7326e-02, -1.1606e-04,  ..., -1.6362e-01,\n",
      "           9.2092e-02, -1.2279e-01],\n",
      "         [ 2.0221e-02,  7.3916e-02,  1.0058e-01,  ..., -1.8365e-01,\n",
      "           4.4177e-02, -6.9415e-02],\n",
      "         ...,\n",
      "         [ 4.4860e-03, -6.8900e-02,  4.5028e-02,  ..., -1.2848e-01,\n",
      "          -7.6967e-02, -1.1159e-02],\n",
      "         [ 1.3645e-01,  7.9588e-02,  1.5848e-01,  ..., -3.6928e-02,\n",
      "           3.9936e-02, -9.0738e-02],\n",
      "         [-7.4590e-02, -1.1855e-01,  1.4096e-02,  ...,  1.2476e-01,\n",
      "           3.4011e-02,  7.7190e-02]]], device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-1.5196e-02,  3.3404e-01,  1.0954e-03,  ..., -2.7502e-01,\n",
      "           1.9330e-01,  1.0341e-01],\n",
      "         [ 9.7553e-02,  6.7326e-02, -1.1606e-04,  ..., -1.6362e-01,\n",
      "           9.2092e-02, -1.2279e-01],\n",
      "         [ 2.0221e-02,  7.3916e-02,  1.0058e-01,  ..., -1.8365e-01,\n",
      "           4.4177e-02, -6.9415e-02],\n",
      "         ...,\n",
      "         [ 4.4860e-03, -6.8900e-02,  4.5028e-02,  ..., -1.2848e-01,\n",
      "          -7.6967e-02, -1.1159e-02],\n",
      "         [ 1.3645e-01,  7.9588e-02,  1.5848e-01,  ..., -3.6928e-02,\n",
      "           3.9936e-02, -9.0738e-02],\n",
      "         [-7.4590e-02, -1.1855e-01,  1.4096e-02,  ...,  1.2476e-01,\n",
      "           3.4011e-02,  7.7190e-02]]], device='cuda:0'),) and output (tensor([[[-0.0356,  0.3483, -0.0366,  ..., -0.2962,  0.2088,  0.1065],\n",
      "         [ 0.1003,  0.0345, -0.0102,  ..., -0.1959,  0.1041, -0.1578],\n",
      "         [ 0.0028,  0.0776,  0.0810,  ..., -0.2731,  0.0387, -0.1309],\n",
      "         ...,\n",
      "         [ 0.0162, -0.1266, -0.0016,  ..., -0.1841, -0.1453, -0.0585],\n",
      "         [ 0.1447,  0.0421,  0.0138,  ..., -0.1680, -0.0269, -0.1504],\n",
      "         [-0.1094, -0.1101,  0.0276,  ...,  0.0066,  0.1093,  0.0204]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.0356,  0.3483, -0.0366,  ..., -0.2962,  0.2088,  0.1065],\n",
      "         [ 0.1003,  0.0345, -0.0102,  ..., -0.1959,  0.1041, -0.1578],\n",
      "         [ 0.0028,  0.0776,  0.0810,  ..., -0.2731,  0.0387, -0.1309],\n",
      "         ...,\n",
      "         [ 0.0162, -0.1266, -0.0016,  ..., -0.1841, -0.1453, -0.0585],\n",
      "         [ 0.1447,  0.0421,  0.0138,  ..., -0.1680, -0.0269, -0.1504],\n",
      "         [-0.1094, -0.1101,  0.0276,  ...,  0.0066,  0.1093,  0.0204]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-1.2733e-02,  3.6610e-01, -3.1563e-02,  ..., -3.6781e-01,\n",
      "           1.9730e-01,  1.0189e-01],\n",
      "         [ 1.0587e-01, -3.7961e-02,  7.3089e-02,  ..., -1.9907e-01,\n",
      "           9.8003e-02, -1.0933e-01],\n",
      "         [ 6.0054e-02,  1.0982e-01,  9.5764e-02,  ..., -2.8272e-01,\n",
      "          -2.3404e-02, -2.0811e-01],\n",
      "         ...,\n",
      "         [-7.1113e-02, -9.9204e-02,  2.4604e-04,  ..., -2.3556e-01,\n",
      "          -1.0251e-01, -1.6524e-01],\n",
      "         [ 1.1532e-01,  6.8955e-02,  8.1347e-02,  ..., -2.1027e-01,\n",
      "          -6.5025e-02, -2.3260e-01],\n",
      "         [-1.6537e-01, -1.1065e-01,  5.3891e-02,  ...,  3.0904e-02,\n",
      "          -1.2295e-02,  9.0256e-02]]], device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-1.2733e-02,  3.6610e-01, -3.1563e-02,  ..., -3.6781e-01,\n",
      "           1.9730e-01,  1.0189e-01],\n",
      "         [ 1.0587e-01, -3.7961e-02,  7.3089e-02,  ..., -1.9907e-01,\n",
      "           9.8003e-02, -1.0933e-01],\n",
      "         [ 6.0054e-02,  1.0982e-01,  9.5764e-02,  ..., -2.8272e-01,\n",
      "          -2.3404e-02, -2.0811e-01],\n",
      "         ...,\n",
      "         [-7.1113e-02, -9.9204e-02,  2.4604e-04,  ..., -2.3556e-01,\n",
      "          -1.0251e-01, -1.6524e-01],\n",
      "         [ 1.1532e-01,  6.8955e-02,  8.1347e-02,  ..., -2.1027e-01,\n",
      "          -6.5025e-02, -2.3260e-01],\n",
      "         [-1.6537e-01, -1.1065e-01,  5.3891e-02,  ...,  3.0904e-02,\n",
      "          -1.2295e-02,  9.0256e-02]]], device='cuda:0'),) and output (tensor([[[-0.0089,  0.3233, -0.0779,  ..., -0.4491,  0.1845,  0.0951],\n",
      "         [ 0.0894, -0.0327,  0.0726,  ..., -0.1333,  0.0693, -0.0648],\n",
      "         [-0.0209,  0.1060,  0.1141,  ..., -0.2487,  0.1028, -0.1526],\n",
      "         ...,\n",
      "         [-0.0578, -0.0771,  0.0422,  ..., -0.2828, -0.1501, -0.1673],\n",
      "         [ 0.0869,  0.1020,  0.1942,  ..., -0.2213, -0.0591, -0.2480],\n",
      "         [-0.1873, -0.1100,  0.2085,  ..., -0.0637, -0.1924,  0.2532]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.0089,  0.3233, -0.0779,  ..., -0.4491,  0.1845,  0.0951],\n",
      "         [ 0.0894, -0.0327,  0.0726,  ..., -0.1333,  0.0693, -0.0648],\n",
      "         [-0.0209,  0.1060,  0.1141,  ..., -0.2487,  0.1028, -0.1526],\n",
      "         ...,\n",
      "         [-0.0578, -0.0771,  0.0422,  ..., -0.2828, -0.1501, -0.1673],\n",
      "         [ 0.0869,  0.1020,  0.1942,  ..., -0.2213, -0.0591, -0.2480],\n",
      "         [-0.1873, -0.1100,  0.2085,  ..., -0.0637, -0.1924,  0.2532]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.0244,  0.2828, -0.0532,  ..., -0.4592,  0.2300,  0.0142],\n",
      "         [ 0.1028, -0.2159,  0.0975,  ..., -0.1872, -0.0075, -0.1031],\n",
      "         [ 0.0348,  0.0559,  0.0756,  ..., -0.3127,  0.1860, -0.3085],\n",
      "         ...,\n",
      "         [ 0.0082, -0.0938,  0.2135,  ..., -0.2693, -0.1618, -0.1939],\n",
      "         [ 0.1484,  0.0214,  0.1836,  ..., -0.3106, -0.1645, -0.1418],\n",
      "         [-0.1933, -0.0768,  0.1281,  ..., -0.0439, -0.2153,  0.2305]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.0244,  0.2828, -0.0532,  ..., -0.4592,  0.2300,  0.0142],\n",
      "         [ 0.1028, -0.2159,  0.0975,  ..., -0.1872, -0.0075, -0.1031],\n",
      "         [ 0.0348,  0.0559,  0.0756,  ..., -0.3127,  0.1860, -0.3085],\n",
      "         ...,\n",
      "         [ 0.0082, -0.0938,  0.2135,  ..., -0.2693, -0.1618, -0.1939],\n",
      "         [ 0.1484,  0.0214,  0.1836,  ..., -0.3106, -0.1645, -0.1418],\n",
      "         [-0.1933, -0.0768,  0.1281,  ..., -0.0439, -0.2153,  0.2305]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.0928,  0.1485,  0.0158,  ..., -0.5380,  0.2664, -0.0333],\n",
      "         [ 0.1411, -0.2619,  0.0116,  ..., -0.2902,  0.1390, -0.1158],\n",
      "         [ 0.0288, -0.0038,  0.0191,  ..., -0.3470,  0.2114, -0.3456],\n",
      "         ...,\n",
      "         [ 0.0409, -0.0490,  0.1836,  ..., -0.2080, -0.1611, -0.1548],\n",
      "         [ 0.1174,  0.0917,  0.1157,  ..., -0.2945, -0.1600, -0.1567],\n",
      "         [-0.2131, -0.2092,  0.0245,  ..., -0.0401, -0.3355,  0.3455]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.0928,  0.1485,  0.0158,  ..., -0.5380,  0.2664, -0.0333],\n",
      "         [ 0.1411, -0.2619,  0.0116,  ..., -0.2902,  0.1390, -0.1158],\n",
      "         [ 0.0288, -0.0038,  0.0191,  ..., -0.3470,  0.2114, -0.3456],\n",
      "         ...,\n",
      "         [ 0.0409, -0.0490,  0.1836,  ..., -0.2080, -0.1611, -0.1548],\n",
      "         [ 0.1174,  0.0917,  0.1157,  ..., -0.2945, -0.1600, -0.1567],\n",
      "         [-0.2131, -0.2092,  0.0245,  ..., -0.0401, -0.3355,  0.3455]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.1000,  0.1323,  0.0241,  ..., -0.5470,  0.2844, -0.0457],\n",
      "         [ 0.0375, -0.2744,  0.0257,  ..., -0.1638,  0.0936, -0.0753],\n",
      "         [ 0.0040, -0.1048, -0.0572,  ..., -0.2364,  0.0334, -0.3391],\n",
      "         ...,\n",
      "         [ 0.0556, -0.1143,  0.1705,  ..., -0.1153, -0.1249, -0.2362],\n",
      "         [ 0.1222,  0.2261,  0.2517,  ..., -0.1537, -0.2291, -0.1944],\n",
      "         [-0.2118, -0.2102,  0.0535,  ..., -0.0044, -0.4620,  0.3542]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.1000,  0.1323,  0.0241,  ..., -0.5470,  0.2844, -0.0457],\n",
      "         [ 0.0375, -0.2744,  0.0257,  ..., -0.1638,  0.0936, -0.0753],\n",
      "         [ 0.0040, -0.1048, -0.0572,  ..., -0.2364,  0.0334, -0.3391],\n",
      "         ...,\n",
      "         [ 0.0556, -0.1143,  0.1705,  ..., -0.1153, -0.1249, -0.2362],\n",
      "         [ 0.1222,  0.2261,  0.2517,  ..., -0.1537, -0.2291, -0.1944],\n",
      "         [-0.2118, -0.2102,  0.0535,  ..., -0.0044, -0.4620,  0.3542]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.1286,  0.0833,  0.0239,  ..., -0.5657,  0.3694, -0.0725],\n",
      "         [-0.0820, -0.2968, -0.1140,  ..., -0.1917,  0.1617, -0.0252],\n",
      "         [ 0.0094, -0.0339, -0.2186,  ..., -0.2384,  0.0508, -0.2344],\n",
      "         ...,\n",
      "         [ 0.0725, -0.1030,  0.0807,  ..., -0.0395, -0.1322, -0.2273],\n",
      "         [ 0.0599,  0.2734,  0.1830,  ..., -0.0665, -0.1772, -0.2500],\n",
      "         [-0.1813, -0.2778, -0.0384,  ..., -0.0956, -0.5120,  0.1964]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.1286,  0.0833,  0.0239,  ..., -0.5657,  0.3694, -0.0725],\n",
      "         [-0.0820, -0.2968, -0.1140,  ..., -0.1917,  0.1617, -0.0252],\n",
      "         [ 0.0094, -0.0339, -0.2186,  ..., -0.2384,  0.0508, -0.2344],\n",
      "         ...,\n",
      "         [ 0.0725, -0.1030,  0.0807,  ..., -0.0395, -0.1322, -0.2273],\n",
      "         [ 0.0599,  0.2734,  0.1830,  ..., -0.0665, -0.1772, -0.2500],\n",
      "         [-0.1813, -0.2778, -0.0384,  ..., -0.0956, -0.5120,  0.1964]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.1417,  0.0703,  0.0343,  ..., -0.5549,  0.3175, -0.0847],\n",
      "         [-0.2735, -0.2750, -0.1724,  ..., -0.3866,  0.2046,  0.0452],\n",
      "         [ 0.1372, -0.0617, -0.3622,  ..., -0.3008,  0.1657, -0.2512],\n",
      "         ...,\n",
      "         [-0.2024, -0.0788,  0.1050,  ...,  0.0295,  0.0174, -0.1415],\n",
      "         [ 0.0339,  0.2447,  0.2332,  ..., -0.0609, -0.0362, -0.2432],\n",
      "         [-0.3164, -0.2653, -0.0680,  ..., -0.1224, -0.4035,  0.1275]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.1417,  0.0703,  0.0343,  ..., -0.5549,  0.3175, -0.0847],\n",
      "         [-0.2735, -0.2750, -0.1724,  ..., -0.3866,  0.2046,  0.0452],\n",
      "         [ 0.1372, -0.0617, -0.3622,  ..., -0.3008,  0.1657, -0.2512],\n",
      "         ...,\n",
      "         [-0.2024, -0.0788,  0.1050,  ...,  0.0295,  0.0174, -0.1415],\n",
      "         [ 0.0339,  0.2447,  0.2332,  ..., -0.0609, -0.0362, -0.2432],\n",
      "         [-0.3164, -0.2653, -0.0680,  ..., -0.1224, -0.4035,  0.1275]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.1528,  0.0725,  0.0609,  ..., -0.5624,  0.3103, -0.0293],\n",
      "         [-0.3733, -0.3736, -0.0804,  ..., -0.4104,  0.2649,  0.1052],\n",
      "         [ 0.0033, -0.0900, -0.2882,  ..., -0.3337, -0.0021, -0.1776],\n",
      "         ...,\n",
      "         [-0.1361, -0.0269,  0.1982,  ...,  0.0661,  0.0309, -0.0705],\n",
      "         [ 0.0315,  0.1350,  0.3209,  ...,  0.0014, -0.0452, -0.2871],\n",
      "         [-0.2953, -0.2693,  0.0252,  ..., -0.1756, -0.2539,  0.0508]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.1528,  0.0725,  0.0609,  ..., -0.5624,  0.3103, -0.0293],\n",
      "         [-0.3733, -0.3736, -0.0804,  ..., -0.4104,  0.2649,  0.1052],\n",
      "         [ 0.0033, -0.0900, -0.2882,  ..., -0.3337, -0.0021, -0.1776],\n",
      "         ...,\n",
      "         [-0.1361, -0.0269,  0.1982,  ...,  0.0661,  0.0309, -0.0705],\n",
      "         [ 0.0315,  0.1350,  0.3209,  ...,  0.0014, -0.0452, -0.2871],\n",
      "         [-0.2953, -0.2693,  0.0252,  ..., -0.1756, -0.2539,  0.0508]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.1559,  0.0520,  0.0566,  ..., -0.5805,  0.2614, -0.0032],\n",
      "         [-0.4456, -0.5353, -0.1770,  ..., -0.4550,  0.1842,  0.0420],\n",
      "         [ 0.1641, -0.2113, -0.3318,  ..., -0.3736, -0.0468, -0.1930],\n",
      "         ...,\n",
      "         [-0.1408,  0.0590,  0.0071,  ...,  0.1770,  0.0016, -0.1170],\n",
      "         [ 0.1111,  0.2018,  0.3164,  ...,  0.0468,  0.1053, -0.3515],\n",
      "         [-0.2629, -0.3890, -0.1262,  ..., -0.1164, -0.2774, -0.0155]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.1559,  0.0520,  0.0566,  ..., -0.5805,  0.2614, -0.0032],\n",
      "         [-0.4456, -0.5353, -0.1770,  ..., -0.4550,  0.1842,  0.0420],\n",
      "         [ 0.1641, -0.2113, -0.3318,  ..., -0.3736, -0.0468, -0.1930],\n",
      "         ...,\n",
      "         [-0.1408,  0.0590,  0.0071,  ...,  0.1770,  0.0016, -0.1170],\n",
      "         [ 0.1111,  0.2018,  0.3164,  ...,  0.0468,  0.1053, -0.3515],\n",
      "         [-0.2629, -0.3890, -0.1262,  ..., -0.1164, -0.2774, -0.0155]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.1488,  0.0445,  0.0765,  ..., -0.5456,  0.2702,  0.0098],\n",
      "         [-0.4186, -0.5586,  0.0014,  ..., -0.4415,  0.2126, -0.1356],\n",
      "         [ 0.0643, -0.3293, -0.3228,  ..., -0.4815, -0.1138, -0.3486],\n",
      "         ...,\n",
      "         [ 0.0611, -0.0409, -0.0285,  ...,  0.0062, -0.0486,  0.1098],\n",
      "         [ 0.1957,  0.3160,  0.2832,  ...,  0.1796,  0.0931, -0.2178],\n",
      "         [-0.1497, -0.2938, -0.1843,  ..., -0.1375, -0.2427,  0.0828]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.1488,  0.0445,  0.0765,  ..., -0.5456,  0.2702,  0.0098],\n",
      "         [-0.4186, -0.5586,  0.0014,  ..., -0.4415,  0.2126, -0.1356],\n",
      "         [ 0.0643, -0.3293, -0.3228,  ..., -0.4815, -0.1138, -0.3486],\n",
      "         ...,\n",
      "         [ 0.0611, -0.0409, -0.0285,  ...,  0.0062, -0.0486,  0.1098],\n",
      "         [ 0.1957,  0.3160,  0.2832,  ...,  0.1796,  0.0931, -0.2178],\n",
      "         [-0.1497, -0.2938, -0.1843,  ..., -0.1375, -0.2427,  0.0828]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-1.2677e-01,  3.3335e-02,  9.3479e-02,  ..., -5.3204e-01,\n",
      "           2.7953e-01,  5.9820e-04],\n",
      "         [-3.8650e-01, -6.6680e-01,  1.1106e-01,  ..., -3.8872e-01,\n",
      "           3.7313e-01, -4.9421e-02],\n",
      "         [ 9.7012e-03, -2.2361e-01, -3.1041e-01,  ..., -4.6693e-01,\n",
      "          -1.1952e-01, -3.0480e-01],\n",
      "         ...,\n",
      "         [ 2.1218e-01, -8.1117e-02,  8.7206e-03,  ..., -1.5109e-01,\n",
      "          -2.9233e-02,  1.1601e-01],\n",
      "         [ 2.8432e-01,  3.8939e-01,  2.3238e-01,  ...,  2.4273e-01,\n",
      "           2.2879e-01, -2.6983e-01],\n",
      "         [-1.1699e-01, -2.1982e-01, -7.5866e-02,  ..., -1.4402e-01,\n",
      "          -1.0073e-01,  5.7836e-02]]], device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-1.2677e-01,  3.3335e-02,  9.3479e-02,  ..., -5.3204e-01,\n",
      "           2.7953e-01,  5.9820e-04],\n",
      "         [-3.8650e-01, -6.6680e-01,  1.1106e-01,  ..., -3.8872e-01,\n",
      "           3.7313e-01, -4.9421e-02],\n",
      "         [ 9.7012e-03, -2.2361e-01, -3.1041e-01,  ..., -4.6693e-01,\n",
      "          -1.1952e-01, -3.0480e-01],\n",
      "         ...,\n",
      "         [ 2.1218e-01, -8.1117e-02,  8.7206e-03,  ..., -1.5109e-01,\n",
      "          -2.9233e-02,  1.1601e-01],\n",
      "         [ 2.8432e-01,  3.8939e-01,  2.3238e-01,  ...,  2.4273e-01,\n",
      "           2.2879e-01, -2.6983e-01],\n",
      "         [-1.1699e-01, -2.1982e-01, -7.5866e-02,  ..., -1.4402e-01,\n",
      "          -1.0073e-01,  5.7836e-02]]], device='cuda:0'),) and output (tensor([[[-0.1355,  0.0227,  0.0554,  ..., -0.5473,  0.2470, -0.0229],\n",
      "         [-0.3325, -0.5974,  0.2409,  ..., -0.3027,  0.4727, -0.0497],\n",
      "         [ 0.0064, -0.2274, -0.3907,  ..., -0.3550, -0.0244, -0.3073],\n",
      "         ...,\n",
      "         [ 0.2568, -0.0235,  0.1041,  ..., -0.2622, -0.1154,  0.1075],\n",
      "         [ 0.2721,  0.2729,  0.2862,  ...,  0.2256,  0.1860, -0.3054],\n",
      "         [-0.0541, -0.1960, -0.0548,  ..., -0.0894,  0.0061,  0.0472]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.1355,  0.0227,  0.0554,  ..., -0.5473,  0.2470, -0.0229],\n",
      "         [-0.3325, -0.5974,  0.2409,  ..., -0.3027,  0.4727, -0.0497],\n",
      "         [ 0.0064, -0.2274, -0.3907,  ..., -0.3550, -0.0244, -0.3073],\n",
      "         ...,\n",
      "         [ 0.2568, -0.0235,  0.1041,  ..., -0.2622, -0.1154,  0.1075],\n",
      "         [ 0.2721,  0.2729,  0.2862,  ...,  0.2256,  0.1860, -0.3054],\n",
      "         [-0.0541, -0.1960, -0.0548,  ..., -0.0894,  0.0061,  0.0472]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.1350,  0.0163,  0.0576,  ..., -0.5555,  0.2435,  0.0051],\n",
      "         [-0.3956, -0.5521,  0.2538,  ..., -0.2086,  0.4252, -0.1838],\n",
      "         [-0.0753, -0.3071, -0.2435,  ..., -0.1863, -0.1174, -0.4081],\n",
      "         ...,\n",
      "         [ 0.2030, -0.1358, -0.0341,  ..., -0.3850, -0.2449,  0.2093],\n",
      "         [ 0.2225,  0.2920,  0.3897,  ...,  0.2267,  0.2060, -0.1561],\n",
      "         [ 0.1546, -0.3049,  0.0308,  ..., -0.0707, -0.0622,  0.0480]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.1350,  0.0163,  0.0576,  ..., -0.5555,  0.2435,  0.0051],\n",
      "         [-0.3956, -0.5521,  0.2538,  ..., -0.2086,  0.4252, -0.1838],\n",
      "         [-0.0753, -0.3071, -0.2435,  ..., -0.1863, -0.1174, -0.4081],\n",
      "         ...,\n",
      "         [ 0.2030, -0.1358, -0.0341,  ..., -0.3850, -0.2449,  0.2093],\n",
      "         [ 0.2225,  0.2920,  0.3897,  ...,  0.2267,  0.2060, -0.1561],\n",
      "         [ 0.1546, -0.3049,  0.0308,  ..., -0.0707, -0.0622,  0.0480]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.1301, -0.0029,  0.0279,  ..., -0.5537,  0.2277,  0.0164],\n",
      "         [-0.3302, -0.4463,  0.5005,  ..., -0.0204,  0.5802, -0.3545],\n",
      "         [-0.2958, -0.3388, -0.1451,  ..., -0.1130, -0.1888, -0.2873],\n",
      "         ...,\n",
      "         [ 0.1435, -0.2048, -0.1301,  ..., -0.3424, -0.2519,  0.1636],\n",
      "         [ 0.1161,  0.3488,  0.3176,  ...,  0.3023,  0.1670, -0.1665],\n",
      "         [ 0.0054, -0.2913,  0.1812,  ..., -0.0690, -0.0308,  0.0894]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.1301, -0.0029,  0.0279,  ..., -0.5537,  0.2277,  0.0164],\n",
      "         [-0.3302, -0.4463,  0.5005,  ..., -0.0204,  0.5802, -0.3545],\n",
      "         [-0.2958, -0.3388, -0.1451,  ..., -0.1130, -0.1888, -0.2873],\n",
      "         ...,\n",
      "         [ 0.1435, -0.2048, -0.1301,  ..., -0.3424, -0.2519,  0.1636],\n",
      "         [ 0.1161,  0.3488,  0.3176,  ...,  0.3023,  0.1670, -0.1665],\n",
      "         [ 0.0054, -0.2913,  0.1812,  ..., -0.0690, -0.0308,  0.0894]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.1219,  0.0061, -0.0021,  ..., -0.5922,  0.2788, -0.0020],\n",
      "         [-0.3139, -0.5882,  0.5996,  ...,  0.0488,  0.5711, -0.4478],\n",
      "         [-0.3281, -0.4325, -0.0417,  ..., -0.1029, -0.0650, -0.3841],\n",
      "         ...,\n",
      "         [ 0.1706, -0.1860,  0.0948,  ..., -0.3304, -0.3166,  0.0671],\n",
      "         [ 0.2207,  0.3355,  0.3536,  ...,  0.4797,  0.1668, -0.1037],\n",
      "         [ 0.1639, -0.4478,  0.1753,  ...,  0.2558, -0.0290,  0.0645]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.1219,  0.0061, -0.0021,  ..., -0.5922,  0.2788, -0.0020],\n",
      "         [-0.3139, -0.5882,  0.5996,  ...,  0.0488,  0.5711, -0.4478],\n",
      "         [-0.3281, -0.4325, -0.0417,  ..., -0.1029, -0.0650, -0.3841],\n",
      "         ...,\n",
      "         [ 0.1706, -0.1860,  0.0948,  ..., -0.3304, -0.3166,  0.0671],\n",
      "         [ 0.2207,  0.3355,  0.3536,  ...,  0.4797,  0.1668, -0.1037],\n",
      "         [ 0.1639, -0.4478,  0.1753,  ...,  0.2558, -0.0290,  0.0645]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.1067,  0.0272,  0.0495,  ..., -0.5550,  0.3449,  0.0266],\n",
      "         [-0.3662, -0.7071,  0.6424,  ..., -0.0091,  0.4016, -0.4627],\n",
      "         [-0.1485, -0.6091, -0.0579,  ..., -0.3597, -0.0659, -0.3385],\n",
      "         ...,\n",
      "         [ 0.1800, -0.1329,  0.1089,  ..., -0.2977, -0.7061,  0.0381],\n",
      "         [-0.0192,  0.5875,  0.5092,  ...,  0.6112, -0.0051,  0.0655],\n",
      "         [ 0.0144, -0.3074,  0.3104,  ...,  0.3328, -0.2117,  0.0098]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.1067,  0.0272,  0.0495,  ..., -0.5550,  0.3449,  0.0266],\n",
      "         [-0.3662, -0.7071,  0.6424,  ..., -0.0091,  0.4016, -0.4627],\n",
      "         [-0.1485, -0.6091, -0.0579,  ..., -0.3597, -0.0659, -0.3385],\n",
      "         ...,\n",
      "         [ 0.1800, -0.1329,  0.1089,  ..., -0.2977, -0.7061,  0.0381],\n",
      "         [-0.0192,  0.5875,  0.5092,  ...,  0.6112, -0.0051,  0.0655],\n",
      "         [ 0.0144, -0.3074,  0.3104,  ...,  0.3328, -0.2117,  0.0098]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-5.9201e-02,  7.3065e-02,  4.9995e-02,  ..., -5.5170e-01,\n",
      "           3.5407e-01,  5.9514e-02],\n",
      "         [-3.9473e-01, -6.7096e-01,  5.2268e-01,  ..., -3.8239e-02,\n",
      "           5.1952e-01, -7.7044e-01],\n",
      "         [ 1.3249e-02, -2.9771e-01, -9.4769e-02,  ..., -1.6937e-01,\n",
      "           2.3542e-01, -2.6554e-01],\n",
      "         ...,\n",
      "         [-5.7455e-02,  1.0986e-01,  8.3067e-02,  ..., -2.2996e-01,\n",
      "          -5.7717e-01, -4.1364e-02],\n",
      "         [-6.1361e-02,  2.8114e-01,  6.4328e-01,  ...,  7.1466e-01,\n",
      "           2.2241e-01, -6.9023e-04],\n",
      "         [ 5.4605e-02, -7.7369e-01,  3.6195e-01,  ...,  3.0117e-01,\n",
      "          -1.0809e-02, -2.0259e-01]]], device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-5.9201e-02,  7.3065e-02,  4.9995e-02,  ..., -5.5170e-01,\n",
      "           3.5407e-01,  5.9514e-02],\n",
      "         [-3.9473e-01, -6.7096e-01,  5.2268e-01,  ..., -3.8239e-02,\n",
      "           5.1952e-01, -7.7044e-01],\n",
      "         [ 1.3249e-02, -2.9771e-01, -9.4769e-02,  ..., -1.6937e-01,\n",
      "           2.3542e-01, -2.6554e-01],\n",
      "         ...,\n",
      "         [-5.7455e-02,  1.0986e-01,  8.3067e-02,  ..., -2.2996e-01,\n",
      "          -5.7717e-01, -4.1364e-02],\n",
      "         [-6.1361e-02,  2.8114e-01,  6.4328e-01,  ...,  7.1466e-01,\n",
      "           2.2241e-01, -6.9023e-04],\n",
      "         [ 5.4605e-02, -7.7369e-01,  3.6195e-01,  ...,  3.0117e-01,\n",
      "          -1.0809e-02, -2.0259e-01]]], device='cuda:0'),) and output (tensor([[[ 0.1143,  0.2237,  0.3292,  ..., -0.7706,  0.2608,  0.0269],\n",
      "         [-0.3155, -0.7192,  0.3833,  ..., -0.2039,  0.5686, -0.6474],\n",
      "         [ 0.0969, -0.6729, -0.0125,  ...,  0.0774,  0.3884, -0.4466],\n",
      "         ...,\n",
      "         [-0.0481, -0.1627, -0.1737,  ..., -0.1994, -0.8209, -0.1401],\n",
      "         [-0.0808, -0.2871,  0.3949,  ...,  1.1005,  0.0770,  0.1227],\n",
      "         [-0.1228, -0.9662,  0.0882,  ...,  0.4491,  0.0432,  0.1395]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[ 0.1143,  0.2237,  0.3292,  ..., -0.7706,  0.2608,  0.0269],\n",
      "         [-0.3155, -0.7192,  0.3833,  ..., -0.2039,  0.5686, -0.6474],\n",
      "         [ 0.0969, -0.6729, -0.0125,  ...,  0.0774,  0.3884, -0.4466],\n",
      "         ...,\n",
      "         [-0.0481, -0.1627, -0.1737,  ..., -0.1994, -0.8209, -0.1401],\n",
      "         [-0.0808, -0.2871,  0.3949,  ...,  1.1005,  0.0770,  0.1227],\n",
      "         [-0.1228, -0.9662,  0.0882,  ...,  0.4491,  0.0432,  0.1395]]],\n",
      "       device='cuda:0'),) and output (tensor([[[ 0.3711,  0.5489,  0.6100,  ..., -0.7499,  0.2080,  0.4195],\n",
      "         [-0.4105, -1.0374,  0.1210,  ..., -0.2601,  0.5486, -1.1186],\n",
      "         [ 0.2439, -1.1054, -0.0569,  ...,  0.5386,  0.4401, -0.6132],\n",
      "         ...,\n",
      "         [-0.1362, -0.2430, -0.1406,  ..., -0.3721, -0.9175, -0.0072],\n",
      "         [-0.1002, -0.1115,  0.5318,  ...,  0.9252,  0.1220, -0.3557],\n",
      "         [-0.4784, -1.3794, -0.1194,  ...,  0.2517, -0.0097, -0.1948]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[ 0.3711,  0.5489,  0.6100,  ..., -0.7499,  0.2080,  0.4195],\n",
      "         [-0.4105, -1.0374,  0.1210,  ..., -0.2601,  0.5486, -1.1186],\n",
      "         [ 0.2439, -1.1054, -0.0569,  ...,  0.5386,  0.4401, -0.6132],\n",
      "         ...,\n",
      "         [-0.1362, -0.2430, -0.1406,  ..., -0.3721, -0.9175, -0.0072],\n",
      "         [-0.1002, -0.1115,  0.5318,  ...,  0.9252,  0.1220, -0.3557],\n",
      "         [-0.4784, -1.3794, -0.1194,  ...,  0.2517, -0.0097, -0.1948]]],\n",
      "       device='cuda:0'),) and output (tensor([[[ 1.1544,  3.2965,  1.6534,  ..., -2.7009,  2.6553,  2.2700],\n",
      "         [-0.2898, -0.6099,  0.7351,  ..., -0.3747, -0.1241, -0.6505],\n",
      "         [ 0.6060, -1.4497, -0.8861,  ..., -0.0638, -0.1957, -1.0963],\n",
      "         ...,\n",
      "         [-0.4061,  0.2532,  0.1007,  ...,  0.2400, -0.8998,  0.2098],\n",
      "         [-0.2175, -0.7473,  1.5864,  ...,  1.9646, -0.3372,  0.3002],\n",
      "         [-0.6077, -2.8554,  0.3996,  ...,  1.6731, -0.6255,  0.1986]]],\n",
      "       device='cuda:0'),)\n",
      "[Debug] Layer names being passed: ['model.layers.0', 'model.layers.1', 'model.layers.2', 'model.layers.3', 'model.layers.4', 'model.layers.5', 'model.layers.6', 'model.layers.7', 'model.layers.8', 'model.layers.9', 'model.layers.10', 'model.layers.11', 'model.layers.12', 'model.layers.13', 'model.layers.14', 'model.layers.15', 'model.layers.16', 'model.layers.17', 'model.layers.18', 'model.layers.19', 'model.layers.20', 'model.layers.21', 'model.layers.22', 'model.layers.23', 'model.layers.24', 'model.layers.25', 'model.layers.26', 'model.layers.27', 'model.layers.28', 'model.layers.29', 'model.layers.30', 'model.layers.31', 'model.embed_tokens']\n",
      "[Debug] Trying to access layer: model.layers.0\n",
      "[Debug] Successfully found layer: model.layers.0\n",
      "[Debug] Trying to access layer: model.layers.1\n",
      "[Debug] Successfully found layer: model.layers.1\n",
      "[Debug] Trying to access layer: model.layers.2\n",
      "[Debug] Successfully found layer: model.layers.2\n",
      "[Debug] Trying to access layer: model.layers.3\n",
      "[Debug] Successfully found layer: model.layers.3\n",
      "[Debug] Trying to access layer: model.layers.4\n",
      "[Debug] Successfully found layer: model.layers.4\n",
      "[Debug] Trying to access layer: model.layers.5\n",
      "[Debug] Successfully found layer: model.layers.5\n",
      "[Debug] Trying to access layer: model.layers.6\n",
      "[Debug] Successfully found layer: model.layers.6\n",
      "[Debug] Trying to access layer: model.layers.7\n",
      "[Debug] Successfully found layer: model.layers.7\n",
      "[Debug] Trying to access layer: model.layers.8\n",
      "[Debug] Successfully found layer: model.layers.8\n",
      "[Debug] Trying to access layer: model.layers.9\n",
      "[Debug] Successfully found layer: model.layers.9\n",
      "[Debug] Trying to access layer: model.layers.10\n",
      "[Debug] Successfully found layer: model.layers.10\n",
      "[Debug] Trying to access layer: model.layers.11\n",
      "[Debug] Successfully found layer: model.layers.11\n",
      "[Debug] Trying to access layer: model.layers.12\n",
      "[Debug] Successfully found layer: model.layers.12\n",
      "[Debug] Trying to access layer: model.layers.13\n",
      "[Debug] Successfully found layer: model.layers.13\n",
      "[Debug] Trying to access layer: model.layers.14\n",
      "[Debug] Successfully found layer: model.layers.14\n",
      "[Debug] Trying to access layer: model.layers.15\n",
      "[Debug] Successfully found layer: model.layers.15\n",
      "[Debug] Trying to access layer: model.layers.16\n",
      "[Debug] Successfully found layer: model.layers.16\n",
      "[Debug] Trying to access layer: model.layers.17\n",
      "[Debug] Successfully found layer: model.layers.17\n",
      "[Debug] Trying to access layer: model.layers.18\n",
      "[Debug] Successfully found layer: model.layers.18\n",
      "[Debug] Trying to access layer: model.layers.19\n",
      "[Debug] Successfully found layer: model.layers.19\n",
      "[Debug] Trying to access layer: model.layers.20\n",
      "[Debug] Successfully found layer: model.layers.20\n",
      "[Debug] Trying to access layer: model.layers.21\n",
      "[Debug] Successfully found layer: model.layers.21\n",
      "[Debug] Trying to access layer: model.layers.22\n",
      "[Debug] Successfully found layer: model.layers.22\n",
      "[Debug] Trying to access layer: model.layers.23\n",
      "[Debug] Successfully found layer: model.layers.23\n",
      "[Debug] Trying to access layer: model.layers.24\n",
      "[Debug] Successfully found layer: model.layers.24\n",
      "[Debug] Trying to access layer: model.layers.25\n",
      "[Debug] Successfully found layer: model.layers.25\n",
      "[Debug] Trying to access layer: model.layers.26\n",
      "[Debug] Successfully found layer: model.layers.26\n",
      "[Debug] Trying to access layer: model.layers.27\n",
      "[Debug] Successfully found layer: model.layers.27\n",
      "[Debug] Trying to access layer: model.layers.28\n",
      "[Debug] Successfully found layer: model.layers.28\n",
      "[Debug] Trying to access layer: model.layers.29\n",
      "[Debug] Successfully found layer: model.layers.29\n",
      "[Debug] Trying to access layer: model.layers.30\n",
      "[Debug] Successfully found layer: model.layers.30\n",
      "[Debug] Trying to access layer: model.layers.31\n",
      "[Debug] Successfully found layer: model.layers.31\n",
      "[Debug] Trying to access layer: model.embed_tokens\n",
      "[Debug] Successfully found layer: model.embed_tokens\n",
      "[Hook] Layer Embedding(128256, 4096) received input (tensor([[128000,  22427,    661,    315,    264,    468,    318,   3368,  32666,\n",
      "            320,  31255,      8,  16807,   5424,    315,  53125,    315,    264,\n",
      "            468,    318,   3368,  32666,    574,    304,  23393,    323,  20037,\n",
      "            709,    389,   6664,    220,    845,     11,    220,   1049,     24,\n",
      "             13]], device='cuda:0'),) and output tensor([[[ 2.6512e-04, -4.9973e-04, -5.8365e-04,  ...,  3.8147e-03,\n",
      "           6.3419e-05,  1.1902e-03],\n",
      "         [-7.1106e-03, -3.2501e-03, -1.5747e-02,  ...,  1.2390e-02,\n",
      "           2.1973e-02, -1.2360e-03],\n",
      "         [-1.9684e-03,  3.7766e-04,  7.0801e-03,  ..., -6.5613e-03,\n",
      "           5.9814e-03,  9.8877e-03],\n",
      "         ...,\n",
      "         [-3.7231e-03,  1.6708e-03, -2.8687e-03,  ...,  8.2397e-04,\n",
      "          -6.2866e-03, -5.1117e-04],\n",
      "         [-8.5449e-03,  2.0905e-03, -3.7956e-04,  ...,  1.1719e-02,\n",
      "           1.4191e-03,  9.8877e-03],\n",
      "         [-6.7520e-04, -5.7602e-04,  4.7684e-04,  ...,  3.3264e-03,\n",
      "           3.9101e-04,  4.7493e-04]]], device='cuda:0')\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[ 2.6512e-04, -4.9973e-04, -5.8365e-04,  ...,  3.8147e-03,\n",
      "           6.3419e-05,  1.1902e-03],\n",
      "         [-7.1106e-03, -3.2501e-03, -1.5747e-02,  ...,  1.2390e-02,\n",
      "           2.1973e-02, -1.2360e-03],\n",
      "         [-1.9684e-03,  3.7766e-04,  7.0801e-03,  ..., -6.5613e-03,\n",
      "           5.9814e-03,  9.8877e-03],\n",
      "         ...,\n",
      "         [-3.7231e-03,  1.6708e-03, -2.8687e-03,  ...,  8.2397e-04,\n",
      "          -6.2866e-03, -5.1117e-04],\n",
      "         [-8.5449e-03,  2.0905e-03, -3.7956e-04,  ...,  1.1719e-02,\n",
      "           1.4191e-03,  9.8877e-03],\n",
      "         [-6.7520e-04, -5.7602e-04,  4.7684e-04,  ...,  3.3264e-03,\n",
      "           3.9101e-04,  4.7493e-04]]], device='cuda:0'),) and output (tensor([[[-0.0004,  0.0146, -0.0020,  ...,  0.0066, -0.0194,  0.0020],\n",
      "         [-0.0045,  0.0012, -0.0142,  ..., -0.0226,  0.0138,  0.0012],\n",
      "         [-0.0341,  0.0056,  0.0363,  ..., -0.0620,  0.0215,  0.0211],\n",
      "         ...,\n",
      "         [ 0.0055,  0.0009, -0.0020,  ..., -0.0280, -0.0133,  0.0153],\n",
      "         [-0.0178,  0.0145, -0.0090,  ..., -0.0132,  0.0089,  0.0281],\n",
      "         [-0.0026,  0.0162,  0.0005,  ..., -0.0062, -0.0007,  0.0111]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.0004,  0.0146, -0.0020,  ...,  0.0066, -0.0194,  0.0020],\n",
      "         [-0.0045,  0.0012, -0.0142,  ..., -0.0226,  0.0138,  0.0012],\n",
      "         [-0.0341,  0.0056,  0.0363,  ..., -0.0620,  0.0215,  0.0211],\n",
      "         ...,\n",
      "         [ 0.0055,  0.0009, -0.0020,  ..., -0.0280, -0.0133,  0.0153],\n",
      "         [-0.0178,  0.0145, -0.0090,  ..., -0.0132,  0.0089,  0.0281],\n",
      "         [-0.0026,  0.0162,  0.0005,  ..., -0.0062, -0.0007,  0.0111]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.0972,  0.0787, -0.0419,  ...,  0.0071,  0.0869,  0.0218],\n",
      "         [-0.0178,  0.0126,  0.0009,  ..., -0.0288, -0.0032,  0.0043],\n",
      "         [-0.0162, -0.0045,  0.0388,  ..., -0.0898,  0.0079,  0.0380],\n",
      "         ...,\n",
      "         [ 0.0039, -0.0041,  0.0261,  ..., -0.0169, -0.0329,  0.0128],\n",
      "         [-0.0136,  0.0060, -0.0316,  ..., -0.0245,  0.0014,  0.0202],\n",
      "         [ 0.0048,  0.0081, -0.0044,  ...,  0.0007, -0.0052,  0.0151]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.0972,  0.0787, -0.0419,  ...,  0.0071,  0.0869,  0.0218],\n",
      "         [-0.0178,  0.0126,  0.0009,  ..., -0.0288, -0.0032,  0.0043],\n",
      "         [-0.0162, -0.0045,  0.0388,  ..., -0.0898,  0.0079,  0.0380],\n",
      "         ...,\n",
      "         [ 0.0039, -0.0041,  0.0261,  ..., -0.0169, -0.0329,  0.0128],\n",
      "         [-0.0136,  0.0060, -0.0316,  ..., -0.0245,  0.0014,  0.0202],\n",
      "         [ 0.0048,  0.0081, -0.0044,  ...,  0.0007, -0.0052,  0.0151]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.1040,  0.0875, -0.0360,  ...,  0.0205,  0.0997,  0.0184],\n",
      "         [-0.0101,  0.0030,  0.0048,  ..., -0.0378, -0.0133, -0.0139],\n",
      "         [-0.0125,  0.0134,  0.0322,  ..., -0.0869, -0.0253,  0.0552],\n",
      "         ...,\n",
      "         [-0.0064, -0.0286,  0.0368,  ..., -0.0887, -0.0424,  0.0173],\n",
      "         [-0.0025,  0.0104, -0.0229,  ..., -0.0040,  0.0078,  0.0487],\n",
      "         [-0.0071,  0.0108,  0.0149,  ..., -0.0180, -0.0122,  0.0186]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.1040,  0.0875, -0.0360,  ...,  0.0205,  0.0997,  0.0184],\n",
      "         [-0.0101,  0.0030,  0.0048,  ..., -0.0378, -0.0133, -0.0139],\n",
      "         [-0.0125,  0.0134,  0.0322,  ..., -0.0869, -0.0253,  0.0552],\n",
      "         ...,\n",
      "         [-0.0064, -0.0286,  0.0368,  ..., -0.0887, -0.0424,  0.0173],\n",
      "         [-0.0025,  0.0104, -0.0229,  ..., -0.0040,  0.0078,  0.0487],\n",
      "         [-0.0071,  0.0108,  0.0149,  ..., -0.0180, -0.0122,  0.0186]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.0870,  0.1049, -0.0173,  ...,  0.0700,  0.0989,  0.0181],\n",
      "         [-0.0161,  0.0497,  0.0121,  ..., -0.0672, -0.0130, -0.0199],\n",
      "         [-0.0483,  0.0652,  0.0551,  ..., -0.0621, -0.0572,  0.0146],\n",
      "         ...,\n",
      "         [-0.0389, -0.0630,  0.0323,  ..., -0.1401,  0.0074, -0.0145],\n",
      "         [-0.0128,  0.0074, -0.0098,  ...,  0.0366,  0.0408,  0.0528],\n",
      "         [-0.0137,  0.0008,  0.0065,  ..., -0.0102,  0.0113,  0.0712]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.0870,  0.1049, -0.0173,  ...,  0.0700,  0.0989,  0.0181],\n",
      "         [-0.0161,  0.0497,  0.0121,  ..., -0.0672, -0.0130, -0.0199],\n",
      "         [-0.0483,  0.0652,  0.0551,  ..., -0.0621, -0.0572,  0.0146],\n",
      "         ...,\n",
      "         [-0.0389, -0.0630,  0.0323,  ..., -0.1401,  0.0074, -0.0145],\n",
      "         [-0.0128,  0.0074, -0.0098,  ...,  0.0366,  0.0408,  0.0528],\n",
      "         [-0.0137,  0.0008,  0.0065,  ..., -0.0102,  0.0113,  0.0712]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.1113,  0.1022, -0.0102,  ...,  0.0436,  0.1283,  0.0321],\n",
      "         [-0.0631,  0.0644,  0.0302,  ..., -0.0448, -0.0058,  0.0031],\n",
      "         [-0.0149,  0.0310,  0.1042,  ..., -0.0754,  0.0273,  0.0741],\n",
      "         ...,\n",
      "         [-0.0002, -0.0265,  0.0869,  ..., -0.1070,  0.0086,  0.0167],\n",
      "         [-0.0057,  0.0518,  0.0037,  ...,  0.0225,  0.0417,  0.0713],\n",
      "         [ 0.0199, -0.0052,  0.0857,  ...,  0.0541, -0.0014,  0.0112]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.1113,  0.1022, -0.0102,  ...,  0.0436,  0.1283,  0.0321],\n",
      "         [-0.0631,  0.0644,  0.0302,  ..., -0.0448, -0.0058,  0.0031],\n",
      "         [-0.0149,  0.0310,  0.1042,  ..., -0.0754,  0.0273,  0.0741],\n",
      "         ...,\n",
      "         [-0.0002, -0.0265,  0.0869,  ..., -0.1070,  0.0086,  0.0167],\n",
      "         [-0.0057,  0.0518,  0.0037,  ...,  0.0225,  0.0417,  0.0713],\n",
      "         [ 0.0199, -0.0052,  0.0857,  ...,  0.0541, -0.0014,  0.0112]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.1034,  0.1132,  0.0096,  ...,  0.0453,  0.1477,  0.0301],\n",
      "         [-0.0358,  0.0600, -0.0038,  ..., -0.0607, -0.0233, -0.0038],\n",
      "         [ 0.0167,  0.0515,  0.0931,  ..., -0.1163,  0.0010,  0.0821],\n",
      "         ...,\n",
      "         [-0.0110, -0.0037,  0.1256,  ..., -0.1629,  0.0049, -0.0057],\n",
      "         [ 0.0117,  0.0447,  0.0042,  ..., -0.0057,  0.0554,  0.0206],\n",
      "         [-0.0195,  0.0233,  0.1059,  ...,  0.0124, -0.0175,  0.0005]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.1034,  0.1132,  0.0096,  ...,  0.0453,  0.1477,  0.0301],\n",
      "         [-0.0358,  0.0600, -0.0038,  ..., -0.0607, -0.0233, -0.0038],\n",
      "         [ 0.0167,  0.0515,  0.0931,  ..., -0.1163,  0.0010,  0.0821],\n",
      "         ...,\n",
      "         [-0.0110, -0.0037,  0.1256,  ..., -0.1629,  0.0049, -0.0057],\n",
      "         [ 0.0117,  0.0447,  0.0042,  ..., -0.0057,  0.0554,  0.0206],\n",
      "         [-0.0195,  0.0233,  0.1059,  ...,  0.0124, -0.0175,  0.0005]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-1.0708e-01,  1.2526e-01,  2.1099e-02,  ...,  1.2627e-02,\n",
      "           1.5859e-01,  2.7839e-02],\n",
      "         [-2.1780e-02,  6.4339e-02, -1.0635e-02,  ..., -4.7686e-02,\n",
      "           7.0978e-02,  3.3698e-02],\n",
      "         [-9.2945e-02, -3.1977e-02,  4.0864e-02,  ..., -1.2001e-01,\n",
      "           3.5955e-02,  2.8451e-02],\n",
      "         ...,\n",
      "         [ 1.5122e-02,  1.9449e-02,  1.6297e-01,  ..., -8.3084e-02,\n",
      "          -5.3737e-02, -2.4019e-02],\n",
      "         [-1.3211e-02,  7.1014e-02,  1.9512e-02,  ...,  8.1241e-03,\n",
      "          -1.3785e-04,  3.2638e-02],\n",
      "         [-9.7550e-02,  6.9028e-02,  8.1925e-02,  ...,  4.2871e-02,\n",
      "          -5.4088e-02,  7.6659e-03]]], device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-1.0708e-01,  1.2526e-01,  2.1099e-02,  ...,  1.2627e-02,\n",
      "           1.5859e-01,  2.7839e-02],\n",
      "         [-2.1780e-02,  6.4339e-02, -1.0635e-02,  ..., -4.7686e-02,\n",
      "           7.0978e-02,  3.3698e-02],\n",
      "         [-9.2945e-02, -3.1977e-02,  4.0864e-02,  ..., -1.2001e-01,\n",
      "           3.5955e-02,  2.8451e-02],\n",
      "         ...,\n",
      "         [ 1.5122e-02,  1.9449e-02,  1.6297e-01,  ..., -8.3084e-02,\n",
      "          -5.3737e-02, -2.4019e-02],\n",
      "         [-1.3211e-02,  7.1014e-02,  1.9512e-02,  ...,  8.1241e-03,\n",
      "          -1.3785e-04,  3.2638e-02],\n",
      "         [-9.7550e-02,  6.9028e-02,  8.1925e-02,  ...,  4.2871e-02,\n",
      "          -5.4088e-02,  7.6659e-03]]], device='cuda:0'),) and output (tensor([[[-0.0945,  0.1848,  0.0036,  ..., -0.0473,  0.1489,  0.0536],\n",
      "         [ 0.0400,  0.0313, -0.0412,  ..., -0.1621, -0.0857, -0.0284],\n",
      "         [-0.0806, -0.0891,  0.0651,  ..., -0.2541,  0.0145,  0.0511],\n",
      "         ...,\n",
      "         [-0.0059, -0.0777,  0.1325,  ..., -0.0641, -0.0367,  0.0046],\n",
      "         [-0.0641,  0.0497, -0.0165,  ...,  0.0393,  0.0261, -0.0219],\n",
      "         [-0.0371,  0.0751,  0.0703,  ...,  0.0470, -0.0431, -0.0244]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.0945,  0.1848,  0.0036,  ..., -0.0473,  0.1489,  0.0536],\n",
      "         [ 0.0400,  0.0313, -0.0412,  ..., -0.1621, -0.0857, -0.0284],\n",
      "         [-0.0806, -0.0891,  0.0651,  ..., -0.2541,  0.0145,  0.0511],\n",
      "         ...,\n",
      "         [-0.0059, -0.0777,  0.1325,  ..., -0.0641, -0.0367,  0.0046],\n",
      "         [-0.0641,  0.0497, -0.0165,  ...,  0.0393,  0.0261, -0.0219],\n",
      "         [-0.0371,  0.0751,  0.0703,  ...,  0.0470, -0.0431, -0.0244]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.1060,  0.2113,  0.0270,  ..., -0.1141,  0.1610,  0.0528],\n",
      "         [ 0.0322,  0.0975, -0.0534,  ..., -0.1238, -0.1068, -0.0611],\n",
      "         [-0.0450, -0.1430,  0.0853,  ..., -0.3321,  0.0663, -0.0121],\n",
      "         ...,\n",
      "         [ 0.0526, -0.0502,  0.1681,  ..., -0.0870, -0.0416, -0.0618],\n",
      "         [-0.1083, -0.0077,  0.0482,  ...,  0.0100,  0.0485, -0.0517],\n",
      "         [-0.0072,  0.0155,  0.1079,  ...,  0.1919, -0.0785,  0.0180]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.1060,  0.2113,  0.0270,  ..., -0.1141,  0.1610,  0.0528],\n",
      "         [ 0.0322,  0.0975, -0.0534,  ..., -0.1238, -0.1068, -0.0611],\n",
      "         [-0.0450, -0.1430,  0.0853,  ..., -0.3321,  0.0663, -0.0121],\n",
      "         ...,\n",
      "         [ 0.0526, -0.0502,  0.1681,  ..., -0.0870, -0.0416, -0.0618],\n",
      "         [-0.1083, -0.0077,  0.0482,  ...,  0.0100,  0.0485, -0.0517],\n",
      "         [-0.0072,  0.0155,  0.1079,  ...,  0.1919, -0.0785,  0.0180]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.0954,  0.2493,  0.0073,  ..., -0.1568,  0.1818,  0.0855],\n",
      "         [-0.0430,  0.0725, -0.0097,  ..., -0.0994, -0.0573, -0.0709],\n",
      "         [-0.1377, -0.1730,  0.0936,  ..., -0.2822,  0.1049, -0.0371],\n",
      "         ...,\n",
      "         [ 0.1065, -0.0291,  0.0370,  ..., -0.0994, -0.0992, -0.0145],\n",
      "         [ 0.0053,  0.0691, -0.0183,  ...,  0.0226,  0.0469,  0.0299],\n",
      "         [ 0.0020,  0.0035,  0.1160,  ...,  0.1815,  0.0317,  0.0784]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.0954,  0.2493,  0.0073,  ..., -0.1568,  0.1818,  0.0855],\n",
      "         [-0.0430,  0.0725, -0.0097,  ..., -0.0994, -0.0573, -0.0709],\n",
      "         [-0.1377, -0.1730,  0.0936,  ..., -0.2822,  0.1049, -0.0371],\n",
      "         ...,\n",
      "         [ 0.1065, -0.0291,  0.0370,  ..., -0.0994, -0.0992, -0.0145],\n",
      "         [ 0.0053,  0.0691, -0.0183,  ...,  0.0226,  0.0469,  0.0299],\n",
      "         [ 0.0020,  0.0035,  0.1160,  ...,  0.1815,  0.0317,  0.0784]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.0152,  0.3340,  0.0011,  ..., -0.2750,  0.1933,  0.1034],\n",
      "         [ 0.0465,  0.0605,  0.0097,  ..., -0.1062, -0.0465, -0.0590],\n",
      "         [-0.1121, -0.1846,  0.0635,  ..., -0.3302,  0.0944, -0.0737],\n",
      "         ...,\n",
      "         [ 0.0027, -0.0301,  0.1351,  ..., -0.0537,  0.0177,  0.0133],\n",
      "         [-0.1053,  0.0454, -0.0021,  ...,  0.0526,  0.0592,  0.0243],\n",
      "         [-0.0435,  0.0058,  0.0513,  ...,  0.1338,  0.0461,  0.1530]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.0152,  0.3340,  0.0011,  ..., -0.2750,  0.1933,  0.1034],\n",
      "         [ 0.0465,  0.0605,  0.0097,  ..., -0.1062, -0.0465, -0.0590],\n",
      "         [-0.1121, -0.1846,  0.0635,  ..., -0.3302,  0.0944, -0.0737],\n",
      "         ...,\n",
      "         [ 0.0027, -0.0301,  0.1351,  ..., -0.0537,  0.0177,  0.0133],\n",
      "         [-0.1053,  0.0454, -0.0021,  ...,  0.0526,  0.0592,  0.0243],\n",
      "         [-0.0435,  0.0058,  0.0513,  ...,  0.1338,  0.0461,  0.1530]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.0356,  0.3483, -0.0366,  ..., -0.2962,  0.2088,  0.1065],\n",
      "         [ 0.0196,  0.0194,  0.0073,  ..., -0.1542, -0.0264, -0.0492],\n",
      "         [-0.1009, -0.1688,  0.0159,  ..., -0.4269,  0.1095, -0.1265],\n",
      "         ...,\n",
      "         [ 0.1861, -0.1003,  0.0635,  ..., -0.1663,  0.1114,  0.0204],\n",
      "         [-0.0673,  0.0553,  0.0507,  ...,  0.0226,  0.1136,  0.0248],\n",
      "         [-0.0321, -0.0312, -0.0081,  ...,  0.1463,  0.1144,  0.0587]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.0356,  0.3483, -0.0366,  ..., -0.2962,  0.2088,  0.1065],\n",
      "         [ 0.0196,  0.0194,  0.0073,  ..., -0.1542, -0.0264, -0.0492],\n",
      "         [-0.1009, -0.1688,  0.0159,  ..., -0.4269,  0.1095, -0.1265],\n",
      "         ...,\n",
      "         [ 0.1861, -0.1003,  0.0635,  ..., -0.1663,  0.1114,  0.0204],\n",
      "         [-0.0673,  0.0553,  0.0507,  ...,  0.0226,  0.1136,  0.0248],\n",
      "         [-0.0321, -0.0312, -0.0081,  ...,  0.1463,  0.1144,  0.0587]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.0127,  0.3661, -0.0316,  ..., -0.3678,  0.1973,  0.1019],\n",
      "         [ 0.0006,  0.0010, -0.0135,  ..., -0.1590, -0.0790,  0.0049],\n",
      "         [-0.0762, -0.1741,  0.0408,  ..., -0.3478,  0.0966, -0.0936],\n",
      "         ...,\n",
      "         [ 0.0769, -0.2064, -0.0203,  ..., -0.1605,  0.1674,  0.0898],\n",
      "         [-0.0899, -0.0132,  0.0561,  ...,  0.0279,  0.1166,  0.0159],\n",
      "         [-0.1416, -0.0063, -0.0217,  ...,  0.1958,  0.1119,  0.1283]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.0127,  0.3661, -0.0316,  ..., -0.3678,  0.1973,  0.1019],\n",
      "         [ 0.0006,  0.0010, -0.0135,  ..., -0.1590, -0.0790,  0.0049],\n",
      "         [-0.0762, -0.1741,  0.0408,  ..., -0.3478,  0.0966, -0.0936],\n",
      "         ...,\n",
      "         [ 0.0769, -0.2064, -0.0203,  ..., -0.1605,  0.1674,  0.0898],\n",
      "         [-0.0899, -0.0132,  0.0561,  ...,  0.0279,  0.1166,  0.0159],\n",
      "         [-0.1416, -0.0063, -0.0217,  ...,  0.1958,  0.1119,  0.1283]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.0089,  0.3233, -0.0779,  ..., -0.4491,  0.1845,  0.0951],\n",
      "         [-0.0332, -0.0295, -0.0243,  ..., -0.1849, -0.1101, -0.0521],\n",
      "         [-0.0743, -0.1288, -0.0178,  ..., -0.4372,  0.1056, -0.1056],\n",
      "         ...,\n",
      "         [ 0.0079, -0.1417, -0.1426,  ..., -0.1494,  0.1339,  0.1112],\n",
      "         [-0.1600, -0.1112,  0.1023,  ..., -0.0558,  0.0507,  0.1401],\n",
      "         [-0.1673, -0.0073,  0.0891,  ...,  0.0420,  0.0784,  0.1698]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.0089,  0.3233, -0.0779,  ..., -0.4491,  0.1845,  0.0951],\n",
      "         [-0.0332, -0.0295, -0.0243,  ..., -0.1849, -0.1101, -0.0521],\n",
      "         [-0.0743, -0.1288, -0.0178,  ..., -0.4372,  0.1056, -0.1056],\n",
      "         ...,\n",
      "         [ 0.0079, -0.1417, -0.1426,  ..., -0.1494,  0.1339,  0.1112],\n",
      "         [-0.1600, -0.1112,  0.1023,  ..., -0.0558,  0.0507,  0.1401],\n",
      "         [-0.1673, -0.0073,  0.0891,  ...,  0.0420,  0.0784,  0.1698]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.0244,  0.2828, -0.0532,  ..., -0.4592,  0.2300,  0.0142],\n",
      "         [-0.0406, -0.0868, -0.0426,  ..., -0.1495, -0.0961, -0.0712],\n",
      "         [-0.0659, -0.1812, -0.0483,  ..., -0.4676,  0.1255, -0.1617],\n",
      "         ...,\n",
      "         [-0.0307, -0.0484, -0.0232,  ..., -0.1024,  0.0871,  0.0706],\n",
      "         [-0.1938, -0.0910,  0.0785,  ..., -0.0362,  0.1169,  0.1242],\n",
      "         [-0.0043,  0.0601,  0.0871,  ...,  0.0763,  0.0076,  0.1286]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.0244,  0.2828, -0.0532,  ..., -0.4592,  0.2300,  0.0142],\n",
      "         [-0.0406, -0.0868, -0.0426,  ..., -0.1495, -0.0961, -0.0712],\n",
      "         [-0.0659, -0.1812, -0.0483,  ..., -0.4676,  0.1255, -0.1617],\n",
      "         ...,\n",
      "         [-0.0307, -0.0484, -0.0232,  ..., -0.1024,  0.0871,  0.0706],\n",
      "         [-0.1938, -0.0910,  0.0785,  ..., -0.0362,  0.1169,  0.1242],\n",
      "         [-0.0043,  0.0601,  0.0871,  ...,  0.0763,  0.0076,  0.1286]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.0928,  0.1485,  0.0158,  ..., -0.5380,  0.2664, -0.0333],\n",
      "         [-0.0828, -0.1499, -0.0833,  ..., -0.1196, -0.0415, -0.1476],\n",
      "         [-0.0117, -0.1342, -0.0882,  ..., -0.4517,  0.1391, -0.1484],\n",
      "         ...,\n",
      "         [-0.0800, -0.1092,  0.0076,  ..., -0.1140,  0.0457,  0.0411],\n",
      "         [-0.1899, -0.0826,  0.1796,  ..., -0.0498, -0.0343,  0.0470],\n",
      "         [-0.0347, -0.0160,  0.0796,  ...,  0.0909, -0.1469,  0.1104]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.0928,  0.1485,  0.0158,  ..., -0.5380,  0.2664, -0.0333],\n",
      "         [-0.0828, -0.1499, -0.0833,  ..., -0.1196, -0.0415, -0.1476],\n",
      "         [-0.0117, -0.1342, -0.0882,  ..., -0.4517,  0.1391, -0.1484],\n",
      "         ...,\n",
      "         [-0.0800, -0.1092,  0.0076,  ..., -0.1140,  0.0457,  0.0411],\n",
      "         [-0.1899, -0.0826,  0.1796,  ..., -0.0498, -0.0343,  0.0470],\n",
      "         [-0.0347, -0.0160,  0.0796,  ...,  0.0909, -0.1469,  0.1104]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.1000,  0.1323,  0.0241,  ..., -0.5470,  0.2844, -0.0457],\n",
      "         [-0.0412, -0.1152, -0.0662,  ..., -0.0821, -0.1176, -0.2111],\n",
      "         [ 0.0018, -0.1828,  0.0045,  ..., -0.4385,  0.0877, -0.1388],\n",
      "         ...,\n",
      "         [-0.2093,  0.0037,  0.0497,  ..., -0.1902,  0.0458, -0.0236],\n",
      "         [-0.0229, -0.0867,  0.1013,  ...,  0.0892,  0.0081,  0.1611],\n",
      "         [ 0.0428,  0.1714,  0.1764,  ...,  0.1976, -0.2028,  0.0809]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.1000,  0.1323,  0.0241,  ..., -0.5470,  0.2844, -0.0457],\n",
      "         [-0.0412, -0.1152, -0.0662,  ..., -0.0821, -0.1176, -0.2111],\n",
      "         [ 0.0018, -0.1828,  0.0045,  ..., -0.4385,  0.0877, -0.1388],\n",
      "         ...,\n",
      "         [-0.2093,  0.0037,  0.0497,  ..., -0.1902,  0.0458, -0.0236],\n",
      "         [-0.0229, -0.0867,  0.1013,  ...,  0.0892,  0.0081,  0.1611],\n",
      "         [ 0.0428,  0.1714,  0.1764,  ...,  0.1976, -0.2028,  0.0809]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.1286,  0.0833,  0.0239,  ..., -0.5657,  0.3694, -0.0725],\n",
      "         [-0.0608, -0.1557, -0.0394,  ..., -0.2385, -0.2481, -0.2570],\n",
      "         [-0.0600, -0.2146, -0.1267,  ..., -0.5606,  0.0696, -0.1515],\n",
      "         ...,\n",
      "         [-0.2841, -0.0755, -0.1853,  ..., -0.1984,  0.0148, -0.1550],\n",
      "         [-0.0312, -0.1022,  0.1364,  ..., -0.0657, -0.0900,  0.2026],\n",
      "         [ 0.0606,  0.0766,  0.1521,  ...,  0.0268, -0.2434, -0.0994]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.1286,  0.0833,  0.0239,  ..., -0.5657,  0.3694, -0.0725],\n",
      "         [-0.0608, -0.1557, -0.0394,  ..., -0.2385, -0.2481, -0.2570],\n",
      "         [-0.0600, -0.2146, -0.1267,  ..., -0.5606,  0.0696, -0.1515],\n",
      "         ...,\n",
      "         [-0.2841, -0.0755, -0.1853,  ..., -0.1984,  0.0148, -0.1550],\n",
      "         [-0.0312, -0.1022,  0.1364,  ..., -0.0657, -0.0900,  0.2026],\n",
      "         [ 0.0606,  0.0766,  0.1521,  ...,  0.0268, -0.2434, -0.0994]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.1417,  0.0703,  0.0343,  ..., -0.5549,  0.3175, -0.0847],\n",
      "         [-0.0498, -0.2699, -0.1379,  ..., -0.2048, -0.0676, -0.3681],\n",
      "         [-0.1116, -0.3004, -0.0837,  ..., -0.4514,  0.1139,  0.0010],\n",
      "         ...,\n",
      "         [-0.1577, -0.1777, -0.1424,  ..., -0.1876,  0.0784, -0.1924],\n",
      "         [-0.1137, -0.0402,  0.2112,  ..., -0.0546, -0.1095,  0.2893],\n",
      "         [ 0.0757,  0.1531,  0.2018,  ...,  0.1022, -0.1794, -0.1630]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.1417,  0.0703,  0.0343,  ..., -0.5549,  0.3175, -0.0847],\n",
      "         [-0.0498, -0.2699, -0.1379,  ..., -0.2048, -0.0676, -0.3681],\n",
      "         [-0.1116, -0.3004, -0.0837,  ..., -0.4514,  0.1139,  0.0010],\n",
      "         ...,\n",
      "         [-0.1577, -0.1777, -0.1424,  ..., -0.1876,  0.0784, -0.1924],\n",
      "         [-0.1137, -0.0402,  0.2112,  ..., -0.0546, -0.1095,  0.2893],\n",
      "         [ 0.0757,  0.1531,  0.2018,  ...,  0.1022, -0.1794, -0.1630]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.1528,  0.0725,  0.0609,  ..., -0.5624,  0.3103, -0.0293],\n",
      "         [-0.1066, -0.3065, -0.1763,  ..., -0.1713,  0.1388, -0.5423],\n",
      "         [-0.0950, -0.1625, -0.0920,  ..., -0.4799,  0.0909, -0.0095],\n",
      "         ...,\n",
      "         [-0.1631, -0.0928, -0.0040,  ..., -0.0448,  0.1826, -0.2434],\n",
      "         [-0.1885, -0.0956,  0.0986,  ..., -0.1335, -0.0946,  0.2613],\n",
      "         [ 0.0683,  0.1002,  0.1216,  ...,  0.0398, -0.0483, -0.0794]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.1528,  0.0725,  0.0609,  ..., -0.5624,  0.3103, -0.0293],\n",
      "         [-0.1066, -0.3065, -0.1763,  ..., -0.1713,  0.1388, -0.5423],\n",
      "         [-0.0950, -0.1625, -0.0920,  ..., -0.4799,  0.0909, -0.0095],\n",
      "         ...,\n",
      "         [-0.1631, -0.0928, -0.0040,  ..., -0.0448,  0.1826, -0.2434],\n",
      "         [-0.1885, -0.0956,  0.0986,  ..., -0.1335, -0.0946,  0.2613],\n",
      "         [ 0.0683,  0.1002,  0.1216,  ...,  0.0398, -0.0483, -0.0794]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.1559,  0.0520,  0.0566,  ..., -0.5805,  0.2614, -0.0032],\n",
      "         [-0.0800, -0.3334, -0.2276,  ..., -0.1455,  0.1198, -0.5992],\n",
      "         [-0.0173, -0.2193,  0.0424,  ..., -0.5072,  0.0463, -0.1627],\n",
      "         ...,\n",
      "         [-0.3465, -0.1676, -0.1385,  ..., -0.1482,  0.1667, -0.1248],\n",
      "         [-0.1902, -0.2594,  0.1460,  ..., -0.0281, -0.1627,  0.3282],\n",
      "         [ 0.1237, -0.0401,  0.0591,  ...,  0.0561, -0.1990, -0.0816]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.1559,  0.0520,  0.0566,  ..., -0.5805,  0.2614, -0.0032],\n",
      "         [-0.0800, -0.3334, -0.2276,  ..., -0.1455,  0.1198, -0.5992],\n",
      "         [-0.0173, -0.2193,  0.0424,  ..., -0.5072,  0.0463, -0.1627],\n",
      "         ...,\n",
      "         [-0.3465, -0.1676, -0.1385,  ..., -0.1482,  0.1667, -0.1248],\n",
      "         [-0.1902, -0.2594,  0.1460,  ..., -0.0281, -0.1627,  0.3282],\n",
      "         [ 0.1237, -0.0401,  0.0591,  ...,  0.0561, -0.1990, -0.0816]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.1488,  0.0445,  0.0765,  ..., -0.5456,  0.2702,  0.0098],\n",
      "         [-0.0761, -0.1535, -0.1397,  ..., -0.0169,  0.0272, -0.5999],\n",
      "         [-0.0113, -0.1822,  0.0195,  ..., -0.4874,  0.0065, -0.1261],\n",
      "         ...,\n",
      "         [-0.2639, -0.1965, -0.1153,  ..., -0.2283,  0.0587, -0.2021],\n",
      "         [-0.2577, -0.2688,  0.0463,  ...,  0.0519, -0.3024,  0.3125],\n",
      "         [ 0.0741, -0.0870, -0.0839,  ...,  0.0026, -0.2004, -0.1856]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.1488,  0.0445,  0.0765,  ..., -0.5456,  0.2702,  0.0098],\n",
      "         [-0.0761, -0.1535, -0.1397,  ..., -0.0169,  0.0272, -0.5999],\n",
      "         [-0.0113, -0.1822,  0.0195,  ..., -0.4874,  0.0065, -0.1261],\n",
      "         ...,\n",
      "         [-0.2639, -0.1965, -0.1153,  ..., -0.2283,  0.0587, -0.2021],\n",
      "         [-0.2577, -0.2688,  0.0463,  ...,  0.0519, -0.3024,  0.3125],\n",
      "         [ 0.0741, -0.0870, -0.0839,  ...,  0.0026, -0.2004, -0.1856]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-1.2677e-01,  3.3335e-02,  9.3478e-02,  ..., -5.3204e-01,\n",
      "           2.7953e-01,  5.9797e-04],\n",
      "         [ 1.1361e-02, -1.6972e-01,  4.4909e-02,  ..., -1.0362e-01,\n",
      "           6.1695e-02, -8.8090e-01],\n",
      "         [-8.3696e-02, -2.3245e-01,  1.0921e-01,  ..., -4.3028e-01,\n",
      "           8.1370e-03, -2.2810e-01],\n",
      "         ...,\n",
      "         [-1.2340e-01, -2.0889e-01, -1.3863e-01,  ..., -2.3999e-01,\n",
      "           1.7676e-01, -2.6414e-01],\n",
      "         [-3.4500e-01, -2.8167e-01, -2.0703e-02,  ...,  1.4690e-01,\n",
      "          -2.9142e-01,  3.1410e-01],\n",
      "         [ 1.0080e-01, -1.0844e-01,  9.2897e-02,  ...,  9.1980e-02,\n",
      "          -2.2254e-01, -1.1958e-01]]], device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-1.2677e-01,  3.3335e-02,  9.3478e-02,  ..., -5.3204e-01,\n",
      "           2.7953e-01,  5.9797e-04],\n",
      "         [ 1.1361e-02, -1.6972e-01,  4.4909e-02,  ..., -1.0362e-01,\n",
      "           6.1695e-02, -8.8090e-01],\n",
      "         [-8.3696e-02, -2.3245e-01,  1.0921e-01,  ..., -4.3028e-01,\n",
      "           8.1370e-03, -2.2810e-01],\n",
      "         ...,\n",
      "         [-1.2340e-01, -2.0889e-01, -1.3863e-01,  ..., -2.3999e-01,\n",
      "           1.7676e-01, -2.6414e-01],\n",
      "         [-3.4500e-01, -2.8167e-01, -2.0703e-02,  ...,  1.4690e-01,\n",
      "          -2.9142e-01,  3.1410e-01],\n",
      "         [ 1.0080e-01, -1.0844e-01,  9.2897e-02,  ...,  9.1980e-02,\n",
      "          -2.2254e-01, -1.1958e-01]]], device='cuda:0'),) and output (tensor([[[-0.1355,  0.0227,  0.0554,  ..., -0.5473,  0.2470, -0.0229],\n",
      "         [-0.1390, -0.2095, -0.1302,  ..., -0.1672,  0.1351, -0.9751],\n",
      "         [ 0.0519, -0.3131,  0.1520,  ..., -0.5850, -0.0350, -0.3805],\n",
      "         ...,\n",
      "         [ 0.2054, -0.3061, -0.0798,  ...,  0.0197,  0.1739, -0.0087],\n",
      "         [-0.3902, -0.3422,  0.0739,  ...,  0.1465, -0.3027,  0.3888],\n",
      "         [ 0.2259, -0.1195,  0.0576,  ...,  0.2996, -0.1766, -0.1353]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.1355,  0.0227,  0.0554,  ..., -0.5473,  0.2470, -0.0229],\n",
      "         [-0.1390, -0.2095, -0.1302,  ..., -0.1672,  0.1351, -0.9751],\n",
      "         [ 0.0519, -0.3131,  0.1520,  ..., -0.5850, -0.0350, -0.3805],\n",
      "         ...,\n",
      "         [ 0.2054, -0.3061, -0.0798,  ...,  0.0197,  0.1739, -0.0087],\n",
      "         [-0.3902, -0.3422,  0.0739,  ...,  0.1465, -0.3027,  0.3888],\n",
      "         [ 0.2259, -0.1195,  0.0576,  ...,  0.2996, -0.1766, -0.1353]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.1350,  0.0163,  0.0576,  ..., -0.5555,  0.2435,  0.0051],\n",
      "         [-0.2325, -0.1649, -0.0502,  ..., -0.1797,  0.0945, -1.1421],\n",
      "         [ 0.0140, -0.3362,  0.0894,  ..., -0.7084,  0.0361, -0.4048],\n",
      "         ...,\n",
      "         [ 0.1342, -0.2540, -0.1662,  ...,  0.0362,  0.2511, -0.0294],\n",
      "         [-0.4055, -0.3359,  0.1126,  ...,  0.1953, -0.2638,  0.3963],\n",
      "         [ 0.2198, -0.1518,  0.0850,  ...,  0.3790, -0.1826, -0.1489]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.1350,  0.0163,  0.0576,  ..., -0.5555,  0.2435,  0.0051],\n",
      "         [-0.2325, -0.1649, -0.0502,  ..., -0.1797,  0.0945, -1.1421],\n",
      "         [ 0.0140, -0.3362,  0.0894,  ..., -0.7084,  0.0361, -0.4048],\n",
      "         ...,\n",
      "         [ 0.1342, -0.2540, -0.1662,  ...,  0.0362,  0.2511, -0.0294],\n",
      "         [-0.4055, -0.3359,  0.1126,  ...,  0.1953, -0.2638,  0.3963],\n",
      "         [ 0.2198, -0.1518,  0.0850,  ...,  0.3790, -0.1826, -0.1489]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-1.3006e-01, -2.8984e-03,  2.7912e-02,  ..., -5.5374e-01,\n",
      "           2.2766e-01,  1.6436e-02],\n",
      "         [-2.7394e-01, -2.9190e-01,  1.2533e-01,  ..., -1.5707e-01,\n",
      "           2.5764e-01, -1.1437e+00],\n",
      "         [ 1.8542e-02, -4.5183e-01,  1.4303e-01,  ..., -7.5559e-01,\n",
      "          -1.0943e-03, -3.5489e-01],\n",
      "         ...,\n",
      "         [ 4.6063e-02, -2.8224e-01, -2.4596e-01,  ...,  1.1537e-02,\n",
      "           2.9960e-01, -4.0447e-02],\n",
      "         [-4.1300e-01, -2.6035e-01,  1.1888e-01,  ...,  2.6375e-01,\n",
      "          -3.0134e-01,  4.1688e-01],\n",
      "         [ 2.7819e-01,  5.3605e-03,  1.5785e-01,  ...,  3.6430e-01,\n",
      "          -1.7866e-01, -2.7624e-01]]], device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-1.3006e-01, -2.8984e-03,  2.7912e-02,  ..., -5.5374e-01,\n",
      "           2.2766e-01,  1.6436e-02],\n",
      "         [-2.7394e-01, -2.9190e-01,  1.2533e-01,  ..., -1.5707e-01,\n",
      "           2.5764e-01, -1.1437e+00],\n",
      "         [ 1.8542e-02, -4.5183e-01,  1.4303e-01,  ..., -7.5559e-01,\n",
      "          -1.0943e-03, -3.5489e-01],\n",
      "         ...,\n",
      "         [ 4.6063e-02, -2.8224e-01, -2.4596e-01,  ...,  1.1537e-02,\n",
      "           2.9960e-01, -4.0447e-02],\n",
      "         [-4.1300e-01, -2.6035e-01,  1.1888e-01,  ...,  2.6375e-01,\n",
      "          -3.0134e-01,  4.1688e-01],\n",
      "         [ 2.7819e-01,  5.3605e-03,  1.5785e-01,  ...,  3.6430e-01,\n",
      "          -1.7866e-01, -2.7624e-01]]], device='cuda:0'),) and output (tensor([[[-0.1219,  0.0061, -0.0021,  ..., -0.5922,  0.2788, -0.0020],\n",
      "         [-0.2371, -0.0584, -0.0183,  ..., -0.2135,  0.3367, -1.1655],\n",
      "         [-0.0724, -0.5074,  0.2909,  ..., -0.7949,  0.1288, -0.5677],\n",
      "         ...,\n",
      "         [-0.2659, -0.1588, -0.3806,  ...,  0.2212,  0.0280, -0.1104],\n",
      "         [-0.5352, -0.2944,  0.2195,  ...,  0.3722, -0.2409,  0.5683],\n",
      "         [ 0.1919, -0.1073,  0.2093,  ...,  0.4335, -0.1762, -0.1123]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.1219,  0.0061, -0.0021,  ..., -0.5922,  0.2788, -0.0020],\n",
      "         [-0.2371, -0.0584, -0.0183,  ..., -0.2135,  0.3367, -1.1655],\n",
      "         [-0.0724, -0.5074,  0.2909,  ..., -0.7949,  0.1288, -0.5677],\n",
      "         ...,\n",
      "         [-0.2659, -0.1588, -0.3806,  ...,  0.2212,  0.0280, -0.1104],\n",
      "         [-0.5352, -0.2944,  0.2195,  ...,  0.3722, -0.2409,  0.5683],\n",
      "         [ 0.1919, -0.1073,  0.2093,  ...,  0.4335, -0.1762, -0.1123]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.1067,  0.0272,  0.0495,  ..., -0.5550,  0.3449,  0.0266],\n",
      "         [-0.1966,  0.0309,  0.0179,  ..., -0.6474,  0.2411, -1.3147],\n",
      "         [-0.0507, -0.3700,  0.6493,  ..., -0.8273, -0.0154, -0.7448],\n",
      "         ...,\n",
      "         [-0.3038,  0.0276, -0.3385,  ...,  0.0233,  0.1775, -0.1594],\n",
      "         [-0.5538, -0.3390,  0.4104,  ...,  0.4728, -0.4538,  0.6242],\n",
      "         [ 0.1966, -0.1042,  0.5056,  ...,  0.4860, -0.5646, -0.2031]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.1067,  0.0272,  0.0495,  ..., -0.5550,  0.3449,  0.0266],\n",
      "         [-0.1966,  0.0309,  0.0179,  ..., -0.6474,  0.2411, -1.3147],\n",
      "         [-0.0507, -0.3700,  0.6493,  ..., -0.8273, -0.0154, -0.7448],\n",
      "         ...,\n",
      "         [-0.3038,  0.0276, -0.3385,  ...,  0.0233,  0.1775, -0.1594],\n",
      "         [-0.5538, -0.3390,  0.4104,  ...,  0.4728, -0.4538,  0.6242],\n",
      "         [ 0.1966, -0.1042,  0.5056,  ...,  0.4860, -0.5646, -0.2031]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.0592,  0.0731,  0.0500,  ..., -0.5517,  0.3541,  0.0595],\n",
      "         [-0.3256, -0.0122,  0.1427,  ..., -0.6539,  0.4752, -1.3057],\n",
      "         [-0.3286, -0.2731,  0.6364,  ..., -0.7146, -0.1600, -0.6720],\n",
      "         ...,\n",
      "         [-0.3501,  0.4560, -0.1681,  ..., -0.0888,  0.1938, -0.1596],\n",
      "         [-0.7849, -0.5436,  0.4170,  ...,  0.5246, -0.5323,  0.1780],\n",
      "         [-0.0101, -0.2247,  0.5594,  ...,  0.2366, -0.6570, -0.5386]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.0592,  0.0731,  0.0500,  ..., -0.5517,  0.3541,  0.0595],\n",
      "         [-0.3256, -0.0122,  0.1427,  ..., -0.6539,  0.4752, -1.3057],\n",
      "         [-0.3286, -0.2731,  0.6364,  ..., -0.7146, -0.1600, -0.6720],\n",
      "         ...,\n",
      "         [-0.3501,  0.4560, -0.1681,  ..., -0.0888,  0.1938, -0.1596],\n",
      "         [-0.7849, -0.5436,  0.4170,  ...,  0.5246, -0.5323,  0.1780],\n",
      "         [-0.0101, -0.2247,  0.5594,  ...,  0.2366, -0.6570, -0.5386]]],\n",
      "       device='cuda:0'),) and output (tensor([[[ 0.1143,  0.2237,  0.3292,  ..., -0.7706,  0.2608,  0.0269],\n",
      "         [-0.4536, -0.2311,  0.1268,  ..., -0.7931,  0.1279, -1.4584],\n",
      "         [-0.1423, -0.3284,  0.7518,  ..., -0.6446, -0.3000, -0.9753],\n",
      "         ...,\n",
      "         [-0.3011,  0.3863, -0.0901,  ..., -0.5631,  0.1544,  0.1294],\n",
      "         [-0.4974, -0.6112,  0.5148,  ...,  0.4646, -0.5955,  0.2077],\n",
      "         [ 0.2000, -0.4856,  0.5814,  ...,  0.2538, -0.8018, -0.5124]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[ 0.1143,  0.2237,  0.3292,  ..., -0.7706,  0.2608,  0.0269],\n",
      "         [-0.4536, -0.2311,  0.1268,  ..., -0.7931,  0.1279, -1.4584],\n",
      "         [-0.1423, -0.3284,  0.7518,  ..., -0.6446, -0.3000, -0.9753],\n",
      "         ...,\n",
      "         [-0.3011,  0.3863, -0.0901,  ..., -0.5631,  0.1544,  0.1294],\n",
      "         [-0.4974, -0.6112,  0.5148,  ...,  0.4646, -0.5955,  0.2077],\n",
      "         [ 0.2000, -0.4856,  0.5814,  ...,  0.2538, -0.8018, -0.5124]]],\n",
      "       device='cuda:0'),) and output (tensor([[[ 3.7106e-01,  5.4886e-01,  6.1002e-01,  ..., -7.4991e-01,\n",
      "           2.0802e-01,  4.1948e-01],\n",
      "         [-7.8906e-01, -2.7812e-01, -1.2297e-01,  ..., -1.3888e+00,\n",
      "           1.3801e-02, -1.6376e+00],\n",
      "         [-1.7202e-02, -9.0600e-01,  6.7615e-01,  ..., -4.6907e-01,\n",
      "          -1.2739e-03, -1.3750e+00],\n",
      "         ...,\n",
      "         [ 5.6897e-02, -2.4235e-01, -1.7253e-01,  ..., -9.7855e-01,\n",
      "          -1.4884e-01, -1.1442e-01],\n",
      "         [-5.3125e-01, -6.4276e-01,  5.6231e-02,  ...,  3.7646e-01,\n",
      "          -2.2976e-01, -3.8852e-01],\n",
      "         [ 1.0215e-02, -7.7201e-01,  1.5568e-01,  ..., -5.6532e-03,\n",
      "          -7.3224e-01, -1.2245e+00]]], device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[ 3.7106e-01,  5.4886e-01,  6.1002e-01,  ..., -7.4991e-01,\n",
      "           2.0802e-01,  4.1948e-01],\n",
      "         [-7.8906e-01, -2.7812e-01, -1.2297e-01,  ..., -1.3888e+00,\n",
      "           1.3801e-02, -1.6376e+00],\n",
      "         [-1.7202e-02, -9.0600e-01,  6.7615e-01,  ..., -4.6907e-01,\n",
      "          -1.2739e-03, -1.3750e+00],\n",
      "         ...,\n",
      "         [ 5.6897e-02, -2.4235e-01, -1.7253e-01,  ..., -9.7855e-01,\n",
      "          -1.4884e-01, -1.1442e-01],\n",
      "         [-5.3125e-01, -6.4276e-01,  5.6231e-02,  ...,  3.7646e-01,\n",
      "          -2.2976e-01, -3.8852e-01],\n",
      "         [ 1.0215e-02, -7.7201e-01,  1.5568e-01,  ..., -5.6532e-03,\n",
      "          -7.3224e-01, -1.2245e+00]]], device='cuda:0'),) and output (tensor([[[ 1.1544e+00,  3.2965e+00,  1.6534e+00,  ..., -2.7009e+00,\n",
      "           2.6553e+00,  2.2700e+00],\n",
      "         [-1.2157e+00,  3.1332e-01, -6.1978e-01,  ..., -1.5957e+00,\n",
      "           3.8471e-01, -9.4237e-01],\n",
      "         [ 4.8460e-01, -1.0387e+00,  4.5175e-01,  ..., -1.0088e+00,\n",
      "          -5.4484e-03, -1.2533e+00],\n",
      "         ...,\n",
      "         [-4.2259e-01, -1.1159e-03,  5.1839e-02,  ..., -1.1241e+00,\n",
      "          -3.8364e-01, -3.5569e-01],\n",
      "         [ 5.4529e-01, -9.9382e-01, -4.4239e-01,  ...,  3.7230e-01,\n",
      "          -5.7790e-01, -1.6448e-01],\n",
      "         [ 8.3304e-02, -1.1536e+00,  4.6944e-01,  ...,  1.8638e-01,\n",
      "          -8.8324e-01, -1.3840e+00]]], device='cuda:0'),)\n",
      "[Debug] Layer names being passed: ['model.layers.0', 'model.layers.1', 'model.layers.2', 'model.layers.3', 'model.layers.4', 'model.layers.5', 'model.layers.6', 'model.layers.7', 'model.layers.8', 'model.layers.9', 'model.layers.10', 'model.layers.11', 'model.layers.12', 'model.layers.13', 'model.layers.14', 'model.layers.15', 'model.layers.16', 'model.layers.17', 'model.layers.18', 'model.layers.19', 'model.layers.20', 'model.layers.21', 'model.layers.22', 'model.layers.23', 'model.layers.24', 'model.layers.25', 'model.layers.26', 'model.layers.27', 'model.layers.28', 'model.layers.29', 'model.layers.30', 'model.layers.31', 'model.embed_tokens']\n",
      "[Debug] Trying to access layer: model.layers.0\n",
      "[Debug] Successfully found layer: model.layers.0\n",
      "[Debug] Trying to access layer: model.layers.1\n",
      "[Debug] Successfully found layer: model.layers.1\n",
      "[Debug] Trying to access layer: model.layers.2\n",
      "[Debug] Successfully found layer: model.layers.2\n",
      "[Debug] Trying to access layer: model.layers.3\n",
      "[Debug] Successfully found layer: model.layers.3\n",
      "[Debug] Trying to access layer: model.layers.4\n",
      "[Debug] Successfully found layer: model.layers.4\n",
      "[Debug] Trying to access layer: model.layers.5\n",
      "[Debug] Successfully found layer: model.layers.5\n",
      "[Debug] Trying to access layer: model.layers.6\n",
      "[Debug] Successfully found layer: model.layers.6\n",
      "[Debug] Trying to access layer: model.layers.7\n",
      "[Debug] Successfully found layer: model.layers.7\n",
      "[Debug] Trying to access layer: model.layers.8\n",
      "[Debug] Successfully found layer: model.layers.8\n",
      "[Debug] Trying to access layer: model.layers.9\n",
      "[Debug] Successfully found layer: model.layers.9\n",
      "[Debug] Trying to access layer: model.layers.10\n",
      "[Debug] Successfully found layer: model.layers.10\n",
      "[Debug] Trying to access layer: model.layers.11\n",
      "[Debug] Successfully found layer: model.layers.11\n",
      "[Debug] Trying to access layer: model.layers.12\n",
      "[Debug] Successfully found layer: model.layers.12\n",
      "[Debug] Trying to access layer: model.layers.13\n",
      "[Debug] Successfully found layer: model.layers.13\n",
      "[Debug] Trying to access layer: model.layers.14\n",
      "[Debug] Successfully found layer: model.layers.14\n",
      "[Debug] Trying to access layer: model.layers.15\n",
      "[Debug] Successfully found layer: model.layers.15\n",
      "[Debug] Trying to access layer: model.layers.16\n",
      "[Debug] Successfully found layer: model.layers.16\n",
      "[Debug] Trying to access layer: model.layers.17\n",
      "[Debug] Successfully found layer: model.layers.17\n",
      "[Debug] Trying to access layer: model.layers.18\n",
      "[Debug] Successfully found layer: model.layers.18\n",
      "[Debug] Trying to access layer: model.layers.19\n",
      "[Debug] Successfully found layer: model.layers.19\n",
      "[Debug] Trying to access layer: model.layers.20\n",
      "[Debug] Successfully found layer: model.layers.20\n",
      "[Debug] Trying to access layer: model.layers.21\n",
      "[Debug] Successfully found layer: model.layers.21\n",
      "[Debug] Trying to access layer: model.layers.22\n",
      "[Debug] Successfully found layer: model.layers.22\n",
      "[Debug] Trying to access layer: model.layers.23\n",
      "[Debug] Successfully found layer: model.layers.23\n",
      "[Debug] Trying to access layer: model.layers.24\n",
      "[Debug] Successfully found layer: model.layers.24\n",
      "[Debug] Trying to access layer: model.layers.25\n",
      "[Debug] Successfully found layer: model.layers.25\n",
      "[Debug] Trying to access layer: model.layers.26\n",
      "[Debug] Successfully found layer: model.layers.26\n",
      "[Debug] Trying to access layer: model.layers.27\n",
      "[Debug] Successfully found layer: model.layers.27\n",
      "[Debug] Trying to access layer: model.layers.28\n",
      "[Debug] Successfully found layer: model.layers.28\n",
      "[Debug] Trying to access layer: model.layers.29\n",
      "[Debug] Successfully found layer: model.layers.29\n",
      "[Debug] Trying to access layer: model.layers.30\n",
      "[Debug] Successfully found layer: model.layers.30\n",
      "[Debug] Trying to access layer: model.layers.31\n",
      "[Debug] Successfully found layer: model.layers.31\n",
      "[Debug] Trying to access layer: model.embed_tokens\n",
      "[Debug] Successfully found layer: model.embed_tokens\n",
      "[Hook] Layer Embedding(128256, 4096) received input (tensor([[128000,   3513,  12019,    315,    279,  16642,  13944,    578,   4632,\n",
      "            596,  44682,   6376,     11,    330,   3957,    273,    409,  15274,\n",
      "          17440,     66,    295,    498,   3967,    311,   1202,  11062,    439,\n",
      "            279,  16488,  66513,     11,    574,  14948,    555,   3892,  25181,\n",
      "            323,   9678,  20543,  10977,  21699,    555,  58097,     11,  90127,\n",
      "            323,  16448,   9581,   5990,    304,  29974,    596,  10335,    265,\n",
      "           6098,    818,  60347,     11,   1455,  35146,    279,  19019,   2781,\n",
      "           3785,  55365,    409,  20263,  15274,     13,   1102,    574,  42508,\n",
      "            304,  10335,    265,   6098,    818,  60347,   6424,   9995,    797,\n",
      "            332,   8032,     20,     60]], device='cuda:0'),) and output tensor([[[ 2.6512e-04, -4.9973e-04, -5.8365e-04,  ...,  3.8147e-03,\n",
      "           6.3419e-05,  1.1902e-03],\n",
      "         [-1.2878e-02, -1.8616e-03, -7.2021e-03,  ...,  9.3994e-03,\n",
      "          -2.3193e-03, -9.2773e-03],\n",
      "         [ 1.2817e-03, -2.2430e-03,  1.2085e-02,  ...,  2.9373e-04,\n",
      "          -9.1553e-03,  1.6785e-03],\n",
      "         ...,\n",
      "         [ 2.1667e-03,  7.7515e-03,  1.2817e-02,  ...,  9.7656e-03,\n",
      "          -8.9111e-03, -1.9836e-03],\n",
      "         [ 3.3722e-03,  7.2021e-03,  1.4420e-03,  ...,  1.4038e-02,\n",
      "          -3.0670e-03, -7.4463e-03],\n",
      "         [ 3.2616e-04,  7.5531e-04,  3.9368e-03,  ...,  1.0132e-02,\n",
      "           6.0425e-03, -1.3542e-04]]], device='cuda:0')\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[ 2.6512e-04, -4.9973e-04, -5.8365e-04,  ...,  3.8147e-03,\n",
      "           6.3419e-05,  1.1902e-03],\n",
      "         [-1.2878e-02, -1.8616e-03, -7.2021e-03,  ...,  9.3994e-03,\n",
      "          -2.3193e-03, -9.2773e-03],\n",
      "         [ 1.2817e-03, -2.2430e-03,  1.2085e-02,  ...,  2.9373e-04,\n",
      "          -9.1553e-03,  1.6785e-03],\n",
      "         ...,\n",
      "         [ 2.1667e-03,  7.7515e-03,  1.2817e-02,  ...,  9.7656e-03,\n",
      "          -8.9111e-03, -1.9836e-03],\n",
      "         [ 3.3722e-03,  7.2021e-03,  1.4420e-03,  ...,  1.4038e-02,\n",
      "          -3.0670e-03, -7.4463e-03],\n",
      "         [ 3.2616e-04,  7.5531e-04,  3.9368e-03,  ...,  1.0132e-02,\n",
      "           6.0425e-03, -1.3542e-04]]], device='cuda:0'),) and output (tensor([[[-0.0004,  0.0146, -0.0020,  ...,  0.0066, -0.0194,  0.0020],\n",
      "         [ 0.0055,  0.0142, -0.0059,  ..., -0.0036, -0.0126, -0.0339],\n",
      "         [-0.0050,  0.0023,  0.0217,  ..., -0.0069,  0.0005, -0.0078],\n",
      "         ...,\n",
      "         [-0.0119,  0.0048,  0.0115,  ...,  0.0158,  0.0069,  0.0065],\n",
      "         [-0.0123, -0.0107,  0.0009,  ...,  0.0262, -0.0067,  0.0042],\n",
      "         [-0.0126,  0.0118, -0.0158,  ...,  0.0009, -0.0045,  0.0146]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.0004,  0.0146, -0.0020,  ...,  0.0066, -0.0194,  0.0020],\n",
      "         [ 0.0055,  0.0142, -0.0059,  ..., -0.0036, -0.0126, -0.0339],\n",
      "         [-0.0050,  0.0023,  0.0217,  ..., -0.0069,  0.0005, -0.0078],\n",
      "         ...,\n",
      "         [-0.0119,  0.0048,  0.0115,  ...,  0.0158,  0.0069,  0.0065],\n",
      "         [-0.0123, -0.0107,  0.0009,  ...,  0.0262, -0.0067,  0.0042],\n",
      "         [-0.0126,  0.0118, -0.0158,  ...,  0.0009, -0.0045,  0.0146]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.0972,  0.0787, -0.0419,  ...,  0.0071,  0.0869,  0.0218],\n",
      "         [ 0.0066,  0.0383, -0.0040,  ..., -0.0153, -0.0333, -0.0264],\n",
      "         [ 0.0072, -0.0189,  0.0427,  ..., -0.0128, -0.0126, -0.0171],\n",
      "         ...,\n",
      "         [-0.0100,  0.0232, -0.0083,  ..., -0.0057, -0.0097,  0.0125],\n",
      "         [-0.0160, -0.0110, -0.0202,  ...,  0.0014, -0.0236,  0.0175],\n",
      "         [-0.0188, -0.0126, -0.0248,  ..., -0.0139, -0.0159,  0.0178]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.0972,  0.0787, -0.0419,  ...,  0.0071,  0.0869,  0.0218],\n",
      "         [ 0.0066,  0.0383, -0.0040,  ..., -0.0153, -0.0333, -0.0264],\n",
      "         [ 0.0072, -0.0189,  0.0427,  ..., -0.0128, -0.0126, -0.0171],\n",
      "         ...,\n",
      "         [-0.0100,  0.0232, -0.0083,  ..., -0.0057, -0.0097,  0.0125],\n",
      "         [-0.0160, -0.0110, -0.0202,  ...,  0.0014, -0.0236,  0.0175],\n",
      "         [-0.0188, -0.0126, -0.0248,  ..., -0.0139, -0.0159,  0.0178]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.1040,  0.0875, -0.0360,  ...,  0.0205,  0.0997,  0.0184],\n",
      "         [ 0.0173,  0.0148,  0.0179,  ..., -0.0110, -0.0402, -0.0199],\n",
      "         [ 0.0180, -0.0256,  0.0700,  ...,  0.0089, -0.0256, -0.0022],\n",
      "         ...,\n",
      "         [-0.0304,  0.0244,  0.0018,  ...,  0.0004,  0.0142,  0.0442],\n",
      "         [-0.0301, -0.0143, -0.0456,  ..., -0.0226, -0.0332,  0.0339],\n",
      "         [-0.0457, -0.0085, -0.0149,  ...,  0.0001, -0.0352,  0.0396]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.1040,  0.0875, -0.0360,  ...,  0.0205,  0.0997,  0.0184],\n",
      "         [ 0.0173,  0.0148,  0.0179,  ..., -0.0110, -0.0402, -0.0199],\n",
      "         [ 0.0180, -0.0256,  0.0700,  ...,  0.0089, -0.0256, -0.0022],\n",
      "         ...,\n",
      "         [-0.0304,  0.0244,  0.0018,  ...,  0.0004,  0.0142,  0.0442],\n",
      "         [-0.0301, -0.0143, -0.0456,  ..., -0.0226, -0.0332,  0.0339],\n",
      "         [-0.0457, -0.0085, -0.0149,  ...,  0.0001, -0.0352,  0.0396]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.0870,  0.1049, -0.0173,  ...,  0.0700,  0.0989,  0.0181],\n",
      "         [ 0.0064,  0.0383,  0.0188,  ..., -0.0650, -0.0747, -0.0904],\n",
      "         [ 0.0159, -0.0034,  0.0835,  ...,  0.0415, -0.0055, -0.0276],\n",
      "         ...,\n",
      "         [-0.0662, -0.0067, -0.0220,  ...,  0.0147, -0.0331,  0.0412],\n",
      "         [-0.0294, -0.0338, -0.0327,  ..., -0.0214, -0.0060,  0.0553],\n",
      "         [-0.0783,  0.0002, -0.0579,  ...,  0.0050, -0.0472,  0.0794]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.0870,  0.1049, -0.0173,  ...,  0.0700,  0.0989,  0.0181],\n",
      "         [ 0.0064,  0.0383,  0.0188,  ..., -0.0650, -0.0747, -0.0904],\n",
      "         [ 0.0159, -0.0034,  0.0835,  ...,  0.0415, -0.0055, -0.0276],\n",
      "         ...,\n",
      "         [-0.0662, -0.0067, -0.0220,  ...,  0.0147, -0.0331,  0.0412],\n",
      "         [-0.0294, -0.0338, -0.0327,  ..., -0.0214, -0.0060,  0.0553],\n",
      "         [-0.0783,  0.0002, -0.0579,  ...,  0.0050, -0.0472,  0.0794]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.1113,  0.1022, -0.0102,  ...,  0.0436,  0.1283,  0.0321],\n",
      "         [ 0.0206,  0.0520,  0.0486,  ..., -0.0386, -0.1163, -0.0188],\n",
      "         [-0.0107,  0.0337,  0.0808,  ...,  0.0198, -0.0010,  0.0015],\n",
      "         ...,\n",
      "         [-0.0200, -0.0283, -0.0581,  ..., -0.0558, -0.0858,  0.0906],\n",
      "         [-0.0717, -0.0244, -0.0442,  ...,  0.0100, -0.0455,  0.1096],\n",
      "         [-0.0627, -0.0493, -0.0560,  ...,  0.0360, -0.0565,  0.0965]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.1113,  0.1022, -0.0102,  ...,  0.0436,  0.1283,  0.0321],\n",
      "         [ 0.0206,  0.0520,  0.0486,  ..., -0.0386, -0.1163, -0.0188],\n",
      "         [-0.0107,  0.0337,  0.0808,  ...,  0.0198, -0.0010,  0.0015],\n",
      "         ...,\n",
      "         [-0.0200, -0.0283, -0.0581,  ..., -0.0558, -0.0858,  0.0906],\n",
      "         [-0.0717, -0.0244, -0.0442,  ...,  0.0100, -0.0455,  0.1096],\n",
      "         [-0.0627, -0.0493, -0.0560,  ...,  0.0360, -0.0565,  0.0965]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.1034,  0.1132,  0.0096,  ...,  0.0453,  0.1477,  0.0301],\n",
      "         [ 0.0290,  0.0389,  0.0400,  ..., -0.0829, -0.1114, -0.0071],\n",
      "         [-0.0057,  0.0595,  0.0667,  ..., -0.0087,  0.0201,  0.0167],\n",
      "         ...,\n",
      "         [-0.0161,  0.0219, -0.0844,  ..., -0.0013, -0.0674,  0.0611],\n",
      "         [-0.0550, -0.0312, -0.0669,  ...,  0.0071,  0.0174,  0.1211],\n",
      "         [-0.0376, -0.0388, -0.0742,  ...,  0.0428, -0.0682,  0.0989]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.1034,  0.1132,  0.0096,  ...,  0.0453,  0.1477,  0.0301],\n",
      "         [ 0.0290,  0.0389,  0.0400,  ..., -0.0829, -0.1114, -0.0071],\n",
      "         [-0.0057,  0.0595,  0.0667,  ..., -0.0087,  0.0201,  0.0167],\n",
      "         ...,\n",
      "         [-0.0161,  0.0219, -0.0844,  ..., -0.0013, -0.0674,  0.0611],\n",
      "         [-0.0550, -0.0312, -0.0669,  ...,  0.0071,  0.0174,  0.1211],\n",
      "         [-0.0376, -0.0388, -0.0742,  ...,  0.0428, -0.0682,  0.0989]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.1071,  0.1253,  0.0211,  ...,  0.0126,  0.1586,  0.0278],\n",
      "         [ 0.0210,  0.0469,  0.0191,  ..., -0.0017, -0.0631,  0.0188],\n",
      "         [-0.0140,  0.0731,  0.0632,  ..., -0.0125,  0.0359, -0.0381],\n",
      "         ...,\n",
      "         [-0.0490, -0.0034, -0.1108,  ...,  0.0356, -0.0803,  0.0656],\n",
      "         [-0.0716,  0.0235, -0.0613,  ..., -0.0543, -0.0163,  0.1035],\n",
      "         [-0.0977, -0.0293, -0.0634,  ...,  0.1059, -0.0782,  0.1271]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.1071,  0.1253,  0.0211,  ...,  0.0126,  0.1586,  0.0278],\n",
      "         [ 0.0210,  0.0469,  0.0191,  ..., -0.0017, -0.0631,  0.0188],\n",
      "         [-0.0140,  0.0731,  0.0632,  ..., -0.0125,  0.0359, -0.0381],\n",
      "         ...,\n",
      "         [-0.0490, -0.0034, -0.1108,  ...,  0.0356, -0.0803,  0.0656],\n",
      "         [-0.0716,  0.0235, -0.0613,  ..., -0.0543, -0.0163,  0.1035],\n",
      "         [-0.0977, -0.0293, -0.0634,  ...,  0.1059, -0.0782,  0.1271]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.0945,  0.1848,  0.0036,  ..., -0.0473,  0.1489,  0.0536],\n",
      "         [ 0.0804, -0.0105,  0.0057,  ..., -0.1038, -0.1487, -0.0538],\n",
      "         [ 0.0309,  0.0993,  0.0533,  ..., -0.0274, -0.0344, -0.0235],\n",
      "         ...,\n",
      "         [-0.0015,  0.0350, -0.0441,  ...,  0.1509, -0.0882,  0.0872],\n",
      "         [-0.0945,  0.0408, -0.0125,  ...,  0.0773, -0.0118,  0.1264],\n",
      "         [-0.1031,  0.0454, -0.0419,  ...,  0.1449, -0.1686,  0.1339]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.0945,  0.1848,  0.0036,  ..., -0.0473,  0.1489,  0.0536],\n",
      "         [ 0.0804, -0.0105,  0.0057,  ..., -0.1038, -0.1487, -0.0538],\n",
      "         [ 0.0309,  0.0993,  0.0533,  ..., -0.0274, -0.0344, -0.0235],\n",
      "         ...,\n",
      "         [-0.0015,  0.0350, -0.0441,  ...,  0.1509, -0.0882,  0.0872],\n",
      "         [-0.0945,  0.0408, -0.0125,  ...,  0.0773, -0.0118,  0.1264],\n",
      "         [-0.1031,  0.0454, -0.0419,  ...,  0.1449, -0.1686,  0.1339]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.1060,  0.2113,  0.0270,  ..., -0.1141,  0.1610,  0.0528],\n",
      "         [ 0.0810,  0.0661, -0.0024,  ..., -0.1130, -0.1703, -0.0764],\n",
      "         [ 0.0715,  0.0620, -0.0080,  ..., -0.0529, -0.0080, -0.0993],\n",
      "         ...,\n",
      "         [ 0.0283, -0.0389, -0.0058,  ...,  0.1890, -0.0897,  0.0459],\n",
      "         [-0.1892, -0.0878, -0.0399,  ...,  0.0480,  0.0062,  0.1438],\n",
      "         [-0.0560, -0.0957, -0.0253,  ...,  0.1482, -0.1474,  0.1197]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.1060,  0.2113,  0.0270,  ..., -0.1141,  0.1610,  0.0528],\n",
      "         [ 0.0810,  0.0661, -0.0024,  ..., -0.1130, -0.1703, -0.0764],\n",
      "         [ 0.0715,  0.0620, -0.0080,  ..., -0.0529, -0.0080, -0.0993],\n",
      "         ...,\n",
      "         [ 0.0283, -0.0389, -0.0058,  ...,  0.1890, -0.0897,  0.0459],\n",
      "         [-0.1892, -0.0878, -0.0399,  ...,  0.0480,  0.0062,  0.1438],\n",
      "         [-0.0560, -0.0957, -0.0253,  ...,  0.1482, -0.1474,  0.1197]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.0954,  0.2493,  0.0073,  ..., -0.1568,  0.1818,  0.0855],\n",
      "         [ 0.0024,  0.0602,  0.0187,  ..., -0.0834, -0.1176, -0.0681],\n",
      "         [ 0.0026, -0.0173, -0.0160,  ..., -0.0628,  0.0491, -0.0694],\n",
      "         ...,\n",
      "         [ 0.0542, -0.0147, -0.0212,  ...,  0.1668, -0.0345,  0.0923],\n",
      "         [-0.1533, -0.0619, -0.0426,  ...,  0.0506,  0.0959,  0.1680],\n",
      "         [-0.0107, -0.0779, -0.0179,  ...,  0.0594, -0.0221,  0.1215]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.0954,  0.2493,  0.0073,  ..., -0.1568,  0.1818,  0.0855],\n",
      "         [ 0.0024,  0.0602,  0.0187,  ..., -0.0834, -0.1176, -0.0681],\n",
      "         [ 0.0026, -0.0173, -0.0160,  ..., -0.0628,  0.0491, -0.0694],\n",
      "         ...,\n",
      "         [ 0.0542, -0.0147, -0.0212,  ...,  0.1668, -0.0345,  0.0923],\n",
      "         [-0.1533, -0.0619, -0.0426,  ...,  0.0506,  0.0959,  0.1680],\n",
      "         [-0.0107, -0.0779, -0.0179,  ...,  0.0594, -0.0221,  0.1215]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.0152,  0.3340,  0.0011,  ..., -0.2750,  0.1933,  0.1034],\n",
      "         [ 0.0728,  0.0569,  0.0289,  ..., -0.0817, -0.1317, -0.0539],\n",
      "         [ 0.0100, -0.0224, -0.0449,  ..., -0.0886, -0.0016, -0.0365],\n",
      "         ...,\n",
      "         [ 0.0293,  0.0194, -0.0840,  ...,  0.1014, -0.0267,  0.1474],\n",
      "         [-0.1017, -0.0248, -0.0599,  ...,  0.0378, -0.0006,  0.2071],\n",
      "         [ 0.0433, -0.0160, -0.0518,  ..., -0.0310, -0.0629,  0.1830]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.0152,  0.3340,  0.0011,  ..., -0.2750,  0.1933,  0.1034],\n",
      "         [ 0.0728,  0.0569,  0.0289,  ..., -0.0817, -0.1317, -0.0539],\n",
      "         [ 0.0100, -0.0224, -0.0449,  ..., -0.0886, -0.0016, -0.0365],\n",
      "         ...,\n",
      "         [ 0.0293,  0.0194, -0.0840,  ...,  0.1014, -0.0267,  0.1474],\n",
      "         [-0.1017, -0.0248, -0.0599,  ...,  0.0378, -0.0006,  0.2071],\n",
      "         [ 0.0433, -0.0160, -0.0518,  ..., -0.0310, -0.0629,  0.1830]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.0356,  0.3483, -0.0366,  ..., -0.2962,  0.2088,  0.1065],\n",
      "         [ 0.0462,  0.0298,  0.0349,  ..., -0.1321, -0.0720, -0.0589],\n",
      "         [-0.0649,  0.0044,  0.0036,  ..., -0.1361,  0.0363, -0.0446],\n",
      "         ...,\n",
      "         [ 0.0093, -0.0371, -0.0641,  ...,  0.0456,  0.1385,  0.1109],\n",
      "         [-0.1187, -0.0443, -0.1193,  ..., -0.0792,  0.1592,  0.0561],\n",
      "         [ 0.0185, -0.0744, -0.0769,  ..., -0.0745,  0.0209,  0.1073]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.0356,  0.3483, -0.0366,  ..., -0.2962,  0.2088,  0.1065],\n",
      "         [ 0.0462,  0.0298,  0.0349,  ..., -0.1321, -0.0720, -0.0589],\n",
      "         [-0.0649,  0.0044,  0.0036,  ..., -0.1361,  0.0363, -0.0446],\n",
      "         ...,\n",
      "         [ 0.0093, -0.0371, -0.0641,  ...,  0.0456,  0.1385,  0.1109],\n",
      "         [-0.1187, -0.0443, -0.1193,  ..., -0.0792,  0.1592,  0.0561],\n",
      "         [ 0.0185, -0.0744, -0.0769,  ..., -0.0745,  0.0209,  0.1073]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-1.2733e-02,  3.6610e-01, -3.1563e-02,  ..., -3.6781e-01,\n",
      "           1.9730e-01,  1.0189e-01],\n",
      "         [ 2.5998e-02,  1.3645e-04,  3.0948e-02,  ..., -1.6858e-01,\n",
      "          -1.3279e-01, -4.9386e-03],\n",
      "         [-4.4125e-03, -4.7764e-02, -1.0496e-02,  ..., -1.2447e-01,\n",
      "           1.4849e-02, -2.9886e-02],\n",
      "         ...,\n",
      "         [ 4.1445e-03, -6.2388e-02,  2.5781e-02,  ...,  3.7593e-02,\n",
      "           1.2291e-01,  1.7230e-01],\n",
      "         [-1.0315e-01, -3.4470e-02, -5.1928e-02,  ..., -2.1524e-02,\n",
      "           6.2139e-02,  6.7447e-02],\n",
      "         [-7.3853e-02, -6.5056e-02, -6.3390e-02,  ...,  5.2317e-02,\n",
      "          -4.9705e-02,  1.3043e-01]]], device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-1.2733e-02,  3.6610e-01, -3.1563e-02,  ..., -3.6781e-01,\n",
      "           1.9730e-01,  1.0189e-01],\n",
      "         [ 2.5998e-02,  1.3645e-04,  3.0948e-02,  ..., -1.6858e-01,\n",
      "          -1.3279e-01, -4.9386e-03],\n",
      "         [-4.4125e-03, -4.7764e-02, -1.0496e-02,  ..., -1.2447e-01,\n",
      "           1.4849e-02, -2.9886e-02],\n",
      "         ...,\n",
      "         [ 4.1445e-03, -6.2388e-02,  2.5781e-02,  ...,  3.7593e-02,\n",
      "           1.2291e-01,  1.7230e-01],\n",
      "         [-1.0315e-01, -3.4470e-02, -5.1928e-02,  ..., -2.1524e-02,\n",
      "           6.2139e-02,  6.7447e-02],\n",
      "         [-7.3853e-02, -6.5056e-02, -6.3390e-02,  ...,  5.2317e-02,\n",
      "          -4.9705e-02,  1.3043e-01]]], device='cuda:0'),) and output (tensor([[[-0.0089,  0.3233, -0.0779,  ..., -0.4491,  0.1845,  0.0951],\n",
      "         [-0.0668, -0.0102,  0.0545,  ..., -0.1756, -0.1458, -0.0784],\n",
      "         [ 0.0159, -0.0088,  0.0790,  ..., -0.1223,  0.0521, -0.0619],\n",
      "         ...,\n",
      "         [ 0.0132, -0.0301,  0.0368,  ..., -0.0180,  0.0290,  0.2338],\n",
      "         [-0.0508, -0.0194, -0.0025,  ..., -0.1024, -0.0551,  0.1200],\n",
      "         [-0.0579, -0.0359,  0.0068,  ...,  0.0471, -0.1594,  0.2546]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.0089,  0.3233, -0.0779,  ..., -0.4491,  0.1845,  0.0951],\n",
      "         [-0.0668, -0.0102,  0.0545,  ..., -0.1756, -0.1458, -0.0784],\n",
      "         [ 0.0159, -0.0088,  0.0790,  ..., -0.1223,  0.0521, -0.0619],\n",
      "         ...,\n",
      "         [ 0.0132, -0.0301,  0.0368,  ..., -0.0180,  0.0290,  0.2338],\n",
      "         [-0.0508, -0.0194, -0.0025,  ..., -0.1024, -0.0551,  0.1200],\n",
      "         [-0.0579, -0.0359,  0.0068,  ...,  0.0471, -0.1594,  0.2546]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.0244,  0.2828, -0.0532,  ..., -0.4592,  0.2300,  0.0142],\n",
      "         [-0.0225, -0.0803, -0.0234,  ..., -0.1598, -0.1542, -0.1672],\n",
      "         [ 0.0418, -0.1003,  0.0954,  ..., -0.1261, -0.0537, -0.0134],\n",
      "         ...,\n",
      "         [ 0.0896, -0.2357,  0.0310,  ..., -0.0494, -0.0279,  0.1723],\n",
      "         [-0.0719, -0.0797, -0.0031,  ..., -0.1417, -0.2175,  0.0011],\n",
      "         [ 0.0239,  0.0052, -0.0025,  ...,  0.0727, -0.2756,  0.2575]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.0244,  0.2828, -0.0532,  ..., -0.4592,  0.2300,  0.0142],\n",
      "         [-0.0225, -0.0803, -0.0234,  ..., -0.1598, -0.1542, -0.1672],\n",
      "         [ 0.0418, -0.1003,  0.0954,  ..., -0.1261, -0.0537, -0.0134],\n",
      "         ...,\n",
      "         [ 0.0896, -0.2357,  0.0310,  ..., -0.0494, -0.0279,  0.1723],\n",
      "         [-0.0719, -0.0797, -0.0031,  ..., -0.1417, -0.2175,  0.0011],\n",
      "         [ 0.0239,  0.0052, -0.0025,  ...,  0.0727, -0.2756,  0.2575]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.0928,  0.1485,  0.0158,  ..., -0.5380,  0.2664, -0.0333],\n",
      "         [-0.1109, -0.1188, -0.0469,  ..., -0.1001, -0.1502, -0.2327],\n",
      "         [ 0.0037,  0.0068,  0.0039,  ..., -0.0020, -0.0230, -0.0829],\n",
      "         ...,\n",
      "         [ 0.2852, -0.3820,  0.0204,  ..., -0.0473, -0.1148,  0.1101],\n",
      "         [ 0.0246, -0.1842, -0.0557,  ..., -0.2088, -0.2553, -0.0018],\n",
      "         [ 0.1326, -0.0674, -0.0880,  ...,  0.0408, -0.3743,  0.2388]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.0928,  0.1485,  0.0158,  ..., -0.5380,  0.2664, -0.0333],\n",
      "         [-0.1109, -0.1188, -0.0469,  ..., -0.1001, -0.1502, -0.2327],\n",
      "         [ 0.0037,  0.0068,  0.0039,  ..., -0.0020, -0.0230, -0.0829],\n",
      "         ...,\n",
      "         [ 0.2852, -0.3820,  0.0204,  ..., -0.0473, -0.1148,  0.1101],\n",
      "         [ 0.0246, -0.1842, -0.0557,  ..., -0.2088, -0.2553, -0.0018],\n",
      "         [ 0.1326, -0.0674, -0.0880,  ...,  0.0408, -0.3743,  0.2388]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.1000,  0.1323,  0.0241,  ..., -0.5470,  0.2844, -0.0457],\n",
      "         [-0.0502, -0.1360,  0.0097,  ..., -0.0586, -0.2939, -0.2671],\n",
      "         [ 0.0641,  0.0682,  0.0707,  ...,  0.1496, -0.0758, -0.0762],\n",
      "         ...,\n",
      "         [ 0.3873, -0.1558, -0.1315,  ...,  0.1147, -0.0408,  0.2843],\n",
      "         [-0.0190, -0.0972, -0.1806,  ..., -0.0993, -0.2276,  0.0400],\n",
      "         [ 0.2243,  0.0966, -0.0878,  ...,  0.2355, -0.4064,  0.1494]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.1000,  0.1323,  0.0241,  ..., -0.5470,  0.2844, -0.0457],\n",
      "         [-0.0502, -0.1360,  0.0097,  ..., -0.0586, -0.2939, -0.2671],\n",
      "         [ 0.0641,  0.0682,  0.0707,  ...,  0.1496, -0.0758, -0.0762],\n",
      "         ...,\n",
      "         [ 0.3873, -0.1558, -0.1315,  ...,  0.1147, -0.0408,  0.2843],\n",
      "         [-0.0190, -0.0972, -0.1806,  ..., -0.0993, -0.2276,  0.0400],\n",
      "         [ 0.2243,  0.0966, -0.0878,  ...,  0.2355, -0.4064,  0.1494]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.1286,  0.0833,  0.0239,  ..., -0.5657,  0.3694, -0.0725],\n",
      "         [-0.0596, -0.2001,  0.1842,  ..., -0.1140, -0.3022, -0.3449],\n",
      "         [ 0.0317,  0.0691,  0.0941,  ...,  0.1415, -0.0462, -0.2053],\n",
      "         ...,\n",
      "         [ 0.6250, -0.2421, -0.0214,  ...,  0.0238, -0.1368,  0.2000],\n",
      "         [ 0.0743, -0.1508, -0.1772,  ..., -0.1739, -0.3024, -0.0809],\n",
      "         [ 0.2429,  0.0355, -0.0436,  ...,  0.1291, -0.4607,  0.0322]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.1286,  0.0833,  0.0239,  ..., -0.5657,  0.3694, -0.0725],\n",
      "         [-0.0596, -0.2001,  0.1842,  ..., -0.1140, -0.3022, -0.3449],\n",
      "         [ 0.0317,  0.0691,  0.0941,  ...,  0.1415, -0.0462, -0.2053],\n",
      "         ...,\n",
      "         [ 0.6250, -0.2421, -0.0214,  ...,  0.0238, -0.1368,  0.2000],\n",
      "         [ 0.0743, -0.1508, -0.1772,  ..., -0.1739, -0.3024, -0.0809],\n",
      "         [ 0.2429,  0.0355, -0.0436,  ...,  0.1291, -0.4607,  0.0322]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.1417,  0.0703,  0.0343,  ..., -0.5549,  0.3175, -0.0847],\n",
      "         [ 0.0630, -0.1640,  0.1162,  ..., -0.1153, -0.4558, -0.5800],\n",
      "         [ 0.0397,  0.0995, -0.0437,  ...,  0.2276, -0.0589, -0.2688],\n",
      "         ...,\n",
      "         [ 0.5211, -0.2482,  0.0175,  ..., -0.0672, -0.0301,  0.1487],\n",
      "         [-0.0649, -0.1788, -0.2119,  ..., -0.3630, -0.2127, -0.2043],\n",
      "         [ 0.2815,  0.0274, -0.0124,  ...,  0.0471, -0.3443, -0.0823]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.1417,  0.0703,  0.0343,  ..., -0.5549,  0.3175, -0.0847],\n",
      "         [ 0.0630, -0.1640,  0.1162,  ..., -0.1153, -0.4558, -0.5800],\n",
      "         [ 0.0397,  0.0995, -0.0437,  ...,  0.2276, -0.0589, -0.2688],\n",
      "         ...,\n",
      "         [ 0.5211, -0.2482,  0.0175,  ..., -0.0672, -0.0301,  0.1487],\n",
      "         [-0.0649, -0.1788, -0.2119,  ..., -0.3630, -0.2127, -0.2043],\n",
      "         [ 0.2815,  0.0274, -0.0124,  ...,  0.0471, -0.3443, -0.0823]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.1528,  0.0725,  0.0609,  ..., -0.5624,  0.3103, -0.0293],\n",
      "         [ 0.0509, -0.0722,  0.0480,  ..., -0.1734, -0.4020, -0.5300],\n",
      "         [ 0.0571,  0.0115, -0.1069,  ...,  0.1752,  0.0658, -0.4213],\n",
      "         ...,\n",
      "         [ 0.4758, -0.2892, -0.0040,  ...,  0.0060,  0.1084,  0.0437],\n",
      "         [-0.0928, -0.1910, -0.1704,  ..., -0.4391, -0.0963, -0.1949],\n",
      "         [ 0.3321, -0.0048,  0.0099,  ...,  0.0202, -0.2920, -0.0526]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.1528,  0.0725,  0.0609,  ..., -0.5624,  0.3103, -0.0293],\n",
      "         [ 0.0509, -0.0722,  0.0480,  ..., -0.1734, -0.4020, -0.5300],\n",
      "         [ 0.0571,  0.0115, -0.1069,  ...,  0.1752,  0.0658, -0.4213],\n",
      "         ...,\n",
      "         [ 0.4758, -0.2892, -0.0040,  ...,  0.0060,  0.1084,  0.0437],\n",
      "         [-0.0928, -0.1910, -0.1704,  ..., -0.4391, -0.0963, -0.1949],\n",
      "         [ 0.3321, -0.0048,  0.0099,  ...,  0.0202, -0.2920, -0.0526]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.1559,  0.0520,  0.0566,  ..., -0.5805,  0.2614, -0.0032],\n",
      "         [ 0.0793,  0.0375,  0.0094,  ..., -0.2099, -0.5025, -0.4481],\n",
      "         [ 0.0374,  0.0175, -0.1660,  ...,  0.2872,  0.1621, -0.4250],\n",
      "         ...,\n",
      "         [ 0.5287, -0.3881, -0.0692,  ...,  0.1740,  0.1168, -0.0101],\n",
      "         [-0.1270, -0.1305, -0.3421,  ..., -0.4399, -0.0743, -0.2857],\n",
      "         [ 0.4179, -0.1485, -0.0719,  ...,  0.1402, -0.1922, -0.0991]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.1559,  0.0520,  0.0566,  ..., -0.5805,  0.2614, -0.0032],\n",
      "         [ 0.0793,  0.0375,  0.0094,  ..., -0.2099, -0.5025, -0.4481],\n",
      "         [ 0.0374,  0.0175, -0.1660,  ...,  0.2872,  0.1621, -0.4250],\n",
      "         ...,\n",
      "         [ 0.5287, -0.3881, -0.0692,  ...,  0.1740,  0.1168, -0.0101],\n",
      "         [-0.1270, -0.1305, -0.3421,  ..., -0.4399, -0.0743, -0.2857],\n",
      "         [ 0.4179, -0.1485, -0.0719,  ...,  0.1402, -0.1922, -0.0991]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.1488,  0.0445,  0.0765,  ..., -0.5456,  0.2702,  0.0098],\n",
      "         [ 0.2049,  0.2656, -0.0662,  ..., -0.1345, -0.5753, -0.5794],\n",
      "         [ 0.0282,  0.1507, -0.2309,  ...,  0.0426,  0.3108, -0.4221],\n",
      "         ...,\n",
      "         [ 0.4569, -0.3975, -0.0917,  ...,  0.1696,  0.0132,  0.1132],\n",
      "         [-0.2590,  0.0392, -0.2504,  ..., -0.5002, -0.1269, -0.1062],\n",
      "         [ 0.3918, -0.1431, -0.0595,  ...,  0.2504, -0.2859, -0.0749]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.1488,  0.0445,  0.0765,  ..., -0.5456,  0.2702,  0.0098],\n",
      "         [ 0.2049,  0.2656, -0.0662,  ..., -0.1345, -0.5753, -0.5794],\n",
      "         [ 0.0282,  0.1507, -0.2309,  ...,  0.0426,  0.3108, -0.4221],\n",
      "         ...,\n",
      "         [ 0.4569, -0.3975, -0.0917,  ...,  0.1696,  0.0132,  0.1132],\n",
      "         [-0.2590,  0.0392, -0.2504,  ..., -0.5002, -0.1269, -0.1062],\n",
      "         [ 0.3918, -0.1431, -0.0595,  ...,  0.2504, -0.2859, -0.0749]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.1268,  0.0333,  0.0935,  ..., -0.5320,  0.2795,  0.0006],\n",
      "         [ 0.2666,  0.2011,  0.0595,  ..., -0.0452, -0.5391, -0.5158],\n",
      "         [-0.0210,  0.2879, -0.3339,  ...,  0.0971,  0.2848, -0.3231],\n",
      "         ...,\n",
      "         [ 0.3655, -0.2874, -0.0068,  ...,  0.3087, -0.0658,  0.1309],\n",
      "         [-0.2092,  0.1083, -0.2211,  ..., -0.5353, -0.1167, -0.1977],\n",
      "         [ 0.4957, -0.0931,  0.1170,  ...,  0.4402, -0.2392, -0.0093]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.1268,  0.0333,  0.0935,  ..., -0.5320,  0.2795,  0.0006],\n",
      "         [ 0.2666,  0.2011,  0.0595,  ..., -0.0452, -0.5391, -0.5158],\n",
      "         [-0.0210,  0.2879, -0.3339,  ...,  0.0971,  0.2848, -0.3231],\n",
      "         ...,\n",
      "         [ 0.3655, -0.2874, -0.0068,  ...,  0.3087, -0.0658,  0.1309],\n",
      "         [-0.2092,  0.1083, -0.2211,  ..., -0.5353, -0.1167, -0.1977],\n",
      "         [ 0.4957, -0.0931,  0.1170,  ...,  0.4402, -0.2392, -0.0093]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.1355,  0.0227,  0.0554,  ..., -0.5473,  0.2470, -0.0229],\n",
      "         [ 0.2893,  0.2468,  0.0553,  ..., -0.1542, -0.6377, -0.4372],\n",
      "         [ 0.0255,  0.3044, -0.4226,  ...,  0.0089,  0.3042, -0.3423],\n",
      "         ...,\n",
      "         [ 0.3135, -0.2441, -0.0332,  ...,  0.1345, -0.0365,  0.1996],\n",
      "         [-0.1867, -0.0458, -0.2049,  ..., -0.5970, -0.0052, -0.0983],\n",
      "         [ 0.5590, -0.2258,  0.1344,  ...,  0.5310, -0.2407, -0.0946]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.1355,  0.0227,  0.0554,  ..., -0.5473,  0.2470, -0.0229],\n",
      "         [ 0.2893,  0.2468,  0.0553,  ..., -0.1542, -0.6377, -0.4372],\n",
      "         [ 0.0255,  0.3044, -0.4226,  ...,  0.0089,  0.3042, -0.3423],\n",
      "         ...,\n",
      "         [ 0.3135, -0.2441, -0.0332,  ...,  0.1345, -0.0365,  0.1996],\n",
      "         [-0.1867, -0.0458, -0.2049,  ..., -0.5970, -0.0052, -0.0983],\n",
      "         [ 0.5590, -0.2258,  0.1344,  ...,  0.5310, -0.2407, -0.0946]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.1350,  0.0163,  0.0576,  ..., -0.5555,  0.2435,  0.0051],\n",
      "         [ 0.2355,  0.3071, -0.0482,  ..., -0.3474, -0.7461, -0.4621],\n",
      "         [ 0.1236,  0.4846, -0.5044,  ..., -0.1222,  0.3344, -0.2640],\n",
      "         ...,\n",
      "         [ 0.3653, -0.3205, -0.1733,  ...,  0.3008,  0.0789,  0.2924],\n",
      "         [-0.1664,  0.1422, -0.1524,  ..., -0.6383, -0.1287, -0.1607],\n",
      "         [ 0.5374, -0.3034,  0.0781,  ...,  0.8236, -0.0473, -0.0960]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.1350,  0.0163,  0.0576,  ..., -0.5555,  0.2435,  0.0051],\n",
      "         [ 0.2355,  0.3071, -0.0482,  ..., -0.3474, -0.7461, -0.4621],\n",
      "         [ 0.1236,  0.4846, -0.5044,  ..., -0.1222,  0.3344, -0.2640],\n",
      "         ...,\n",
      "         [ 0.3653, -0.3205, -0.1733,  ...,  0.3008,  0.0789,  0.2924],\n",
      "         [-0.1664,  0.1422, -0.1524,  ..., -0.6383, -0.1287, -0.1607],\n",
      "         [ 0.5374, -0.3034,  0.0781,  ...,  0.8236, -0.0473, -0.0960]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.1301, -0.0029,  0.0279,  ..., -0.5537,  0.2277,  0.0164],\n",
      "         [ 0.1918,  0.3238, -0.0537,  ..., -0.2626, -0.7967, -0.3142],\n",
      "         [ 0.1132,  0.4618, -0.4283,  ..., -0.1713,  0.5377, -0.2321],\n",
      "         ...,\n",
      "         [ 0.3497, -0.2043,  0.0634,  ...,  0.1898,  0.0069,  0.3800],\n",
      "         [-0.2446, -0.0196, -0.1887,  ..., -0.8407,  0.0366, -0.2673],\n",
      "         [ 0.5026, -0.1073,  0.0239,  ...,  0.8689, -0.0317, -0.0351]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.1301, -0.0029,  0.0279,  ..., -0.5537,  0.2277,  0.0164],\n",
      "         [ 0.1918,  0.3238, -0.0537,  ..., -0.2626, -0.7967, -0.3142],\n",
      "         [ 0.1132,  0.4618, -0.4283,  ..., -0.1713,  0.5377, -0.2321],\n",
      "         ...,\n",
      "         [ 0.3497, -0.2043,  0.0634,  ...,  0.1898,  0.0069,  0.3800],\n",
      "         [-0.2446, -0.0196, -0.1887,  ..., -0.8407,  0.0366, -0.2673],\n",
      "         [ 0.5026, -0.1073,  0.0239,  ...,  0.8689, -0.0317, -0.0351]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.1219,  0.0061, -0.0021,  ..., -0.5922,  0.2788, -0.0020],\n",
      "         [ 0.0516,  0.3172, -0.3561,  ..., -0.1298, -0.9021, -0.2543],\n",
      "         [ 0.1539,  0.3426, -0.5090,  ..., -0.1174,  0.5003, -0.0680],\n",
      "         ...,\n",
      "         [ 0.4603, -0.1087,  0.0583,  ...,  0.3766,  0.0228,  0.3992],\n",
      "         [-0.1897, -0.0568, -0.2409,  ..., -0.8521,  0.1953, -0.1673],\n",
      "         [ 0.5584, -0.1697,  0.0537,  ...,  0.9750, -0.1495,  0.1177]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.1219,  0.0061, -0.0021,  ..., -0.5922,  0.2788, -0.0020],\n",
      "         [ 0.0516,  0.3172, -0.3561,  ..., -0.1298, -0.9021, -0.2543],\n",
      "         [ 0.1539,  0.3426, -0.5090,  ..., -0.1174,  0.5003, -0.0680],\n",
      "         ...,\n",
      "         [ 0.4603, -0.1087,  0.0583,  ...,  0.3766,  0.0228,  0.3992],\n",
      "         [-0.1897, -0.0568, -0.2409,  ..., -0.8521,  0.1953, -0.1673],\n",
      "         [ 0.5584, -0.1697,  0.0537,  ...,  0.9750, -0.1495,  0.1177]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.1067,  0.0272,  0.0495,  ..., -0.5550,  0.3449,  0.0266],\n",
      "         [ 0.0030,  0.4310, -0.4001,  ..., -0.1070, -1.0737, -0.3328],\n",
      "         [ 0.0031,  0.3301, -0.4639,  ..., -0.0599,  0.5790, -0.0222],\n",
      "         ...,\n",
      "         [ 0.3821,  0.0270,  0.0438,  ...,  0.4287, -0.2827,  0.6594],\n",
      "         [-0.0326,  0.1470, -0.1880,  ..., -0.7391,  0.0573, -0.0748],\n",
      "         [ 0.4736, -0.3665, -0.1161,  ...,  1.0094, -0.3122,  0.3149]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.1067,  0.0272,  0.0495,  ..., -0.5550,  0.3449,  0.0266],\n",
      "         [ 0.0030,  0.4310, -0.4001,  ..., -0.1070, -1.0737, -0.3328],\n",
      "         [ 0.0031,  0.3301, -0.4639,  ..., -0.0599,  0.5790, -0.0222],\n",
      "         ...,\n",
      "         [ 0.3821,  0.0270,  0.0438,  ...,  0.4287, -0.2827,  0.6594],\n",
      "         [-0.0326,  0.1470, -0.1880,  ..., -0.7391,  0.0573, -0.0748],\n",
      "         [ 0.4736, -0.3665, -0.1161,  ...,  1.0094, -0.3122,  0.3149]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.0592,  0.0731,  0.0500,  ..., -0.5517,  0.3541,  0.0595],\n",
      "         [-0.0152,  0.3132, -0.1248,  ..., -0.0515, -0.8257, -0.2937],\n",
      "         [ 0.2519,  0.2475, -0.4471,  ..., -0.1518,  0.8188, -0.0107],\n",
      "         ...,\n",
      "         [ 0.4004, -0.1131,  0.5758,  ...,  0.3478, -0.1611,  0.4401],\n",
      "         [-0.0504,  0.0705, -0.2033,  ..., -1.1594,  0.1800, -0.2177],\n",
      "         [ 0.2514, -0.4683, -0.1488,  ...,  1.0137, -0.2811,  0.3120]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.0592,  0.0731,  0.0500,  ..., -0.5517,  0.3541,  0.0595],\n",
      "         [-0.0152,  0.3132, -0.1248,  ..., -0.0515, -0.8257, -0.2937],\n",
      "         [ 0.2519,  0.2475, -0.4471,  ..., -0.1518,  0.8188, -0.0107],\n",
      "         ...,\n",
      "         [ 0.4004, -0.1131,  0.5758,  ...,  0.3478, -0.1611,  0.4401],\n",
      "         [-0.0504,  0.0705, -0.2033,  ..., -1.1594,  0.1800, -0.2177],\n",
      "         [ 0.2514, -0.4683, -0.1488,  ...,  1.0137, -0.2811,  0.3120]]],\n",
      "       device='cuda:0'),) and output (tensor([[[ 0.1143,  0.2237,  0.3292,  ..., -0.7706,  0.2608,  0.0269],\n",
      "         [ 0.2160,  0.4125, -0.3778,  ..., -0.0621, -1.2652, -0.3365],\n",
      "         [ 0.4303,  0.2992, -0.3946,  ..., -0.1232,  0.6271,  0.2989],\n",
      "         ...,\n",
      "         [ 0.3517, -0.2923,  0.5924,  ...,  0.3504, -0.0915,  0.7293],\n",
      "         [ 0.0066, -0.0552, -0.5737,  ..., -1.1444, -0.0347, -0.1040],\n",
      "         [ 0.0980, -0.4980, -0.0145,  ...,  1.1200, -0.3426,  0.4796]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[ 0.1143,  0.2237,  0.3292,  ..., -0.7706,  0.2608,  0.0269],\n",
      "         [ 0.2160,  0.4125, -0.3778,  ..., -0.0621, -1.2652, -0.3365],\n",
      "         [ 0.4303,  0.2992, -0.3946,  ..., -0.1232,  0.6271,  0.2989],\n",
      "         ...,\n",
      "         [ 0.3517, -0.2923,  0.5924,  ...,  0.3504, -0.0915,  0.7293],\n",
      "         [ 0.0066, -0.0552, -0.5737,  ..., -1.1444, -0.0347, -0.1040],\n",
      "         [ 0.0980, -0.4980, -0.0145,  ...,  1.1200, -0.3426,  0.4796]]],\n",
      "       device='cuda:0'),) and output (tensor([[[ 0.3711,  0.5489,  0.6100,  ..., -0.7499,  0.2080,  0.4195],\n",
      "         [ 0.1289,  0.5740, -0.8998,  ..., -0.1417, -1.2287, -0.4084],\n",
      "         [ 0.7976,  0.1791, -0.2995,  ..., -0.5437,  0.3638,  0.1964],\n",
      "         ...,\n",
      "         [ 0.3191, -0.5055,  0.2682,  ...,  0.1869, -0.4592,  0.2463],\n",
      "         [-0.5350, -0.2270, -0.8182,  ..., -0.9436,  0.0250, -0.8134],\n",
      "         [ 0.0957, -1.0664, -0.3032,  ...,  1.2510, -0.3474,  0.0333]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[ 0.3711,  0.5489,  0.6100,  ..., -0.7499,  0.2080,  0.4195],\n",
      "         [ 0.1289,  0.5740, -0.8998,  ..., -0.1417, -1.2287, -0.4084],\n",
      "         [ 0.7976,  0.1791, -0.2995,  ..., -0.5437,  0.3638,  0.1964],\n",
      "         ...,\n",
      "         [ 0.3191, -0.5055,  0.2682,  ...,  0.1869, -0.4592,  0.2463],\n",
      "         [-0.5350, -0.2270, -0.8182,  ..., -0.9436,  0.0250, -0.8134],\n",
      "         [ 0.0957, -1.0664, -0.3032,  ...,  1.2510, -0.3474,  0.0333]]],\n",
      "       device='cuda:0'),) and output (tensor([[[ 1.1544,  3.2965,  1.6534,  ..., -2.7009,  2.6553,  2.2700],\n",
      "         [-0.4826,  1.1279, -2.0377,  ..., -1.0675, -1.8136,  0.5726],\n",
      "         [ 0.9234,  0.3427, -0.1031,  ..., -1.3464,  0.5416,  0.6494],\n",
      "         ...,\n",
      "         [-0.7960, -1.4722,  0.4406,  ...,  0.2429,  0.0368,  0.8364],\n",
      "         [-0.6580, -1.2585, -0.7493,  ..., -0.1492, -0.7798, -0.0348],\n",
      "         [ 0.1321, -1.3325, -0.7360,  ...,  2.5342,  0.0526,  0.6984]]],\n",
      "       device='cuda:0'),)\n",
      "[Debug] Layer names being passed: ['model.layers.0', 'model.layers.1', 'model.layers.2', 'model.layers.3', 'model.layers.4', 'model.layers.5', 'model.layers.6', 'model.layers.7', 'model.layers.8', 'model.layers.9', 'model.layers.10', 'model.layers.11', 'model.layers.12', 'model.layers.13', 'model.layers.14', 'model.layers.15', 'model.layers.16', 'model.layers.17', 'model.layers.18', 'model.layers.19', 'model.layers.20', 'model.layers.21', 'model.layers.22', 'model.layers.23', 'model.layers.24', 'model.layers.25', 'model.layers.26', 'model.layers.27', 'model.layers.28', 'model.layers.29', 'model.layers.30', 'model.layers.31', 'model.embed_tokens']\n",
      "[Debug] Trying to access layer: model.layers.0\n",
      "[Debug] Successfully found layer: model.layers.0\n",
      "[Debug] Trying to access layer: model.layers.1\n",
      "[Debug] Successfully found layer: model.layers.1\n",
      "[Debug] Trying to access layer: model.layers.2\n",
      "[Debug] Successfully found layer: model.layers.2\n",
      "[Debug] Trying to access layer: model.layers.3\n",
      "[Debug] Successfully found layer: model.layers.3\n",
      "[Debug] Trying to access layer: model.layers.4\n",
      "[Debug] Successfully found layer: model.layers.4\n",
      "[Debug] Trying to access layer: model.layers.5\n",
      "[Debug] Successfully found layer: model.layers.5\n",
      "[Debug] Trying to access layer: model.layers.6\n",
      "[Debug] Successfully found layer: model.layers.6\n",
      "[Debug] Trying to access layer: model.layers.7\n",
      "[Debug] Successfully found layer: model.layers.7\n",
      "[Debug] Trying to access layer: model.layers.8\n",
      "[Debug] Successfully found layer: model.layers.8\n",
      "[Debug] Trying to access layer: model.layers.9\n",
      "[Debug] Successfully found layer: model.layers.9\n",
      "[Debug] Trying to access layer: model.layers.10\n",
      "[Debug] Successfully found layer: model.layers.10\n",
      "[Debug] Trying to access layer: model.layers.11\n",
      "[Debug] Successfully found layer: model.layers.11\n",
      "[Debug] Trying to access layer: model.layers.12\n",
      "[Debug] Successfully found layer: model.layers.12\n",
      "[Debug] Trying to access layer: model.layers.13\n",
      "[Debug] Successfully found layer: model.layers.13\n",
      "[Debug] Trying to access layer: model.layers.14\n",
      "[Debug] Successfully found layer: model.layers.14\n",
      "[Debug] Trying to access layer: model.layers.15\n",
      "[Debug] Successfully found layer: model.layers.15\n",
      "[Debug] Trying to access layer: model.layers.16\n",
      "[Debug] Successfully found layer: model.layers.16\n",
      "[Debug] Trying to access layer: model.layers.17\n",
      "[Debug] Successfully found layer: model.layers.17\n",
      "[Debug] Trying to access layer: model.layers.18\n",
      "[Debug] Successfully found layer: model.layers.18\n",
      "[Debug] Trying to access layer: model.layers.19\n",
      "[Debug] Successfully found layer: model.layers.19\n",
      "[Debug] Trying to access layer: model.layers.20\n",
      "[Debug] Successfully found layer: model.layers.20\n",
      "[Debug] Trying to access layer: model.layers.21\n",
      "[Debug] Successfully found layer: model.layers.21\n",
      "[Debug] Trying to access layer: model.layers.22\n",
      "[Debug] Successfully found layer: model.layers.22\n",
      "[Debug] Trying to access layer: model.layers.23\n",
      "[Debug] Successfully found layer: model.layers.23\n",
      "[Debug] Trying to access layer: model.layers.24\n",
      "[Debug] Successfully found layer: model.layers.24\n",
      "[Debug] Trying to access layer: model.layers.25\n",
      "[Debug] Successfully found layer: model.layers.25\n",
      "[Debug] Trying to access layer: model.layers.26\n",
      "[Debug] Successfully found layer: model.layers.26\n",
      "[Debug] Trying to access layer: model.layers.27\n",
      "[Debug] Successfully found layer: model.layers.27\n",
      "[Debug] Trying to access layer: model.layers.28\n",
      "[Debug] Successfully found layer: model.layers.28\n",
      "[Debug] Trying to access layer: model.layers.29\n",
      "[Debug] Successfully found layer: model.layers.29\n",
      "[Debug] Trying to access layer: model.layers.30\n",
      "[Debug] Successfully found layer: model.layers.30\n",
      "[Debug] Trying to access layer: model.layers.31\n",
      "[Debug] Successfully found layer: model.layers.31\n",
      "[Debug] Trying to access layer: model.embed_tokens\n",
      "[Debug] Successfully found layer: model.embed_tokens\n",
      "[Hook] Layer Embedding(128256, 4096) received input (tensor([[128000,     44,    980,  61023,    386,    980,    285,   3145,  12446,\n",
      "            304,  11427,  44104,     82,    323,  16700,  86497,    320,  21470,\n",
      "            439,  10913,  13936,     11,    477,    279,   4892,   3778,   8681,\n",
      "          63911,    570,   4314,  22484,    617,  35901,   1027,  16917,  28160,\n",
      "            555,   4027,    323,  44611,  10696,    354,  65916,    505,  44304,\n",
      "           1778,    439,  61495,    323,   9578,  56741,     82,     13,   1102,\n",
      "            574,  13240,    430,    304,    220,   1049,     18,     11,   1193,\n",
      "            220,    975,    311,    220,   1627,   3346,    315,  16763,   1974,\n",
      "          61951,   2103,  14958,    304,    264,  12309,   5933,   1614,    320,\n",
      "           9210,    374,     11,    814,   1051,    539,   1511,    369,  30029,\n",
      "           4245,    311,    279,  48111,    315,    279,    362,  35174,    570,\n",
      "          63388,    750,     11,    814,   4097,   4056,     22,      4,    315,\n",
      "          10054,  12862,   4363,   3158,     13,   1666,    279,   1917,    596,\n",
      "           1455,  17744,  43024,  27331,  17614,   2015,     11,    279,    386,\n",
      "            980,    285,   3145,   4097,    832,    315,    279,    810,  47379,\n",
      "           3062,  17614,  10373,     13]], device='cuda:0'),) and output tensor([[[ 2.6512e-04, -4.9973e-04, -5.8365e-04,  ...,  3.8147e-03,\n",
      "           6.3419e-05,  1.1902e-03],\n",
      "         [-1.7578e-02, -4.5776e-03,  1.2100e-05,  ...,  4.8828e-04,\n",
      "           1.4526e-02, -4.5471e-03],\n",
      "         [-6.9885e-03, -1.6113e-02,  1.0803e-02,  ...,  3.1433e-03,\n",
      "           2.0996e-02, -8.0566e-03],\n",
      "         ...,\n",
      "         [-1.3000e-02,  1.1597e-02,  8.6670e-03,  ...,  6.3171e-03,\n",
      "           8.9111e-03,  1.4954e-02],\n",
      "         [-5.2795e-03, -5.8594e-03,  4.1008e-04,  ...,  8.7280e-03,\n",
      "          -3.9368e-03, -3.3112e-03],\n",
      "         [-6.7520e-04, -5.7602e-04,  4.7684e-04,  ...,  3.3264e-03,\n",
      "           3.9101e-04,  4.7493e-04]]], device='cuda:0')\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[ 2.6512e-04, -4.9973e-04, -5.8365e-04,  ...,  3.8147e-03,\n",
      "           6.3419e-05,  1.1902e-03],\n",
      "         [-1.7578e-02, -4.5776e-03,  1.2100e-05,  ...,  4.8828e-04,\n",
      "           1.4526e-02, -4.5471e-03],\n",
      "         [-6.9885e-03, -1.6113e-02,  1.0803e-02,  ...,  3.1433e-03,\n",
      "           2.0996e-02, -8.0566e-03],\n",
      "         ...,\n",
      "         [-1.3000e-02,  1.1597e-02,  8.6670e-03,  ...,  6.3171e-03,\n",
      "           8.9111e-03,  1.4954e-02],\n",
      "         [-5.2795e-03, -5.8594e-03,  4.1008e-04,  ...,  8.7280e-03,\n",
      "          -3.9368e-03, -3.3112e-03],\n",
      "         [-6.7520e-04, -5.7602e-04,  4.7684e-04,  ...,  3.3264e-03,\n",
      "           3.9101e-04,  4.7493e-04]]], device='cuda:0'),) and output (tensor([[[-0.0004,  0.0146, -0.0020,  ...,  0.0066, -0.0194,  0.0020],\n",
      "         [-0.0156,  0.0055, -0.0086,  ..., -0.0090,  0.0079, -0.0181],\n",
      "         [-0.0108, -0.0046,  0.0034,  ..., -0.0246,  0.0211, -0.0467],\n",
      "         ...,\n",
      "         [-0.0099,  0.0128,  0.0117,  ...,  0.0196,  0.0054,  0.0038],\n",
      "         [ 0.0067, -0.0142, -0.0140,  ..., -0.0249, -0.0067,  0.0001],\n",
      "         [ 0.0045, -0.0030,  0.0060,  ...,  0.0126,  0.0050, -0.0023]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.0004,  0.0146, -0.0020,  ...,  0.0066, -0.0194,  0.0020],\n",
      "         [-0.0156,  0.0055, -0.0086,  ..., -0.0090,  0.0079, -0.0181],\n",
      "         [-0.0108, -0.0046,  0.0034,  ..., -0.0246,  0.0211, -0.0467],\n",
      "         ...,\n",
      "         [-0.0099,  0.0128,  0.0117,  ...,  0.0196,  0.0054,  0.0038],\n",
      "         [ 0.0067, -0.0142, -0.0140,  ..., -0.0249, -0.0067,  0.0001],\n",
      "         [ 0.0045, -0.0030,  0.0060,  ...,  0.0126,  0.0050, -0.0023]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.0972,  0.0787, -0.0419,  ...,  0.0071,  0.0869,  0.0218],\n",
      "         [-0.0106,  0.0075, -0.0282,  ..., -0.0144, -0.0104, -0.0150],\n",
      "         [-0.0221,  0.0041,  0.0228,  ..., -0.0703,  0.0091, -0.0327],\n",
      "         ...,\n",
      "         [-0.0039,  0.0233,  0.0062,  ...,  0.0634,  0.0065,  0.0067],\n",
      "         [ 0.0100, -0.0011, -0.0210,  ..., -0.0694, -0.0110, -0.0309],\n",
      "         [ 0.0062, -0.0050, -0.0039,  ...,  0.0247,  0.0026, -0.0029]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.0972,  0.0787, -0.0419,  ...,  0.0071,  0.0869,  0.0218],\n",
      "         [-0.0106,  0.0075, -0.0282,  ..., -0.0144, -0.0104, -0.0150],\n",
      "         [-0.0221,  0.0041,  0.0228,  ..., -0.0703,  0.0091, -0.0327],\n",
      "         ...,\n",
      "         [-0.0039,  0.0233,  0.0062,  ...,  0.0634,  0.0065,  0.0067],\n",
      "         [ 0.0100, -0.0011, -0.0210,  ..., -0.0694, -0.0110, -0.0309],\n",
      "         [ 0.0062, -0.0050, -0.0039,  ...,  0.0247,  0.0026, -0.0029]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.1040,  0.0875, -0.0360,  ...,  0.0205,  0.0997,  0.0184],\n",
      "         [-0.0214, -0.0150, -0.0318,  ...,  0.0080, -0.0286,  0.0006],\n",
      "         [-0.0055,  0.0133,  0.0387,  ..., -0.0633, -0.0016, -0.0536],\n",
      "         ...,\n",
      "         [-0.0078,  0.0063,  0.0398,  ...,  0.0810,  0.0130, -0.0262],\n",
      "         [-0.0168, -0.0012, -0.0191,  ..., -0.0700, -0.0072, -0.0359],\n",
      "         [-0.0097, -0.0051,  0.0092,  ...,  0.0382, -0.0002, -0.0040]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.1040,  0.0875, -0.0360,  ...,  0.0205,  0.0997,  0.0184],\n",
      "         [-0.0214, -0.0150, -0.0318,  ...,  0.0080, -0.0286,  0.0006],\n",
      "         [-0.0055,  0.0133,  0.0387,  ..., -0.0633, -0.0016, -0.0536],\n",
      "         ...,\n",
      "         [-0.0078,  0.0063,  0.0398,  ...,  0.0810,  0.0130, -0.0262],\n",
      "         [-0.0168, -0.0012, -0.0191,  ..., -0.0700, -0.0072, -0.0359],\n",
      "         [-0.0097, -0.0051,  0.0092,  ...,  0.0382, -0.0002, -0.0040]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.0870,  0.1049, -0.0173,  ...,  0.0700,  0.0989,  0.0181],\n",
      "         [-0.0322,  0.0362, -0.0509,  ..., -0.0366, -0.0298, -0.0565],\n",
      "         [ 0.0263,  0.0423,  0.0520,  ..., -0.0743,  0.0150, -0.0378],\n",
      "         ...,\n",
      "         [-0.0602, -0.0243,  0.1069,  ...,  0.0207,  0.0005, -0.0336],\n",
      "         [-0.0191,  0.0057,  0.0298,  ..., -0.0948, -0.0216, -0.0555],\n",
      "         [-0.0196, -0.0111, -0.0152,  ...,  0.0224, -0.0122, -0.0257]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.0870,  0.1049, -0.0173,  ...,  0.0700,  0.0989,  0.0181],\n",
      "         [-0.0322,  0.0362, -0.0509,  ..., -0.0366, -0.0298, -0.0565],\n",
      "         [ 0.0263,  0.0423,  0.0520,  ..., -0.0743,  0.0150, -0.0378],\n",
      "         ...,\n",
      "         [-0.0602, -0.0243,  0.1069,  ...,  0.0207,  0.0005, -0.0336],\n",
      "         [-0.0191,  0.0057,  0.0298,  ..., -0.0948, -0.0216, -0.0555],\n",
      "         [-0.0196, -0.0111, -0.0152,  ...,  0.0224, -0.0122, -0.0257]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.1113,  0.1022, -0.0102,  ...,  0.0436,  0.1283,  0.0321],\n",
      "         [-0.0404,  0.0243, -0.0168,  ...,  0.0161, -0.0432, -0.0095],\n",
      "         [ 0.0106,  0.0494,  0.0094,  ..., -0.0904,  0.0065, -0.0440],\n",
      "         ...,\n",
      "         [-0.0307, -0.0367,  0.0846,  ...,  0.1432, -0.0211, -0.0503],\n",
      "         [ 0.0422,  0.0220,  0.0688,  ..., -0.0314, -0.0297, -0.0329],\n",
      "         [ 0.0051,  0.0085, -0.0055,  ...,  0.0989, -0.0048, -0.0746]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.1113,  0.1022, -0.0102,  ...,  0.0436,  0.1283,  0.0321],\n",
      "         [-0.0404,  0.0243, -0.0168,  ...,  0.0161, -0.0432, -0.0095],\n",
      "         [ 0.0106,  0.0494,  0.0094,  ..., -0.0904,  0.0065, -0.0440],\n",
      "         ...,\n",
      "         [-0.0307, -0.0367,  0.0846,  ...,  0.1432, -0.0211, -0.0503],\n",
      "         [ 0.0422,  0.0220,  0.0688,  ..., -0.0314, -0.0297, -0.0329],\n",
      "         [ 0.0051,  0.0085, -0.0055,  ...,  0.0989, -0.0048, -0.0746]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.1034,  0.1132,  0.0096,  ...,  0.0453,  0.1477,  0.0301],\n",
      "         [-0.0101,  0.0164, -0.0234,  ..., -0.0258, -0.0481,  0.0192],\n",
      "         [ 0.0384,  0.0082,  0.0296,  ..., -0.1857,  0.0477, -0.0354],\n",
      "         ...,\n",
      "         [-0.0593, -0.0625,  0.0537,  ...,  0.0678, -0.0361, -0.0619],\n",
      "         [ 0.0585,  0.0112,  0.0734,  ...,  0.0167, -0.0504, -0.0640],\n",
      "         [ 0.0067,  0.0165, -0.0389,  ...,  0.1168,  0.0097, -0.0973]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.1034,  0.1132,  0.0096,  ...,  0.0453,  0.1477,  0.0301],\n",
      "         [-0.0101,  0.0164, -0.0234,  ..., -0.0258, -0.0481,  0.0192],\n",
      "         [ 0.0384,  0.0082,  0.0296,  ..., -0.1857,  0.0477, -0.0354],\n",
      "         ...,\n",
      "         [-0.0593, -0.0625,  0.0537,  ...,  0.0678, -0.0361, -0.0619],\n",
      "         [ 0.0585,  0.0112,  0.0734,  ...,  0.0167, -0.0504, -0.0640],\n",
      "         [ 0.0067,  0.0165, -0.0389,  ...,  0.1168,  0.0097, -0.0973]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.1071,  0.1253,  0.0211,  ...,  0.0126,  0.1586,  0.0278],\n",
      "         [-0.0009,  0.0305, -0.0369,  ...,  0.0222,  0.0129,  0.0224],\n",
      "         [ 0.0306, -0.0746,  0.0637,  ..., -0.1131,  0.0405, -0.0705],\n",
      "         ...,\n",
      "         [-0.0862, -0.0611,  0.0694,  ...,  0.1098, -0.0072, -0.0194],\n",
      "         [ 0.0352,  0.0144,  0.0496,  ..., -0.0324, -0.0537, -0.0096],\n",
      "         [ 0.0405,  0.0499, -0.0276,  ...,  0.1341, -0.0695, -0.0632]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.1071,  0.1253,  0.0211,  ...,  0.0126,  0.1586,  0.0278],\n",
      "         [-0.0009,  0.0305, -0.0369,  ...,  0.0222,  0.0129,  0.0224],\n",
      "         [ 0.0306, -0.0746,  0.0637,  ..., -0.1131,  0.0405, -0.0705],\n",
      "         ...,\n",
      "         [-0.0862, -0.0611,  0.0694,  ...,  0.1098, -0.0072, -0.0194],\n",
      "         [ 0.0352,  0.0144,  0.0496,  ..., -0.0324, -0.0537, -0.0096],\n",
      "         [ 0.0405,  0.0499, -0.0276,  ...,  0.1341, -0.0695, -0.0632]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.0945,  0.1848,  0.0036,  ..., -0.0473,  0.1489,  0.0536],\n",
      "         [ 0.0915,  0.0165, -0.0529,  ..., -0.0484, -0.1050, -0.0297],\n",
      "         [ 0.0377, -0.0764, -0.0046,  ..., -0.2297, -0.0228, -0.0829],\n",
      "         ...,\n",
      "         [-0.0489, -0.0799,  0.1183,  ...,  0.1533, -0.0202, -0.0518],\n",
      "         [ 0.0384, -0.0093,  0.0566,  ...,  0.0089, -0.0793,  0.0146],\n",
      "         [ 0.0057,  0.0351,  0.0476,  ...,  0.1508, -0.1131, -0.0637]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.0945,  0.1848,  0.0036,  ..., -0.0473,  0.1489,  0.0536],\n",
      "         [ 0.0915,  0.0165, -0.0529,  ..., -0.0484, -0.1050, -0.0297],\n",
      "         [ 0.0377, -0.0764, -0.0046,  ..., -0.2297, -0.0228, -0.0829],\n",
      "         ...,\n",
      "         [-0.0489, -0.0799,  0.1183,  ...,  0.1533, -0.0202, -0.0518],\n",
      "         [ 0.0384, -0.0093,  0.0566,  ...,  0.0089, -0.0793,  0.0146],\n",
      "         [ 0.0057,  0.0351,  0.0476,  ...,  0.1508, -0.1131, -0.0637]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.1060,  0.2113,  0.0270,  ..., -0.1141,  0.1610,  0.0528],\n",
      "         [ 0.1157,  0.0882, -0.0731,  ..., -0.0179, -0.1438, -0.0448],\n",
      "         [ 0.0715, -0.0669, -0.0079,  ..., -0.1993, -0.0338, -0.1051],\n",
      "         ...,\n",
      "         [-0.1251, -0.1375,  0.0661,  ...,  0.1389, -0.0149, -0.1452],\n",
      "         [-0.0272, -0.1453,  0.0401,  ...,  0.0207, -0.1052, -0.0822],\n",
      "         [-0.0233, -0.0594,  0.1538,  ...,  0.1491, -0.0090, -0.0679]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.1060,  0.2113,  0.0270,  ..., -0.1141,  0.1610,  0.0528],\n",
      "         [ 0.1157,  0.0882, -0.0731,  ..., -0.0179, -0.1438, -0.0448],\n",
      "         [ 0.0715, -0.0669, -0.0079,  ..., -0.1993, -0.0338, -0.1051],\n",
      "         ...,\n",
      "         [-0.1251, -0.1375,  0.0661,  ...,  0.1389, -0.0149, -0.1452],\n",
      "         [-0.0272, -0.1453,  0.0401,  ...,  0.0207, -0.1052, -0.0822],\n",
      "         [-0.0233, -0.0594,  0.1538,  ...,  0.1491, -0.0090, -0.0679]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-9.5415e-02,  2.4926e-01,  7.3336e-03,  ..., -1.5681e-01,\n",
      "           1.8185e-01,  8.5524e-02],\n",
      "         [ 3.7089e-02,  8.2143e-02, -1.8715e-02,  ...,  2.1825e-03,\n",
      "          -8.6159e-02, -2.9206e-02],\n",
      "         [-1.5731e-02, -1.2406e-01,  1.8297e-04,  ..., -1.7006e-01,\n",
      "           4.7145e-02, -6.7738e-02],\n",
      "         ...,\n",
      "         [-2.1424e-01, -1.0271e-01,  6.0035e-02,  ...,  1.0321e-01,\n",
      "           2.1759e-02, -5.9358e-02],\n",
      "         [-1.3463e-02, -9.5619e-02,  1.1177e-01,  ...,  3.4360e-02,\n",
      "          -1.0515e-03, -2.8637e-02],\n",
      "         [ 5.3165e-02, -9.4702e-02,  9.0260e-02,  ...,  2.1855e-01,\n",
      "           1.3619e-01, -1.0811e-01]]], device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-9.5415e-02,  2.4926e-01,  7.3336e-03,  ..., -1.5681e-01,\n",
      "           1.8185e-01,  8.5524e-02],\n",
      "         [ 3.7089e-02,  8.2143e-02, -1.8715e-02,  ...,  2.1825e-03,\n",
      "          -8.6159e-02, -2.9206e-02],\n",
      "         [-1.5731e-02, -1.2406e-01,  1.8297e-04,  ..., -1.7006e-01,\n",
      "           4.7145e-02, -6.7738e-02],\n",
      "         ...,\n",
      "         [-2.1424e-01, -1.0271e-01,  6.0035e-02,  ...,  1.0321e-01,\n",
      "           2.1759e-02, -5.9358e-02],\n",
      "         [-1.3463e-02, -9.5619e-02,  1.1177e-01,  ...,  3.4360e-02,\n",
      "          -1.0515e-03, -2.8637e-02],\n",
      "         [ 5.3165e-02, -9.4702e-02,  9.0260e-02,  ...,  2.1855e-01,\n",
      "           1.3619e-01, -1.0811e-01]]], device='cuda:0'),) and output (tensor([[[-0.0152,  0.3340,  0.0011,  ..., -0.2750,  0.1933,  0.1034],\n",
      "         [ 0.0873,  0.0593,  0.0111,  ...,  0.0015, -0.0911, -0.0201],\n",
      "         [ 0.0328, -0.1141, -0.0423,  ..., -0.1849,  0.0551, -0.0774],\n",
      "         ...,\n",
      "         [-0.1207, -0.1232,  0.0638,  ...,  0.1630,  0.0485,  0.0387],\n",
      "         [ 0.0289, -0.0962,  0.1823,  ...,  0.1470, -0.0332,  0.0520],\n",
      "         [ 0.0353, -0.0895,  0.1343,  ...,  0.1849, -0.0163, -0.2151]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.0152,  0.3340,  0.0011,  ..., -0.2750,  0.1933,  0.1034],\n",
      "         [ 0.0873,  0.0593,  0.0111,  ...,  0.0015, -0.0911, -0.0201],\n",
      "         [ 0.0328, -0.1141, -0.0423,  ..., -0.1849,  0.0551, -0.0774],\n",
      "         ...,\n",
      "         [-0.1207, -0.1232,  0.0638,  ...,  0.1630,  0.0485,  0.0387],\n",
      "         [ 0.0289, -0.0962,  0.1823,  ...,  0.1470, -0.0332,  0.0520],\n",
      "         [ 0.0353, -0.0895,  0.1343,  ...,  0.1849, -0.0163, -0.2151]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.0356,  0.3483, -0.0366,  ..., -0.2962,  0.2088,  0.1065],\n",
      "         [ 0.0704,  0.0491,  0.0137,  ..., -0.0201, -0.0566, -0.0140],\n",
      "         [-0.0019, -0.1195,  0.0184,  ..., -0.1688,  0.1203, -0.0356],\n",
      "         ...,\n",
      "         [-0.1538, -0.0835, -0.0595,  ...,  0.1541, -0.0272,  0.0302],\n",
      "         [-0.0587, -0.1796,  0.1238,  ...,  0.1445, -0.0373, -0.0330],\n",
      "         [ 0.0404,  0.0288,  0.0394,  ...,  0.1641, -0.0736, -0.2138]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.0356,  0.3483, -0.0366,  ..., -0.2962,  0.2088,  0.1065],\n",
      "         [ 0.0704,  0.0491,  0.0137,  ..., -0.0201, -0.0566, -0.0140],\n",
      "         [-0.0019, -0.1195,  0.0184,  ..., -0.1688,  0.1203, -0.0356],\n",
      "         ...,\n",
      "         [-0.1538, -0.0835, -0.0595,  ...,  0.1541, -0.0272,  0.0302],\n",
      "         [-0.0587, -0.1796,  0.1238,  ...,  0.1445, -0.0373, -0.0330],\n",
      "         [ 0.0404,  0.0288,  0.0394,  ...,  0.1641, -0.0736, -0.2138]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.0127,  0.3661, -0.0316,  ..., -0.3678,  0.1973,  0.1019],\n",
      "         [ 0.0486,  0.0154,  0.0134,  ..., -0.0451, -0.1183,  0.0463],\n",
      "         [ 0.0310, -0.1187,  0.0409,  ..., -0.1317,  0.0800, -0.0143],\n",
      "         ...,\n",
      "         [-0.1290, -0.0978, -0.0062,  ...,  0.1406, -0.0198, -0.0133],\n",
      "         [ 0.0708, -0.0677,  0.1604,  ...,  0.1429, -0.0798, -0.0976],\n",
      "         [ 0.0790, -0.0871,  0.1508,  ...,  0.1375, -0.2248, -0.1442]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.0127,  0.3661, -0.0316,  ..., -0.3678,  0.1973,  0.1019],\n",
      "         [ 0.0486,  0.0154,  0.0134,  ..., -0.0451, -0.1183,  0.0463],\n",
      "         [ 0.0310, -0.1187,  0.0409,  ..., -0.1317,  0.0800, -0.0143],\n",
      "         ...,\n",
      "         [-0.1290, -0.0978, -0.0062,  ...,  0.1406, -0.0198, -0.0133],\n",
      "         [ 0.0708, -0.0677,  0.1604,  ...,  0.1429, -0.0798, -0.0976],\n",
      "         [ 0.0790, -0.0871,  0.1508,  ...,  0.1375, -0.2248, -0.1442]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.0089,  0.3233, -0.0779,  ..., -0.4491,  0.1845,  0.0951],\n",
      "         [-0.1038,  0.0203,  0.0225,  ..., -0.0957, -0.1294, -0.0272],\n",
      "         [-0.0130, -0.0434, -0.0069,  ..., -0.1277,  0.0815, -0.0636],\n",
      "         ...,\n",
      "         [-0.0911, -0.0669, -0.0192,  ...,  0.0841, -0.1132,  0.0594],\n",
      "         [ 0.0443, -0.0917,  0.1959,  ...,  0.0909, -0.2096, -0.1216],\n",
      "         [ 0.1189, -0.0537,  0.2174,  ...,  0.0285, -0.1527, -0.0286]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.0089,  0.3233, -0.0779,  ..., -0.4491,  0.1845,  0.0951],\n",
      "         [-0.1038,  0.0203,  0.0225,  ..., -0.0957, -0.1294, -0.0272],\n",
      "         [-0.0130, -0.0434, -0.0069,  ..., -0.1277,  0.0815, -0.0636],\n",
      "         ...,\n",
      "         [-0.0911, -0.0669, -0.0192,  ...,  0.0841, -0.1132,  0.0594],\n",
      "         [ 0.0443, -0.0917,  0.1959,  ...,  0.0909, -0.2096, -0.1216],\n",
      "         [ 0.1189, -0.0537,  0.2174,  ...,  0.0285, -0.1527, -0.0286]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.0244,  0.2828, -0.0532,  ..., -0.4592,  0.2300,  0.0142],\n",
      "         [-0.0480, -0.0673, -0.0312,  ..., -0.1289, -0.1389, -0.0713],\n",
      "         [-0.0138, -0.0681, -0.0787,  ..., -0.0351,  0.0369, -0.0534],\n",
      "         ...,\n",
      "         [-0.2096, -0.0861,  0.1110,  ...,  0.0541, -0.1143, -0.0006],\n",
      "         [ 0.0286, -0.1203,  0.1937,  ...,  0.0065, -0.1185, -0.0592],\n",
      "         [ 0.0041, -0.0740,  0.1777,  ...,  0.0344, -0.1198, -0.0132]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.0244,  0.2828, -0.0532,  ..., -0.4592,  0.2300,  0.0142],\n",
      "         [-0.0480, -0.0673, -0.0312,  ..., -0.1289, -0.1389, -0.0713],\n",
      "         [-0.0138, -0.0681, -0.0787,  ..., -0.0351,  0.0369, -0.0534],\n",
      "         ...,\n",
      "         [-0.2096, -0.0861,  0.1110,  ...,  0.0541, -0.1143, -0.0006],\n",
      "         [ 0.0286, -0.1203,  0.1937,  ...,  0.0065, -0.1185, -0.0592],\n",
      "         [ 0.0041, -0.0740,  0.1777,  ...,  0.0344, -0.1198, -0.0132]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.0928,  0.1485,  0.0158,  ..., -0.5380,  0.2664, -0.0333],\n",
      "         [-0.1377, -0.1962,  0.0231,  ..., -0.1440, -0.0563, -0.1441],\n",
      "         [-0.0591,  0.0342, -0.0495,  ...,  0.0292,  0.1437, -0.1378],\n",
      "         ...,\n",
      "         [-0.0837, -0.0880,  0.2445,  ...,  0.0045, -0.1292,  0.0459],\n",
      "         [ 0.1750,  0.0246,  0.3133,  ...,  0.0043, -0.2157, -0.0136],\n",
      "         [ 0.1332, -0.0834,  0.2010,  ..., -0.0692, -0.2003,  0.0664]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.0928,  0.1485,  0.0158,  ..., -0.5380,  0.2664, -0.0333],\n",
      "         [-0.1377, -0.1962,  0.0231,  ..., -0.1440, -0.0563, -0.1441],\n",
      "         [-0.0591,  0.0342, -0.0495,  ...,  0.0292,  0.1437, -0.1378],\n",
      "         ...,\n",
      "         [-0.0837, -0.0880,  0.2445,  ...,  0.0045, -0.1292,  0.0459],\n",
      "         [ 0.1750,  0.0246,  0.3133,  ...,  0.0043, -0.2157, -0.0136],\n",
      "         [ 0.1332, -0.0834,  0.2010,  ..., -0.0692, -0.2003,  0.0664]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.1000,  0.1323,  0.0241,  ..., -0.5470,  0.2844, -0.0457],\n",
      "         [-0.0961, -0.2788,  0.0785,  ..., -0.0467, -0.1335, -0.2066],\n",
      "         [ 0.0297,  0.0249, -0.0964,  ...,  0.1046,  0.1128, -0.0249],\n",
      "         ...,\n",
      "         [ 0.0360, -0.1820,  0.2648,  ...,  0.0987, -0.0925,  0.1252],\n",
      "         [ 0.1741, -0.0661,  0.1956,  ...,  0.2070, -0.2543, -0.0790],\n",
      "         [ 0.0384, -0.0386,  0.2138,  ...,  0.0616, -0.3678, -0.0118]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.1000,  0.1323,  0.0241,  ..., -0.5470,  0.2844, -0.0457],\n",
      "         [-0.0961, -0.2788,  0.0785,  ..., -0.0467, -0.1335, -0.2066],\n",
      "         [ 0.0297,  0.0249, -0.0964,  ...,  0.1046,  0.1128, -0.0249],\n",
      "         ...,\n",
      "         [ 0.0360, -0.1820,  0.2648,  ...,  0.0987, -0.0925,  0.1252],\n",
      "         [ 0.1741, -0.0661,  0.1956,  ...,  0.2070, -0.2543, -0.0790],\n",
      "         [ 0.0384, -0.0386,  0.2138,  ...,  0.0616, -0.3678, -0.0118]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.1286,  0.0833,  0.0239,  ..., -0.5657,  0.3694, -0.0725],\n",
      "         [-0.2082, -0.2146,  0.1783,  ..., -0.0760, -0.1919, -0.2075],\n",
      "         [-0.0141,  0.0910, -0.0518,  ...,  0.0349,  0.0727, -0.0534],\n",
      "         ...,\n",
      "         [-0.0756, -0.0179,  0.2566,  ...,  0.1463, -0.1267,  0.0371],\n",
      "         [ 0.0610,  0.1105,  0.1701,  ...,  0.0163, -0.2404, -0.0722],\n",
      "         [ 0.0715,  0.0032,  0.2514,  ..., -0.0107, -0.4976, -0.0868]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.1286,  0.0833,  0.0239,  ..., -0.5657,  0.3694, -0.0725],\n",
      "         [-0.2082, -0.2146,  0.1783,  ..., -0.0760, -0.1919, -0.2075],\n",
      "         [-0.0141,  0.0910, -0.0518,  ...,  0.0349,  0.0727, -0.0534],\n",
      "         ...,\n",
      "         [-0.0756, -0.0179,  0.2566,  ...,  0.1463, -0.1267,  0.0371],\n",
      "         [ 0.0610,  0.1105,  0.1701,  ...,  0.0163, -0.2404, -0.0722],\n",
      "         [ 0.0715,  0.0032,  0.2514,  ..., -0.0107, -0.4976, -0.0868]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.1417,  0.0703,  0.0343,  ..., -0.5549,  0.3175, -0.0847],\n",
      "         [-0.2405, -0.3103,  0.2406,  ..., -0.0248, -0.2163, -0.2033],\n",
      "         [-0.0772,  0.0404, -0.1617,  ...,  0.0934,  0.1440, -0.1236],\n",
      "         ...,\n",
      "         [-0.0880, -0.0558,  0.4123,  ...,  0.2156, -0.3049,  0.0626],\n",
      "         [ 0.1272,  0.1065,  0.2350,  ...,  0.0732, -0.2664, -0.0757],\n",
      "         [-0.0410, -0.0082,  0.2583,  ..., -0.0176, -0.5403, -0.2132]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.1417,  0.0703,  0.0343,  ..., -0.5549,  0.3175, -0.0847],\n",
      "         [-0.2405, -0.3103,  0.2406,  ..., -0.0248, -0.2163, -0.2033],\n",
      "         [-0.0772,  0.0404, -0.1617,  ...,  0.0934,  0.1440, -0.1236],\n",
      "         ...,\n",
      "         [-0.0880, -0.0558,  0.4123,  ...,  0.2156, -0.3049,  0.0626],\n",
      "         [ 0.1272,  0.1065,  0.2350,  ...,  0.0732, -0.2664, -0.0757],\n",
      "         [-0.0410, -0.0082,  0.2583,  ..., -0.0176, -0.5403, -0.2132]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.1528,  0.0725,  0.0609,  ..., -0.5624,  0.3103, -0.0293],\n",
      "         [-0.1619, -0.2959,  0.1902,  ...,  0.0397, -0.2134, -0.2929],\n",
      "         [-0.2645,  0.0717, -0.1870,  ...,  0.1766,  0.0998, -0.0517],\n",
      "         ...,\n",
      "         [-0.2165, -0.1631,  0.3121,  ...,  0.1061, -0.3570,  0.0630],\n",
      "         [ 0.0932, -0.0768,  0.0910,  ..., -0.1374, -0.1755, -0.0160],\n",
      "         [-0.0697, -0.1145,  0.2832,  ..., -0.1571, -0.4467, -0.2434]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.1528,  0.0725,  0.0609,  ..., -0.5624,  0.3103, -0.0293],\n",
      "         [-0.1619, -0.2959,  0.1902,  ...,  0.0397, -0.2134, -0.2929],\n",
      "         [-0.2645,  0.0717, -0.1870,  ...,  0.1766,  0.0998, -0.0517],\n",
      "         ...,\n",
      "         [-0.2165, -0.1631,  0.3121,  ...,  0.1061, -0.3570,  0.0630],\n",
      "         [ 0.0932, -0.0768,  0.0910,  ..., -0.1374, -0.1755, -0.0160],\n",
      "         [-0.0697, -0.1145,  0.2832,  ..., -0.1571, -0.4467, -0.2434]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.1559,  0.0520,  0.0566,  ..., -0.5805,  0.2614, -0.0032],\n",
      "         [-0.2100, -0.4028,  0.1289,  ...,  0.0733, -0.2617, -0.2125],\n",
      "         [-0.1567,  0.0123, -0.2428,  ...,  0.1144,  0.1282, -0.0372],\n",
      "         ...,\n",
      "         [-0.2540, -0.0787,  0.3399,  ...,  0.1475, -0.3554, -0.0680],\n",
      "         [ 0.1111,  0.1044,  0.1704,  ...,  0.0602, -0.0974, -0.0636],\n",
      "         [ 0.0743, -0.1665,  0.2060,  ..., -0.1194, -0.3994, -0.2694]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.1559,  0.0520,  0.0566,  ..., -0.5805,  0.2614, -0.0032],\n",
      "         [-0.2100, -0.4028,  0.1289,  ...,  0.0733, -0.2617, -0.2125],\n",
      "         [-0.1567,  0.0123, -0.2428,  ...,  0.1144,  0.1282, -0.0372],\n",
      "         ...,\n",
      "         [-0.2540, -0.0787,  0.3399,  ...,  0.1475, -0.3554, -0.0680],\n",
      "         [ 0.1111,  0.1044,  0.1704,  ...,  0.0602, -0.0974, -0.0636],\n",
      "         [ 0.0743, -0.1665,  0.2060,  ..., -0.1194, -0.3994, -0.2694]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.1488,  0.0445,  0.0765,  ..., -0.5456,  0.2702,  0.0098],\n",
      "         [-0.2783, -0.5755,  0.2253,  ...,  0.1655, -0.3041, -0.1889],\n",
      "         [-0.3509,  0.1601, -0.2736,  ...,  0.0930,  0.0994, -0.0481],\n",
      "         ...,\n",
      "         [-0.2630, -0.1628,  0.4082,  ...,  0.2949, -0.4147, -0.0251],\n",
      "         [-0.0443,  0.2790,  0.3464,  ...,  0.1752, -0.1755, -0.0273],\n",
      "         [-0.0079, -0.2418,  0.2014,  ..., -0.0550, -0.4030, -0.4494]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.1488,  0.0445,  0.0765,  ..., -0.5456,  0.2702,  0.0098],\n",
      "         [-0.2783, -0.5755,  0.2253,  ...,  0.1655, -0.3041, -0.1889],\n",
      "         [-0.3509,  0.1601, -0.2736,  ...,  0.0930,  0.0994, -0.0481],\n",
      "         ...,\n",
      "         [-0.2630, -0.1628,  0.4082,  ...,  0.2949, -0.4147, -0.0251],\n",
      "         [-0.0443,  0.2790,  0.3464,  ...,  0.1752, -0.1755, -0.0273],\n",
      "         [-0.0079, -0.2418,  0.2014,  ..., -0.0550, -0.4030, -0.4494]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-1.2677e-01,  3.3335e-02,  9.3478e-02,  ..., -5.3204e-01,\n",
      "           2.7953e-01,  5.9812e-04],\n",
      "         [-2.8385e-01, -4.8828e-01,  3.0940e-01,  ...,  1.6451e-01,\n",
      "          -2.2718e-01, -2.2586e-01],\n",
      "         [-3.4300e-01,  1.6835e-01, -2.6297e-01,  ...,  1.6119e-01,\n",
      "           1.4791e-01, -9.2852e-02],\n",
      "         ...,\n",
      "         [-4.6061e-01, -2.5593e-01,  3.6475e-01,  ...,  1.8645e-01,\n",
      "          -6.1321e-01, -1.5516e-01],\n",
      "         [-9.3750e-02,  1.4408e-01,  2.4577e-01,  ...,  1.8336e-01,\n",
      "          -7.4665e-02, -1.1978e-01],\n",
      "         [ 2.2466e-03, -2.8664e-01,  2.7735e-01,  ..., -2.7000e-02,\n",
      "          -5.1042e-01, -5.4616e-01]]], device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-1.2677e-01,  3.3335e-02,  9.3478e-02,  ..., -5.3204e-01,\n",
      "           2.7953e-01,  5.9812e-04],\n",
      "         [-2.8385e-01, -4.8828e-01,  3.0940e-01,  ...,  1.6451e-01,\n",
      "          -2.2718e-01, -2.2586e-01],\n",
      "         [-3.4300e-01,  1.6835e-01, -2.6297e-01,  ...,  1.6119e-01,\n",
      "           1.4791e-01, -9.2852e-02],\n",
      "         ...,\n",
      "         [-4.6061e-01, -2.5593e-01,  3.6475e-01,  ...,  1.8645e-01,\n",
      "          -6.1321e-01, -1.5516e-01],\n",
      "         [-9.3750e-02,  1.4408e-01,  2.4577e-01,  ...,  1.8336e-01,\n",
      "          -7.4665e-02, -1.1978e-01],\n",
      "         [ 2.2466e-03, -2.8664e-01,  2.7735e-01,  ..., -2.7000e-02,\n",
      "          -5.1042e-01, -5.4616e-01]]], device='cuda:0'),) and output (tensor([[[-0.1355,  0.0227,  0.0554,  ..., -0.5473,  0.2470, -0.0229],\n",
      "         [-0.2683, -0.4340,  0.2426,  ...,  0.1124, -0.3369, -0.2980],\n",
      "         [-0.3298,  0.1016, -0.2282,  ...,  0.0997,  0.0076,  0.0364],\n",
      "         ...,\n",
      "         [-0.5403, -0.1668,  0.2847,  ...,  0.2489, -0.8206, -0.0633],\n",
      "         [-0.1491,  0.1972,  0.3093,  ...,  0.2683, -0.0403, -0.0041],\n",
      "         [ 0.0347, -0.2307,  0.3534,  ...,  0.0418, -0.4906, -0.5569]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.1355,  0.0227,  0.0554,  ..., -0.5473,  0.2470, -0.0229],\n",
      "         [-0.2683, -0.4340,  0.2426,  ...,  0.1124, -0.3369, -0.2980],\n",
      "         [-0.3298,  0.1016, -0.2282,  ...,  0.0997,  0.0076,  0.0364],\n",
      "         ...,\n",
      "         [-0.5403, -0.1668,  0.2847,  ...,  0.2489, -0.8206, -0.0633],\n",
      "         [-0.1491,  0.1972,  0.3093,  ...,  0.2683, -0.0403, -0.0041],\n",
      "         [ 0.0347, -0.2307,  0.3534,  ...,  0.0418, -0.4906, -0.5569]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.1350,  0.0163,  0.0576,  ..., -0.5555,  0.2435,  0.0051],\n",
      "         [-0.2106, -0.3849,  0.1989,  ...,  0.1278, -0.3601, -0.2699],\n",
      "         [-0.3128,  0.1647, -0.2131,  ...,  0.0540,  0.0550,  0.0351],\n",
      "         ...,\n",
      "         [-0.5992, -0.2293,  0.3179,  ...,  0.2098, -0.8618,  0.2080],\n",
      "         [-0.0345,  0.2003,  0.3655,  ...,  0.1752, -0.0011,  0.2828],\n",
      "         [ 0.0533, -0.3344,  0.4058,  ...,  0.0996, -0.5401, -0.5020]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.1350,  0.0163,  0.0576,  ..., -0.5555,  0.2435,  0.0051],\n",
      "         [-0.2106, -0.3849,  0.1989,  ...,  0.1278, -0.3601, -0.2699],\n",
      "         [-0.3128,  0.1647, -0.2131,  ...,  0.0540,  0.0550,  0.0351],\n",
      "         ...,\n",
      "         [-0.5992, -0.2293,  0.3179,  ...,  0.2098, -0.8618,  0.2080],\n",
      "         [-0.0345,  0.2003,  0.3655,  ...,  0.1752, -0.0011,  0.2828],\n",
      "         [ 0.0533, -0.3344,  0.4058,  ...,  0.0996, -0.5401, -0.5020]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.1301, -0.0029,  0.0279,  ..., -0.5537,  0.2277,  0.0164],\n",
      "         [-0.1888, -0.4633,  0.1796,  ...,  0.0542, -0.4552, -0.3303],\n",
      "         [-0.2199,  0.1747, -0.1952,  ...,  0.0032,  0.1254,  0.0648],\n",
      "         ...,\n",
      "         [-0.9657, -0.3221,  0.3715,  ...,  0.0918, -0.8268,  0.1196],\n",
      "         [ 0.0134,  0.1478,  0.4980,  ..., -0.0051,  0.1992,  0.2513],\n",
      "         [-0.0157, -0.3126,  0.5385,  ...,  0.0631, -0.4693, -0.5793]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.1301, -0.0029,  0.0279,  ..., -0.5537,  0.2277,  0.0164],\n",
      "         [-0.1888, -0.4633,  0.1796,  ...,  0.0542, -0.4552, -0.3303],\n",
      "         [-0.2199,  0.1747, -0.1952,  ...,  0.0032,  0.1254,  0.0648],\n",
      "         ...,\n",
      "         [-0.9657, -0.3221,  0.3715,  ...,  0.0918, -0.8268,  0.1196],\n",
      "         [ 0.0134,  0.1478,  0.4980,  ..., -0.0051,  0.1992,  0.2513],\n",
      "         [-0.0157, -0.3126,  0.5385,  ...,  0.0631, -0.4693, -0.5793]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.1219,  0.0061, -0.0021,  ..., -0.5922,  0.2788, -0.0020],\n",
      "         [-0.3080, -0.5390,  0.1700,  ...,  0.1091, -0.5435, -0.3056],\n",
      "         [-0.1156,  0.0832, -0.2455,  ..., -0.1197,  0.1969, -0.0405],\n",
      "         ...,\n",
      "         [-1.0688, -0.5224,  0.3032,  ...,  0.2583, -0.8424, -0.1527],\n",
      "         [-0.1828,  0.2142,  0.6681,  ...,  0.1231,  0.3223,  0.3714],\n",
      "         [-0.0666, -0.3417,  0.6552,  ...,  0.2031, -0.4748, -0.4971]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.1219,  0.0061, -0.0021,  ..., -0.5922,  0.2788, -0.0020],\n",
      "         [-0.3080, -0.5390,  0.1700,  ...,  0.1091, -0.5435, -0.3056],\n",
      "         [-0.1156,  0.0832, -0.2455,  ..., -0.1197,  0.1969, -0.0405],\n",
      "         ...,\n",
      "         [-1.0688, -0.5224,  0.3032,  ...,  0.2583, -0.8424, -0.1527],\n",
      "         [-0.1828,  0.2142,  0.6681,  ...,  0.1231,  0.3223,  0.3714],\n",
      "         [-0.0666, -0.3417,  0.6552,  ...,  0.2031, -0.4748, -0.4971]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.1067,  0.0272,  0.0495,  ..., -0.5550,  0.3449,  0.0266],\n",
      "         [-0.3782, -0.6704,  0.1138,  ...,  0.1904, -0.6502, -0.3568],\n",
      "         [-0.1389, -0.0192, -0.2586,  ..., -0.3268,  0.3408, -0.1081],\n",
      "         ...,\n",
      "         [-1.1311, -0.5975,  0.3911,  ...,  0.2901, -0.9619, -0.0666],\n",
      "         [-0.3594,  0.0659,  0.8325,  ...,  0.3344,  0.2824,  0.2715],\n",
      "         [-0.0432, -0.4086,  0.6091,  ...,  0.1780, -0.5460, -0.5009]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.1067,  0.0272,  0.0495,  ..., -0.5550,  0.3449,  0.0266],\n",
      "         [-0.3782, -0.6704,  0.1138,  ...,  0.1904, -0.6502, -0.3568],\n",
      "         [-0.1389, -0.0192, -0.2586,  ..., -0.3268,  0.3408, -0.1081],\n",
      "         ...,\n",
      "         [-1.1311, -0.5975,  0.3911,  ...,  0.2901, -0.9619, -0.0666],\n",
      "         [-0.3594,  0.0659,  0.8325,  ...,  0.3344,  0.2824,  0.2715],\n",
      "         [-0.0432, -0.4086,  0.6091,  ...,  0.1780, -0.5460, -0.5009]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.0592,  0.0731,  0.0500,  ..., -0.5517,  0.3541,  0.0595],\n",
      "         [-0.4399, -0.7497,  0.2923,  ...,  0.3191, -0.6928, -0.3154],\n",
      "         [-0.1111, -0.1868, -0.3099,  ..., -0.2334,  0.5070, -0.1822],\n",
      "         ...,\n",
      "         [-1.4515, -0.8903,  0.6332,  ...,  0.1440, -1.0102,  0.0766],\n",
      "         [-0.5908, -0.2005,  0.8874,  ...,  0.2563,  0.4874,  0.3485],\n",
      "         [-0.1195, -0.5655,  0.6910,  ...,  0.0102, -0.5397, -0.5426]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.0592,  0.0731,  0.0500,  ..., -0.5517,  0.3541,  0.0595],\n",
      "         [-0.4399, -0.7497,  0.2923,  ...,  0.3191, -0.6928, -0.3154],\n",
      "         [-0.1111, -0.1868, -0.3099,  ..., -0.2334,  0.5070, -0.1822],\n",
      "         ...,\n",
      "         [-1.4515, -0.8903,  0.6332,  ...,  0.1440, -1.0102,  0.0766],\n",
      "         [-0.5908, -0.2005,  0.8874,  ...,  0.2563,  0.4874,  0.3485],\n",
      "         [-0.1195, -0.5655,  0.6910,  ...,  0.0102, -0.5397, -0.5426]]],\n",
      "       device='cuda:0'),) and output (tensor([[[ 0.1143,  0.2237,  0.3292,  ..., -0.7706,  0.2608,  0.0269],\n",
      "         [-0.4202, -1.0429,  0.4147,  ...,  0.3729, -0.7638, -0.5359],\n",
      "         [ 0.0491, -0.3440, -0.3250,  ..., -0.1696,  0.3522, -0.4167],\n",
      "         ...,\n",
      "         [-1.1169, -1.1418,  0.5133,  ...,  0.2865, -1.0198, -0.0027],\n",
      "         [-0.1643, -0.0802,  0.5847,  ...,  0.3326, -0.0729,  0.4989],\n",
      "         [ 0.0201, -0.9596,  0.8516,  ...,  0.0782, -0.4173, -0.6704]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[ 0.1143,  0.2237,  0.3292,  ..., -0.7706,  0.2608,  0.0269],\n",
      "         [-0.4202, -1.0429,  0.4147,  ...,  0.3729, -0.7638, -0.5359],\n",
      "         [ 0.0491, -0.3440, -0.3250,  ..., -0.1696,  0.3522, -0.4167],\n",
      "         ...,\n",
      "         [-1.1169, -1.1418,  0.5133,  ...,  0.2865, -1.0198, -0.0027],\n",
      "         [-0.1643, -0.0802,  0.5847,  ...,  0.3326, -0.0729,  0.4989],\n",
      "         [ 0.0201, -0.9596,  0.8516,  ...,  0.0782, -0.4173, -0.6704]]],\n",
      "       device='cuda:0'),) and output (tensor([[[ 0.3711,  0.5489,  0.6100,  ..., -0.7499,  0.2080,  0.4195],\n",
      "         [-0.2012, -1.3779, -0.0362,  ...,  0.5451, -0.7853, -0.9669],\n",
      "         [-0.1825, -0.8087, -0.6496,  ..., -0.1510,  0.1681, -0.7248],\n",
      "         ...,\n",
      "         [-1.0013, -1.2117,  0.4376,  ...,  0.2727, -1.3855, -0.2751],\n",
      "         [-0.0387, -0.3258,  0.5873,  ...,  0.7432, -0.3550,  0.3609],\n",
      "         [-0.4124, -1.7020,  0.4680,  ..., -0.0397, -0.6729, -1.1288]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[ 0.3711,  0.5489,  0.6100,  ..., -0.7499,  0.2080,  0.4195],\n",
      "         [-0.2012, -1.3779, -0.0362,  ...,  0.5451, -0.7853, -0.9669],\n",
      "         [-0.1825, -0.8087, -0.6496,  ..., -0.1510,  0.1681, -0.7248],\n",
      "         ...,\n",
      "         [-1.0013, -1.2117,  0.4376,  ...,  0.2727, -1.3855, -0.2751],\n",
      "         [-0.0387, -0.3258,  0.5873,  ...,  0.7432, -0.3550,  0.3609],\n",
      "         [-0.4124, -1.7020,  0.4680,  ..., -0.0397, -0.6729, -1.1288]]],\n",
      "       device='cuda:0'),) and output (tensor([[[ 1.1544,  3.2965,  1.6534,  ..., -2.7009,  2.6553,  2.2700],\n",
      "         [-0.3297, -1.5788, -0.6439,  ..., -1.2777, -0.8950, -1.4603],\n",
      "         [ 0.1660, -0.2390, -0.3779,  ...,  0.3862, -0.1053, -0.2535],\n",
      "         ...,\n",
      "         [-1.2453, -2.0392,  1.0448,  ...,  1.9111, -1.2495, -0.0522],\n",
      "         [-0.8001, -1.0794,  0.1746,  ...,  2.5433, -1.7282,  1.2439],\n",
      "         [-0.7628, -2.8971,  0.6325,  ...,  1.6464, -1.3421, -0.4702]]],\n",
      "       device='cuda:0'),)\n",
      "[Debug] Layer names being passed: ['model.layers.0', 'model.layers.1', 'model.layers.2', 'model.layers.3', 'model.layers.4', 'model.layers.5', 'model.layers.6', 'model.layers.7', 'model.layers.8', 'model.layers.9', 'model.layers.10', 'model.layers.11', 'model.layers.12', 'model.layers.13', 'model.layers.14', 'model.layers.15', 'model.layers.16', 'model.layers.17', 'model.layers.18', 'model.layers.19', 'model.layers.20', 'model.layers.21', 'model.layers.22', 'model.layers.23', 'model.layers.24', 'model.layers.25', 'model.layers.26', 'model.layers.27', 'model.layers.28', 'model.layers.29', 'model.layers.30', 'model.layers.31', 'model.embed_tokens']\n",
      "[Debug] Trying to access layer: model.layers.0\n",
      "[Debug] Successfully found layer: model.layers.0\n",
      "[Debug] Trying to access layer: model.layers.1\n",
      "[Debug] Successfully found layer: model.layers.1\n",
      "[Debug] Trying to access layer: model.layers.2\n",
      "[Debug] Successfully found layer: model.layers.2\n",
      "[Debug] Trying to access layer: model.layers.3\n",
      "[Debug] Successfully found layer: model.layers.3\n",
      "[Debug] Trying to access layer: model.layers.4\n",
      "[Debug] Successfully found layer: model.layers.4\n",
      "[Debug] Trying to access layer: model.layers.5\n",
      "[Debug] Successfully found layer: model.layers.5\n",
      "[Debug] Trying to access layer: model.layers.6\n",
      "[Debug] Successfully found layer: model.layers.6\n",
      "[Debug] Trying to access layer: model.layers.7\n",
      "[Debug] Successfully found layer: model.layers.7\n",
      "[Debug] Trying to access layer: model.layers.8\n",
      "[Debug] Successfully found layer: model.layers.8\n",
      "[Debug] Trying to access layer: model.layers.9\n",
      "[Debug] Successfully found layer: model.layers.9\n",
      "[Debug] Trying to access layer: model.layers.10\n",
      "[Debug] Successfully found layer: model.layers.10\n",
      "[Debug] Trying to access layer: model.layers.11\n",
      "[Debug] Successfully found layer: model.layers.11\n",
      "[Debug] Trying to access layer: model.layers.12\n",
      "[Debug] Successfully found layer: model.layers.12\n",
      "[Debug] Trying to access layer: model.layers.13\n",
      "[Debug] Successfully found layer: model.layers.13\n",
      "[Debug] Trying to access layer: model.layers.14\n",
      "[Debug] Successfully found layer: model.layers.14\n",
      "[Debug] Trying to access layer: model.layers.15\n",
      "[Debug] Successfully found layer: model.layers.15\n",
      "[Debug] Trying to access layer: model.layers.16\n",
      "[Debug] Successfully found layer: model.layers.16\n",
      "[Debug] Trying to access layer: model.layers.17\n",
      "[Debug] Successfully found layer: model.layers.17\n",
      "[Debug] Trying to access layer: model.layers.18\n",
      "[Debug] Successfully found layer: model.layers.18\n",
      "[Debug] Trying to access layer: model.layers.19\n",
      "[Debug] Successfully found layer: model.layers.19\n",
      "[Debug] Trying to access layer: model.layers.20\n",
      "[Debug] Successfully found layer: model.layers.20\n",
      "[Debug] Trying to access layer: model.layers.21\n",
      "[Debug] Successfully found layer: model.layers.21\n",
      "[Debug] Trying to access layer: model.layers.22\n",
      "[Debug] Successfully found layer: model.layers.22\n",
      "[Debug] Trying to access layer: model.layers.23\n",
      "[Debug] Successfully found layer: model.layers.23\n",
      "[Debug] Trying to access layer: model.layers.24\n",
      "[Debug] Successfully found layer: model.layers.24\n",
      "[Debug] Trying to access layer: model.layers.25\n",
      "[Debug] Successfully found layer: model.layers.25\n",
      "[Debug] Trying to access layer: model.layers.26\n",
      "[Debug] Successfully found layer: model.layers.26\n",
      "[Debug] Trying to access layer: model.layers.27\n",
      "[Debug] Successfully found layer: model.layers.27\n",
      "[Debug] Trying to access layer: model.layers.28\n",
      "[Debug] Successfully found layer: model.layers.28\n",
      "[Debug] Trying to access layer: model.layers.29\n",
      "[Debug] Successfully found layer: model.layers.29\n",
      "[Debug] Trying to access layer: model.layers.30\n",
      "[Debug] Successfully found layer: model.layers.30\n",
      "[Debug] Trying to access layer: model.layers.31\n",
      "[Debug] Successfully found layer: model.layers.31\n",
      "[Debug] Trying to access layer: model.embed_tokens\n",
      "[Debug] Successfully found layer: model.embed_tokens\n",
      "[Hook] Layer Embedding(128256, 4096) received input (tensor([[128000,     37,  11975,    596,   5492,    369,  77533,   3367,  23323,\n",
      "          14583,  72785,    268,  50178,    279,   4101,   1306,  48810,   1403,\n",
      "          12875,    505,    459,  10065,  23756,    323,  19486,    279,   7434,\n",
      "            311,  51052,   4885,     13,    578,   1501,   1176,  85170,    389,\n",
      "          69530,   8304,    389,   6287,    220,   1032,     11,    220,   1049,\n",
      "             19,     11,    439,    264,    220,   1954,  24401,  12707,   4632,\n",
      "             13,   1952,   6287,    220,    508,     11,    433,   6137,   1202,\n",
      "           4725,   1629,    315,  17510,   4791,   7716,  16938,  24401,  18243,\n",
      "            389,  80523,     11,    520,    220,     22,   9012,     13,    578,\n",
      "           4101,   8220,   1202,   1629,    389,   3297,    220,     18,     11,\n",
      "            220,   1049,     24,     11,    449,    264,   2860,    315,   4848,\n",
      "          15956,    323,  71049,  86703,  18243,     13,  14583,  72785,    268,\n",
      "           2163,  69530,   8304,  20193,   1306,    279,   4101,   9670,     13,\n",
      "            432,    261,  11099,    617,  43087,    389,   2577,  26429,    526,\n",
      "            505,   6287,    220,    806,     11,    220,    679,     17,    311,\n",
      "           6841,    220,     18,     11,    220,    679,     18,    323,   1578,\n",
      "            505,   5651,    220,     16,     11,    220,    679,     19,    311,\n",
      "           5936,    220,     18,     11,    220,    679,     22,     13]],\n",
      "       device='cuda:0'),) and output tensor([[[ 2.6512e-04, -4.9973e-04, -5.8365e-04,  ...,  3.8147e-03,\n",
      "           6.3419e-05,  1.1902e-03],\n",
      "         [-1.0376e-02,  2.2430e-03,  5.9204e-03,  ...,  4.5471e-03,\n",
      "           1.5381e-02, -1.0147e-03],\n",
      "         [-4.0627e-04, -2.7924e-03,  6.5308e-03,  ...,  5.4626e-03,\n",
      "           1.5030e-03, -1.3733e-02],\n",
      "         ...,\n",
      "         [ 2.4872e-03, -1.5411e-03, -4.8218e-03,  ...,  4.9438e-03,\n",
      "           5.5542e-03, -4.2725e-03],\n",
      "         [-2.6093e-03,  4.0894e-03, -1.2493e-04,  ...,  1.5869e-02,\n",
      "           7.5989e-03,  7.7820e-03],\n",
      "         [-6.7520e-04, -5.7602e-04,  4.7684e-04,  ...,  3.3264e-03,\n",
      "           3.9101e-04,  4.7493e-04]]], device='cuda:0')\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[ 2.6512e-04, -4.9973e-04, -5.8365e-04,  ...,  3.8147e-03,\n",
      "           6.3419e-05,  1.1902e-03],\n",
      "         [-1.0376e-02,  2.2430e-03,  5.9204e-03,  ...,  4.5471e-03,\n",
      "           1.5381e-02, -1.0147e-03],\n",
      "         [-4.0627e-04, -2.7924e-03,  6.5308e-03,  ...,  5.4626e-03,\n",
      "           1.5030e-03, -1.3733e-02],\n",
      "         ...,\n",
      "         [ 2.4872e-03, -1.5411e-03, -4.8218e-03,  ...,  4.9438e-03,\n",
      "           5.5542e-03, -4.2725e-03],\n",
      "         [-2.6093e-03,  4.0894e-03, -1.2493e-04,  ...,  1.5869e-02,\n",
      "           7.5989e-03,  7.7820e-03],\n",
      "         [-6.7520e-04, -5.7602e-04,  4.7684e-04,  ...,  3.3264e-03,\n",
      "           3.9101e-04,  4.7493e-04]]], device='cuda:0'),) and output (tensor([[[-0.0004,  0.0146, -0.0020,  ...,  0.0066, -0.0194,  0.0020],\n",
      "         [-0.0030,  0.0139, -0.0036,  ..., -0.0107, -0.0062, -0.0079],\n",
      "         [ 0.0047, -0.0335,  0.0094,  ..., -0.0213, -0.0114,  0.0045],\n",
      "         ...,\n",
      "         [ 0.0075,  0.0042, -0.0113,  ...,  0.0002,  0.0048,  0.0055],\n",
      "         [ 0.0093,  0.0188,  0.0051,  ..., -0.0122,  0.0308,  0.0255],\n",
      "         [-0.0057,  0.0096,  0.0008,  ..., -0.0020,  0.0048,  0.0079]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.0004,  0.0146, -0.0020,  ...,  0.0066, -0.0194,  0.0020],\n",
      "         [-0.0030,  0.0139, -0.0036,  ..., -0.0107, -0.0062, -0.0079],\n",
      "         [ 0.0047, -0.0335,  0.0094,  ..., -0.0213, -0.0114,  0.0045],\n",
      "         ...,\n",
      "         [ 0.0075,  0.0042, -0.0113,  ...,  0.0002,  0.0048,  0.0055],\n",
      "         [ 0.0093,  0.0188,  0.0051,  ..., -0.0122,  0.0308,  0.0255],\n",
      "         [-0.0057,  0.0096,  0.0008,  ..., -0.0020,  0.0048,  0.0079]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.0972,  0.0787, -0.0419,  ...,  0.0071,  0.0869,  0.0218],\n",
      "         [ 0.0120,  0.0238, -0.0161,  ..., -0.0213, -0.0195, -0.0092],\n",
      "         [-0.0035, -0.0183,  0.0329,  ..., -0.0590,  0.0146, -0.0079],\n",
      "         ...,\n",
      "         [-0.0094,  0.0067,  0.0071,  ..., -0.0252,  0.0113, -0.0026],\n",
      "         [-0.0093,  0.0216,  0.0005,  ..., -0.0579,  0.0265,  0.0372],\n",
      "         [-0.0132, -0.0059,  0.0050,  ..., -0.0074, -0.0063,  0.0100]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.0972,  0.0787, -0.0419,  ...,  0.0071,  0.0869,  0.0218],\n",
      "         [ 0.0120,  0.0238, -0.0161,  ..., -0.0213, -0.0195, -0.0092],\n",
      "         [-0.0035, -0.0183,  0.0329,  ..., -0.0590,  0.0146, -0.0079],\n",
      "         ...,\n",
      "         [-0.0094,  0.0067,  0.0071,  ..., -0.0252,  0.0113, -0.0026],\n",
      "         [-0.0093,  0.0216,  0.0005,  ..., -0.0579,  0.0265,  0.0372],\n",
      "         [-0.0132, -0.0059,  0.0050,  ..., -0.0074, -0.0063,  0.0100]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.1040,  0.0875, -0.0360,  ...,  0.0205,  0.0997,  0.0184],\n",
      "         [ 0.0046, -0.0078, -0.0004,  ...,  0.0167, -0.0345, -0.0073],\n",
      "         [-0.0265, -0.0103,  0.0126,  ..., -0.0222, -0.0181,  0.0216],\n",
      "         ...,\n",
      "         [-0.0042, -0.0134,  0.0320,  ..., -0.0843, -0.0116,  0.0031],\n",
      "         [ 0.0328, -0.0012,  0.0053,  ..., -0.0768,  0.0250,  0.0351],\n",
      "         [-0.0125,  0.0047,  0.0255,  ...,  0.0282, -0.0142,  0.0238]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.1040,  0.0875, -0.0360,  ...,  0.0205,  0.0997,  0.0184],\n",
      "         [ 0.0046, -0.0078, -0.0004,  ...,  0.0167, -0.0345, -0.0073],\n",
      "         [-0.0265, -0.0103,  0.0126,  ..., -0.0222, -0.0181,  0.0216],\n",
      "         ...,\n",
      "         [-0.0042, -0.0134,  0.0320,  ..., -0.0843, -0.0116,  0.0031],\n",
      "         [ 0.0328, -0.0012,  0.0053,  ..., -0.0768,  0.0250,  0.0351],\n",
      "         [-0.0125,  0.0047,  0.0255,  ...,  0.0282, -0.0142,  0.0238]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.0870,  0.1049, -0.0173,  ...,  0.0700,  0.0989,  0.0181],\n",
      "         [-0.0238,  0.0417, -0.0232,  ..., -0.0381, -0.0264, -0.0492],\n",
      "         [-0.0197, -0.0069,  0.0172,  ...,  0.0023, -0.0086,  0.0055],\n",
      "         ...,\n",
      "         [-0.0356,  0.0101,  0.0147,  ..., -0.1357, -0.0050, -0.0463],\n",
      "         [-0.0012, -0.0070, -0.0151,  ..., -0.0937,  0.0186, -0.0273],\n",
      "         [-0.0233,  0.0063,  0.0169,  ...,  0.0243, -0.0404,  0.0273]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.0870,  0.1049, -0.0173,  ...,  0.0700,  0.0989,  0.0181],\n",
      "         [-0.0238,  0.0417, -0.0232,  ..., -0.0381, -0.0264, -0.0492],\n",
      "         [-0.0197, -0.0069,  0.0172,  ...,  0.0023, -0.0086,  0.0055],\n",
      "         ...,\n",
      "         [-0.0356,  0.0101,  0.0147,  ..., -0.1357, -0.0050, -0.0463],\n",
      "         [-0.0012, -0.0070, -0.0151,  ..., -0.0937,  0.0186, -0.0273],\n",
      "         [-0.0233,  0.0063,  0.0169,  ...,  0.0243, -0.0404,  0.0273]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.1113,  0.1022, -0.0102,  ...,  0.0436,  0.1283,  0.0321],\n",
      "         [-0.0321,  0.0369,  0.0077,  ...,  0.0060, -0.0282, -0.0110],\n",
      "         [-0.0203, -0.0220, -0.0146,  ...,  0.0331, -0.0095,  0.0033],\n",
      "         ...,\n",
      "         [-0.0296,  0.0346,  0.0475,  ..., -0.1197,  0.0028, -0.0056],\n",
      "         [-0.0022,  0.0314,  0.0338,  ..., -0.0743,  0.0449, -0.0290],\n",
      "         [-0.0221, -0.0015,  0.0364,  ...,  0.1018, -0.0105, -0.0115]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.1113,  0.1022, -0.0102,  ...,  0.0436,  0.1283,  0.0321],\n",
      "         [-0.0321,  0.0369,  0.0077,  ...,  0.0060, -0.0282, -0.0110],\n",
      "         [-0.0203, -0.0220, -0.0146,  ...,  0.0331, -0.0095,  0.0033],\n",
      "         ...,\n",
      "         [-0.0296,  0.0346,  0.0475,  ..., -0.1197,  0.0028, -0.0056],\n",
      "         [-0.0022,  0.0314,  0.0338,  ..., -0.0743,  0.0449, -0.0290],\n",
      "         [-0.0221, -0.0015,  0.0364,  ...,  0.1018, -0.0105, -0.0115]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.1034,  0.1132,  0.0096,  ...,  0.0453,  0.1477,  0.0301],\n",
      "         [-0.0172,  0.0210, -0.0079,  ..., -0.0368, -0.0499,  0.0104],\n",
      "         [ 0.0583, -0.0273, -0.0631,  ..., -0.0125, -0.0049,  0.0296],\n",
      "         ...,\n",
      "         [ 0.0018,  0.0142,  0.0382,  ..., -0.1763,  0.0115,  0.0227],\n",
      "         [-0.0085,  0.0355,  0.0681,  ..., -0.0520,  0.0286,  0.0159],\n",
      "         [-0.0449, -0.0522,  0.0524,  ...,  0.0957, -0.0633, -0.0146]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.1034,  0.1132,  0.0096,  ...,  0.0453,  0.1477,  0.0301],\n",
      "         [-0.0172,  0.0210, -0.0079,  ..., -0.0368, -0.0499,  0.0104],\n",
      "         [ 0.0583, -0.0273, -0.0631,  ..., -0.0125, -0.0049,  0.0296],\n",
      "         ...,\n",
      "         [ 0.0018,  0.0142,  0.0382,  ..., -0.1763,  0.0115,  0.0227],\n",
      "         [-0.0085,  0.0355,  0.0681,  ..., -0.0520,  0.0286,  0.0159],\n",
      "         [-0.0449, -0.0522,  0.0524,  ...,  0.0957, -0.0633, -0.0146]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.1071,  0.1253,  0.0211,  ...,  0.0126,  0.1586,  0.0278],\n",
      "         [-0.0022,  0.0480, -0.0169,  ...,  0.0206,  0.0039,  0.0229],\n",
      "         [ 0.0138, -0.0985, -0.0575,  ..., -0.0156, -0.0312,  0.0495],\n",
      "         ...,\n",
      "         [-0.0332,  0.0222,  0.1160,  ..., -0.1502,  0.0684,  0.1014],\n",
      "         [ 0.0471, -0.0585,  0.0897,  ..., -0.0426, -0.0187,  0.0292],\n",
      "         [-0.0184, -0.0622,  0.0383,  ...,  0.0845, -0.0269,  0.0032]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.1071,  0.1253,  0.0211,  ...,  0.0126,  0.1586,  0.0278],\n",
      "         [-0.0022,  0.0480, -0.0169,  ...,  0.0206,  0.0039,  0.0229],\n",
      "         [ 0.0138, -0.0985, -0.0575,  ..., -0.0156, -0.0312,  0.0495],\n",
      "         ...,\n",
      "         [-0.0332,  0.0222,  0.1160,  ..., -0.1502,  0.0684,  0.1014],\n",
      "         [ 0.0471, -0.0585,  0.0897,  ..., -0.0426, -0.0187,  0.0292],\n",
      "         [-0.0184, -0.0622,  0.0383,  ...,  0.0845, -0.0269,  0.0032]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.0945,  0.1848,  0.0036,  ..., -0.0473,  0.1489,  0.0536],\n",
      "         [ 0.0824,  0.0158, -0.0337,  ..., -0.0474, -0.1099, -0.0253],\n",
      "         [ 0.0623, -0.0962, -0.0615,  ..., -0.0358, -0.0633,  0.0438],\n",
      "         ...,\n",
      "         [-0.0210, -0.0465,  0.0783,  ..., -0.0487,  0.1109,  0.0734],\n",
      "         [ 0.0403, -0.0371,  0.0630,  ...,  0.0660,  0.0969, -0.0168],\n",
      "         [-0.0405, -0.1177,  0.0306,  ...,  0.1747,  0.0307, -0.0817]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.0945,  0.1848,  0.0036,  ..., -0.0473,  0.1489,  0.0536],\n",
      "         [ 0.0824,  0.0158, -0.0337,  ..., -0.0474, -0.1099, -0.0253],\n",
      "         [ 0.0623, -0.0962, -0.0615,  ..., -0.0358, -0.0633,  0.0438],\n",
      "         ...,\n",
      "         [-0.0210, -0.0465,  0.0783,  ..., -0.0487,  0.1109,  0.0734],\n",
      "         [ 0.0403, -0.0371,  0.0630,  ...,  0.0660,  0.0969, -0.0168],\n",
      "         [-0.0405, -0.1177,  0.0306,  ...,  0.1747,  0.0307, -0.0817]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.1060,  0.2113,  0.0270,  ..., -0.1141,  0.1610,  0.0528],\n",
      "         [ 0.1203,  0.0937, -0.0661,  ..., -0.0224, -0.1505, -0.0468],\n",
      "         [ 0.0294, -0.0769, -0.0551,  ..., -0.0161, -0.0875,  0.0210],\n",
      "         ...,\n",
      "         [-0.0868, -0.0120,  0.0112,  ..., -0.0747,  0.1084,  0.1186],\n",
      "         [ 0.0244,  0.0480, -0.0195,  ...,  0.0404,  0.0521,  0.0201],\n",
      "         [ 0.0168, -0.1188,  0.1416,  ...,  0.1402,  0.0828,  0.0218]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.1060,  0.2113,  0.0270,  ..., -0.1141,  0.1610,  0.0528],\n",
      "         [ 0.1203,  0.0937, -0.0661,  ..., -0.0224, -0.1505, -0.0468],\n",
      "         [ 0.0294, -0.0769, -0.0551,  ..., -0.0161, -0.0875,  0.0210],\n",
      "         ...,\n",
      "         [-0.0868, -0.0120,  0.0112,  ..., -0.0747,  0.1084,  0.1186],\n",
      "         [ 0.0244,  0.0480, -0.0195,  ...,  0.0404,  0.0521,  0.0201],\n",
      "         [ 0.0168, -0.1188,  0.1416,  ...,  0.1402,  0.0828,  0.0218]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.0954,  0.2493,  0.0073,  ..., -0.1568,  0.1818,  0.0855],\n",
      "         [ 0.0422,  0.0823, -0.0049,  ..., -0.0012, -0.0984, -0.0360],\n",
      "         [ 0.0136, -0.1108, -0.0581,  ..., -0.0073, -0.1092,  0.0315],\n",
      "         ...,\n",
      "         [-0.1059,  0.0195,  0.0552,  ...,  0.0941,  0.0341,  0.1603],\n",
      "         [-0.0835,  0.1255,  0.1008,  ...,  0.1803,  0.0628,  0.0550],\n",
      "         [ 0.0624, -0.1150,  0.1830,  ...,  0.1462,  0.0805, -0.0051]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.0954,  0.2493,  0.0073,  ..., -0.1568,  0.1818,  0.0855],\n",
      "         [ 0.0422,  0.0823, -0.0049,  ..., -0.0012, -0.0984, -0.0360],\n",
      "         [ 0.0136, -0.1108, -0.0581,  ..., -0.0073, -0.1092,  0.0315],\n",
      "         ...,\n",
      "         [-0.1059,  0.0195,  0.0552,  ...,  0.0941,  0.0341,  0.1603],\n",
      "         [-0.0835,  0.1255,  0.1008,  ...,  0.1803,  0.0628,  0.0550],\n",
      "         [ 0.0624, -0.1150,  0.1830,  ...,  0.1462,  0.0805, -0.0051]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.0152,  0.3340,  0.0011,  ..., -0.2750,  0.1933,  0.1034],\n",
      "         [ 0.1023,  0.0574,  0.0244,  ..., -0.0103, -0.0984, -0.0204],\n",
      "         [ 0.0262, -0.1079, -0.1016,  ..., -0.0925, -0.1241,  0.0307],\n",
      "         ...,\n",
      "         [-0.1467, -0.1088,  0.0209,  ...,  0.0386, -0.0439,  0.1014],\n",
      "         [-0.0433,  0.1129,  0.0615,  ...,  0.1692, -0.0030,  0.0292],\n",
      "         [ 0.0363, -0.1275,  0.1505,  ...,  0.0517,  0.0012,  0.0026]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.0152,  0.3340,  0.0011,  ..., -0.2750,  0.1933,  0.1034],\n",
      "         [ 0.1023,  0.0574,  0.0244,  ..., -0.0103, -0.0984, -0.0204],\n",
      "         [ 0.0262, -0.1079, -0.1016,  ..., -0.0925, -0.1241,  0.0307],\n",
      "         ...,\n",
      "         [-0.1467, -0.1088,  0.0209,  ...,  0.0386, -0.0439,  0.1014],\n",
      "         [-0.0433,  0.1129,  0.0615,  ...,  0.1692, -0.0030,  0.0292],\n",
      "         [ 0.0363, -0.1275,  0.1505,  ...,  0.0517,  0.0012,  0.0026]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.0356,  0.3483, -0.0366,  ..., -0.2962,  0.2088,  0.1065],\n",
      "         [ 0.0888,  0.0592,  0.0317,  ..., -0.0299, -0.0680, -0.0121],\n",
      "         [ 0.0154, -0.1012, -0.0206,  ..., -0.1767, -0.0805,  0.0363],\n",
      "         ...,\n",
      "         [-0.1002, -0.0360, -0.0450,  ..., -0.0807,  0.1904,  0.0885],\n",
      "         [ 0.0054,  0.0472,  0.0602,  ...,  0.0922,  0.0942,  0.0391],\n",
      "         [-0.0028, -0.2189,  0.1129,  ..., -0.0958,  0.1185, -0.0197]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.0356,  0.3483, -0.0366,  ..., -0.2962,  0.2088,  0.1065],\n",
      "         [ 0.0888,  0.0592,  0.0317,  ..., -0.0299, -0.0680, -0.0121],\n",
      "         [ 0.0154, -0.1012, -0.0206,  ..., -0.1767, -0.0805,  0.0363],\n",
      "         ...,\n",
      "         [-0.1002, -0.0360, -0.0450,  ..., -0.0807,  0.1904,  0.0885],\n",
      "         [ 0.0054,  0.0472,  0.0602,  ...,  0.0922,  0.0942,  0.0391],\n",
      "         [-0.0028, -0.2189,  0.1129,  ..., -0.0958,  0.1185, -0.0197]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.0127,  0.3661, -0.0316,  ..., -0.3678,  0.1973,  0.1019],\n",
      "         [ 0.0705,  0.0265,  0.0313,  ..., -0.0665, -0.1321,  0.0410],\n",
      "         [ 0.0285, -0.1553,  0.0127,  ..., -0.2292, -0.0721,  0.0422],\n",
      "         ...,\n",
      "         [-0.1342, -0.0570, -0.0339,  ..., -0.0573,  0.0998,  0.0699],\n",
      "         [ 0.0892,  0.0227,  0.1830,  ...,  0.1913,  0.0526, -0.0476],\n",
      "         [-0.0631, -0.0599,  0.1417,  ...,  0.0011,  0.0172, -0.0303]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.0127,  0.3661, -0.0316,  ..., -0.3678,  0.1973,  0.1019],\n",
      "         [ 0.0705,  0.0265,  0.0313,  ..., -0.0665, -0.1321,  0.0410],\n",
      "         [ 0.0285, -0.1553,  0.0127,  ..., -0.2292, -0.0721,  0.0422],\n",
      "         ...,\n",
      "         [-0.1342, -0.0570, -0.0339,  ..., -0.0573,  0.0998,  0.0699],\n",
      "         [ 0.0892,  0.0227,  0.1830,  ...,  0.1913,  0.0526, -0.0476],\n",
      "         [-0.0631, -0.0599,  0.1417,  ...,  0.0011,  0.0172, -0.0303]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.0089,  0.3233, -0.0779,  ..., -0.4491,  0.1845,  0.0951],\n",
      "         [-0.0863,  0.0463,  0.0332,  ..., -0.1038, -0.1459, -0.0254],\n",
      "         [ 0.1012, -0.1778, -0.0642,  ..., -0.2366, -0.0300,  0.0253],\n",
      "         ...,\n",
      "         [-0.0158,  0.0634, -0.0783,  ..., -0.1762,  0.2059,  0.0461],\n",
      "         [ 0.0321, -0.0712,  0.2163,  ...,  0.0808,  0.0356, -0.0604],\n",
      "         [-0.0719,  0.0384,  0.2260,  ..., -0.1381,  0.0103,  0.1331]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.0089,  0.3233, -0.0779,  ..., -0.4491,  0.1845,  0.0951],\n",
      "         [-0.0863,  0.0463,  0.0332,  ..., -0.1038, -0.1459, -0.0254],\n",
      "         [ 0.1012, -0.1778, -0.0642,  ..., -0.2366, -0.0300,  0.0253],\n",
      "         ...,\n",
      "         [-0.0158,  0.0634, -0.0783,  ..., -0.1762,  0.2059,  0.0461],\n",
      "         [ 0.0321, -0.0712,  0.2163,  ...,  0.0808,  0.0356, -0.0604],\n",
      "         [-0.0719,  0.0384,  0.2260,  ..., -0.1381,  0.0103,  0.1331]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.0244,  0.2828, -0.0532,  ..., -0.4592,  0.2300,  0.0142],\n",
      "         [-0.0407, -0.0648, -0.0111,  ..., -0.1073, -0.1317, -0.0720],\n",
      "         [ 0.0202, -0.1954, -0.0423,  ..., -0.1063, -0.0894, -0.0192],\n",
      "         ...,\n",
      "         [ 0.0566,  0.0993, -0.0367,  ..., -0.0762,  0.0678,  0.0663],\n",
      "         [-0.0367,  0.0817,  0.1333,  ...,  0.0993, -0.0563, -0.0191],\n",
      "         [-0.0564,  0.0689,  0.1280,  ..., -0.1170, -0.0857,  0.1263]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.0244,  0.2828, -0.0532,  ..., -0.4592,  0.2300,  0.0142],\n",
      "         [-0.0407, -0.0648, -0.0111,  ..., -0.1073, -0.1317, -0.0720],\n",
      "         [ 0.0202, -0.1954, -0.0423,  ..., -0.1063, -0.0894, -0.0192],\n",
      "         ...,\n",
      "         [ 0.0566,  0.0993, -0.0367,  ..., -0.0762,  0.0678,  0.0663],\n",
      "         [-0.0367,  0.0817,  0.1333,  ...,  0.0993, -0.0563, -0.0191],\n",
      "         [-0.0564,  0.0689,  0.1280,  ..., -0.1170, -0.0857,  0.1263]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.0928,  0.1485,  0.0158,  ..., -0.5380,  0.2664, -0.0333],\n",
      "         [-0.1184, -0.1740, -0.0136,  ..., -0.0927, -0.0638, -0.0746],\n",
      "         [ 0.0421, -0.1649, -0.0176,  ..., -0.1652, -0.0329,  0.0128],\n",
      "         ...,\n",
      "         [-0.1161,  0.1516,  0.1269,  ..., -0.1341,  0.0540,  0.2881],\n",
      "         [-0.0451,  0.0799,  0.1695,  ...,  0.1416, -0.0557,  0.0572],\n",
      "         [ 0.0345,  0.0736,  0.0477,  ..., -0.1050, -0.1945,  0.2442]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.0928,  0.1485,  0.0158,  ..., -0.5380,  0.2664, -0.0333],\n",
      "         [-0.1184, -0.1740, -0.0136,  ..., -0.0927, -0.0638, -0.0746],\n",
      "         [ 0.0421, -0.1649, -0.0176,  ..., -0.1652, -0.0329,  0.0128],\n",
      "         ...,\n",
      "         [-0.1161,  0.1516,  0.1269,  ..., -0.1341,  0.0540,  0.2881],\n",
      "         [-0.0451,  0.0799,  0.1695,  ...,  0.1416, -0.0557,  0.0572],\n",
      "         [ 0.0345,  0.0736,  0.0477,  ..., -0.1050, -0.1945,  0.2442]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.1000,  0.1323,  0.0241,  ..., -0.5470,  0.2844, -0.0457],\n",
      "         [-0.0583, -0.2512,  0.0298,  ...,  0.0325, -0.1373, -0.1316],\n",
      "         [ 0.1936, -0.2308,  0.1023,  ...,  0.0216,  0.0091, -0.1009],\n",
      "         ...,\n",
      "         [-0.0424,  0.1546,  0.0537,  ..., -0.2543,  0.0678,  0.1194],\n",
      "         [ 0.1090,  0.0915,  0.2042,  ...,  0.1691, -0.1992,  0.1034],\n",
      "         [ 0.1819,  0.1291, -0.0342,  ..., -0.0300, -0.3496,  0.2659]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.1000,  0.1323,  0.0241,  ..., -0.5470,  0.2844, -0.0457],\n",
      "         [-0.0583, -0.2512,  0.0298,  ...,  0.0325, -0.1373, -0.1316],\n",
      "         [ 0.1936, -0.2308,  0.1023,  ...,  0.0216,  0.0091, -0.1009],\n",
      "         ...,\n",
      "         [-0.0424,  0.1546,  0.0537,  ..., -0.2543,  0.0678,  0.1194],\n",
      "         [ 0.1090,  0.0915,  0.2042,  ...,  0.1691, -0.1992,  0.1034],\n",
      "         [ 0.1819,  0.1291, -0.0342,  ..., -0.0300, -0.3496,  0.2659]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.1286,  0.0833,  0.0239,  ..., -0.5657,  0.3694, -0.0725],\n",
      "         [-0.0743, -0.1278,  0.0966,  ..., -0.0564, -0.1650, -0.0955],\n",
      "         [ 0.1111, -0.2656,  0.0487,  ..., -0.1732,  0.0192, -0.1092],\n",
      "         ...,\n",
      "         [-0.0608, -0.0394,  0.0131,  ..., -0.0457, -0.0119,  0.1160],\n",
      "         [ 0.0860,  0.1122,  0.1759,  ...,  0.1481, -0.2002,  0.1731],\n",
      "         [ 0.1690,  0.1487, -0.0930,  ..., -0.0588, -0.4429,  0.2761]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.1286,  0.0833,  0.0239,  ..., -0.5657,  0.3694, -0.0725],\n",
      "         [-0.0743, -0.1278,  0.0966,  ..., -0.0564, -0.1650, -0.0955],\n",
      "         [ 0.1111, -0.2656,  0.0487,  ..., -0.1732,  0.0192, -0.1092],\n",
      "         ...,\n",
      "         [-0.0608, -0.0394,  0.0131,  ..., -0.0457, -0.0119,  0.1160],\n",
      "         [ 0.0860,  0.1122,  0.1759,  ...,  0.1481, -0.2002,  0.1731],\n",
      "         [ 0.1690,  0.1487, -0.0930,  ..., -0.0588, -0.4429,  0.2761]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.1417,  0.0703,  0.0343,  ..., -0.5549,  0.3175, -0.0847],\n",
      "         [-0.0943, -0.1372,  0.0391,  ..., -0.0778, -0.1138, -0.0732],\n",
      "         [-0.0795, -0.2764, -0.1192,  ..., -0.1677, -0.1099, -0.1740],\n",
      "         ...,\n",
      "         [ 0.0217, -0.1050, -0.0452,  ..., -0.0300, -0.0817, -0.0124],\n",
      "         [ 0.2178,  0.0888,  0.1157,  ...,  0.0977, -0.2484,  0.2218],\n",
      "         [ 0.1656,  0.1340, -0.0582,  ...,  0.0113, -0.4719,  0.3187]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.1417,  0.0703,  0.0343,  ..., -0.5549,  0.3175, -0.0847],\n",
      "         [-0.0943, -0.1372,  0.0391,  ..., -0.0778, -0.1138, -0.0732],\n",
      "         [-0.0795, -0.2764, -0.1192,  ..., -0.1677, -0.1099, -0.1740],\n",
      "         ...,\n",
      "         [ 0.0217, -0.1050, -0.0452,  ..., -0.0300, -0.0817, -0.0124],\n",
      "         [ 0.2178,  0.0888,  0.1157,  ...,  0.0977, -0.2484,  0.2218],\n",
      "         [ 0.1656,  0.1340, -0.0582,  ...,  0.0113, -0.4719,  0.3187]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.1528,  0.0725,  0.0609,  ..., -0.5624,  0.3103, -0.0293],\n",
      "         [-0.1146, -0.1187,  0.0884,  ..., -0.0363, -0.0841, -0.1003],\n",
      "         [-0.1375, -0.2893, -0.1295,  ..., -0.2207, -0.0565, -0.1087],\n",
      "         ...,\n",
      "         [ 0.1384, -0.0437,  0.0983,  ..., -0.0087, -0.0264, -0.0474],\n",
      "         [ 0.0926,  0.0090,  0.0503,  ...,  0.0780, -0.1625,  0.2513],\n",
      "         [ 0.0516,  0.1097, -0.1417,  ..., -0.1055, -0.4582,  0.2524]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.1528,  0.0725,  0.0609,  ..., -0.5624,  0.3103, -0.0293],\n",
      "         [-0.1146, -0.1187,  0.0884,  ..., -0.0363, -0.0841, -0.1003],\n",
      "         [-0.1375, -0.2893, -0.1295,  ..., -0.2207, -0.0565, -0.1087],\n",
      "         ...,\n",
      "         [ 0.1384, -0.0437,  0.0983,  ..., -0.0087, -0.0264, -0.0474],\n",
      "         [ 0.0926,  0.0090,  0.0503,  ...,  0.0780, -0.1625,  0.2513],\n",
      "         [ 0.0516,  0.1097, -0.1417,  ..., -0.1055, -0.4582,  0.2524]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.1559,  0.0520,  0.0566,  ..., -0.5805,  0.2614, -0.0032],\n",
      "         [-0.1547, -0.1337,  0.0514,  ..., -0.0746, -0.0887, -0.0601],\n",
      "         [-0.0136, -0.3586, -0.2246,  ..., -0.3050, -0.0919, -0.0337],\n",
      "         ...,\n",
      "         [ 0.1572,  0.0783,  0.2286,  ...,  0.1620, -0.0299, -0.0697],\n",
      "         [ 0.1447, -0.0227,  0.0827,  ...,  0.1432, -0.2016,  0.2960],\n",
      "         [ 0.0905,  0.0353, -0.0768,  ..., -0.2077, -0.4152,  0.3716]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.1559,  0.0520,  0.0566,  ..., -0.5805,  0.2614, -0.0032],\n",
      "         [-0.1547, -0.1337,  0.0514,  ..., -0.0746, -0.0887, -0.0601],\n",
      "         [-0.0136, -0.3586, -0.2246,  ..., -0.3050, -0.0919, -0.0337],\n",
      "         ...,\n",
      "         [ 0.1572,  0.0783,  0.2286,  ...,  0.1620, -0.0299, -0.0697],\n",
      "         [ 0.1447, -0.0227,  0.0827,  ...,  0.1432, -0.2016,  0.2960],\n",
      "         [ 0.0905,  0.0353, -0.0768,  ..., -0.2077, -0.4152,  0.3716]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.1488,  0.0445,  0.0765,  ..., -0.5456,  0.2702,  0.0098],\n",
      "         [-0.1820, -0.1409,  0.0618,  ..., -0.0134, -0.1458, -0.1489],\n",
      "         [ 0.1541, -0.2953, -0.2177,  ..., -0.3127, -0.0731, -0.0232],\n",
      "         ...,\n",
      "         [ 0.1655,  0.1588,  0.1171,  ...,  0.2223, -0.0700,  0.0089],\n",
      "         [ 0.1546,  0.0451,  0.0370,  ...,  0.1717, -0.2978,  0.2493],\n",
      "         [ 0.1670,  0.1993, -0.2202,  ..., -0.2491, -0.5148,  0.3361]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.1488,  0.0445,  0.0765,  ..., -0.5456,  0.2702,  0.0098],\n",
      "         [-0.1820, -0.1409,  0.0618,  ..., -0.0134, -0.1458, -0.1489],\n",
      "         [ 0.1541, -0.2953, -0.2177,  ..., -0.3127, -0.0731, -0.0232],\n",
      "         ...,\n",
      "         [ 0.1655,  0.1588,  0.1171,  ...,  0.2223, -0.0700,  0.0089],\n",
      "         [ 0.1546,  0.0451,  0.0370,  ...,  0.1717, -0.2978,  0.2493],\n",
      "         [ 0.1670,  0.1993, -0.2202,  ..., -0.2491, -0.5148,  0.3361]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.1268,  0.0333,  0.0935,  ..., -0.5320,  0.2795,  0.0006],\n",
      "         [-0.1952, -0.1381,  0.0799,  ..., -0.0260, -0.0164, -0.1538],\n",
      "         [ 0.2540, -0.2907, -0.0560,  ..., -0.2684, -0.0897, -0.0838],\n",
      "         ...,\n",
      "         [ 0.0677,  0.1072,  0.0910,  ...,  0.1606, -0.1300, -0.0685],\n",
      "         [-0.0269,  0.1126,  0.0673,  ...,  0.1112, -0.2217,  0.3106],\n",
      "         [ 0.0881,  0.2199, -0.0733,  ..., -0.1549, -0.5287,  0.4282]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.1268,  0.0333,  0.0935,  ..., -0.5320,  0.2795,  0.0006],\n",
      "         [-0.1952, -0.1381,  0.0799,  ..., -0.0260, -0.0164, -0.1538],\n",
      "         [ 0.2540, -0.2907, -0.0560,  ..., -0.2684, -0.0897, -0.0838],\n",
      "         ...,\n",
      "         [ 0.0677,  0.1072,  0.0910,  ...,  0.1606, -0.1300, -0.0685],\n",
      "         [-0.0269,  0.1126,  0.0673,  ...,  0.1112, -0.2217,  0.3106],\n",
      "         [ 0.0881,  0.2199, -0.0733,  ..., -0.1549, -0.5287,  0.4282]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.1355,  0.0227,  0.0554,  ..., -0.5473,  0.2470, -0.0229],\n",
      "         [-0.2616, -0.0596,  0.1333,  ..., -0.1543, -0.0621, -0.2159],\n",
      "         [ 0.3522, -0.2019, -0.0668,  ..., -0.3996, -0.1920, -0.0745],\n",
      "         ...,\n",
      "         [ 0.2084,  0.0635,  0.0237,  ...,  0.0905, -0.1712, -0.0608],\n",
      "         [-0.0311, -0.0448,  0.0679,  ...,  0.0760, -0.2338,  0.1269],\n",
      "         [ 0.1135,  0.1555, -0.1544,  ..., -0.0261, -0.4941,  0.3073]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.1355,  0.0227,  0.0554,  ..., -0.5473,  0.2470, -0.0229],\n",
      "         [-0.2616, -0.0596,  0.1333,  ..., -0.1543, -0.0621, -0.2159],\n",
      "         [ 0.3522, -0.2019, -0.0668,  ..., -0.3996, -0.1920, -0.0745],\n",
      "         ...,\n",
      "         [ 0.2084,  0.0635,  0.0237,  ...,  0.0905, -0.1712, -0.0608],\n",
      "         [-0.0311, -0.0448,  0.0679,  ...,  0.0760, -0.2338,  0.1269],\n",
      "         [ 0.1135,  0.1555, -0.1544,  ..., -0.0261, -0.4941,  0.3073]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-1.3499e-01,  1.6291e-02,  5.7584e-02,  ..., -5.5546e-01,\n",
      "           2.4354e-01,  5.0920e-03],\n",
      "         [-2.8955e-01, -4.2504e-02,  1.2533e-01,  ..., -1.9028e-01,\n",
      "          -3.4684e-04, -1.7965e-01],\n",
      "         [ 2.4485e-01, -1.9010e-01, -5.2919e-02,  ..., -4.0279e-01,\n",
      "          -1.9726e-01, -1.0793e-01],\n",
      "         ...,\n",
      "         [ 2.1596e-01,  7.8494e-03, -1.1973e-01,  ...,  1.8187e-01,\n",
      "          -1.8883e-01, -8.4099e-03],\n",
      "         [-2.1780e-02, -8.3232e-02,  1.6427e-01,  ...,  1.5688e-01,\n",
      "          -2.1864e-01,  2.7431e-01],\n",
      "         [ 1.4816e-01,  2.3547e-01, -1.2200e-01,  ...,  4.8116e-02,\n",
      "          -4.0625e-01,  3.2910e-01]]], device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-1.3499e-01,  1.6291e-02,  5.7584e-02,  ..., -5.5546e-01,\n",
      "           2.4354e-01,  5.0920e-03],\n",
      "         [-2.8955e-01, -4.2504e-02,  1.2533e-01,  ..., -1.9028e-01,\n",
      "          -3.4684e-04, -1.7965e-01],\n",
      "         [ 2.4485e-01, -1.9010e-01, -5.2919e-02,  ..., -4.0279e-01,\n",
      "          -1.9726e-01, -1.0793e-01],\n",
      "         ...,\n",
      "         [ 2.1596e-01,  7.8494e-03, -1.1973e-01,  ...,  1.8187e-01,\n",
      "          -1.8883e-01, -8.4099e-03],\n",
      "         [-2.1780e-02, -8.3232e-02,  1.6427e-01,  ...,  1.5688e-01,\n",
      "          -2.1864e-01,  2.7431e-01],\n",
      "         [ 1.4816e-01,  2.3547e-01, -1.2200e-01,  ...,  4.8116e-02,\n",
      "          -4.0625e-01,  3.2910e-01]]], device='cuda:0'),) and output (tensor([[[-0.1301, -0.0029,  0.0279,  ..., -0.5537,  0.2277,  0.0164],\n",
      "         [-0.3277, -0.0129,  0.1072,  ..., -0.2369, -0.0422, -0.1156],\n",
      "         [ 0.3772, -0.2348, -0.0381,  ..., -0.4082, -0.3338, -0.1562],\n",
      "         ...,\n",
      "         [ 0.2278, -0.0437, -0.0230,  ...,  0.0688, -0.1335, -0.1812],\n",
      "         [-0.0508, -0.0455,  0.1426,  ...,  0.1495, -0.1217,  0.2419],\n",
      "         [ 0.1199,  0.1563, -0.0046,  ...,  0.0801, -0.4960,  0.3246]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.1301, -0.0029,  0.0279,  ..., -0.5537,  0.2277,  0.0164],\n",
      "         [-0.3277, -0.0129,  0.1072,  ..., -0.2369, -0.0422, -0.1156],\n",
      "         [ 0.3772, -0.2348, -0.0381,  ..., -0.4082, -0.3338, -0.1562],\n",
      "         ...,\n",
      "         [ 0.2278, -0.0437, -0.0230,  ...,  0.0688, -0.1335, -0.1812],\n",
      "         [-0.0508, -0.0455,  0.1426,  ...,  0.1495, -0.1217,  0.2419],\n",
      "         [ 0.1199,  0.1563, -0.0046,  ...,  0.0801, -0.4960,  0.3246]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.1219,  0.0061, -0.0021,  ..., -0.5922,  0.2788, -0.0020],\n",
      "         [-0.3654,  0.1109, -0.0443,  ..., -0.2378, -0.0531, -0.1664],\n",
      "         [ 0.5268, -0.2163, -0.1473,  ..., -0.3655, -0.3302, -0.2089],\n",
      "         ...,\n",
      "         [ 0.2277, -0.0602, -0.0757,  ...,  0.1049, -0.1978, -0.2365],\n",
      "         [-0.1746,  0.0161,  0.2868,  ...,  0.2130, -0.2322,  0.3502],\n",
      "         [ 0.2249, -0.0199, -0.0434,  ...,  0.0519, -0.4447,  0.6374]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.1219,  0.0061, -0.0021,  ..., -0.5922,  0.2788, -0.0020],\n",
      "         [-0.3654,  0.1109, -0.0443,  ..., -0.2378, -0.0531, -0.1664],\n",
      "         [ 0.5268, -0.2163, -0.1473,  ..., -0.3655, -0.3302, -0.2089],\n",
      "         ...,\n",
      "         [ 0.2277, -0.0602, -0.0757,  ...,  0.1049, -0.1978, -0.2365],\n",
      "         [-0.1746,  0.0161,  0.2868,  ...,  0.2130, -0.2322,  0.3502],\n",
      "         [ 0.2249, -0.0199, -0.0434,  ...,  0.0519, -0.4447,  0.6374]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.1067,  0.0272,  0.0495,  ..., -0.5550,  0.3449,  0.0266],\n",
      "         [-0.4038,  0.0728, -0.0671,  ..., -0.0978, -0.1513, -0.1260],\n",
      "         [ 0.5356, -0.1349, -0.0878,  ..., -0.3153, -0.4752, -0.1900],\n",
      "         ...,\n",
      "         [ 0.1499, -0.1463, -0.0163,  ...,  0.1665, -0.0240,  0.0212],\n",
      "         [-0.3684,  0.0185,  0.3578,  ...,  0.3952, -0.2512,  0.5411],\n",
      "         [ 0.0288,  0.1114, -0.2324,  ...,  0.1338, -0.6237,  0.5826]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.1067,  0.0272,  0.0495,  ..., -0.5550,  0.3449,  0.0266],\n",
      "         [-0.4038,  0.0728, -0.0671,  ..., -0.0978, -0.1513, -0.1260],\n",
      "         [ 0.5356, -0.1349, -0.0878,  ..., -0.3153, -0.4752, -0.1900],\n",
      "         ...,\n",
      "         [ 0.1499, -0.1463, -0.0163,  ...,  0.1665, -0.0240,  0.0212],\n",
      "         [-0.3684,  0.0185,  0.3578,  ...,  0.3952, -0.2512,  0.5411],\n",
      "         [ 0.0288,  0.1114, -0.2324,  ...,  0.1338, -0.6237,  0.5826]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.0592,  0.0731,  0.0500,  ..., -0.5517,  0.3541,  0.0595],\n",
      "         [-0.5285,  0.0632, -0.0265,  ...,  0.0635, -0.1387, -0.0029],\n",
      "         [ 0.3270, -0.1205,  0.0814,  ..., -0.3493, -0.3611, -0.2318],\n",
      "         ...,\n",
      "         [ 0.0982, -0.1386,  0.3352,  ...,  0.1359, -0.2014, -0.1888],\n",
      "         [-0.6922, -0.3073,  0.4463,  ...,  0.3962, -0.1819,  0.3407],\n",
      "         [-0.1378, -0.1434, -0.1251,  ..., -0.0496, -0.7138,  0.3442]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.0592,  0.0731,  0.0500,  ..., -0.5517,  0.3541,  0.0595],\n",
      "         [-0.5285,  0.0632, -0.0265,  ...,  0.0635, -0.1387, -0.0029],\n",
      "         [ 0.3270, -0.1205,  0.0814,  ..., -0.3493, -0.3611, -0.2318],\n",
      "         ...,\n",
      "         [ 0.0982, -0.1386,  0.3352,  ...,  0.1359, -0.2014, -0.1888],\n",
      "         [-0.6922, -0.3073,  0.4463,  ...,  0.3962, -0.1819,  0.3407],\n",
      "         [-0.1378, -0.1434, -0.1251,  ..., -0.0496, -0.7138,  0.3442]]],\n",
      "       device='cuda:0'),) and output (tensor([[[ 0.1143,  0.2237,  0.3292,  ..., -0.7706,  0.2608,  0.0269],\n",
      "         [-0.5776,  0.0059, -0.0710,  ...,  0.2528, -0.3285, -0.2386],\n",
      "         [ 0.5370, -0.4183,  0.1772,  ..., -0.3914, -0.4436, -0.1182],\n",
      "         ...,\n",
      "         [ 0.3412, -0.3140,  0.2882,  ..., -0.0586, -0.7302, -0.2365],\n",
      "         [-0.5670, -0.2131,  0.5072,  ...,  0.3311, -0.2152,  0.4913],\n",
      "         [-0.1027, -0.0219, -0.0553,  ..., -0.0991, -0.8725,  0.2230]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[ 0.1143,  0.2237,  0.3292,  ..., -0.7706,  0.2608,  0.0269],\n",
      "         [-0.5776,  0.0059, -0.0710,  ...,  0.2528, -0.3285, -0.2386],\n",
      "         [ 0.5370, -0.4183,  0.1772,  ..., -0.3914, -0.4436, -0.1182],\n",
      "         ...,\n",
      "         [ 0.3412, -0.3140,  0.2882,  ..., -0.0586, -0.7302, -0.2365],\n",
      "         [-0.5670, -0.2131,  0.5072,  ...,  0.3311, -0.2152,  0.4913],\n",
      "         [-0.1027, -0.0219, -0.0553,  ..., -0.0991, -0.8725,  0.2230]]],\n",
      "       device='cuda:0'),) and output (tensor([[[ 0.3711,  0.5489,  0.6100,  ..., -0.7499,  0.2080,  0.4195],\n",
      "         [-0.5777, -0.4228, -0.5790,  ...,  0.0627, -0.4417, -0.8926],\n",
      "         [ 0.5353, -0.4487,  0.0517,  ..., -0.2644, -0.3064,  0.0129],\n",
      "         ...,\n",
      "         [ 0.2158, -0.5128,  0.1260,  ..., -0.4655, -1.2999, -0.8079],\n",
      "         [-0.9565, -0.7317,  0.0448,  ...,  0.1215, -0.4580, -0.1885],\n",
      "         [-0.2227, -0.4497, -0.3876,  ..., -0.5331, -1.1016, -0.3504]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[ 0.3711,  0.5489,  0.6100,  ..., -0.7499,  0.2080,  0.4195],\n",
      "         [-0.5777, -0.4228, -0.5790,  ...,  0.0627, -0.4417, -0.8926],\n",
      "         [ 0.5353, -0.4487,  0.0517,  ..., -0.2644, -0.3064,  0.0129],\n",
      "         ...,\n",
      "         [ 0.2158, -0.5128,  0.1260,  ..., -0.4655, -1.2999, -0.8079],\n",
      "         [-0.9565, -0.7317,  0.0448,  ...,  0.1215, -0.4580, -0.1885],\n",
      "         [-0.2227, -0.4497, -0.3876,  ..., -0.5331, -1.1016, -0.3504]]],\n",
      "       device='cuda:0'),) and output (tensor([[[ 1.1544,  3.2965,  1.6534,  ..., -2.7009,  2.6553,  2.2700],\n",
      "         [-0.9244, -0.1957, -2.4944,  ..., -0.6075, -0.3109, -0.8584],\n",
      "         [ 0.7592,  0.2118, -0.1062,  ..., -0.4450,  0.0987, -0.2160],\n",
      "         ...,\n",
      "         [-0.0877, -0.6660,  0.9150,  ...,  0.5032, -1.0561, -0.7907],\n",
      "         [-0.6696, -0.8279, -0.0440,  ...,  1.1713, -0.8770,  0.4854],\n",
      "         [-0.7853, -0.9944,  0.8422,  ...,  0.3942, -0.8357,  0.1597]]],\n",
      "       device='cuda:0'),)\n",
      "[Debug] Layer names being passed: ['model.layers.0', 'model.layers.1', 'model.layers.2', 'model.layers.3', 'model.layers.4', 'model.layers.5', 'model.layers.6', 'model.layers.7', 'model.layers.8', 'model.layers.9', 'model.layers.10', 'model.layers.11', 'model.layers.12', 'model.layers.13', 'model.layers.14', 'model.layers.15', 'model.layers.16', 'model.layers.17', 'model.layers.18', 'model.layers.19', 'model.layers.20', 'model.layers.21', 'model.layers.22', 'model.layers.23', 'model.layers.24', 'model.layers.25', 'model.layers.26', 'model.layers.27', 'model.layers.28', 'model.layers.29', 'model.layers.30', 'model.layers.31', 'model.embed_tokens']\n",
      "[Debug] Trying to access layer: model.layers.0\n",
      "[Debug] Successfully found layer: model.layers.0\n",
      "[Debug] Trying to access layer: model.layers.1\n",
      "[Debug] Successfully found layer: model.layers.1\n",
      "[Debug] Trying to access layer: model.layers.2\n",
      "[Debug] Successfully found layer: model.layers.2\n",
      "[Debug] Trying to access layer: model.layers.3\n",
      "[Debug] Successfully found layer: model.layers.3\n",
      "[Debug] Trying to access layer: model.layers.4\n",
      "[Debug] Successfully found layer: model.layers.4\n",
      "[Debug] Trying to access layer: model.layers.5\n",
      "[Debug] Successfully found layer: model.layers.5\n",
      "[Debug] Trying to access layer: model.layers.6\n",
      "[Debug] Successfully found layer: model.layers.6\n",
      "[Debug] Trying to access layer: model.layers.7\n",
      "[Debug] Successfully found layer: model.layers.7\n",
      "[Debug] Trying to access layer: model.layers.8\n",
      "[Debug] Successfully found layer: model.layers.8\n",
      "[Debug] Trying to access layer: model.layers.9\n",
      "[Debug] Successfully found layer: model.layers.9\n",
      "[Debug] Trying to access layer: model.layers.10\n",
      "[Debug] Successfully found layer: model.layers.10\n",
      "[Debug] Trying to access layer: model.layers.11\n",
      "[Debug] Successfully found layer: model.layers.11\n",
      "[Debug] Trying to access layer: model.layers.12\n",
      "[Debug] Successfully found layer: model.layers.12\n",
      "[Debug] Trying to access layer: model.layers.13\n",
      "[Debug] Successfully found layer: model.layers.13\n",
      "[Debug] Trying to access layer: model.layers.14\n",
      "[Debug] Successfully found layer: model.layers.14\n",
      "[Debug] Trying to access layer: model.layers.15\n",
      "[Debug] Successfully found layer: model.layers.15\n",
      "[Debug] Trying to access layer: model.layers.16\n",
      "[Debug] Successfully found layer: model.layers.16\n",
      "[Debug] Trying to access layer: model.layers.17\n",
      "[Debug] Successfully found layer: model.layers.17\n",
      "[Debug] Trying to access layer: model.layers.18\n",
      "[Debug] Successfully found layer: model.layers.18\n",
      "[Debug] Trying to access layer: model.layers.19\n",
      "[Debug] Successfully found layer: model.layers.19\n",
      "[Debug] Trying to access layer: model.layers.20\n",
      "[Debug] Successfully found layer: model.layers.20\n",
      "[Debug] Trying to access layer: model.layers.21\n",
      "[Debug] Successfully found layer: model.layers.21\n",
      "[Debug] Trying to access layer: model.layers.22\n",
      "[Debug] Successfully found layer: model.layers.22\n",
      "[Debug] Trying to access layer: model.layers.23\n",
      "[Debug] Successfully found layer: model.layers.23\n",
      "[Debug] Trying to access layer: model.layers.24\n",
      "[Debug] Successfully found layer: model.layers.24\n",
      "[Debug] Trying to access layer: model.layers.25\n",
      "[Debug] Successfully found layer: model.layers.25\n",
      "[Debug] Trying to access layer: model.layers.26\n",
      "[Debug] Successfully found layer: model.layers.26\n",
      "[Debug] Trying to access layer: model.layers.27\n",
      "[Debug] Successfully found layer: model.layers.27\n",
      "[Debug] Trying to access layer: model.layers.28\n",
      "[Debug] Successfully found layer: model.layers.28\n",
      "[Debug] Trying to access layer: model.layers.29\n",
      "[Debug] Successfully found layer: model.layers.29\n",
      "[Debug] Trying to access layer: model.layers.30\n",
      "[Debug] Successfully found layer: model.layers.30\n",
      "[Debug] Trying to access layer: model.layers.31\n",
      "[Debug] Successfully found layer: model.layers.31\n",
      "[Debug] Trying to access layer: model.embed_tokens\n",
      "[Debug] Successfully found layer: model.embed_tokens\n",
      "[Hook] Layer Embedding(128256, 4096) received input (tensor([[128000,   9207,    315,    279,   3723,   4273,    304,    279,  23315,\n",
      "           5111,    578,   3560,    315,    279,   3723,   4273,    304,    279,\n",
      "          23315,   5111,   6137,   1306,   4435,   5111,   8105,    323,  81700,\n",
      "           1139,   2539,  15507,   2391,    279,  23315,   5111,    505,    220,\n",
      "           6280,     20,    311,    220,   4468,     20,     13]],\n",
      "       device='cuda:0'),) and output tensor([[[ 2.6512e-04, -4.9973e-04, -5.8365e-04,  ...,  3.8147e-03,\n",
      "           6.3419e-05,  1.1902e-03],\n",
      "         [-1.3962e-03, -6.1646e-03,  2.0313e-04,  ...,  3.6011e-03,\n",
      "           3.5706e-03, -8.7280e-03],\n",
      "         [ 1.2817e-03,  9.1171e-04,  2.0905e-03,  ...,  1.6251e-03,\n",
      "           4.0894e-03, -4.0283e-03],\n",
      "         ...,\n",
      "         [ 4.9438e-03,  1.3046e-03,  2.2278e-03,  ..., -1.5335e-03,\n",
      "          -6.2866e-03, -2.6398e-03],\n",
      "         [ 3.3722e-03,  7.2021e-03,  1.4420e-03,  ...,  1.4038e-02,\n",
      "          -3.0670e-03, -7.4463e-03],\n",
      "         [-6.7520e-04, -5.7602e-04,  4.7684e-04,  ...,  3.3264e-03,\n",
      "           3.9101e-04,  4.7493e-04]]], device='cuda:0')\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[ 2.6512e-04, -4.9973e-04, -5.8365e-04,  ...,  3.8147e-03,\n",
      "           6.3419e-05,  1.1902e-03],\n",
      "         [-1.3962e-03, -6.1646e-03,  2.0313e-04,  ...,  3.6011e-03,\n",
      "           3.5706e-03, -8.7280e-03],\n",
      "         [ 1.2817e-03,  9.1171e-04,  2.0905e-03,  ...,  1.6251e-03,\n",
      "           4.0894e-03, -4.0283e-03],\n",
      "         ...,\n",
      "         [ 4.9438e-03,  1.3046e-03,  2.2278e-03,  ..., -1.5335e-03,\n",
      "          -6.2866e-03, -2.6398e-03],\n",
      "         [ 3.3722e-03,  7.2021e-03,  1.4420e-03,  ...,  1.4038e-02,\n",
      "          -3.0670e-03, -7.4463e-03],\n",
      "         [-6.7520e-04, -5.7602e-04,  4.7684e-04,  ...,  3.3264e-03,\n",
      "           3.9101e-04,  4.7493e-04]]], device='cuda:0'),) and output (tensor([[[-0.0004,  0.0146, -0.0020,  ...,  0.0066, -0.0194,  0.0020],\n",
      "         [ 0.0021,  0.0018, -0.0010,  ...,  0.0022,  0.0052, -0.0413],\n",
      "         [-0.0071,  0.0060,  0.0077,  ...,  0.0114,  0.0214, -0.0058],\n",
      "         ...,\n",
      "         [ 0.0157, -0.0100,  0.0005,  ..., -0.0294, -0.0256,  0.0072],\n",
      "         [ 0.0122, -0.0057,  0.0026,  ...,  0.0030, -0.0083, -0.0090],\n",
      "         [-0.0028,  0.0160,  0.0008,  ..., -0.0064,  0.0043,  0.0065]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.0004,  0.0146, -0.0020,  ...,  0.0066, -0.0194,  0.0020],\n",
      "         [ 0.0021,  0.0018, -0.0010,  ...,  0.0022,  0.0052, -0.0413],\n",
      "         [-0.0071,  0.0060,  0.0077,  ...,  0.0114,  0.0214, -0.0058],\n",
      "         ...,\n",
      "         [ 0.0157, -0.0100,  0.0005,  ..., -0.0294, -0.0256,  0.0072],\n",
      "         [ 0.0122, -0.0057,  0.0026,  ...,  0.0030, -0.0083, -0.0090],\n",
      "         [-0.0028,  0.0160,  0.0008,  ..., -0.0064,  0.0043,  0.0065]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.0972,  0.0787, -0.0419,  ...,  0.0071,  0.0869,  0.0218],\n",
      "         [ 0.0115, -0.0003,  0.0118,  ..., -0.0019,  0.0099, -0.0484],\n",
      "         [-0.0082,  0.0063,  0.0066,  ...,  0.0354,  0.0328, -0.0312],\n",
      "         ...,\n",
      "         [ 0.0271, -0.0043,  0.0298,  ..., -0.0400, -0.0236,  0.0061],\n",
      "         [ 0.0100, -0.0236,  0.0089,  ..., -0.0362, -0.0140, -0.0058],\n",
      "         [-0.0010,  0.0042,  0.0041,  ..., -0.0228, -0.0091,  0.0094]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.0972,  0.0787, -0.0419,  ...,  0.0071,  0.0869,  0.0218],\n",
      "         [ 0.0115, -0.0003,  0.0118,  ..., -0.0019,  0.0099, -0.0484],\n",
      "         [-0.0082,  0.0063,  0.0066,  ...,  0.0354,  0.0328, -0.0312],\n",
      "         ...,\n",
      "         [ 0.0271, -0.0043,  0.0298,  ..., -0.0400, -0.0236,  0.0061],\n",
      "         [ 0.0100, -0.0236,  0.0089,  ..., -0.0362, -0.0140, -0.0058],\n",
      "         [-0.0010,  0.0042,  0.0041,  ..., -0.0228, -0.0091,  0.0094]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.1040,  0.0875, -0.0360,  ...,  0.0205,  0.0997,  0.0184],\n",
      "         [ 0.0175, -0.0171,  0.0262,  ...,  0.0238,  0.0163, -0.0699],\n",
      "         [ 0.0084, -0.0042,  0.0256,  ...,  0.0864,  0.0075, -0.0285],\n",
      "         ...,\n",
      "         [ 0.0295, -0.0071,  0.0144,  ..., -0.1291, -0.0257,  0.0143],\n",
      "         [ 0.0149, -0.0310,  0.0209,  ..., -0.0512, -0.0393,  0.0045],\n",
      "         [-0.0054,  0.0119,  0.0021,  ..., -0.0230, -0.0330,  0.0188]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.1040,  0.0875, -0.0360,  ...,  0.0205,  0.0997,  0.0184],\n",
      "         [ 0.0175, -0.0171,  0.0262,  ...,  0.0238,  0.0163, -0.0699],\n",
      "         [ 0.0084, -0.0042,  0.0256,  ...,  0.0864,  0.0075, -0.0285],\n",
      "         ...,\n",
      "         [ 0.0295, -0.0071,  0.0144,  ..., -0.1291, -0.0257,  0.0143],\n",
      "         [ 0.0149, -0.0310,  0.0209,  ..., -0.0512, -0.0393,  0.0045],\n",
      "         [-0.0054,  0.0119,  0.0021,  ..., -0.0230, -0.0330,  0.0188]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.0870,  0.1049, -0.0173,  ...,  0.0700,  0.0989,  0.0181],\n",
      "         [-0.0007,  0.0124,  0.0171,  ...,  0.0100,  0.0026, -0.0877],\n",
      "         [ 0.0074, -0.0346,  0.0691,  ...,  0.0590,  0.0391, -0.0622],\n",
      "         ...,\n",
      "         [ 0.0789, -0.0107, -0.0379,  ..., -0.1607, -0.0316, -0.0375],\n",
      "         [ 0.0639, -0.0054, -0.0272,  ..., -0.0593, -0.0393, -0.0197],\n",
      "         [-0.0240,  0.0114,  0.0163,  ..., -0.0156,  0.0087,  0.0604]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.0870,  0.1049, -0.0173,  ...,  0.0700,  0.0989,  0.0181],\n",
      "         [-0.0007,  0.0124,  0.0171,  ...,  0.0100,  0.0026, -0.0877],\n",
      "         [ 0.0074, -0.0346,  0.0691,  ...,  0.0590,  0.0391, -0.0622],\n",
      "         ...,\n",
      "         [ 0.0789, -0.0107, -0.0379,  ..., -0.1607, -0.0316, -0.0375],\n",
      "         [ 0.0639, -0.0054, -0.0272,  ..., -0.0593, -0.0393, -0.0197],\n",
      "         [-0.0240,  0.0114,  0.0163,  ..., -0.0156,  0.0087,  0.0604]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.1113,  0.1022, -0.0102,  ...,  0.0436,  0.1283,  0.0321],\n",
      "         [-0.0397, -0.0190,  0.0415,  ...,  0.0426,  0.0352, -0.0709],\n",
      "         [-0.0458, -0.0786,  0.1481,  ...,  0.0739,  0.0650, -0.0808],\n",
      "         ...,\n",
      "         [ 0.0381,  0.0394, -0.0119,  ..., -0.1095, -0.0431,  0.0234],\n",
      "         [ 0.0931,  0.0075, -0.0005,  ..., -0.0927, -0.0958,  0.0109],\n",
      "         [-0.0159, -0.0435,  0.0521,  ..., -0.0046, -0.0689, -0.0236]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.1113,  0.1022, -0.0102,  ...,  0.0436,  0.1283,  0.0321],\n",
      "         [-0.0397, -0.0190,  0.0415,  ...,  0.0426,  0.0352, -0.0709],\n",
      "         [-0.0458, -0.0786,  0.1481,  ...,  0.0739,  0.0650, -0.0808],\n",
      "         ...,\n",
      "         [ 0.0381,  0.0394, -0.0119,  ..., -0.1095, -0.0431,  0.0234],\n",
      "         [ 0.0931,  0.0075, -0.0005,  ..., -0.0927, -0.0958,  0.0109],\n",
      "         [-0.0159, -0.0435,  0.0521,  ..., -0.0046, -0.0689, -0.0236]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.1034,  0.1132,  0.0096,  ...,  0.0453,  0.1477,  0.0301],\n",
      "         [ 0.0173,  0.0450,  0.0646,  ...,  0.0634,  0.0202, -0.0547],\n",
      "         [-0.0399, -0.0122,  0.1516,  ...,  0.0972,  0.1052, -0.0693],\n",
      "         ...,\n",
      "         [ 0.0507,  0.0392,  0.0199,  ..., -0.1205, -0.0544, -0.0157],\n",
      "         [ 0.0696,  0.0391,  0.0331,  ..., -0.0623, -0.1024, -0.0148],\n",
      "         [-0.0681,  0.0007,  0.0625,  ...,  0.0832, -0.0595,  0.0158]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.1034,  0.1132,  0.0096,  ...,  0.0453,  0.1477,  0.0301],\n",
      "         [ 0.0173,  0.0450,  0.0646,  ...,  0.0634,  0.0202, -0.0547],\n",
      "         [-0.0399, -0.0122,  0.1516,  ...,  0.0972,  0.1052, -0.0693],\n",
      "         ...,\n",
      "         [ 0.0507,  0.0392,  0.0199,  ..., -0.1205, -0.0544, -0.0157],\n",
      "         [ 0.0696,  0.0391,  0.0331,  ..., -0.0623, -0.1024, -0.0148],\n",
      "         [-0.0681,  0.0007,  0.0625,  ...,  0.0832, -0.0595,  0.0158]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.1071,  0.1253,  0.0211,  ...,  0.0126,  0.1586,  0.0278],\n",
      "         [-0.0050,  0.0237,  0.0056,  ...,  0.1170,  0.0375, -0.0277],\n",
      "         [-0.0487,  0.0164,  0.0785,  ...,  0.0865,  0.0662, -0.0611],\n",
      "         ...,\n",
      "         [ 0.0286,  0.0240,  0.0355,  ..., -0.0903, -0.0544, -0.0470],\n",
      "         [ 0.0596, -0.0068, -0.0241,  ...,  0.0213, -0.1079, -0.0641],\n",
      "         [-0.0668, -0.0045,  0.0649,  ...,  0.0641, -0.1125,  0.0508]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.1071,  0.1253,  0.0211,  ...,  0.0126,  0.1586,  0.0278],\n",
      "         [-0.0050,  0.0237,  0.0056,  ...,  0.1170,  0.0375, -0.0277],\n",
      "         [-0.0487,  0.0164,  0.0785,  ...,  0.0865,  0.0662, -0.0611],\n",
      "         ...,\n",
      "         [ 0.0286,  0.0240,  0.0355,  ..., -0.0903, -0.0544, -0.0470],\n",
      "         [ 0.0596, -0.0068, -0.0241,  ...,  0.0213, -0.1079, -0.0641],\n",
      "         [-0.0668, -0.0045,  0.0649,  ...,  0.0641, -0.1125,  0.0508]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.0945,  0.1848,  0.0036,  ..., -0.0473,  0.1489,  0.0536],\n",
      "         [ 0.0446, -0.0267, -0.0071,  ...,  0.0572, -0.0144, -0.0923],\n",
      "         [ 0.0075,  0.0548,  0.1114,  ..., -0.0078,  0.0203, -0.1345],\n",
      "         ...,\n",
      "         [ 0.0745, -0.0021, -0.0331,  ...,  0.0109, -0.0085,  0.0015],\n",
      "         [ 0.0214,  0.0313, -0.0244,  ...,  0.0584, -0.1093,  0.0025],\n",
      "         [-0.0535,  0.0877,  0.0030,  ...,  0.0778, -0.0784, -0.0058]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.0945,  0.1848,  0.0036,  ..., -0.0473,  0.1489,  0.0536],\n",
      "         [ 0.0446, -0.0267, -0.0071,  ...,  0.0572, -0.0144, -0.0923],\n",
      "         [ 0.0075,  0.0548,  0.1114,  ..., -0.0078,  0.0203, -0.1345],\n",
      "         ...,\n",
      "         [ 0.0745, -0.0021, -0.0331,  ...,  0.0109, -0.0085,  0.0015],\n",
      "         [ 0.0214,  0.0313, -0.0244,  ...,  0.0584, -0.1093,  0.0025],\n",
      "         [-0.0535,  0.0877,  0.0030,  ...,  0.0778, -0.0784, -0.0058]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.1060,  0.2113,  0.0270,  ..., -0.1141,  0.1610,  0.0528],\n",
      "         [ 0.0545,  0.0219, -0.0118,  ...,  0.0123, -0.0270, -0.1135],\n",
      "         [-0.0150,  0.0362,  0.1001,  ..., -0.0967,  0.0098, -0.1438],\n",
      "         ...,\n",
      "         [ 0.0008,  0.0178, -0.0487,  ...,  0.0490,  0.0303, -0.0849],\n",
      "         [-0.0211,  0.0439, -0.0031,  ...,  0.0374, -0.0614, -0.1004],\n",
      "         [-0.0482, -0.0158, -0.0204,  ...,  0.0211, -0.1126,  0.0094]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.1060,  0.2113,  0.0270,  ..., -0.1141,  0.1610,  0.0528],\n",
      "         [ 0.0545,  0.0219, -0.0118,  ...,  0.0123, -0.0270, -0.1135],\n",
      "         [-0.0150,  0.0362,  0.1001,  ..., -0.0967,  0.0098, -0.1438],\n",
      "         ...,\n",
      "         [ 0.0008,  0.0178, -0.0487,  ...,  0.0490,  0.0303, -0.0849],\n",
      "         [-0.0211,  0.0439, -0.0031,  ...,  0.0374, -0.0614, -0.1004],\n",
      "         [-0.0482, -0.0158, -0.0204,  ...,  0.0211, -0.1126,  0.0094]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-9.5416e-02,  2.4926e-01,  7.3335e-03,  ..., -1.5681e-01,\n",
      "           1.8185e-01,  8.5524e-02],\n",
      "         [-1.2285e-02,  1.4773e-02,  1.7517e-02,  ...,  7.6470e-02,\n",
      "          -1.4691e-04, -9.9904e-02],\n",
      "         [-8.9516e-02, -4.8848e-02,  1.4984e-01,  ..., -7.5898e-02,\n",
      "           3.7649e-02, -2.0706e-01],\n",
      "         ...,\n",
      "         [-8.9933e-02, -1.4878e-01,  1.9618e-02,  ...,  7.6961e-02,\n",
      "           8.2706e-03,  2.1830e-02],\n",
      "         [-5.8641e-02, -6.9321e-02,  2.9870e-02,  ...,  7.1630e-02,\n",
      "           2.9380e-03, -4.0393e-02],\n",
      "         [-2.3229e-02, -1.3552e-01,  2.6561e-02,  ...,  6.0708e-02,\n",
      "          -6.9518e-02,  2.4400e-02]]], device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-9.5416e-02,  2.4926e-01,  7.3335e-03,  ..., -1.5681e-01,\n",
      "           1.8185e-01,  8.5524e-02],\n",
      "         [-1.2285e-02,  1.4773e-02,  1.7517e-02,  ...,  7.6470e-02,\n",
      "          -1.4691e-04, -9.9904e-02],\n",
      "         [-8.9516e-02, -4.8848e-02,  1.4984e-01,  ..., -7.5898e-02,\n",
      "           3.7649e-02, -2.0706e-01],\n",
      "         ...,\n",
      "         [-8.9933e-02, -1.4878e-01,  1.9618e-02,  ...,  7.6961e-02,\n",
      "           8.2706e-03,  2.1830e-02],\n",
      "         [-5.8641e-02, -6.9321e-02,  2.9870e-02,  ...,  7.1630e-02,\n",
      "           2.9380e-03, -4.0393e-02],\n",
      "         [-2.3229e-02, -1.3552e-01,  2.6561e-02,  ...,  6.0708e-02,\n",
      "          -6.9518e-02,  2.4400e-02]]], device='cuda:0'),) and output (tensor([[[-0.0152,  0.3340,  0.0011,  ..., -0.2750,  0.1933,  0.1034],\n",
      "         [ 0.0374,  0.0341,  0.0315,  ...,  0.0726,  0.0113, -0.0510],\n",
      "         [-0.0977, -0.0667,  0.0939,  ..., -0.0767, -0.0268, -0.1746],\n",
      "         ...,\n",
      "         [-0.0497, -0.1716,  0.1026,  ..., -0.0146, -0.0847,  0.0419],\n",
      "         [-0.0333, -0.0256,  0.1047,  ..., -0.0217, -0.1289,  0.0097],\n",
      "         [-0.0450, -0.1096, -0.0211,  ...,  0.0822, -0.2163,  0.0477]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.0152,  0.3340,  0.0011,  ..., -0.2750,  0.1933,  0.1034],\n",
      "         [ 0.0374,  0.0341,  0.0315,  ...,  0.0726,  0.0113, -0.0510],\n",
      "         [-0.0977, -0.0667,  0.0939,  ..., -0.0767, -0.0268, -0.1746],\n",
      "         ...,\n",
      "         [-0.0497, -0.1716,  0.1026,  ..., -0.0146, -0.0847,  0.0419],\n",
      "         [-0.0333, -0.0256,  0.1047,  ..., -0.0217, -0.1289,  0.0097],\n",
      "         [-0.0450, -0.1096, -0.0211,  ...,  0.0822, -0.2163,  0.0477]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.0356,  0.3483, -0.0366,  ..., -0.2962,  0.2088,  0.1065],\n",
      "         [-0.0189, -0.0160,  0.0257,  ...,  0.0183,  0.0681, -0.0623],\n",
      "         [-0.1105, -0.0757,  0.0832,  ..., -0.1729,  0.1181, -0.2015],\n",
      "         ...,\n",
      "         [-0.0482, -0.1428,  0.1067,  ...,  0.0132, -0.0052, -0.0368],\n",
      "         [ 0.0044,  0.0494,  0.1619,  ...,  0.0145, -0.1077, -0.0081],\n",
      "         [-0.0557, -0.1245, -0.0350,  ...,  0.0582, -0.2355, -0.0264]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.0356,  0.3483, -0.0366,  ..., -0.2962,  0.2088,  0.1065],\n",
      "         [-0.0189, -0.0160,  0.0257,  ...,  0.0183,  0.0681, -0.0623],\n",
      "         [-0.1105, -0.0757,  0.0832,  ..., -0.1729,  0.1181, -0.2015],\n",
      "         ...,\n",
      "         [-0.0482, -0.1428,  0.1067,  ...,  0.0132, -0.0052, -0.0368],\n",
      "         [ 0.0044,  0.0494,  0.1619,  ...,  0.0145, -0.1077, -0.0081],\n",
      "         [-0.0557, -0.1245, -0.0350,  ...,  0.0582, -0.2355, -0.0264]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.0127,  0.3661, -0.0316,  ..., -0.3678,  0.1973,  0.1019],\n",
      "         [-0.0080, -0.0412,  0.1029,  ...,  0.0051,  0.0736, -0.0044],\n",
      "         [-0.0254, -0.1667,  0.1697,  ..., -0.2008,  0.0410, -0.1674],\n",
      "         ...,\n",
      "         [-0.1200, -0.2412, -0.0143,  ...,  0.1386,  0.0386, -0.0473],\n",
      "         [-0.0843, -0.0527,  0.0376,  ...,  0.1090, -0.1231,  0.0357],\n",
      "         [-0.1565, -0.1116, -0.0062,  ..., -0.0045, -0.2491,  0.0601]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.0127,  0.3661, -0.0316,  ..., -0.3678,  0.1973,  0.1019],\n",
      "         [-0.0080, -0.0412,  0.1029,  ...,  0.0051,  0.0736, -0.0044],\n",
      "         [-0.0254, -0.1667,  0.1697,  ..., -0.2008,  0.0410, -0.1674],\n",
      "         ...,\n",
      "         [-0.1200, -0.2412, -0.0143,  ...,  0.1386,  0.0386, -0.0473],\n",
      "         [-0.0843, -0.0527,  0.0376,  ...,  0.1090, -0.1231,  0.0357],\n",
      "         [-0.1565, -0.1116, -0.0062,  ..., -0.0045, -0.2491,  0.0601]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.0089,  0.3233, -0.0779,  ..., -0.4491,  0.1845,  0.0951],\n",
      "         [-0.0841, -0.0381,  0.0958,  ...,  0.0079,  0.1165, -0.0566],\n",
      "         [-0.1136, -0.1442,  0.2428,  ..., -0.2143,  0.0508, -0.2048],\n",
      "         ...,\n",
      "         [-0.0187, -0.1119,  0.0911,  ...,  0.0159,  0.0499, -0.0570],\n",
      "         [-0.1278, -0.1077,  0.1329,  ...,  0.0275, -0.1172, -0.0085],\n",
      "         [-0.2201, -0.0898,  0.1073,  ..., -0.0536, -0.1746,  0.0920]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.0089,  0.3233, -0.0779,  ..., -0.4491,  0.1845,  0.0951],\n",
      "         [-0.0841, -0.0381,  0.0958,  ...,  0.0079,  0.1165, -0.0566],\n",
      "         [-0.1136, -0.1442,  0.2428,  ..., -0.2143,  0.0508, -0.2048],\n",
      "         ...,\n",
      "         [-0.0187, -0.1119,  0.0911,  ...,  0.0159,  0.0499, -0.0570],\n",
      "         [-0.1278, -0.1077,  0.1329,  ...,  0.0275, -0.1172, -0.0085],\n",
      "         [-0.2201, -0.0898,  0.1073,  ..., -0.0536, -0.1746,  0.0920]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.0244,  0.2828, -0.0532,  ..., -0.4592,  0.2300,  0.0142],\n",
      "         [-0.0601, -0.0857, -0.0139,  ...,  0.0129,  0.0154, -0.0923],\n",
      "         [-0.0884, -0.2267,  0.0922,  ..., -0.2001,  0.0837, -0.2924],\n",
      "         ...,\n",
      "         [-0.0989, -0.0415,  0.1667,  ..., -0.0438, -0.0406, -0.0660],\n",
      "         [-0.1472, -0.0308,  0.0131,  ..., -0.0102, -0.2070, -0.0716],\n",
      "         [-0.1690, -0.0542, -0.0276,  ..., -0.0012, -0.2117, -0.0245]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.0244,  0.2828, -0.0532,  ..., -0.4592,  0.2300,  0.0142],\n",
      "         [-0.0601, -0.0857, -0.0139,  ...,  0.0129,  0.0154, -0.0923],\n",
      "         [-0.0884, -0.2267,  0.0922,  ..., -0.2001,  0.0837, -0.2924],\n",
      "         ...,\n",
      "         [-0.0989, -0.0415,  0.1667,  ..., -0.0438, -0.0406, -0.0660],\n",
      "         [-0.1472, -0.0308,  0.0131,  ..., -0.0102, -0.2070, -0.0716],\n",
      "         [-0.1690, -0.0542, -0.0276,  ..., -0.0012, -0.2117, -0.0245]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-9.2775e-02,  1.4845e-01,  1.5798e-02,  ..., -5.3800e-01,\n",
      "           2.6636e-01, -3.3290e-02],\n",
      "         [-9.4566e-02, -1.1275e-01, -9.3414e-02,  ...,  2.0840e-04,\n",
      "          -1.1604e-01, -1.5408e-01],\n",
      "         [-8.1647e-02, -1.7708e-01, -1.1016e-02,  ..., -1.5560e-01,\n",
      "          -9.3455e-02, -3.4521e-01],\n",
      "         ...,\n",
      "         [-2.1684e-01, -9.2883e-03,  7.4872e-02,  ...,  3.7443e-02,\n",
      "          -1.9368e-01, -3.3830e-02],\n",
      "         [-1.6102e-01,  5.1412e-02,  6.7792e-02,  ..., -7.3439e-02,\n",
      "          -2.8860e-01, -5.9255e-02],\n",
      "         [-3.8373e-02, -8.4169e-02, -6.6584e-03,  ..., -1.0233e-01,\n",
      "          -3.5834e-01,  1.6637e-02]]], device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-9.2775e-02,  1.4845e-01,  1.5798e-02,  ..., -5.3800e-01,\n",
      "           2.6636e-01, -3.3290e-02],\n",
      "         [-9.4566e-02, -1.1275e-01, -9.3414e-02,  ...,  2.0840e-04,\n",
      "          -1.1604e-01, -1.5408e-01],\n",
      "         [-8.1647e-02, -1.7708e-01, -1.1016e-02,  ..., -1.5560e-01,\n",
      "          -9.3455e-02, -3.4521e-01],\n",
      "         ...,\n",
      "         [-2.1684e-01, -9.2883e-03,  7.4872e-02,  ...,  3.7443e-02,\n",
      "          -1.9368e-01, -3.3830e-02],\n",
      "         [-1.6102e-01,  5.1412e-02,  6.7792e-02,  ..., -7.3439e-02,\n",
      "          -2.8860e-01, -5.9255e-02],\n",
      "         [-3.8373e-02, -8.4169e-02, -6.6584e-03,  ..., -1.0233e-01,\n",
      "          -3.5834e-01,  1.6637e-02]]], device='cuda:0'),) and output (tensor([[[-0.1000,  0.1323,  0.0241,  ..., -0.5470,  0.2844, -0.0457],\n",
      "         [-0.0506, -0.1200, -0.0142,  ...,  0.0869, -0.2421, -0.1165],\n",
      "         [-0.0103, -0.1926, -0.0252,  ..., -0.0615, -0.0681, -0.3480],\n",
      "         ...,\n",
      "         [-0.2169,  0.0209,  0.0557,  ...,  0.0506, -0.2853, -0.0580],\n",
      "         [-0.0894, -0.1165,  0.1896,  ...,  0.0299, -0.3320,  0.0096],\n",
      "         [ 0.0495,  0.0230,  0.0852,  ...,  0.0268, -0.4470, -0.0407]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.1000,  0.1323,  0.0241,  ..., -0.5470,  0.2844, -0.0457],\n",
      "         [-0.0506, -0.1200, -0.0142,  ...,  0.0869, -0.2421, -0.1165],\n",
      "         [-0.0103, -0.1926, -0.0252,  ..., -0.0615, -0.0681, -0.3480],\n",
      "         ...,\n",
      "         [-0.2169,  0.0209,  0.0557,  ...,  0.0506, -0.2853, -0.0580],\n",
      "         [-0.0894, -0.1165,  0.1896,  ...,  0.0299, -0.3320,  0.0096],\n",
      "         [ 0.0495,  0.0230,  0.0852,  ...,  0.0268, -0.4470, -0.0407]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.1286,  0.0833,  0.0239,  ..., -0.5657,  0.3694, -0.0725],\n",
      "         [-0.0130, -0.1409, -0.0495,  ...,  0.0804, -0.3102, -0.0660],\n",
      "         [-0.0893, -0.2056, -0.0762,  ..., -0.1854, -0.1167, -0.3099],\n",
      "         ...,\n",
      "         [-0.1738,  0.0242, -0.0049,  ...,  0.1330, -0.1527, -0.0430],\n",
      "         [-0.0802, -0.0251,  0.0919,  ...,  0.0486, -0.3719,  0.0452],\n",
      "         [-0.0348,  0.1832,  0.1307,  ...,  0.0033, -0.4195, -0.1311]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.1286,  0.0833,  0.0239,  ..., -0.5657,  0.3694, -0.0725],\n",
      "         [-0.0130, -0.1409, -0.0495,  ...,  0.0804, -0.3102, -0.0660],\n",
      "         [-0.0893, -0.2056, -0.0762,  ..., -0.1854, -0.1167, -0.3099],\n",
      "         ...,\n",
      "         [-0.1738,  0.0242, -0.0049,  ...,  0.1330, -0.1527, -0.0430],\n",
      "         [-0.0802, -0.0251,  0.0919,  ...,  0.0486, -0.3719,  0.0452],\n",
      "         [-0.0348,  0.1832,  0.1307,  ...,  0.0033, -0.4195, -0.1311]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.1417,  0.0703,  0.0343,  ..., -0.5549,  0.3175, -0.0847],\n",
      "         [-0.1504, -0.2764, -0.0786,  ...,  0.1901, -0.3811, -0.2030],\n",
      "         [-0.1133, -0.3610, -0.1422,  ..., -0.0869, -0.0319, -0.2676],\n",
      "         ...,\n",
      "         [-0.1129,  0.0685,  0.1489,  ...,  0.0476, -0.1903,  0.0164],\n",
      "         [-0.0470, -0.0211,  0.1001,  ...,  0.0508, -0.4457, -0.0621],\n",
      "         [-0.0674, -0.0013,  0.1045,  ...,  0.0318, -0.4461, -0.3394]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.1417,  0.0703,  0.0343,  ..., -0.5549,  0.3175, -0.0847],\n",
      "         [-0.1504, -0.2764, -0.0786,  ...,  0.1901, -0.3811, -0.2030],\n",
      "         [-0.1133, -0.3610, -0.1422,  ..., -0.0869, -0.0319, -0.2676],\n",
      "         ...,\n",
      "         [-0.1129,  0.0685,  0.1489,  ...,  0.0476, -0.1903,  0.0164],\n",
      "         [-0.0470, -0.0211,  0.1001,  ...,  0.0508, -0.4457, -0.0621],\n",
      "         [-0.0674, -0.0013,  0.1045,  ...,  0.0318, -0.4461, -0.3394]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.1528,  0.0725,  0.0609,  ..., -0.5624,  0.3103, -0.0293],\n",
      "         [-0.1715, -0.2198, -0.0147,  ...,  0.1226, -0.5031, -0.2505],\n",
      "         [-0.0297, -0.4007, -0.1636,  ..., -0.1107, -0.0576, -0.2133],\n",
      "         ...,\n",
      "         [ 0.0877,  0.2524,  0.2289,  ...,  0.1791, -0.1082, -0.1452],\n",
      "         [-0.0878, -0.0637,  0.0740,  ...,  0.1020, -0.4183,  0.0144],\n",
      "         [ 0.0578,  0.0092,  0.1553,  ...,  0.0661, -0.2942, -0.3541]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.1528,  0.0725,  0.0609,  ..., -0.5624,  0.3103, -0.0293],\n",
      "         [-0.1715, -0.2198, -0.0147,  ...,  0.1226, -0.5031, -0.2505],\n",
      "         [-0.0297, -0.4007, -0.1636,  ..., -0.1107, -0.0576, -0.2133],\n",
      "         ...,\n",
      "         [ 0.0877,  0.2524,  0.2289,  ...,  0.1791, -0.1082, -0.1452],\n",
      "         [-0.0878, -0.0637,  0.0740,  ...,  0.1020, -0.4183,  0.0144],\n",
      "         [ 0.0578,  0.0092,  0.1553,  ...,  0.0661, -0.2942, -0.3541]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.1559,  0.0520,  0.0566,  ..., -0.5805,  0.2614, -0.0032],\n",
      "         [-0.1972, -0.3388, -0.1918,  ...,  0.0797, -0.5531, -0.2898],\n",
      "         [-0.0768, -0.4569, -0.1050,  ..., -0.1556, -0.1413, -0.1358],\n",
      "         ...,\n",
      "         [-0.0178,  0.1837,  0.1001,  ...,  0.1996, -0.1519, -0.2170],\n",
      "         [-0.1240, -0.1834, -0.0174,  ...,  0.1203, -0.3372,  0.1113],\n",
      "         [ 0.1252, -0.0314,  0.0120,  ...,  0.0854, -0.2213, -0.2349]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.1559,  0.0520,  0.0566,  ..., -0.5805,  0.2614, -0.0032],\n",
      "         [-0.1972, -0.3388, -0.1918,  ...,  0.0797, -0.5531, -0.2898],\n",
      "         [-0.0768, -0.4569, -0.1050,  ..., -0.1556, -0.1413, -0.1358],\n",
      "         ...,\n",
      "         [-0.0178,  0.1837,  0.1001,  ...,  0.1996, -0.1519, -0.2170],\n",
      "         [-0.1240, -0.1834, -0.0174,  ...,  0.1203, -0.3372,  0.1113],\n",
      "         [ 0.1252, -0.0314,  0.0120,  ...,  0.0854, -0.2213, -0.2349]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.1488,  0.0445,  0.0765,  ..., -0.5456,  0.2702,  0.0098],\n",
      "         [-0.2125, -0.3813, -0.2464,  ...,  0.1303, -0.4445, -0.5869],\n",
      "         [-0.1573, -0.4866,  0.0692,  ..., -0.0525, -0.3591, -0.2077],\n",
      "         ...,\n",
      "         [ 0.1848,  0.1806,  0.0028,  ...,  0.2271, -0.2315, -0.2585],\n",
      "         [-0.0843, -0.1180, -0.0448,  ...,  0.1143, -0.3616,  0.1082],\n",
      "         [ 0.1327, -0.0153, -0.0181,  ..., -0.0369, -0.2191, -0.1908]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.1488,  0.0445,  0.0765,  ..., -0.5456,  0.2702,  0.0098],\n",
      "         [-0.2125, -0.3813, -0.2464,  ...,  0.1303, -0.4445, -0.5869],\n",
      "         [-0.1573, -0.4866,  0.0692,  ..., -0.0525, -0.3591, -0.2077],\n",
      "         ...,\n",
      "         [ 0.1848,  0.1806,  0.0028,  ...,  0.2271, -0.2315, -0.2585],\n",
      "         [-0.0843, -0.1180, -0.0448,  ...,  0.1143, -0.3616,  0.1082],\n",
      "         [ 0.1327, -0.0153, -0.0181,  ..., -0.0369, -0.2191, -0.1908]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.1268,  0.0333,  0.0935,  ..., -0.5320,  0.2795,  0.0006],\n",
      "         [-0.3471, -0.2620, -0.2107,  ...,  0.1913, -0.5602, -0.5741],\n",
      "         [-0.2966, -0.3988,  0.1355,  ..., -0.1367, -0.4881, -0.1139],\n",
      "         ...,\n",
      "         [ 0.1220,  0.1777,  0.1064,  ...,  0.2834, -0.2272, -0.4442],\n",
      "         [-0.1208,  0.0034, -0.0536,  ...,  0.0201, -0.2928,  0.1051],\n",
      "         [ 0.1721,  0.0389, -0.0059,  ..., -0.0293, -0.1293, -0.1709]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.1268,  0.0333,  0.0935,  ..., -0.5320,  0.2795,  0.0006],\n",
      "         [-0.3471, -0.2620, -0.2107,  ...,  0.1913, -0.5602, -0.5741],\n",
      "         [-0.2966, -0.3988,  0.1355,  ..., -0.1367, -0.4881, -0.1139],\n",
      "         ...,\n",
      "         [ 0.1220,  0.1777,  0.1064,  ...,  0.2834, -0.2272, -0.4442],\n",
      "         [-0.1208,  0.0034, -0.0536,  ...,  0.0201, -0.2928,  0.1051],\n",
      "         [ 0.1721,  0.0389, -0.0059,  ..., -0.0293, -0.1293, -0.1709]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.1355,  0.0227,  0.0554,  ..., -0.5473,  0.2470, -0.0229],\n",
      "         [-0.5027, -0.4278, -0.2989,  ...,  0.1159, -0.5893, -0.3820],\n",
      "         [-0.3019, -0.4918,  0.0238,  ...,  0.0192, -0.5307, -0.0877],\n",
      "         ...,\n",
      "         [ 0.4120,  0.0139, -0.1348,  ...,  0.1202, -0.1283, -0.5199],\n",
      "         [ 0.0042,  0.0192,  0.0646,  ..., -0.0123, -0.3195,  0.0237],\n",
      "         [ 0.2628,  0.0211,  0.0984,  ..., -0.0291, -0.1955, -0.1852]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.1355,  0.0227,  0.0554,  ..., -0.5473,  0.2470, -0.0229],\n",
      "         [-0.5027, -0.4278, -0.2989,  ...,  0.1159, -0.5893, -0.3820],\n",
      "         [-0.3019, -0.4918,  0.0238,  ...,  0.0192, -0.5307, -0.0877],\n",
      "         ...,\n",
      "         [ 0.4120,  0.0139, -0.1348,  ...,  0.1202, -0.1283, -0.5199],\n",
      "         [ 0.0042,  0.0192,  0.0646,  ..., -0.0123, -0.3195,  0.0237],\n",
      "         [ 0.2628,  0.0211,  0.0984,  ..., -0.0291, -0.1955, -0.1852]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.1350,  0.0163,  0.0576,  ..., -0.5555,  0.2435,  0.0051],\n",
      "         [-0.4906, -0.4103, -0.2673,  ...,  0.1708, -0.5807, -0.2392],\n",
      "         [-0.3791, -0.5204, -0.0014,  ...,  0.0650, -0.5631, -0.0586],\n",
      "         ...,\n",
      "         [ 0.5365, -0.0962,  0.0901,  ...,  0.2300,  0.0020, -0.5400],\n",
      "         [-0.0476, -0.0289,  0.2678,  ...,  0.0228, -0.2942,  0.1447],\n",
      "         [ 0.3820, -0.1197,  0.1299,  ..., -0.0722, -0.1914, -0.2285]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.1350,  0.0163,  0.0576,  ..., -0.5555,  0.2435,  0.0051],\n",
      "         [-0.4906, -0.4103, -0.2673,  ...,  0.1708, -0.5807, -0.2392],\n",
      "         [-0.3791, -0.5204, -0.0014,  ...,  0.0650, -0.5631, -0.0586],\n",
      "         ...,\n",
      "         [ 0.5365, -0.0962,  0.0901,  ...,  0.2300,  0.0020, -0.5400],\n",
      "         [-0.0476, -0.0289,  0.2678,  ...,  0.0228, -0.2942,  0.1447],\n",
      "         [ 0.3820, -0.1197,  0.1299,  ..., -0.0722, -0.1914, -0.2285]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.1301, -0.0029,  0.0279,  ..., -0.5537,  0.2277,  0.0164],\n",
      "         [-0.5277, -0.4016, -0.0717,  ...,  0.1516, -0.5321, -0.2409],\n",
      "         [-0.4737, -0.5889,  0.0584,  ...,  0.0443, -0.4363,  0.1322],\n",
      "         ...,\n",
      "         [ 0.5056, -0.1838,  0.2011,  ...,  0.3396,  0.1863, -0.3331],\n",
      "         [-0.1836, -0.0616,  0.3835,  ...,  0.1417, -0.2019,  0.0746],\n",
      "         [ 0.2295, -0.1768,  0.2425,  ..., -0.0909, -0.2339, -0.3109]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.1301, -0.0029,  0.0279,  ..., -0.5537,  0.2277,  0.0164],\n",
      "         [-0.5277, -0.4016, -0.0717,  ...,  0.1516, -0.5321, -0.2409],\n",
      "         [-0.4737, -0.5889,  0.0584,  ...,  0.0443, -0.4363,  0.1322],\n",
      "         ...,\n",
      "         [ 0.5056, -0.1838,  0.2011,  ...,  0.3396,  0.1863, -0.3331],\n",
      "         [-0.1836, -0.0616,  0.3835,  ...,  0.1417, -0.2019,  0.0746],\n",
      "         [ 0.2295, -0.1768,  0.2425,  ..., -0.0909, -0.2339, -0.3109]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.1219,  0.0061, -0.0021,  ..., -0.5922,  0.2788, -0.0020],\n",
      "         [-0.5408, -0.4615,  0.0152,  ...,  0.3255, -0.5326, -0.3436],\n",
      "         [-0.5072, -0.6243,  0.0346,  ...,  0.1065, -0.3910,  0.2741],\n",
      "         ...,\n",
      "         [ 0.7631, -0.2391,  0.4509,  ...,  0.2416,  0.1324, -0.5070],\n",
      "         [-0.3815, -0.0699,  0.5202,  ...,  0.1561, -0.0260,  0.3107],\n",
      "         [ 0.1656, -0.3386,  0.1704,  ...,  0.0519, -0.2349, -0.2977]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.1219,  0.0061, -0.0021,  ..., -0.5922,  0.2788, -0.0020],\n",
      "         [-0.5408, -0.4615,  0.0152,  ...,  0.3255, -0.5326, -0.3436],\n",
      "         [-0.5072, -0.6243,  0.0346,  ...,  0.1065, -0.3910,  0.2741],\n",
      "         ...,\n",
      "         [ 0.7631, -0.2391,  0.4509,  ...,  0.2416,  0.1324, -0.5070],\n",
      "         [-0.3815, -0.0699,  0.5202,  ...,  0.1561, -0.0260,  0.3107],\n",
      "         [ 0.1656, -0.3386,  0.1704,  ...,  0.0519, -0.2349, -0.2977]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.1067,  0.0272,  0.0495,  ..., -0.5550,  0.3449,  0.0266],\n",
      "         [-0.6717, -0.5494,  0.0339,  ...,  0.1675, -0.6339, -0.1055],\n",
      "         [-0.5163, -0.7544, -0.1413,  ...,  0.0033, -0.4445,  0.1592],\n",
      "         ...,\n",
      "         [ 0.6587, -0.2612,  0.2352,  ...,  0.3306,  0.1203, -0.4325],\n",
      "         [-0.4442, -0.0922,  0.6424,  ...,  0.2302, -0.1715,  0.4069],\n",
      "         [ 0.2273, -0.4461,  0.3100,  ...,  0.0989, -0.5639, -0.3341]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.1067,  0.0272,  0.0495,  ..., -0.5550,  0.3449,  0.0266],\n",
      "         [-0.6717, -0.5494,  0.0339,  ...,  0.1675, -0.6339, -0.1055],\n",
      "         [-0.5163, -0.7544, -0.1413,  ...,  0.0033, -0.4445,  0.1592],\n",
      "         ...,\n",
      "         [ 0.6587, -0.2612,  0.2352,  ...,  0.3306,  0.1203, -0.4325],\n",
      "         [-0.4442, -0.0922,  0.6424,  ...,  0.2302, -0.1715,  0.4069],\n",
      "         [ 0.2273, -0.4461,  0.3100,  ...,  0.0989, -0.5639, -0.3341]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.0592,  0.0731,  0.0500,  ..., -0.5517,  0.3541,  0.0595],\n",
      "         [-0.8301, -0.4414, -0.0880,  ...,  0.0550, -0.4741,  0.0217],\n",
      "         [-0.8419, -0.9289, -0.1285,  ..., -0.1555, -0.4876,  0.3898],\n",
      "         ...,\n",
      "         [ 0.1432, -0.8172,  0.4889,  ...,  0.2828, -0.1158, -0.5970],\n",
      "         [-0.5687, -0.4080,  0.8408,  ...,  0.0900, -0.0764,  0.2957],\n",
      "         [ 0.1071, -0.5676,  0.4112,  ..., -0.1923, -0.7012, -0.2595]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.0592,  0.0731,  0.0500,  ..., -0.5517,  0.3541,  0.0595],\n",
      "         [-0.8301, -0.4414, -0.0880,  ...,  0.0550, -0.4741,  0.0217],\n",
      "         [-0.8419, -0.9289, -0.1285,  ..., -0.1555, -0.4876,  0.3898],\n",
      "         ...,\n",
      "         [ 0.1432, -0.8172,  0.4889,  ...,  0.2828, -0.1158, -0.5970],\n",
      "         [-0.5687, -0.4080,  0.8408,  ...,  0.0900, -0.0764,  0.2957],\n",
      "         [ 0.1071, -0.5676,  0.4112,  ..., -0.1923, -0.7012, -0.2595]]],\n",
      "       device='cuda:0'),) and output (tensor([[[ 0.1143,  0.2237,  0.3292,  ..., -0.7706,  0.2608,  0.0269],\n",
      "         [-0.5409, -0.7010, -0.0964,  ..., -0.0698, -0.6366,  0.0020],\n",
      "         [-0.7788, -1.0169, -0.2463,  ...,  0.0380, -0.6460,  0.5000],\n",
      "         ...,\n",
      "         [ 0.3825, -1.0852,  0.1718,  ...,  0.0910, -0.4188, -0.8713],\n",
      "         [-0.4098, -0.5284,  1.1383,  ..., -0.0186, -0.2136, -0.0304],\n",
      "         [ 0.2119, -0.4493,  0.4753,  ..., -0.1174, -0.8514, -0.4504]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[ 0.1143,  0.2237,  0.3292,  ..., -0.7706,  0.2608,  0.0269],\n",
      "         [-0.5409, -0.7010, -0.0964,  ..., -0.0698, -0.6366,  0.0020],\n",
      "         [-0.7788, -1.0169, -0.2463,  ...,  0.0380, -0.6460,  0.5000],\n",
      "         ...,\n",
      "         [ 0.3825, -1.0852,  0.1718,  ...,  0.0910, -0.4188, -0.8713],\n",
      "         [-0.4098, -0.5284,  1.1383,  ..., -0.0186, -0.2136, -0.0304],\n",
      "         [ 0.2119, -0.4493,  0.4753,  ..., -0.1174, -0.8514, -0.4504]]],\n",
      "       device='cuda:0'),) and output (tensor([[[ 0.3711,  0.5489,  0.6100,  ..., -0.7499,  0.2080,  0.4195],\n",
      "         [-0.4239, -1.3264, -0.2474,  ...,  0.0211, -0.7978,  0.0676],\n",
      "         [-0.8252, -1.4197, -0.3725,  ..., -0.1234, -0.8771,  0.3855],\n",
      "         ...,\n",
      "         [ 0.2757, -1.5254, -0.3194,  ..., -0.1681, -0.5480, -1.0100],\n",
      "         [-0.6137, -0.6608,  0.7971,  ...,  0.4362, -0.3713,  0.1416],\n",
      "         [ 0.0543, -1.0728,  0.1864,  ..., -0.2423, -0.9476, -0.7477]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[ 0.3711,  0.5489,  0.6100,  ..., -0.7499,  0.2080,  0.4195],\n",
      "         [-0.4239, -1.3264, -0.2474,  ...,  0.0211, -0.7978,  0.0676],\n",
      "         [-0.8252, -1.4197, -0.3725,  ..., -0.1234, -0.8771,  0.3855],\n",
      "         ...,\n",
      "         [ 0.2757, -1.5254, -0.3194,  ..., -0.1681, -0.5480, -1.0100],\n",
      "         [-0.6137, -0.6608,  0.7971,  ...,  0.4362, -0.3713,  0.1416],\n",
      "         [ 0.0543, -1.0728,  0.1864,  ..., -0.2423, -0.9476, -0.7477]]],\n",
      "       device='cuda:0'),) and output (tensor([[[ 1.1544,  3.2965,  1.6534,  ..., -2.7009,  2.6553,  2.2700],\n",
      "         [ 0.1638, -0.9234,  0.3327,  ...,  0.1979, -1.0411,  0.4373],\n",
      "         [-0.7962, -0.2024, -1.0150,  ...,  0.1548, -1.6453,  1.4704],\n",
      "         ...,\n",
      "         [-0.3516, -2.1624,  0.3137,  ...,  0.2842,  0.7741, -0.8494],\n",
      "         [ 0.3851, -1.1222,  0.1499,  ...,  0.6736, -0.7477,  0.7669],\n",
      "         [ 0.0333, -1.7609,  0.0036,  ...,  0.1767, -1.2385, -0.1783]]],\n",
      "       device='cuda:0'),)\n",
      "[Debug] Layer names being passed: ['model.layers.0', 'model.layers.1', 'model.layers.2', 'model.layers.3', 'model.layers.4', 'model.layers.5', 'model.layers.6', 'model.layers.7', 'model.layers.8', 'model.layers.9', 'model.layers.10', 'model.layers.11', 'model.layers.12', 'model.layers.13', 'model.layers.14', 'model.layers.15', 'model.layers.16', 'model.layers.17', 'model.layers.18', 'model.layers.19', 'model.layers.20', 'model.layers.21', 'model.layers.22', 'model.layers.23', 'model.layers.24', 'model.layers.25', 'model.layers.26', 'model.layers.27', 'model.layers.28', 'model.layers.29', 'model.layers.30', 'model.layers.31', 'model.embed_tokens']\n",
      "[Debug] Trying to access layer: model.layers.0\n",
      "[Debug] Successfully found layer: model.layers.0\n",
      "[Debug] Trying to access layer: model.layers.1\n",
      "[Debug] Successfully found layer: model.layers.1\n",
      "[Debug] Trying to access layer: model.layers.2\n",
      "[Debug] Successfully found layer: model.layers.2\n",
      "[Debug] Trying to access layer: model.layers.3\n",
      "[Debug] Successfully found layer: model.layers.3\n",
      "[Debug] Trying to access layer: model.layers.4\n",
      "[Debug] Successfully found layer: model.layers.4\n",
      "[Debug] Trying to access layer: model.layers.5\n",
      "[Debug] Successfully found layer: model.layers.5\n",
      "[Debug] Trying to access layer: model.layers.6\n",
      "[Debug] Successfully found layer: model.layers.6\n",
      "[Debug] Trying to access layer: model.layers.7\n",
      "[Debug] Successfully found layer: model.layers.7\n",
      "[Debug] Trying to access layer: model.layers.8\n",
      "[Debug] Successfully found layer: model.layers.8\n",
      "[Debug] Trying to access layer: model.layers.9\n",
      "[Debug] Successfully found layer: model.layers.9\n",
      "[Debug] Trying to access layer: model.layers.10\n",
      "[Debug] Successfully found layer: model.layers.10\n",
      "[Debug] Trying to access layer: model.layers.11\n",
      "[Debug] Successfully found layer: model.layers.11\n",
      "[Debug] Trying to access layer: model.layers.12\n",
      "[Debug] Successfully found layer: model.layers.12\n",
      "[Debug] Trying to access layer: model.layers.13\n",
      "[Debug] Successfully found layer: model.layers.13\n",
      "[Debug] Trying to access layer: model.layers.14\n",
      "[Debug] Successfully found layer: model.layers.14\n",
      "[Debug] Trying to access layer: model.layers.15\n",
      "[Debug] Successfully found layer: model.layers.15\n",
      "[Debug] Trying to access layer: model.layers.16\n",
      "[Debug] Successfully found layer: model.layers.16\n",
      "[Debug] Trying to access layer: model.layers.17\n",
      "[Debug] Successfully found layer: model.layers.17\n",
      "[Debug] Trying to access layer: model.layers.18\n",
      "[Debug] Successfully found layer: model.layers.18\n",
      "[Debug] Trying to access layer: model.layers.19\n",
      "[Debug] Successfully found layer: model.layers.19\n",
      "[Debug] Trying to access layer: model.layers.20\n",
      "[Debug] Successfully found layer: model.layers.20\n",
      "[Debug] Trying to access layer: model.layers.21\n",
      "[Debug] Successfully found layer: model.layers.21\n",
      "[Debug] Trying to access layer: model.layers.22\n",
      "[Debug] Successfully found layer: model.layers.22\n",
      "[Debug] Trying to access layer: model.layers.23\n",
      "[Debug] Successfully found layer: model.layers.23\n",
      "[Debug] Trying to access layer: model.layers.24\n",
      "[Debug] Successfully found layer: model.layers.24\n",
      "[Debug] Trying to access layer: model.layers.25\n",
      "[Debug] Successfully found layer: model.layers.25\n",
      "[Debug] Trying to access layer: model.layers.26\n",
      "[Debug] Successfully found layer: model.layers.26\n",
      "[Debug] Trying to access layer: model.layers.27\n",
      "[Debug] Successfully found layer: model.layers.27\n",
      "[Debug] Trying to access layer: model.layers.28\n",
      "[Debug] Successfully found layer: model.layers.28\n",
      "[Debug] Trying to access layer: model.layers.29\n",
      "[Debug] Successfully found layer: model.layers.29\n",
      "[Debug] Trying to access layer: model.layers.30\n",
      "[Debug] Successfully found layer: model.layers.30\n",
      "[Debug] Trying to access layer: model.layers.31\n",
      "[Debug] Successfully found layer: model.layers.31\n",
      "[Debug] Trying to access layer: model.embed_tokens\n",
      "[Debug] Successfully found layer: model.embed_tokens\n",
      "[Hook] Layer Embedding(128256, 4096) received input (tensor([[128000,  25499,    315,    279,  10734,    315,   2418,    323,    315,\n",
      "            279,  47317,    578,  19476,    304,    279,  42021,   2586,    505,\n",
      "            279,  41903,    323,   5054,  22006,    315,    279,  92931,     11,\n",
      "           1778,    439,   3927,   2191,     11,    279,   3674,   5226,    439,\n",
      "          46820,   1534,    555,    279,  24983,  16023,  55475,  98989,   2933,\n",
      "             11,    323,    279,  25768,    315,  13736,  16948,  37588,    555,\n",
      "            279,  54007,    409,   9995,    288,    447,  26235,     13,   1666,\n",
      "            649,    387,   3970,    304,    279,  22755,     11,    279,   8753,\n",
      "          18489,    574,  17345,  28160,    555,    279,   5054,  19675,    315,\n",
      "            279,  92931,    323,  16565,    315,   3823,   3268,    439,    574,\n",
      "            279,    549,    815,     13,  42021,    315,  44177,    902,  53580,\n",
      "            433,    320,     19,   5887,    220,  11242,     21,    570]],\n",
      "       device='cuda:0'),) and output tensor([[[ 2.6512e-04, -4.9973e-04, -5.8365e-04,  ...,  3.8147e-03,\n",
      "           6.3419e-05,  1.1902e-03],\n",
      "         [ 1.3916e-02, -1.5137e-02,  3.1433e-03,  ...,  1.6357e-02,\n",
      "          -7.7820e-03, -2.7161e-03],\n",
      "         [ 1.2817e-03,  9.1171e-04,  2.0905e-03,  ...,  1.6251e-03,\n",
      "           4.0894e-03, -4.0283e-03],\n",
      "         ...,\n",
      "         [-2.0752e-02,  2.0905e-03, -1.8082e-03,  ..., -6.5002e-03,\n",
      "           6.7139e-03,  1.9836e-03],\n",
      "         [ 4.9133e-03,  4.5776e-03,  1.6251e-03,  ...,  1.2573e-02,\n",
      "           1.4343e-03,  2.9755e-03],\n",
      "         [-6.6528e-03,  7.6294e-04,  2.1973e-03,  ...,  1.2360e-03,\n",
      "           6.8359e-03,  5.3406e-03]]], device='cuda:0')\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[ 2.6512e-04, -4.9973e-04, -5.8365e-04,  ...,  3.8147e-03,\n",
      "           6.3419e-05,  1.1902e-03],\n",
      "         [ 1.3916e-02, -1.5137e-02,  3.1433e-03,  ...,  1.6357e-02,\n",
      "          -7.7820e-03, -2.7161e-03],\n",
      "         [ 1.2817e-03,  9.1171e-04,  2.0905e-03,  ...,  1.6251e-03,\n",
      "           4.0894e-03, -4.0283e-03],\n",
      "         ...,\n",
      "         [-2.0752e-02,  2.0905e-03, -1.8082e-03,  ..., -6.5002e-03,\n",
      "           6.7139e-03,  1.9836e-03],\n",
      "         [ 4.9133e-03,  4.5776e-03,  1.6251e-03,  ...,  1.2573e-02,\n",
      "           1.4343e-03,  2.9755e-03],\n",
      "         [-6.6528e-03,  7.6294e-04,  2.1973e-03,  ...,  1.2360e-03,\n",
      "           6.8359e-03,  5.3406e-03]]], device='cuda:0'),) and output (tensor([[[-4.3565e-04,  1.4559e-02, -2.0488e-03,  ...,  6.5782e-03,\n",
      "          -1.9362e-02,  1.9588e-03],\n",
      "         [ 2.0080e-02, -2.0874e-02, -7.2702e-03,  ..., -1.6714e-03,\n",
      "          -1.7058e-02,  1.2370e-03],\n",
      "         [-1.6641e-02,  1.1307e-02, -9.9433e-03,  ..., -3.2762e-02,\n",
      "          -2.3200e-05,  7.8577e-04],\n",
      "         ...,\n",
      "         [-3.7635e-02,  4.0065e-04,  7.7389e-03,  ..., -5.3110e-02,\n",
      "           3.4359e-03, -1.0055e-03],\n",
      "         [-7.0611e-03, -1.5645e-02,  2.1334e-02,  ...,  1.2443e-02,\n",
      "          -1.1090e-02,  6.6154e-03],\n",
      "         [ 2.8335e-04, -8.9640e-04,  5.1048e-03,  ..., -1.0682e-03,\n",
      "           1.2185e-02,  1.0806e-03]]], device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-4.3565e-04,  1.4559e-02, -2.0488e-03,  ...,  6.5782e-03,\n",
      "          -1.9362e-02,  1.9588e-03],\n",
      "         [ 2.0080e-02, -2.0874e-02, -7.2702e-03,  ..., -1.6714e-03,\n",
      "          -1.7058e-02,  1.2370e-03],\n",
      "         [-1.6641e-02,  1.1307e-02, -9.9433e-03,  ..., -3.2762e-02,\n",
      "          -2.3200e-05,  7.8577e-04],\n",
      "         ...,\n",
      "         [-3.7635e-02,  4.0065e-04,  7.7389e-03,  ..., -5.3110e-02,\n",
      "           3.4359e-03, -1.0055e-03],\n",
      "         [-7.0611e-03, -1.5645e-02,  2.1334e-02,  ...,  1.2443e-02,\n",
      "          -1.1090e-02,  6.6154e-03],\n",
      "         [ 2.8335e-04, -8.9640e-04,  5.1048e-03,  ..., -1.0682e-03,\n",
      "           1.2185e-02,  1.0806e-03]]], device='cuda:0'),) and output (tensor([[[-0.0972,  0.0787, -0.0419,  ...,  0.0071,  0.0869,  0.0218],\n",
      "         [ 0.0190, -0.0348, -0.0227,  ..., -0.0227, -0.0231,  0.0154],\n",
      "         [-0.0193, -0.0130,  0.0005,  ..., -0.0630,  0.0249,  0.0214],\n",
      "         ...,\n",
      "         [-0.0627,  0.0156,  0.0160,  ..., -0.0976,  0.0014,  0.0001],\n",
      "         [-0.0064, -0.0299,  0.0616,  ..., -0.0123, -0.0087, -0.0087],\n",
      "         [ 0.0009, -0.0070,  0.0109,  ..., -0.0117, -0.0015,  0.0011]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.0972,  0.0787, -0.0419,  ...,  0.0071,  0.0869,  0.0218],\n",
      "         [ 0.0190, -0.0348, -0.0227,  ..., -0.0227, -0.0231,  0.0154],\n",
      "         [-0.0193, -0.0130,  0.0005,  ..., -0.0630,  0.0249,  0.0214],\n",
      "         ...,\n",
      "         [-0.0627,  0.0156,  0.0160,  ..., -0.0976,  0.0014,  0.0001],\n",
      "         [-0.0064, -0.0299,  0.0616,  ..., -0.0123, -0.0087, -0.0087],\n",
      "         [ 0.0009, -0.0070,  0.0109,  ..., -0.0117, -0.0015,  0.0011]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.1040,  0.0875, -0.0360,  ...,  0.0205,  0.0997,  0.0184],\n",
      "         [-0.0007, -0.0261,  0.0065,  ..., -0.0002, -0.0390, -0.0185],\n",
      "         [-0.0405,  0.0160,  0.0115,  ...,  0.0024, -0.0116,  0.0071],\n",
      "         ...,\n",
      "         [-0.0346,  0.0161,  0.0253,  ..., -0.1181,  0.0034,  0.0074],\n",
      "         [-0.0125, -0.0433,  0.0203,  ..., -0.0145, -0.0196, -0.0138],\n",
      "         [-0.0152,  0.0072,  0.0120,  ...,  0.0271, -0.0176,  0.0095]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.1040,  0.0875, -0.0360,  ...,  0.0205,  0.0997,  0.0184],\n",
      "         [-0.0007, -0.0261,  0.0065,  ..., -0.0002, -0.0390, -0.0185],\n",
      "         [-0.0405,  0.0160,  0.0115,  ...,  0.0024, -0.0116,  0.0071],\n",
      "         ...,\n",
      "         [-0.0346,  0.0161,  0.0253,  ..., -0.1181,  0.0034,  0.0074],\n",
      "         [-0.0125, -0.0433,  0.0203,  ..., -0.0145, -0.0196, -0.0138],\n",
      "         [-0.0152,  0.0072,  0.0120,  ...,  0.0271, -0.0176,  0.0095]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.0870,  0.1049, -0.0173,  ...,  0.0700,  0.0989,  0.0181],\n",
      "         [-0.0200,  0.0149, -0.0291,  ..., -0.0178,  0.0031, -0.0881],\n",
      "         [-0.0191,  0.0022,  0.0157,  ..., -0.0342,  0.0961, -0.0555],\n",
      "         ...,\n",
      "         [-0.0377,  0.0188,  0.0232,  ..., -0.1480,  0.0268,  0.0008],\n",
      "         [-0.0128, -0.0560,  0.0116,  ..., -0.0223,  0.0140, -0.0289],\n",
      "         [-0.0195, -0.0164, -0.0077,  ...,  0.0270, -0.0192,  0.0273]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.0870,  0.1049, -0.0173,  ...,  0.0700,  0.0989,  0.0181],\n",
      "         [-0.0200,  0.0149, -0.0291,  ..., -0.0178,  0.0031, -0.0881],\n",
      "         [-0.0191,  0.0022,  0.0157,  ..., -0.0342,  0.0961, -0.0555],\n",
      "         ...,\n",
      "         [-0.0377,  0.0188,  0.0232,  ..., -0.1480,  0.0268,  0.0008],\n",
      "         [-0.0128, -0.0560,  0.0116,  ..., -0.0223,  0.0140, -0.0289],\n",
      "         [-0.0195, -0.0164, -0.0077,  ...,  0.0270, -0.0192,  0.0273]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.1113,  0.1022, -0.0102,  ...,  0.0436,  0.1283,  0.0321],\n",
      "         [-0.0203,  0.0414,  0.0132,  ...,  0.0345,  0.0262, -0.0871],\n",
      "         [ 0.0136,  0.0457,  0.0448,  ..., -0.0636,  0.0560, -0.0869],\n",
      "         ...,\n",
      "         [-0.0988,  0.0314,  0.0750,  ..., -0.1853,  0.0493,  0.0205],\n",
      "         [-0.0262, -0.0942,  0.0155,  ..., -0.0582,  0.0540, -0.0194],\n",
      "         [ 0.0019, -0.0393,  0.0269,  ...,  0.0449, -0.0320, -0.0069]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.1113,  0.1022, -0.0102,  ...,  0.0436,  0.1283,  0.0321],\n",
      "         [-0.0203,  0.0414,  0.0132,  ...,  0.0345,  0.0262, -0.0871],\n",
      "         [ 0.0136,  0.0457,  0.0448,  ..., -0.0636,  0.0560, -0.0869],\n",
      "         ...,\n",
      "         [-0.0988,  0.0314,  0.0750,  ..., -0.1853,  0.0493,  0.0205],\n",
      "         [-0.0262, -0.0942,  0.0155,  ..., -0.0582,  0.0540, -0.0194],\n",
      "         [ 0.0019, -0.0393,  0.0269,  ...,  0.0449, -0.0320, -0.0069]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.1034,  0.1132,  0.0096,  ...,  0.0453,  0.1477,  0.0301],\n",
      "         [ 0.0018,  0.0963, -0.0087,  ..., -0.0219,  0.0386, -0.0917],\n",
      "         [-0.0014,  0.0802, -0.0016,  ..., -0.1288,  0.0735, -0.1082],\n",
      "         ...,\n",
      "         [-0.0527,  0.0211,  0.0399,  ..., -0.1752,  0.0428, -0.0703],\n",
      "         [ 0.0322, -0.0509,  0.0176,  ..., -0.1229,  0.0040, -0.0938],\n",
      "         [-0.0291, -0.0132,  0.0250,  ...,  0.0431, -0.0956, -0.0009]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.1034,  0.1132,  0.0096,  ...,  0.0453,  0.1477,  0.0301],\n",
      "         [ 0.0018,  0.0963, -0.0087,  ..., -0.0219,  0.0386, -0.0917],\n",
      "         [-0.0014,  0.0802, -0.0016,  ..., -0.1288,  0.0735, -0.1082],\n",
      "         ...,\n",
      "         [-0.0527,  0.0211,  0.0399,  ..., -0.1752,  0.0428, -0.0703],\n",
      "         [ 0.0322, -0.0509,  0.0176,  ..., -0.1229,  0.0040, -0.0938],\n",
      "         [-0.0291, -0.0132,  0.0250,  ...,  0.0431, -0.0956, -0.0009]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.1071,  0.1253,  0.0211,  ...,  0.0126,  0.1586,  0.0278],\n",
      "         [-0.0356,  0.1078, -0.0125,  ..., -0.0385, -0.0003, -0.0981],\n",
      "         [-0.0932,  0.1158,  0.0248,  ..., -0.1205,  0.0639, -0.1059],\n",
      "         ...,\n",
      "         [-0.0230,  0.0413,  0.0588,  ..., -0.1882, -0.0348, -0.0018],\n",
      "         [ 0.0974, -0.0549, -0.0201,  ..., -0.0201, -0.0545, -0.0523],\n",
      "         [ 0.0055, -0.0366,  0.1016,  ...,  0.0653, -0.1153,  0.0107]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.1071,  0.1253,  0.0211,  ...,  0.0126,  0.1586,  0.0278],\n",
      "         [-0.0356,  0.1078, -0.0125,  ..., -0.0385, -0.0003, -0.0981],\n",
      "         [-0.0932,  0.1158,  0.0248,  ..., -0.1205,  0.0639, -0.1059],\n",
      "         ...,\n",
      "         [-0.0230,  0.0413,  0.0588,  ..., -0.1882, -0.0348, -0.0018],\n",
      "         [ 0.0974, -0.0549, -0.0201,  ..., -0.0201, -0.0545, -0.0523],\n",
      "         [ 0.0055, -0.0366,  0.1016,  ...,  0.0653, -0.1153,  0.0107]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.0945,  0.1848,  0.0036,  ..., -0.0473,  0.1489,  0.0536],\n",
      "         [-0.0381,  0.0754, -0.0012,  ..., -0.1162, -0.0469, -0.1329],\n",
      "         [-0.1006,  0.0854,  0.0388,  ..., -0.2807,  0.0692, -0.1181],\n",
      "         ...,\n",
      "         [-0.0739, -0.0566,  0.0756,  ..., -0.0624, -0.0251,  0.0433],\n",
      "         [ 0.1117, -0.0362, -0.0920,  ...,  0.0454,  0.0461,  0.0297],\n",
      "         [-0.0434, -0.0555,  0.0728,  ...,  0.0889, -0.0879, -0.0549]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.0945,  0.1848,  0.0036,  ..., -0.0473,  0.1489,  0.0536],\n",
      "         [-0.0381,  0.0754, -0.0012,  ..., -0.1162, -0.0469, -0.1329],\n",
      "         [-0.1006,  0.0854,  0.0388,  ..., -0.2807,  0.0692, -0.1181],\n",
      "         ...,\n",
      "         [-0.0739, -0.0566,  0.0756,  ..., -0.0624, -0.0251,  0.0433],\n",
      "         [ 0.1117, -0.0362, -0.0920,  ...,  0.0454,  0.0461,  0.0297],\n",
      "         [-0.0434, -0.0555,  0.0728,  ...,  0.0889, -0.0879, -0.0549]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.1060,  0.2113,  0.0270,  ..., -0.1141,  0.1610,  0.0528],\n",
      "         [-0.0114,  0.1045,  0.0136,  ..., -0.2285, -0.0608, -0.1433],\n",
      "         [-0.0006,  0.1103,  0.0910,  ..., -0.2958,  0.0366, -0.1089],\n",
      "         ...,\n",
      "         [-0.0298, -0.0006,  0.0132,  ..., -0.0765, -0.0886,  0.0478],\n",
      "         [ 0.1744,  0.0487, -0.0855,  ..., -0.0284, -0.0278, -0.0337],\n",
      "         [ 0.0641, -0.0234,  0.0621,  ...,  0.0594,  0.0182, -0.0636]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.1060,  0.2113,  0.0270,  ..., -0.1141,  0.1610,  0.0528],\n",
      "         [-0.0114,  0.1045,  0.0136,  ..., -0.2285, -0.0608, -0.1433],\n",
      "         [-0.0006,  0.1103,  0.0910,  ..., -0.2958,  0.0366, -0.1089],\n",
      "         ...,\n",
      "         [-0.0298, -0.0006,  0.0132,  ..., -0.0765, -0.0886,  0.0478],\n",
      "         [ 0.1744,  0.0487, -0.0855,  ..., -0.0284, -0.0278, -0.0337],\n",
      "         [ 0.0641, -0.0234,  0.0621,  ...,  0.0594,  0.0182, -0.0636]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.0954,  0.2493,  0.0073,  ..., -0.1568,  0.1818,  0.0855],\n",
      "         [-0.1086,  0.0013, -0.0149,  ..., -0.1689, -0.0285, -0.1465],\n",
      "         [-0.0813,  0.0939,  0.0903,  ..., -0.2837,  0.0917, -0.0713],\n",
      "         ...,\n",
      "         [-0.0195,  0.0452,  0.0035,  ..., -0.0508,  0.0243,  0.0356],\n",
      "         [ 0.1572,  0.1073, -0.1264,  ..., -0.0567,  0.0105,  0.0120],\n",
      "         [ 0.0412, -0.0277,  0.0735,  ..., -0.0328,  0.0315, -0.0314]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.0954,  0.2493,  0.0073,  ..., -0.1568,  0.1818,  0.0855],\n",
      "         [-0.1086,  0.0013, -0.0149,  ..., -0.1689, -0.0285, -0.1465],\n",
      "         [-0.0813,  0.0939,  0.0903,  ..., -0.2837,  0.0917, -0.0713],\n",
      "         ...,\n",
      "         [-0.0195,  0.0452,  0.0035,  ..., -0.0508,  0.0243,  0.0356],\n",
      "         [ 0.1572,  0.1073, -0.1264,  ..., -0.0567,  0.0105,  0.0120],\n",
      "         [ 0.0412, -0.0277,  0.0735,  ..., -0.0328,  0.0315, -0.0314]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.0152,  0.3340,  0.0011,  ..., -0.2750,  0.1933,  0.1034],\n",
      "         [-0.0614,  0.0681, -0.0362,  ..., -0.2263, -0.0492, -0.1839],\n",
      "         [-0.0157,  0.1155,  0.0856,  ..., -0.3384,  0.0167, -0.0613],\n",
      "         ...,\n",
      "         [-0.0290,  0.0421,  0.0016,  ..., -0.0799,  0.1056,  0.0153],\n",
      "         [ 0.1021,  0.1110, -0.1150,  ..., -0.0924,  0.0588,  0.0053],\n",
      "         [ 0.0942, -0.1260,  0.0581,  ...,  0.0088, -0.0078,  0.0380]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.0152,  0.3340,  0.0011,  ..., -0.2750,  0.1933,  0.1034],\n",
      "         [-0.0614,  0.0681, -0.0362,  ..., -0.2263, -0.0492, -0.1839],\n",
      "         [-0.0157,  0.1155,  0.0856,  ..., -0.3384,  0.0167, -0.0613],\n",
      "         ...,\n",
      "         [-0.0290,  0.0421,  0.0016,  ..., -0.0799,  0.1056,  0.0153],\n",
      "         [ 0.1021,  0.1110, -0.1150,  ..., -0.0924,  0.0588,  0.0053],\n",
      "         [ 0.0942, -0.1260,  0.0581,  ...,  0.0088, -0.0078,  0.0380]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.0356,  0.3483, -0.0366,  ..., -0.2962,  0.2088,  0.1065],\n",
      "         [-0.0962,  0.0164, -0.0488,  ..., -0.3058, -0.0108, -0.2034],\n",
      "         [-0.1504,  0.1070,  0.1039,  ..., -0.3901,  0.0821, -0.1222],\n",
      "         ...,\n",
      "         [-0.0852,  0.0035, -0.0924,  ..., -0.1573,  0.1795, -0.0067],\n",
      "         [ 0.1504,  0.0761, -0.0836,  ..., -0.0329,  0.1348, -0.0785],\n",
      "         [ 0.0478, -0.1650,  0.0772,  ...,  0.0100,  0.0069,  0.0387]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.0356,  0.3483, -0.0366,  ..., -0.2962,  0.2088,  0.1065],\n",
      "         [-0.0962,  0.0164, -0.0488,  ..., -0.3058, -0.0108, -0.2034],\n",
      "         [-0.1504,  0.1070,  0.1039,  ..., -0.3901,  0.0821, -0.1222],\n",
      "         ...,\n",
      "         [-0.0852,  0.0035, -0.0924,  ..., -0.1573,  0.1795, -0.0067],\n",
      "         [ 0.1504,  0.0761, -0.0836,  ..., -0.0329,  0.1348, -0.0785],\n",
      "         [ 0.0478, -0.1650,  0.0772,  ...,  0.0100,  0.0069,  0.0387]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.0127,  0.3661, -0.0316,  ..., -0.3678,  0.1973,  0.1019],\n",
      "         [-0.1080, -0.0314,  0.0240,  ..., -0.2941, -0.0312, -0.1715],\n",
      "         [-0.1558,  0.0652,  0.1502,  ..., -0.3528, -0.0124, -0.1074],\n",
      "         ...,\n",
      "         [-0.0578, -0.0676, -0.1417,  ..., -0.0953,  0.1761, -0.1179],\n",
      "         [ 0.1134,  0.0425, -0.0129,  ...,  0.0727,  0.2035, -0.1306],\n",
      "         [ 0.0389, -0.1740,  0.0883,  ...,  0.1022, -0.0395,  0.0300]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.0127,  0.3661, -0.0316,  ..., -0.3678,  0.1973,  0.1019],\n",
      "         [-0.1080, -0.0314,  0.0240,  ..., -0.2941, -0.0312, -0.1715],\n",
      "         [-0.1558,  0.0652,  0.1502,  ..., -0.3528, -0.0124, -0.1074],\n",
      "         ...,\n",
      "         [-0.0578, -0.0676, -0.1417,  ..., -0.0953,  0.1761, -0.1179],\n",
      "         [ 0.1134,  0.0425, -0.0129,  ...,  0.0727,  0.2035, -0.1306],\n",
      "         [ 0.0389, -0.1740,  0.0883,  ...,  0.1022, -0.0395,  0.0300]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.0089,  0.3233, -0.0779,  ..., -0.4491,  0.1845,  0.0951],\n",
      "         [-0.1206, -0.0055,  0.0175,  ..., -0.2588, -0.0045, -0.1718],\n",
      "         [-0.1268,  0.0799,  0.1505,  ..., -0.3641,  0.0843, -0.0757],\n",
      "         ...,\n",
      "         [-0.0753, -0.0338, -0.0670,  ..., -0.1953,  0.1570, -0.1579],\n",
      "         [ 0.0450, -0.0232,  0.0091,  ..., -0.0343,  0.1051, -0.0739],\n",
      "         [ 0.0885, -0.0826,  0.1595,  ...,  0.0448, -0.0049,  0.1255]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.0089,  0.3233, -0.0779,  ..., -0.4491,  0.1845,  0.0951],\n",
      "         [-0.1206, -0.0055,  0.0175,  ..., -0.2588, -0.0045, -0.1718],\n",
      "         [-0.1268,  0.0799,  0.1505,  ..., -0.3641,  0.0843, -0.0757],\n",
      "         ...,\n",
      "         [-0.0753, -0.0338, -0.0670,  ..., -0.1953,  0.1570, -0.1579],\n",
      "         [ 0.0450, -0.0232,  0.0091,  ..., -0.0343,  0.1051, -0.0739],\n",
      "         [ 0.0885, -0.0826,  0.1595,  ...,  0.0448, -0.0049,  0.1255]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.0244,  0.2828, -0.0532,  ..., -0.4592,  0.2300,  0.0142],\n",
      "         [-0.1312, -0.1104, -0.0611,  ..., -0.2012, -0.0498, -0.1630],\n",
      "         [-0.0970, -0.0355,  0.1170,  ..., -0.3050,  0.1132, -0.1155],\n",
      "         ...,\n",
      "         [-0.1053, -0.0157,  0.0242,  ..., -0.1283,  0.0713, -0.1133],\n",
      "         [-0.0746,  0.0861,  0.0368,  ..., -0.0190,  0.0687,  0.0182],\n",
      "         [ 0.1232, -0.1006,  0.0860,  ...,  0.0405, -0.1479,  0.1015]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.0244,  0.2828, -0.0532,  ..., -0.4592,  0.2300,  0.0142],\n",
      "         [-0.1312, -0.1104, -0.0611,  ..., -0.2012, -0.0498, -0.1630],\n",
      "         [-0.0970, -0.0355,  0.1170,  ..., -0.3050,  0.1132, -0.1155],\n",
      "         ...,\n",
      "         [-0.1053, -0.0157,  0.0242,  ..., -0.1283,  0.0713, -0.1133],\n",
      "         [-0.0746,  0.0861,  0.0368,  ..., -0.0190,  0.0687,  0.0182],\n",
      "         [ 0.1232, -0.1006,  0.0860,  ...,  0.0405, -0.1479,  0.1015]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.0928,  0.1485,  0.0158,  ..., -0.5380,  0.2664, -0.0333],\n",
      "         [-0.1886, -0.1210, -0.1424,  ..., -0.2293, -0.0991, -0.1123],\n",
      "         [-0.1789, -0.0181, -0.0323,  ..., -0.2585,  0.0663, -0.0154],\n",
      "         ...,\n",
      "         [-0.1450, -0.1068, -0.0563,  ..., -0.1464, -0.0306, -0.0210],\n",
      "         [-0.1726, -0.0094,  0.0816,  ..., -0.0685,  0.0590, -0.0139],\n",
      "         [ 0.1884, -0.1375,  0.1039,  ...,  0.0384, -0.2351,  0.1121]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.0928,  0.1485,  0.0158,  ..., -0.5380,  0.2664, -0.0333],\n",
      "         [-0.1886, -0.1210, -0.1424,  ..., -0.2293, -0.0991, -0.1123],\n",
      "         [-0.1789, -0.0181, -0.0323,  ..., -0.2585,  0.0663, -0.0154],\n",
      "         ...,\n",
      "         [-0.1450, -0.1068, -0.0563,  ..., -0.1464, -0.0306, -0.0210],\n",
      "         [-0.1726, -0.0094,  0.0816,  ..., -0.0685,  0.0590, -0.0139],\n",
      "         [ 0.1884, -0.1375,  0.1039,  ...,  0.0384, -0.2351,  0.1121]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.1000,  0.1323,  0.0241,  ..., -0.5470,  0.2844, -0.0457],\n",
      "         [-0.1777, -0.2332, -0.0185,  ..., -0.2363, -0.1471, -0.0907],\n",
      "         [-0.1680, -0.1758,  0.1444,  ..., -0.3666, -0.0145, -0.1803],\n",
      "         ...,\n",
      "         [-0.1685, -0.2272, -0.1157,  ..., -0.0962,  0.0201, -0.0597],\n",
      "         [-0.1676, -0.0284,  0.0241,  ..., -0.0593,  0.0305, -0.0928],\n",
      "         [ 0.1270, -0.0867,  0.1760,  ...,  0.0503, -0.2657,  0.0420]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.1000,  0.1323,  0.0241,  ..., -0.5470,  0.2844, -0.0457],\n",
      "         [-0.1777, -0.2332, -0.0185,  ..., -0.2363, -0.1471, -0.0907],\n",
      "         [-0.1680, -0.1758,  0.1444,  ..., -0.3666, -0.0145, -0.1803],\n",
      "         ...,\n",
      "         [-0.1685, -0.2272, -0.1157,  ..., -0.0962,  0.0201, -0.0597],\n",
      "         [-0.1676, -0.0284,  0.0241,  ..., -0.0593,  0.0305, -0.0928],\n",
      "         [ 0.1270, -0.0867,  0.1760,  ...,  0.0503, -0.2657,  0.0420]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.1286,  0.0833,  0.0239,  ..., -0.5657,  0.3694, -0.0725],\n",
      "         [-0.1932, -0.1933, -0.0916,  ..., -0.2343, -0.1163,  0.0224],\n",
      "         [-0.0455, -0.3169, -0.0409,  ..., -0.3832,  0.0275,  0.0803],\n",
      "         ...,\n",
      "         [-0.2282, -0.2812, -0.0739,  ...,  0.0349, -0.0594, -0.1027],\n",
      "         [-0.1153, -0.0964, -0.0752,  ..., -0.1878, -0.0142, -0.0061],\n",
      "         [ 0.1447, -0.1270,  0.1112,  ...,  0.0441, -0.3489, -0.1001]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.1286,  0.0833,  0.0239,  ..., -0.5657,  0.3694, -0.0725],\n",
      "         [-0.1932, -0.1933, -0.0916,  ..., -0.2343, -0.1163,  0.0224],\n",
      "         [-0.0455, -0.3169, -0.0409,  ..., -0.3832,  0.0275,  0.0803],\n",
      "         ...,\n",
      "         [-0.2282, -0.2812, -0.0739,  ...,  0.0349, -0.0594, -0.1027],\n",
      "         [-0.1153, -0.0964, -0.0752,  ..., -0.1878, -0.0142, -0.0061],\n",
      "         [ 0.1447, -0.1270,  0.1112,  ...,  0.0441, -0.3489, -0.1001]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.1417,  0.0703,  0.0343,  ..., -0.5549,  0.3175, -0.0847],\n",
      "         [-0.3224, -0.3094, -0.1983,  ..., -0.2239, -0.0315,  0.0613],\n",
      "         [-0.0853, -0.3705,  0.0277,  ..., -0.3139,  0.1749,  0.2055],\n",
      "         ...,\n",
      "         [-0.1321, -0.3057, -0.2242,  ...,  0.0939,  0.0592, -0.2103],\n",
      "         [-0.2917,  0.0635, -0.1141,  ..., -0.1748,  0.1136,  0.0661],\n",
      "         [ 0.0177, -0.0520,  0.1003,  ...,  0.0058, -0.2720, -0.2233]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.1417,  0.0703,  0.0343,  ..., -0.5549,  0.3175, -0.0847],\n",
      "         [-0.3224, -0.3094, -0.1983,  ..., -0.2239, -0.0315,  0.0613],\n",
      "         [-0.0853, -0.3705,  0.0277,  ..., -0.3139,  0.1749,  0.2055],\n",
      "         ...,\n",
      "         [-0.1321, -0.3057, -0.2242,  ...,  0.0939,  0.0592, -0.2103],\n",
      "         [-0.2917,  0.0635, -0.1141,  ..., -0.1748,  0.1136,  0.0661],\n",
      "         [ 0.0177, -0.0520,  0.1003,  ...,  0.0058, -0.2720, -0.2233]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.1528,  0.0725,  0.0609,  ..., -0.5624,  0.3103, -0.0293],\n",
      "         [-0.3750, -0.2350, -0.2473,  ..., -0.2658, -0.0059,  0.0175],\n",
      "         [-0.1041, -0.2936, -0.0469,  ..., -0.3185,  0.2386,  0.2190],\n",
      "         ...,\n",
      "         [-0.2472, -0.4061, -0.1252,  ...,  0.2702,  0.0199, -0.1703],\n",
      "         [-0.3173, -0.0324, -0.0967,  ..., -0.1356,  0.0050,  0.1258],\n",
      "         [ 0.0391, -0.2120,  0.1311,  ...,  0.0848, -0.1746, -0.1839]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.1528,  0.0725,  0.0609,  ..., -0.5624,  0.3103, -0.0293],\n",
      "         [-0.3750, -0.2350, -0.2473,  ..., -0.2658, -0.0059,  0.0175],\n",
      "         [-0.1041, -0.2936, -0.0469,  ..., -0.3185,  0.2386,  0.2190],\n",
      "         ...,\n",
      "         [-0.2472, -0.4061, -0.1252,  ...,  0.2702,  0.0199, -0.1703],\n",
      "         [-0.3173, -0.0324, -0.0967,  ..., -0.1356,  0.0050,  0.1258],\n",
      "         [ 0.0391, -0.2120,  0.1311,  ...,  0.0848, -0.1746, -0.1839]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.1559,  0.0520,  0.0566,  ..., -0.5805,  0.2614, -0.0032],\n",
      "         [-0.4775, -0.2655, -0.3211,  ..., -0.2378, -0.1585,  0.1203],\n",
      "         [-0.1319, -0.2740,  0.0449,  ..., -0.1638,  0.1172,  0.2750],\n",
      "         ...,\n",
      "         [-0.2541, -0.3103, -0.2890,  ...,  0.3008, -0.0435, -0.1945],\n",
      "         [-0.2892, -0.0306, -0.0100,  ..., -0.0798, -0.1692,  0.1497],\n",
      "         [ 0.0377, -0.4308,  0.0450,  ...,  0.1789, -0.2428, -0.2844]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.1559,  0.0520,  0.0566,  ..., -0.5805,  0.2614, -0.0032],\n",
      "         [-0.4775, -0.2655, -0.3211,  ..., -0.2378, -0.1585,  0.1203],\n",
      "         [-0.1319, -0.2740,  0.0449,  ..., -0.1638,  0.1172,  0.2750],\n",
      "         ...,\n",
      "         [-0.2541, -0.3103, -0.2890,  ...,  0.3008, -0.0435, -0.1945],\n",
      "         [-0.2892, -0.0306, -0.0100,  ..., -0.0798, -0.1692,  0.1497],\n",
      "         [ 0.0377, -0.4308,  0.0450,  ...,  0.1789, -0.2428, -0.2844]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.1488,  0.0445,  0.0765,  ..., -0.5456,  0.2702,  0.0098],\n",
      "         [-0.6652, -0.2144, -0.4550,  ..., -0.2284, -0.1702,  0.1169],\n",
      "         [-0.2448, -0.3642, -0.0172,  ..., -0.0748,  0.0104,  0.2578],\n",
      "         ...,\n",
      "         [-0.3857, -0.2039, -0.4337,  ...,  0.2774,  0.0290, -0.3999],\n",
      "         [-0.3953,  0.0642, -0.1984,  ...,  0.0395, -0.1948,  0.1671],\n",
      "         [ 0.0022, -0.4250,  0.0024,  ...,  0.1261, -0.2729, -0.3160]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.1488,  0.0445,  0.0765,  ..., -0.5456,  0.2702,  0.0098],\n",
      "         [-0.6652, -0.2144, -0.4550,  ..., -0.2284, -0.1702,  0.1169],\n",
      "         [-0.2448, -0.3642, -0.0172,  ..., -0.0748,  0.0104,  0.2578],\n",
      "         ...,\n",
      "         [-0.3857, -0.2039, -0.4337,  ...,  0.2774,  0.0290, -0.3999],\n",
      "         [-0.3953,  0.0642, -0.1984,  ...,  0.0395, -0.1948,  0.1671],\n",
      "         [ 0.0022, -0.4250,  0.0024,  ...,  0.1261, -0.2729, -0.3160]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-1.2677e-01,  3.3335e-02,  9.3479e-02,  ..., -5.3204e-01,\n",
      "           2.7953e-01,  5.9814e-04],\n",
      "         [-6.9238e-01, -1.0976e-01, -4.2238e-01,  ..., -1.9094e-01,\n",
      "          -2.1172e-01,  1.5269e-01],\n",
      "         [-2.7881e-01, -4.2012e-01,  9.2704e-02,  ..., -1.7010e-01,\n",
      "          -1.1844e-01,  3.7695e-01],\n",
      "         ...,\n",
      "         [-2.0773e-01, -1.2458e-02, -4.1692e-01,  ...,  3.1835e-01,\n",
      "           1.1161e-01, -4.7835e-01],\n",
      "         [-4.5580e-01, -7.9639e-02, -2.6204e-01,  ...,  4.1423e-02,\n",
      "          -1.6439e-01,  1.1543e-01],\n",
      "         [-4.3387e-02, -3.7197e-01,  2.3200e-02,  ...,  6.9655e-02,\n",
      "          -2.9263e-01, -1.9500e-01]]], device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-1.2677e-01,  3.3335e-02,  9.3479e-02,  ..., -5.3204e-01,\n",
      "           2.7953e-01,  5.9814e-04],\n",
      "         [-6.9238e-01, -1.0976e-01, -4.2238e-01,  ..., -1.9094e-01,\n",
      "          -2.1172e-01,  1.5269e-01],\n",
      "         [-2.7881e-01, -4.2012e-01,  9.2704e-02,  ..., -1.7010e-01,\n",
      "          -1.1844e-01,  3.7695e-01],\n",
      "         ...,\n",
      "         [-2.0773e-01, -1.2458e-02, -4.1692e-01,  ...,  3.1835e-01,\n",
      "           1.1161e-01, -4.7835e-01],\n",
      "         [-4.5580e-01, -7.9639e-02, -2.6204e-01,  ...,  4.1423e-02,\n",
      "          -1.6439e-01,  1.1543e-01],\n",
      "         [-4.3387e-02, -3.7197e-01,  2.3200e-02,  ...,  6.9655e-02,\n",
      "          -2.9263e-01, -1.9500e-01]]], device='cuda:0'),) and output (tensor([[[-1.3547e-01,  2.2723e-02,  5.5414e-02,  ..., -5.4726e-01,\n",
      "           2.4703e-01, -2.2850e-02],\n",
      "         [-7.4087e-01, -1.4167e-01, -4.4087e-01,  ..., -4.1942e-01,\n",
      "          -2.4002e-01,  2.5326e-01],\n",
      "         [-3.7320e-01, -4.4735e-01,  5.3400e-02,  ..., -1.2079e-01,\n",
      "          -1.7736e-01,  6.0605e-01],\n",
      "         ...,\n",
      "         [-2.4540e-01, -1.5264e-02, -3.1214e-01,  ...,  2.8863e-01,\n",
      "           7.2345e-04, -4.6579e-01],\n",
      "         [-5.2938e-01, -1.2051e-01, -2.6196e-01,  ..., -4.5141e-02,\n",
      "          -1.4740e-01,  6.4754e-02],\n",
      "         [-9.3302e-02, -3.1616e-01, -3.0889e-02,  ...,  1.6486e-01,\n",
      "          -1.7879e-01, -1.6795e-01]]], device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-1.3547e-01,  2.2723e-02,  5.5414e-02,  ..., -5.4726e-01,\n",
      "           2.4703e-01, -2.2850e-02],\n",
      "         [-7.4087e-01, -1.4167e-01, -4.4087e-01,  ..., -4.1942e-01,\n",
      "          -2.4002e-01,  2.5326e-01],\n",
      "         [-3.7320e-01, -4.4735e-01,  5.3400e-02,  ..., -1.2079e-01,\n",
      "          -1.7736e-01,  6.0605e-01],\n",
      "         ...,\n",
      "         [-2.4540e-01, -1.5264e-02, -3.1214e-01,  ...,  2.8863e-01,\n",
      "           7.2345e-04, -4.6579e-01],\n",
      "         [-5.2938e-01, -1.2051e-01, -2.6196e-01,  ..., -4.5141e-02,\n",
      "          -1.4740e-01,  6.4754e-02],\n",
      "         [-9.3302e-02, -3.1616e-01, -3.0889e-02,  ...,  1.6486e-01,\n",
      "          -1.7879e-01, -1.6795e-01]]], device='cuda:0'),) and output (tensor([[[-0.1350,  0.0163,  0.0576,  ..., -0.5555,  0.2435,  0.0051],\n",
      "         [-0.7671, -0.1512, -0.4586,  ..., -0.3802, -0.1851,  0.2629],\n",
      "         [-0.3635, -0.3972,  0.0782,  ..., -0.0348, -0.1271,  0.4946],\n",
      "         ...,\n",
      "         [-0.1348, -0.0783, -0.4908,  ...,  0.3620,  0.1889, -0.5323],\n",
      "         [-0.6280, -0.0405, -0.1927,  ...,  0.0116, -0.0442,  0.1502],\n",
      "         [-0.1991, -0.4575, -0.0101,  ...,  0.4327, -0.0077, -0.1562]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.1350,  0.0163,  0.0576,  ..., -0.5555,  0.2435,  0.0051],\n",
      "         [-0.7671, -0.1512, -0.4586,  ..., -0.3802, -0.1851,  0.2629],\n",
      "         [-0.3635, -0.3972,  0.0782,  ..., -0.0348, -0.1271,  0.4946],\n",
      "         ...,\n",
      "         [-0.1348, -0.0783, -0.4908,  ...,  0.3620,  0.1889, -0.5323],\n",
      "         [-0.6280, -0.0405, -0.1927,  ...,  0.0116, -0.0442,  0.1502],\n",
      "         [-0.1991, -0.4575, -0.0101,  ...,  0.4327, -0.0077, -0.1562]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.1301, -0.0029,  0.0279,  ..., -0.5537,  0.2277,  0.0164],\n",
      "         [-0.7599, -0.2344, -0.4168,  ..., -0.3132, -0.3847,  0.3069],\n",
      "         [-0.2990, -0.3593,  0.0363,  ..., -0.0213, -0.1368,  0.5528],\n",
      "         ...,\n",
      "         [-0.0454, -0.1943, -0.5012,  ...,  0.2742,  0.1689, -0.5513],\n",
      "         [-0.7921, -0.1287, -0.3478,  ...,  0.0417,  0.0611,  0.1251],\n",
      "         [-0.2576, -0.3804,  0.0661,  ...,  0.5921,  0.1020, -0.1966]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.1301, -0.0029,  0.0279,  ..., -0.5537,  0.2277,  0.0164],\n",
      "         [-0.7599, -0.2344, -0.4168,  ..., -0.3132, -0.3847,  0.3069],\n",
      "         [-0.2990, -0.3593,  0.0363,  ..., -0.0213, -0.1368,  0.5528],\n",
      "         ...,\n",
      "         [-0.0454, -0.1943, -0.5012,  ...,  0.2742,  0.1689, -0.5513],\n",
      "         [-0.7921, -0.1287, -0.3478,  ...,  0.0417,  0.0611,  0.1251],\n",
      "         [-0.2576, -0.3804,  0.0661,  ...,  0.5921,  0.1020, -0.1966]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.1219,  0.0061, -0.0021,  ..., -0.5922,  0.2788, -0.0020],\n",
      "         [-0.7599, -0.2663, -0.4536,  ..., -0.2747, -0.4388,  0.3005],\n",
      "         [-0.4435, -0.3795, -0.0190,  ..., -0.0101, -0.0990,  0.7307],\n",
      "         ...,\n",
      "         [-0.1142, -0.0894, -0.1574,  ...,  0.1722,  0.0791, -0.5716],\n",
      "         [-0.8820, -0.0950, -0.3750,  ...,  0.2020, -0.0323,  0.3413],\n",
      "         [-0.2575, -0.4037,  0.0891,  ...,  0.7337,  0.2019, -0.1613]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.1219,  0.0061, -0.0021,  ..., -0.5922,  0.2788, -0.0020],\n",
      "         [-0.7599, -0.2663, -0.4536,  ..., -0.2747, -0.4388,  0.3005],\n",
      "         [-0.4435, -0.3795, -0.0190,  ..., -0.0101, -0.0990,  0.7307],\n",
      "         ...,\n",
      "         [-0.1142, -0.0894, -0.1574,  ...,  0.1722,  0.0791, -0.5716],\n",
      "         [-0.8820, -0.0950, -0.3750,  ...,  0.2020, -0.0323,  0.3413],\n",
      "         [-0.2575, -0.4037,  0.0891,  ...,  0.7337,  0.2019, -0.1613]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.1067,  0.0272,  0.0495,  ..., -0.5550,  0.3449,  0.0266],\n",
      "         [-0.8282, -0.2892, -0.3403,  ..., -0.2512, -0.4486,  0.3516],\n",
      "         [-0.4053, -0.2605, -0.1726,  ..., -0.0650, -0.1002,  0.6771],\n",
      "         ...,\n",
      "         [-0.3574, -0.0832, -0.0572,  ...,  0.2149,  0.2830, -0.3554],\n",
      "         [-0.7897, -0.2700, -0.2678,  ...,  0.3844, -0.1473,  0.1681],\n",
      "         [-0.2930, -0.3424, -0.0383,  ...,  0.7271,  0.0442, -0.1436]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.1067,  0.0272,  0.0495,  ..., -0.5550,  0.3449,  0.0266],\n",
      "         [-0.8282, -0.2892, -0.3403,  ..., -0.2512, -0.4486,  0.3516],\n",
      "         [-0.4053, -0.2605, -0.1726,  ..., -0.0650, -0.1002,  0.6771],\n",
      "         ...,\n",
      "         [-0.3574, -0.0832, -0.0572,  ...,  0.2149,  0.2830, -0.3554],\n",
      "         [-0.7897, -0.2700, -0.2678,  ...,  0.3844, -0.1473,  0.1681],\n",
      "         [-0.2930, -0.3424, -0.0383,  ...,  0.7271,  0.0442, -0.1436]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.0592,  0.0731,  0.0500,  ..., -0.5517,  0.3541,  0.0595],\n",
      "         [-0.9034, -0.4221, -0.3541,  ..., -0.3490, -0.3966,  0.1871],\n",
      "         [-0.4093, -0.4986, -0.2364,  ...,  0.0148,  0.0291,  0.7250],\n",
      "         ...,\n",
      "         [-0.7547, -0.6559,  0.8811,  ..., -0.1500,  0.5354, -0.6127],\n",
      "         [-1.2037, -0.7078, -0.0833,  ...,  0.1978, -0.1068, -0.1170],\n",
      "         [-0.4852, -0.5938, -0.1735,  ...,  0.6032,  0.2893, -0.0560]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.0592,  0.0731,  0.0500,  ..., -0.5517,  0.3541,  0.0595],\n",
      "         [-0.9034, -0.4221, -0.3541,  ..., -0.3490, -0.3966,  0.1871],\n",
      "         [-0.4093, -0.4986, -0.2364,  ...,  0.0148,  0.0291,  0.7250],\n",
      "         ...,\n",
      "         [-0.7547, -0.6559,  0.8811,  ..., -0.1500,  0.5354, -0.6127],\n",
      "         [-1.2037, -0.7078, -0.0833,  ...,  0.1978, -0.1068, -0.1170],\n",
      "         [-0.4852, -0.5938, -0.1735,  ...,  0.6032,  0.2893, -0.0560]]],\n",
      "       device='cuda:0'),) and output (tensor([[[ 0.1143,  0.2237,  0.3292,  ..., -0.7706,  0.2608,  0.0269],\n",
      "         [-0.9385, -0.4706, -0.1471,  ..., -0.5487, -0.4252,  0.0903],\n",
      "         [-0.3570, -0.6097, -0.3187,  ...,  0.2552,  0.1549,  0.7868],\n",
      "         ...,\n",
      "         [-0.5649, -0.8970,  0.3185,  ..., -0.3391,  0.0447, -0.3946],\n",
      "         [-0.8857, -1.1639, -0.3935,  ...,  0.2281, -0.3614, -0.1507],\n",
      "         [-0.4213, -0.9125, -0.3842,  ...,  0.6075,  0.3544, -0.0111]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[ 0.1143,  0.2237,  0.3292,  ..., -0.7706,  0.2608,  0.0269],\n",
      "         [-0.9385, -0.4706, -0.1471,  ..., -0.5487, -0.4252,  0.0903],\n",
      "         [-0.3570, -0.6097, -0.3187,  ...,  0.2552,  0.1549,  0.7868],\n",
      "         ...,\n",
      "         [-0.5649, -0.8970,  0.3185,  ..., -0.3391,  0.0447, -0.3946],\n",
      "         [-0.8857, -1.1639, -0.3935,  ...,  0.2281, -0.3614, -0.1507],\n",
      "         [-0.4213, -0.9125, -0.3842,  ...,  0.6075,  0.3544, -0.0111]]],\n",
      "       device='cuda:0'),) and output (tensor([[[ 0.3711,  0.5489,  0.6100,  ..., -0.7499,  0.2080,  0.4195],\n",
      "         [-0.9636, -0.8911, -0.2352,  ..., -0.2033, -0.3616, -0.0725],\n",
      "         [-0.3776, -0.8487, -0.7226,  ...,  0.6694, -0.1653,  0.5521],\n",
      "         ...,\n",
      "         [-0.8052, -1.0816, -0.0056,  ..., -0.6302,  0.1760, -0.5994],\n",
      "         [-1.2497, -0.9603, -0.1856,  ...,  0.0273, -0.6836, -1.1966],\n",
      "         [-0.5399, -1.2040, -0.7306,  ...,  0.7586,  0.4677, -0.5897]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[ 0.3711,  0.5489,  0.6100,  ..., -0.7499,  0.2080,  0.4195],\n",
      "         [-0.9636, -0.8911, -0.2352,  ..., -0.2033, -0.3616, -0.0725],\n",
      "         [-0.3776, -0.8487, -0.7226,  ...,  0.6694, -0.1653,  0.5521],\n",
      "         ...,\n",
      "         [-0.8052, -1.0816, -0.0056,  ..., -0.6302,  0.1760, -0.5994],\n",
      "         [-1.2497, -0.9603, -0.1856,  ...,  0.0273, -0.6836, -1.1966],\n",
      "         [-0.5399, -1.2040, -0.7306,  ...,  0.7586,  0.4677, -0.5897]]],\n",
      "       device='cuda:0'),) and output (tensor([[[ 1.1544,  3.2965,  1.6534,  ..., -2.7009,  2.6553,  2.2700],\n",
      "         [ 0.0899, -0.5011, -0.1999,  ..., -0.3568, -0.4225,  0.4133],\n",
      "         [-0.9740, -0.3197, -2.6367,  ...,  0.7985, -0.3114,  0.8239],\n",
      "         ...,\n",
      "         [-2.4322, -2.0259, -0.2269,  ...,  0.2721,  0.1452, -0.0504],\n",
      "         [-1.5503, -1.1854, -0.5724,  ...,  0.5880, -1.1395, -0.4384],\n",
      "         [-0.8288, -1.9681, -0.3054,  ...,  1.6883,  0.2137,  0.0993]]],\n",
      "       device='cuda:0'),)\n",
      "[Debug] Layer names being passed: ['model.layers.0', 'model.layers.1', 'model.layers.2', 'model.layers.3', 'model.layers.4', 'model.layers.5', 'model.layers.6', 'model.layers.7', 'model.layers.8', 'model.layers.9', 'model.layers.10', 'model.layers.11', 'model.layers.12', 'model.layers.13', 'model.layers.14', 'model.layers.15', 'model.layers.16', 'model.layers.17', 'model.layers.18', 'model.layers.19', 'model.layers.20', 'model.layers.21', 'model.layers.22', 'model.layers.23', 'model.layers.24', 'model.layers.25', 'model.layers.26', 'model.layers.27', 'model.layers.28', 'model.layers.29', 'model.layers.30', 'model.layers.31', 'model.embed_tokens']\n",
      "[Debug] Trying to access layer: model.layers.0\n",
      "[Debug] Successfully found layer: model.layers.0\n",
      "[Debug] Trying to access layer: model.layers.1\n",
      "[Debug] Successfully found layer: model.layers.1\n",
      "[Debug] Trying to access layer: model.layers.2\n",
      "[Debug] Successfully found layer: model.layers.2\n",
      "[Debug] Trying to access layer: model.layers.3\n",
      "[Debug] Successfully found layer: model.layers.3\n",
      "[Debug] Trying to access layer: model.layers.4\n",
      "[Debug] Successfully found layer: model.layers.4\n",
      "[Debug] Trying to access layer: model.layers.5\n",
      "[Debug] Successfully found layer: model.layers.5\n",
      "[Debug] Trying to access layer: model.layers.6\n",
      "[Debug] Successfully found layer: model.layers.6\n",
      "[Debug] Trying to access layer: model.layers.7\n",
      "[Debug] Successfully found layer: model.layers.7\n",
      "[Debug] Trying to access layer: model.layers.8\n",
      "[Debug] Successfully found layer: model.layers.8\n",
      "[Debug] Trying to access layer: model.layers.9\n",
      "[Debug] Successfully found layer: model.layers.9\n",
      "[Debug] Trying to access layer: model.layers.10\n",
      "[Debug] Successfully found layer: model.layers.10\n",
      "[Debug] Trying to access layer: model.layers.11\n",
      "[Debug] Successfully found layer: model.layers.11\n",
      "[Debug] Trying to access layer: model.layers.12\n",
      "[Debug] Successfully found layer: model.layers.12\n",
      "[Debug] Trying to access layer: model.layers.13\n",
      "[Debug] Successfully found layer: model.layers.13\n",
      "[Debug] Trying to access layer: model.layers.14\n",
      "[Debug] Successfully found layer: model.layers.14\n",
      "[Debug] Trying to access layer: model.layers.15\n",
      "[Debug] Successfully found layer: model.layers.15\n",
      "[Debug] Trying to access layer: model.layers.16\n",
      "[Debug] Successfully found layer: model.layers.16\n",
      "[Debug] Trying to access layer: model.layers.17\n",
      "[Debug] Successfully found layer: model.layers.17\n",
      "[Debug] Trying to access layer: model.layers.18\n",
      "[Debug] Successfully found layer: model.layers.18\n",
      "[Debug] Trying to access layer: model.layers.19\n",
      "[Debug] Successfully found layer: model.layers.19\n",
      "[Debug] Trying to access layer: model.layers.20\n",
      "[Debug] Successfully found layer: model.layers.20\n",
      "[Debug] Trying to access layer: model.layers.21\n",
      "[Debug] Successfully found layer: model.layers.21\n",
      "[Debug] Trying to access layer: model.layers.22\n",
      "[Debug] Successfully found layer: model.layers.22\n",
      "[Debug] Trying to access layer: model.layers.23\n",
      "[Debug] Successfully found layer: model.layers.23\n",
      "[Debug] Trying to access layer: model.layers.24\n",
      "[Debug] Successfully found layer: model.layers.24\n",
      "[Debug] Trying to access layer: model.layers.25\n",
      "[Debug] Successfully found layer: model.layers.25\n",
      "[Debug] Trying to access layer: model.layers.26\n",
      "[Debug] Successfully found layer: model.layers.26\n",
      "[Debug] Trying to access layer: model.layers.27\n",
      "[Debug] Successfully found layer: model.layers.27\n",
      "[Debug] Trying to access layer: model.layers.28\n",
      "[Debug] Successfully found layer: model.layers.28\n",
      "[Debug] Trying to access layer: model.layers.29\n",
      "[Debug] Successfully found layer: model.layers.29\n",
      "[Debug] Trying to access layer: model.layers.30\n",
      "[Debug] Successfully found layer: model.layers.30\n",
      "[Debug] Trying to access layer: model.layers.31\n",
      "[Debug] Successfully found layer: model.layers.31\n",
      "[Debug] Trying to access layer: model.embed_tokens\n",
      "[Debug] Successfully found layer: model.embed_tokens\n",
      "[Hook] Layer Embedding(128256, 4096) received input (tensor([[128000,     42,   1003,  20531,  12324,    578,  50280,  12324,    374,\n",
      "            264,  52482,  12324,  15871,   1990,   6890,    323,  17076,     11,\n",
      "           3515,   3940,   1120,   1306,    279,  17071,    315,   6890,    304,\n",
      "            220,   6393,     22,     13,   5734,    706,    520,   3115,   6476,\n",
      "            264,   9099,   3560,   8032,     17,     60,   6890,    323,  17076,\n",
      "            617,  21095,   2380,  25981,    927,  50280,     11,   2737,    279,\n",
      "          76985,   9483,    587,  40422,  15317,    315,    220,   6393,     22,\n",
      "            323,    220,   5162,     20,     11,    439,   1664,    439,    279,\n",
      "            735,    867,    321,   5111,    315,    220,   2550,     24,     13,\n",
      "            578,   1403,   5961,    617,   1101,   1027,   6532,    304,   3892,\n",
      "          96380,  21168,    927,   2585,    315,    279,  12095,  46799,  96486,\n",
      "             13]], device='cuda:0'),) and output tensor([[[ 2.6512e-04, -4.9973e-04, -5.8365e-04,  ...,  3.8147e-03,\n",
      "           6.3419e-05,  1.1902e-03],\n",
      "         [-2.3956e-03, -4.1199e-03, -3.6240e-04,  ...,  1.3184e-02,\n",
      "           9.0942e-03,  3.0365e-03],\n",
      "         [-9.7656e-03,  8.8501e-03, -4.5586e-04,  ...,  1.3351e-03,\n",
      "          -1.1780e-02, -8.4229e-03],\n",
      "         ...,\n",
      "         [-2.0905e-03, -9.4891e-05,  1.8311e-02,  ...,  9.7656e-03,\n",
      "          -4.5395e-04, -1.1597e-02],\n",
      "         [-7.1106e-03,  3.6621e-04, -8.6670e-03,  ...,  2.0752e-02,\n",
      "           2.8839e-03,  1.7334e-02],\n",
      "         [-6.7520e-04, -5.7602e-04,  4.7684e-04,  ...,  3.3264e-03,\n",
      "           3.9101e-04,  4.7493e-04]]], device='cuda:0')\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[ 2.6512e-04, -4.9973e-04, -5.8365e-04,  ...,  3.8147e-03,\n",
      "           6.3419e-05,  1.1902e-03],\n",
      "         [-2.3956e-03, -4.1199e-03, -3.6240e-04,  ...,  1.3184e-02,\n",
      "           9.0942e-03,  3.0365e-03],\n",
      "         [-9.7656e-03,  8.8501e-03, -4.5586e-04,  ...,  1.3351e-03,\n",
      "          -1.1780e-02, -8.4229e-03],\n",
      "         ...,\n",
      "         [-2.0905e-03, -9.4891e-05,  1.8311e-02,  ...,  9.7656e-03,\n",
      "          -4.5395e-04, -1.1597e-02],\n",
      "         [-7.1106e-03,  3.6621e-04, -8.6670e-03,  ...,  2.0752e-02,\n",
      "           2.8839e-03,  1.7334e-02],\n",
      "         [-6.7520e-04, -5.7602e-04,  4.7684e-04,  ...,  3.3264e-03,\n",
      "           3.9101e-04,  4.7493e-04]]], device='cuda:0'),) and output (tensor([[[-0.0004,  0.0146, -0.0020,  ...,  0.0066, -0.0194,  0.0020],\n",
      "         [-0.0177, -0.0057,  0.0126,  ...,  0.0104, -0.0125,  0.0018],\n",
      "         [-0.0213,  0.0182,  0.0016,  ..., -0.0101, -0.0055, -0.0112],\n",
      "         ...,\n",
      "         [-0.0119, -0.0026,  0.0193,  ...,  0.0175,  0.0091, -0.0136],\n",
      "         [-0.0168, -0.0116,  0.0016,  ..., -0.0283,  0.0066, -0.0008],\n",
      "         [ 0.0042, -0.0062,  0.0042,  ...,  0.0080,  0.0049, -0.0033]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.0004,  0.0146, -0.0020,  ...,  0.0066, -0.0194,  0.0020],\n",
      "         [-0.0177, -0.0057,  0.0126,  ...,  0.0104, -0.0125,  0.0018],\n",
      "         [-0.0213,  0.0182,  0.0016,  ..., -0.0101, -0.0055, -0.0112],\n",
      "         ...,\n",
      "         [-0.0119, -0.0026,  0.0193,  ...,  0.0175,  0.0091, -0.0136],\n",
      "         [-0.0168, -0.0116,  0.0016,  ..., -0.0283,  0.0066, -0.0008],\n",
      "         [ 0.0042, -0.0062,  0.0042,  ...,  0.0080,  0.0049, -0.0033]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.0972,  0.0787, -0.0419,  ...,  0.0071,  0.0869,  0.0218],\n",
      "         [-0.0230, -0.0013,  0.0052,  ...,  0.0063, -0.0178,  0.0033],\n",
      "         [-0.0221,  0.0125, -0.0104,  ..., -0.0296, -0.0383, -0.0023],\n",
      "         ...,\n",
      "         [ 0.0012, -0.0127,  0.0039,  ..., -0.0383, -0.0455, -0.0195],\n",
      "         [-0.0034, -0.0119, -0.0020,  ..., -0.0982, -0.0095,  0.0027],\n",
      "         [ 0.0043,  0.0014, -0.0139,  ...,  0.0024, -0.0083, -0.0064]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.0972,  0.0787, -0.0419,  ...,  0.0071,  0.0869,  0.0218],\n",
      "         [-0.0230, -0.0013,  0.0052,  ...,  0.0063, -0.0178,  0.0033],\n",
      "         [-0.0221,  0.0125, -0.0104,  ..., -0.0296, -0.0383, -0.0023],\n",
      "         ...,\n",
      "         [ 0.0012, -0.0127,  0.0039,  ..., -0.0383, -0.0455, -0.0195],\n",
      "         [-0.0034, -0.0119, -0.0020,  ..., -0.0982, -0.0095,  0.0027],\n",
      "         [ 0.0043,  0.0014, -0.0139,  ...,  0.0024, -0.0083, -0.0064]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.1040,  0.0875, -0.0360,  ...,  0.0205,  0.0997,  0.0184],\n",
      "         [-0.0383, -0.0205,  0.0033,  ...,  0.0048, -0.0371,  0.0002],\n",
      "         [-0.0404,  0.0362, -0.0135,  ..., -0.0497, -0.0819, -0.0135],\n",
      "         ...,\n",
      "         [ 0.0089, -0.0008,  0.0007,  ...,  0.0260, -0.0859, -0.0450],\n",
      "         [-0.0162, -0.0165, -0.0180,  ..., -0.0040, -0.0191, -0.0102],\n",
      "         [-0.0204,  0.0157, -0.0116,  ...,  0.0210, -0.0118, -0.0070]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.1040,  0.0875, -0.0360,  ...,  0.0205,  0.0997,  0.0184],\n",
      "         [-0.0383, -0.0205,  0.0033,  ...,  0.0048, -0.0371,  0.0002],\n",
      "         [-0.0404,  0.0362, -0.0135,  ..., -0.0497, -0.0819, -0.0135],\n",
      "         ...,\n",
      "         [ 0.0089, -0.0008,  0.0007,  ...,  0.0260, -0.0859, -0.0450],\n",
      "         [-0.0162, -0.0165, -0.0180,  ..., -0.0040, -0.0191, -0.0102],\n",
      "         [-0.0204,  0.0157, -0.0116,  ...,  0.0210, -0.0118, -0.0070]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.0870,  0.1049, -0.0173,  ...,  0.0700,  0.0989,  0.0181],\n",
      "         [-0.0663,  0.0354, -0.0275,  ..., -0.0349, -0.0483, -0.0446],\n",
      "         [-0.0817,  0.0503,  0.0273,  ..., -0.0377, -0.0682, -0.0487],\n",
      "         ...,\n",
      "         [ 0.0197,  0.0007,  0.0011,  ...,  0.0310, -0.1266, -0.0141],\n",
      "         [-0.0250, -0.0122,  0.0244,  ..., -0.0136,  0.0108, -0.0259],\n",
      "         [-0.0552,  0.0149,  0.0107,  ...,  0.0483, -0.0247,  0.0057]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.0870,  0.1049, -0.0173,  ...,  0.0700,  0.0989,  0.0181],\n",
      "         [-0.0663,  0.0354, -0.0275,  ..., -0.0349, -0.0483, -0.0446],\n",
      "         [-0.0817,  0.0503,  0.0273,  ..., -0.0377, -0.0682, -0.0487],\n",
      "         ...,\n",
      "         [ 0.0197,  0.0007,  0.0011,  ...,  0.0310, -0.1266, -0.0141],\n",
      "         [-0.0250, -0.0122,  0.0244,  ..., -0.0136,  0.0108, -0.0259],\n",
      "         [-0.0552,  0.0149,  0.0107,  ...,  0.0483, -0.0247,  0.0057]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.1113,  0.1022, -0.0102,  ...,  0.0436,  0.1283,  0.0321],\n",
      "         [-0.0719,  0.0367,  0.0061,  ...,  0.0244, -0.0444, -0.0076],\n",
      "         [-0.0712,  0.0977,  0.0203,  ..., -0.0394, -0.0728, -0.0187],\n",
      "         ...,\n",
      "         [ 0.0470, -0.0024, -0.0167,  ...,  0.0359, -0.1276, -0.0249],\n",
      "         [-0.0624,  0.0002,  0.0126,  ..., -0.0425,  0.0107, -0.0542],\n",
      "         [-0.0716, -0.0018, -0.0155,  ...,  0.0435, -0.0791, -0.0218]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.1113,  0.1022, -0.0102,  ...,  0.0436,  0.1283,  0.0321],\n",
      "         [-0.0719,  0.0367,  0.0061,  ...,  0.0244, -0.0444, -0.0076],\n",
      "         [-0.0712,  0.0977,  0.0203,  ..., -0.0394, -0.0728, -0.0187],\n",
      "         ...,\n",
      "         [ 0.0470, -0.0024, -0.0167,  ...,  0.0359, -0.1276, -0.0249],\n",
      "         [-0.0624,  0.0002,  0.0126,  ..., -0.0425,  0.0107, -0.0542],\n",
      "         [-0.0716, -0.0018, -0.0155,  ...,  0.0435, -0.0791, -0.0218]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.1034,  0.1132,  0.0096,  ...,  0.0453,  0.1477,  0.0301],\n",
      "         [-0.0506,  0.0203, -0.0077,  ..., -0.0122, -0.0488,  0.0169],\n",
      "         [-0.0393,  0.0119,  0.0312,  ..., -0.0054, -0.0439,  0.0350],\n",
      "         ...,\n",
      "         [ 0.0808,  0.0848, -0.0402,  ..., -0.1002, -0.1020, -0.0712],\n",
      "         [-0.0272,  0.0166,  0.0116,  ..., -0.1133,  0.0009, -0.0239],\n",
      "         [-0.0652, -0.0078,  0.0419,  ...,  0.0451, -0.0679,  0.0349]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.1034,  0.1132,  0.0096,  ...,  0.0453,  0.1477,  0.0301],\n",
      "         [-0.0506,  0.0203, -0.0077,  ..., -0.0122, -0.0488,  0.0169],\n",
      "         [-0.0393,  0.0119,  0.0312,  ..., -0.0054, -0.0439,  0.0350],\n",
      "         ...,\n",
      "         [ 0.0808,  0.0848, -0.0402,  ..., -0.1002, -0.1020, -0.0712],\n",
      "         [-0.0272,  0.0166,  0.0116,  ..., -0.1133,  0.0009, -0.0239],\n",
      "         [-0.0652, -0.0078,  0.0419,  ...,  0.0451, -0.0679,  0.0349]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.1071,  0.1253,  0.0211,  ...,  0.0126,  0.1586,  0.0278],\n",
      "         [-0.0526,  0.0165, -0.0192,  ...,  0.0490, -0.0090,  0.0326],\n",
      "         [-0.0176, -0.0019,  0.0768,  ..., -0.0150, -0.0203,  0.0988],\n",
      "         ...,\n",
      "         [ 0.1120,  0.0199,  0.0438,  ...,  0.0154, -0.1281, -0.0480],\n",
      "         [-0.1191,  0.0788,  0.0008,  ..., -0.0061,  0.0722,  0.0728],\n",
      "         [-0.0177, -0.0239,  0.0016,  ...,  0.0982, -0.0158,  0.0195]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.1071,  0.1253,  0.0211,  ...,  0.0126,  0.1586,  0.0278],\n",
      "         [-0.0526,  0.0165, -0.0192,  ...,  0.0490, -0.0090,  0.0326],\n",
      "         [-0.0176, -0.0019,  0.0768,  ..., -0.0150, -0.0203,  0.0988],\n",
      "         ...,\n",
      "         [ 0.1120,  0.0199,  0.0438,  ...,  0.0154, -0.1281, -0.0480],\n",
      "         [-0.1191,  0.0788,  0.0008,  ..., -0.0061,  0.0722,  0.0728],\n",
      "         [-0.0177, -0.0239,  0.0016,  ...,  0.0982, -0.0158,  0.0195]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.0945,  0.1848,  0.0036,  ..., -0.0473,  0.1489,  0.0536],\n",
      "         [ 0.0245, -0.0112, -0.0290,  ..., -0.0235, -0.1342, -0.0230],\n",
      "         [ 0.0114,  0.0404,  0.0421,  ..., -0.0733, -0.0786,  0.0225],\n",
      "         ...,\n",
      "         [ 0.2485,  0.0845,  0.0392,  ...,  0.0502, -0.1436, -0.0597],\n",
      "         [-0.0634,  0.0736,  0.0097,  ...,  0.1100,  0.0679,  0.0263],\n",
      "         [-0.0355, -0.0612, -0.0364,  ...,  0.0382, -0.0864, -0.0502]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.0945,  0.1848,  0.0036,  ..., -0.0473,  0.1489,  0.0536],\n",
      "         [ 0.0245, -0.0112, -0.0290,  ..., -0.0235, -0.1342, -0.0230],\n",
      "         [ 0.0114,  0.0404,  0.0421,  ..., -0.0733, -0.0786,  0.0225],\n",
      "         ...,\n",
      "         [ 0.2485,  0.0845,  0.0392,  ...,  0.0502, -0.1436, -0.0597],\n",
      "         [-0.0634,  0.0736,  0.0097,  ...,  0.1100,  0.0679,  0.0263],\n",
      "         [-0.0355, -0.0612, -0.0364,  ...,  0.0382, -0.0864, -0.0502]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.1060,  0.2113,  0.0270,  ..., -0.1141,  0.1610,  0.0528],\n",
      "         [ 0.0487,  0.0654, -0.0532,  ...,  0.0009, -0.1567, -0.0495],\n",
      "         [ 0.0114,  0.0418, -0.0174,  ..., -0.0941, -0.1351, -0.0206],\n",
      "         ...,\n",
      "         [ 0.2503,  0.0708,  0.0705,  ...,  0.0194, -0.2444, -0.1752],\n",
      "         [-0.1060, -0.0491,  0.0859,  ...,  0.1261,  0.0123, -0.0756],\n",
      "         [-0.0608, -0.0624, -0.0285,  ...,  0.1242, -0.0792, -0.0454]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.1060,  0.2113,  0.0270,  ..., -0.1141,  0.1610,  0.0528],\n",
      "         [ 0.0487,  0.0654, -0.0532,  ...,  0.0009, -0.1567, -0.0495],\n",
      "         [ 0.0114,  0.0418, -0.0174,  ..., -0.0941, -0.1351, -0.0206],\n",
      "         ...,\n",
      "         [ 0.2503,  0.0708,  0.0705,  ...,  0.0194, -0.2444, -0.1752],\n",
      "         [-0.1060, -0.0491,  0.0859,  ...,  0.1261,  0.0123, -0.0756],\n",
      "         [-0.0608, -0.0624, -0.0285,  ...,  0.1242, -0.0792, -0.0454]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.0954,  0.2493,  0.0073,  ..., -0.1568,  0.1818,  0.0855],\n",
      "         [-0.0032,  0.0489, -0.0059,  ...,  0.0119, -0.0937, -0.0453],\n",
      "         [-0.0212, -0.0582, -0.0205,  ..., -0.1179, -0.1062, -0.0188],\n",
      "         ...,\n",
      "         [ 0.2890, -0.0352,  0.0017,  ...,  0.0282, -0.0892, -0.1643],\n",
      "         [-0.0540, -0.1626,  0.0092,  ...,  0.0965,  0.0032, -0.0441],\n",
      "         [-0.0656, -0.0830, -0.0665,  ...,  0.0835, -0.0209,  0.0111]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.0954,  0.2493,  0.0073,  ..., -0.1568,  0.1818,  0.0855],\n",
      "         [-0.0032,  0.0489, -0.0059,  ...,  0.0119, -0.0937, -0.0453],\n",
      "         [-0.0212, -0.0582, -0.0205,  ..., -0.1179, -0.1062, -0.0188],\n",
      "         ...,\n",
      "         [ 0.2890, -0.0352,  0.0017,  ...,  0.0282, -0.0892, -0.1643],\n",
      "         [-0.0540, -0.1626,  0.0092,  ...,  0.0965,  0.0032, -0.0441],\n",
      "         [-0.0656, -0.0830, -0.0665,  ...,  0.0835, -0.0209,  0.0111]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.0152,  0.3340,  0.0011,  ..., -0.2750,  0.1933,  0.1034],\n",
      "         [ 0.0737,  0.0346,  0.0065,  ...,  0.0109, -0.0994, -0.0360],\n",
      "         [-0.0283, -0.0690, -0.0305,  ..., -0.1196, -0.1354, -0.0429],\n",
      "         ...,\n",
      "         [ 0.3189, -0.0392, -0.0103,  ...,  0.0940, -0.0809, -0.1733],\n",
      "         [-0.0214, -0.1736,  0.0703,  ...,  0.0828, -0.0502, -0.0759],\n",
      "         [-0.0013, -0.0472, -0.0794,  ...,  0.0598, -0.0152,  0.0007]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.0152,  0.3340,  0.0011,  ..., -0.2750,  0.1933,  0.1034],\n",
      "         [ 0.0737,  0.0346,  0.0065,  ...,  0.0109, -0.0994, -0.0360],\n",
      "         [-0.0283, -0.0690, -0.0305,  ..., -0.1196, -0.1354, -0.0429],\n",
      "         ...,\n",
      "         [ 0.3189, -0.0392, -0.0103,  ...,  0.0940, -0.0809, -0.1733],\n",
      "         [-0.0214, -0.1736,  0.0703,  ...,  0.0828, -0.0502, -0.0759],\n",
      "         [-0.0013, -0.0472, -0.0794,  ...,  0.0598, -0.0152,  0.0007]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.0356,  0.3483, -0.0366,  ..., -0.2962,  0.2088,  0.1065],\n",
      "         [ 0.0643,  0.0260,  0.0061,  ..., -0.0060, -0.0646, -0.0219],\n",
      "         [-0.0557, -0.0858,  0.0081,  ..., -0.1140, -0.0658, -0.0036],\n",
      "         ...,\n",
      "         [ 0.2606, -0.0158, -0.0242,  ...,  0.0897, -0.1788, -0.2333],\n",
      "         [-0.0299, -0.1035,  0.0340,  ...,  0.0623,  0.0357, -0.0341],\n",
      "         [-0.0746, -0.0271, -0.1333,  ..., -0.0182,  0.0192, -0.0288]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.0356,  0.3483, -0.0366,  ..., -0.2962,  0.2088,  0.1065],\n",
      "         [ 0.0643,  0.0260,  0.0061,  ..., -0.0060, -0.0646, -0.0219],\n",
      "         [-0.0557, -0.0858,  0.0081,  ..., -0.1140, -0.0658, -0.0036],\n",
      "         ...,\n",
      "         [ 0.2606, -0.0158, -0.0242,  ...,  0.0897, -0.1788, -0.2333],\n",
      "         [-0.0299, -0.1035,  0.0340,  ...,  0.0623,  0.0357, -0.0341],\n",
      "         [-0.0746, -0.0271, -0.1333,  ..., -0.0182,  0.0192, -0.0288]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.0127,  0.3661, -0.0316,  ..., -0.3678,  0.1973,  0.1019],\n",
      "         [ 0.0463, -0.0028, -0.0075,  ..., -0.0171, -0.1020,  0.0405],\n",
      "         [-0.0109, -0.0958,  0.0509,  ..., -0.1020, -0.0719,  0.0136],\n",
      "         ...,\n",
      "         [ 0.3321,  0.0534, -0.0442,  ...,  0.1076, -0.1886, -0.3030],\n",
      "         [-0.0624, -0.0997, -0.0326,  ...,  0.1948, -0.0689, -0.0705],\n",
      "         [-0.1157, -0.0394, -0.1007,  ...,  0.0626, -0.0092,  0.0109]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.0127,  0.3661, -0.0316,  ..., -0.3678,  0.1973,  0.1019],\n",
      "         [ 0.0463, -0.0028, -0.0075,  ..., -0.0171, -0.1020,  0.0405],\n",
      "         [-0.0109, -0.0958,  0.0509,  ..., -0.1020, -0.0719,  0.0136],\n",
      "         ...,\n",
      "         [ 0.3321,  0.0534, -0.0442,  ...,  0.1076, -0.1886, -0.3030],\n",
      "         [-0.0624, -0.0997, -0.0326,  ...,  0.1948, -0.0689, -0.0705],\n",
      "         [-0.1157, -0.0394, -0.1007,  ...,  0.0626, -0.0092,  0.0109]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.0089,  0.3233, -0.0779,  ..., -0.4491,  0.1845,  0.0951],\n",
      "         [-0.1046,  0.0070, -0.0163,  ..., -0.0753, -0.1079, -0.0005],\n",
      "         [-0.0818, -0.0176,  0.0120,  ..., -0.1304, -0.0304, -0.0373],\n",
      "         ...,\n",
      "         [ 0.3031, -0.0444, -0.0645,  ...,  0.1237, -0.1298, -0.2502],\n",
      "         [-0.0070, -0.1308,  0.0148,  ...,  0.0859, -0.1326,  0.0329],\n",
      "         [-0.0598, -0.0105, -0.0656,  ..., -0.0464, -0.1618,  0.1654]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.0089,  0.3233, -0.0779,  ..., -0.4491,  0.1845,  0.0951],\n",
      "         [-0.1046,  0.0070, -0.0163,  ..., -0.0753, -0.1079, -0.0005],\n",
      "         [-0.0818, -0.0176,  0.0120,  ..., -0.1304, -0.0304, -0.0373],\n",
      "         ...,\n",
      "         [ 0.3031, -0.0444, -0.0645,  ...,  0.1237, -0.1298, -0.2502],\n",
      "         [-0.0070, -0.1308,  0.0148,  ...,  0.0859, -0.1326,  0.0329],\n",
      "         [-0.0598, -0.0105, -0.0656,  ..., -0.0464, -0.1618,  0.1654]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.0244,  0.2828, -0.0532,  ..., -0.4592,  0.2300,  0.0142],\n",
      "         [-0.0788, -0.0784, -0.0752,  ..., -0.0776, -0.1053, -0.0251],\n",
      "         [-0.1754, -0.1181,  0.0072,  ..., -0.0564, -0.0991, -0.0386],\n",
      "         ...,\n",
      "         [ 0.2213, -0.2009, -0.0395,  ...,  0.0904, -0.1668, -0.2167],\n",
      "         [-0.1029, -0.0794,  0.0022,  ...,  0.0757, -0.0990,  0.0855],\n",
      "         [-0.0506, -0.0827, -0.0660,  ...,  0.0137, -0.2198,  0.1267]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.0244,  0.2828, -0.0532,  ..., -0.4592,  0.2300,  0.0142],\n",
      "         [-0.0788, -0.0784, -0.0752,  ..., -0.0776, -0.1053, -0.0251],\n",
      "         [-0.1754, -0.1181,  0.0072,  ..., -0.0564, -0.0991, -0.0386],\n",
      "         ...,\n",
      "         [ 0.2213, -0.2009, -0.0395,  ...,  0.0904, -0.1668, -0.2167],\n",
      "         [-0.1029, -0.0794,  0.0022,  ...,  0.0757, -0.0990,  0.0855],\n",
      "         [-0.0506, -0.0827, -0.0660,  ...,  0.0137, -0.2198,  0.1267]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.0928,  0.1485,  0.0158,  ..., -0.5380,  0.2664, -0.0333],\n",
      "         [-0.1912, -0.1790, -0.1212,  ..., -0.0926, -0.0447, -0.0045],\n",
      "         [-0.1382, -0.1937, -0.0155,  ...,  0.0635, -0.0705,  0.0245],\n",
      "         ...,\n",
      "         [ 0.3602, -0.1870, -0.0574,  ...,  0.1673, -0.3086, -0.1224],\n",
      "         [-0.0371, -0.1586,  0.0626,  ...,  0.0470, -0.1103,  0.0024],\n",
      "         [ 0.0979, -0.1140, -0.0553,  ...,  0.0667, -0.3298,  0.1159]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.0928,  0.1485,  0.0158,  ..., -0.5380,  0.2664, -0.0333],\n",
      "         [-0.1912, -0.1790, -0.1212,  ..., -0.0926, -0.0447, -0.0045],\n",
      "         [-0.1382, -0.1937, -0.0155,  ...,  0.0635, -0.0705,  0.0245],\n",
      "         ...,\n",
      "         [ 0.3602, -0.1870, -0.0574,  ...,  0.1673, -0.3086, -0.1224],\n",
      "         [-0.0371, -0.1586,  0.0626,  ...,  0.0470, -0.1103,  0.0024],\n",
      "         [ 0.0979, -0.1140, -0.0553,  ...,  0.0667, -0.3298,  0.1159]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.1000,  0.1323,  0.0241,  ..., -0.5470,  0.2844, -0.0457],\n",
      "         [-0.1847, -0.2759, -0.0226,  ..., -0.0416, -0.1330,  0.0162],\n",
      "         [-0.2673, -0.0755,  0.0014,  ...,  0.1547, -0.1404, -0.0131],\n",
      "         ...,\n",
      "         [ 0.4484, -0.1813,  0.1131,  ...,  0.2151, -0.2042, -0.0820],\n",
      "         [ 0.0776, -0.1771,  0.1654,  ...,  0.1186, -0.0633,  0.0205],\n",
      "         [ 0.0733, -0.0487,  0.0416,  ...,  0.0130, -0.4958,  0.1634]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.1000,  0.1323,  0.0241,  ..., -0.5470,  0.2844, -0.0457],\n",
      "         [-0.1847, -0.2759, -0.0226,  ..., -0.0416, -0.1330,  0.0162],\n",
      "         [-0.2673, -0.0755,  0.0014,  ...,  0.1547, -0.1404, -0.0131],\n",
      "         ...,\n",
      "         [ 0.4484, -0.1813,  0.1131,  ...,  0.2151, -0.2042, -0.0820],\n",
      "         [ 0.0776, -0.1771,  0.1654,  ...,  0.1186, -0.0633,  0.0205],\n",
      "         [ 0.0733, -0.0487,  0.0416,  ...,  0.0130, -0.4958,  0.1634]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.1286,  0.0833,  0.0239,  ..., -0.5657,  0.3694, -0.0725],\n",
      "         [-0.2443, -0.2250,  0.0591,  ..., -0.0757, -0.1737, -0.0168],\n",
      "         [-0.1920, -0.0804, -0.0248,  ..., -0.0979, -0.2242, -0.0200],\n",
      "         ...,\n",
      "         [ 0.3912, -0.1034,  0.0543,  ...,  0.1752, -0.2684, -0.0573],\n",
      "         [ 0.0844, -0.0929,  0.0552,  ...,  0.1529, -0.0659,  0.0370],\n",
      "         [ 0.2342, -0.1432, -0.0357,  ..., -0.0296, -0.5726,  0.1130]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.1286,  0.0833,  0.0239,  ..., -0.5657,  0.3694, -0.0725],\n",
      "         [-0.2443, -0.2250,  0.0591,  ..., -0.0757, -0.1737, -0.0168],\n",
      "         [-0.1920, -0.0804, -0.0248,  ..., -0.0979, -0.2242, -0.0200],\n",
      "         ...,\n",
      "         [ 0.3912, -0.1034,  0.0543,  ...,  0.1752, -0.2684, -0.0573],\n",
      "         [ 0.0844, -0.0929,  0.0552,  ...,  0.1529, -0.0659,  0.0370],\n",
      "         [ 0.2342, -0.1432, -0.0357,  ..., -0.0296, -0.5726,  0.1130]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.1417,  0.0703,  0.0343,  ..., -0.5549,  0.3175, -0.0847],\n",
      "         [-0.2596, -0.2425,  0.0647,  ..., -0.1207, -0.1567, -0.0944],\n",
      "         [-0.3348, -0.0251, -0.0762,  ..., -0.0709, -0.1344,  0.0599],\n",
      "         ...,\n",
      "         [ 0.4829, -0.1533,  0.0121,  ...,  0.1039, -0.1383, -0.1041],\n",
      "         [ 0.1621, -0.0916,  0.0367,  ...,  0.0924,  0.0443,  0.0608],\n",
      "         [ 0.1529, -0.0197, -0.0290,  ..., -0.0534, -0.4910, -0.0598]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.1417,  0.0703,  0.0343,  ..., -0.5549,  0.3175, -0.0847],\n",
      "         [-0.2596, -0.2425,  0.0647,  ..., -0.1207, -0.1567, -0.0944],\n",
      "         [-0.3348, -0.0251, -0.0762,  ..., -0.0709, -0.1344,  0.0599],\n",
      "         ...,\n",
      "         [ 0.4829, -0.1533,  0.0121,  ...,  0.1039, -0.1383, -0.1041],\n",
      "         [ 0.1621, -0.0916,  0.0367,  ...,  0.0924,  0.0443,  0.0608],\n",
      "         [ 0.1529, -0.0197, -0.0290,  ..., -0.0534, -0.4910, -0.0598]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.1528,  0.0725,  0.0609,  ..., -0.5624,  0.3103, -0.0293],\n",
      "         [-0.1860, -0.1222,  0.0716,  ..., -0.2188, -0.0616, -0.1324],\n",
      "         [-0.4185,  0.0057, -0.0970,  ..., -0.0267, -0.0707,  0.1616],\n",
      "         ...,\n",
      "         [ 0.6478, -0.0765,  0.1823,  ...,  0.1352, -0.0514, -0.1814],\n",
      "         [ 0.2523, -0.1027,  0.0529,  ...,  0.1025, -0.0025,  0.1349],\n",
      "         [ 0.2062, -0.0882,  0.0441,  ..., -0.0548, -0.3368, -0.0795]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.1528,  0.0725,  0.0609,  ..., -0.5624,  0.3103, -0.0293],\n",
      "         [-0.1860, -0.1222,  0.0716,  ..., -0.2188, -0.0616, -0.1324],\n",
      "         [-0.4185,  0.0057, -0.0970,  ..., -0.0267, -0.0707,  0.1616],\n",
      "         ...,\n",
      "         [ 0.6478, -0.0765,  0.1823,  ...,  0.1352, -0.0514, -0.1814],\n",
      "         [ 0.2523, -0.1027,  0.0529,  ...,  0.1025, -0.0025,  0.1349],\n",
      "         [ 0.2062, -0.0882,  0.0441,  ..., -0.0548, -0.3368, -0.0795]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.1559,  0.0520,  0.0566,  ..., -0.5805,  0.2614, -0.0032],\n",
      "         [-0.2096, -0.2104,  0.1087,  ..., -0.2408, -0.1605,  0.0109],\n",
      "         [-0.3645,  0.0635, -0.2316,  ...,  0.0479, -0.1630,  0.0153],\n",
      "         ...,\n",
      "         [ 0.5901,  0.1107,  0.0725,  ...,  0.1884, -0.0750, -0.1264],\n",
      "         [ 0.1409,  0.0763, -0.0055,  ...,  0.0358,  0.0075, -0.0421],\n",
      "         [ 0.2342, -0.1643, -0.2037,  ..., -0.0009, -0.3557, -0.0499]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.1559,  0.0520,  0.0566,  ..., -0.5805,  0.2614, -0.0032],\n",
      "         [-0.2096, -0.2104,  0.1087,  ..., -0.2408, -0.1605,  0.0109],\n",
      "         [-0.3645,  0.0635, -0.2316,  ...,  0.0479, -0.1630,  0.0153],\n",
      "         ...,\n",
      "         [ 0.5901,  0.1107,  0.0725,  ...,  0.1884, -0.0750, -0.1264],\n",
      "         [ 0.1409,  0.0763, -0.0055,  ...,  0.0358,  0.0075, -0.0421],\n",
      "         [ 0.2342, -0.1643, -0.2037,  ..., -0.0009, -0.3557, -0.0499]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.1488,  0.0445,  0.0765,  ..., -0.5456,  0.2702,  0.0098],\n",
      "         [-0.3098, -0.2929,  0.2013,  ..., -0.2331, -0.1555,  0.0863],\n",
      "         [-0.4422,  0.1203, -0.2499,  ...,  0.1422, -0.1211, -0.0273],\n",
      "         ...,\n",
      "         [ 0.8445,  0.0319,  0.0558,  ...,  0.2328, -0.0137, -0.0960],\n",
      "         [ 0.1852,  0.0554,  0.0846,  ...,  0.0908,  0.0382,  0.0136],\n",
      "         [ 0.1452, -0.1729, -0.1990,  ...,  0.0221, -0.3491, -0.0171]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.1488,  0.0445,  0.0765,  ..., -0.5456,  0.2702,  0.0098],\n",
      "         [-0.3098, -0.2929,  0.2013,  ..., -0.2331, -0.1555,  0.0863],\n",
      "         [-0.4422,  0.1203, -0.2499,  ...,  0.1422, -0.1211, -0.0273],\n",
      "         ...,\n",
      "         [ 0.8445,  0.0319,  0.0558,  ...,  0.2328, -0.0137, -0.0960],\n",
      "         [ 0.1852,  0.0554,  0.0846,  ...,  0.0908,  0.0382,  0.0136],\n",
      "         [ 0.1452, -0.1729, -0.1990,  ...,  0.0221, -0.3491, -0.0171]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-1.2677e-01,  3.3335e-02,  9.3479e-02,  ..., -5.3204e-01,\n",
      "           2.7953e-01,  5.9814e-04],\n",
      "         [-3.5587e-01, -2.7776e-01,  2.6715e-01,  ..., -3.4160e-01,\n",
      "          -1.0886e-01, -4.7130e-02],\n",
      "         [-2.4911e-01,  8.3101e-03, -2.3243e-01,  ...,  2.5116e-02,\n",
      "          -4.5046e-02, -1.6862e-02],\n",
      "         ...,\n",
      "         [ 8.7622e-01,  1.4620e-01, -1.7671e-01,  ...,  2.1888e-01,\n",
      "          -5.5914e-02,  4.8093e-02],\n",
      "         [ 2.2715e-01,  1.1785e-01, -1.9210e-01,  ...,  5.7877e-03,\n",
      "           1.0744e-01, -3.5223e-02],\n",
      "         [ 1.8990e-01, -1.2198e-01, -1.4776e-01,  ...,  1.3012e-01,\n",
      "          -3.7325e-01,  4.9865e-02]]], device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-1.2677e-01,  3.3335e-02,  9.3479e-02,  ..., -5.3204e-01,\n",
      "           2.7953e-01,  5.9814e-04],\n",
      "         [-3.5587e-01, -2.7776e-01,  2.6715e-01,  ..., -3.4160e-01,\n",
      "          -1.0886e-01, -4.7130e-02],\n",
      "         [-2.4911e-01,  8.3101e-03, -2.3243e-01,  ...,  2.5116e-02,\n",
      "          -4.5046e-02, -1.6862e-02],\n",
      "         ...,\n",
      "         [ 8.7622e-01,  1.4620e-01, -1.7671e-01,  ...,  2.1888e-01,\n",
      "          -5.5914e-02,  4.8093e-02],\n",
      "         [ 2.2715e-01,  1.1785e-01, -1.9210e-01,  ...,  5.7877e-03,\n",
      "           1.0744e-01, -3.5223e-02],\n",
      "         [ 1.8990e-01, -1.2198e-01, -1.4776e-01,  ...,  1.3012e-01,\n",
      "          -3.7325e-01,  4.9865e-02]]], device='cuda:0'),) and output (tensor([[[-0.1355,  0.0227,  0.0554,  ..., -0.5473,  0.2470, -0.0229],\n",
      "         [-0.3088, -0.1943,  0.3198,  ..., -0.4310, -0.0903, -0.0647],\n",
      "         [-0.2480, -0.0220, -0.1143,  ...,  0.0086, -0.1596, -0.2732],\n",
      "         ...,\n",
      "         [ 0.8483,  0.0394, -0.1191,  ...,  0.0499, -0.2640, -0.0662],\n",
      "         [ 0.1910,  0.1440, -0.1544,  ..., -0.0624,  0.0044, -0.1107],\n",
      "         [ 0.3070, -0.0769, -0.0906,  ...,  0.2005, -0.3660,  0.0140]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.1355,  0.0227,  0.0554,  ..., -0.5473,  0.2470, -0.0229],\n",
      "         [-0.3088, -0.1943,  0.3198,  ..., -0.4310, -0.0903, -0.0647],\n",
      "         [-0.2480, -0.0220, -0.1143,  ...,  0.0086, -0.1596, -0.2732],\n",
      "         ...,\n",
      "         [ 0.8483,  0.0394, -0.1191,  ...,  0.0499, -0.2640, -0.0662],\n",
      "         [ 0.1910,  0.1440, -0.1544,  ..., -0.0624,  0.0044, -0.1107],\n",
      "         [ 0.3070, -0.0769, -0.0906,  ...,  0.2005, -0.3660,  0.0140]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.1350,  0.0163,  0.0576,  ..., -0.5555,  0.2435,  0.0051],\n",
      "         [-0.2914, -0.2086,  0.2427,  ..., -0.5017, -0.1051, -0.0892],\n",
      "         [-0.2434,  0.0485, -0.1004,  ..., -0.0226, -0.2870, -0.2261],\n",
      "         ...,\n",
      "         [ 0.8821,  0.0198, -0.0137,  ...,  0.0351, -0.4410, -0.0513],\n",
      "         [ 0.2617,  0.0928, -0.1484,  ..., -0.0992,  0.0599,  0.0311],\n",
      "         [ 0.4034, -0.1165,  0.0409,  ...,  0.2541, -0.2131,  0.0047]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.1350,  0.0163,  0.0576,  ..., -0.5555,  0.2435,  0.0051],\n",
      "         [-0.2914, -0.2086,  0.2427,  ..., -0.5017, -0.1051, -0.0892],\n",
      "         [-0.2434,  0.0485, -0.1004,  ..., -0.0226, -0.2870, -0.2261],\n",
      "         ...,\n",
      "         [ 0.8821,  0.0198, -0.0137,  ...,  0.0351, -0.4410, -0.0513],\n",
      "         [ 0.2617,  0.0928, -0.1484,  ..., -0.0992,  0.0599,  0.0311],\n",
      "         [ 0.4034, -0.1165,  0.0409,  ...,  0.2541, -0.2131,  0.0047]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.1301, -0.0029,  0.0279,  ..., -0.5537,  0.2277,  0.0164],\n",
      "         [-0.2384, -0.1866,  0.3342,  ..., -0.5758, -0.2975,  0.0272],\n",
      "         [-0.1837,  0.1566, -0.0583,  ...,  0.1361, -0.3142, -0.1013],\n",
      "         ...,\n",
      "         [ 0.9586, -0.0184,  0.2970,  ...,  0.0022, -0.3879, -0.0044],\n",
      "         [ 0.3211, -0.0513, -0.1124,  ..., -0.1648,  0.1582,  0.0432],\n",
      "         [ 0.2675, -0.0598,  0.2738,  ...,  0.4031, -0.1226, -0.0768]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.1301, -0.0029,  0.0279,  ..., -0.5537,  0.2277,  0.0164],\n",
      "         [-0.2384, -0.1866,  0.3342,  ..., -0.5758, -0.2975,  0.0272],\n",
      "         [-0.1837,  0.1566, -0.0583,  ...,  0.1361, -0.3142, -0.1013],\n",
      "         ...,\n",
      "         [ 0.9586, -0.0184,  0.2970,  ...,  0.0022, -0.3879, -0.0044],\n",
      "         [ 0.3211, -0.0513, -0.1124,  ..., -0.1648,  0.1582,  0.0432],\n",
      "         [ 0.2675, -0.0598,  0.2738,  ...,  0.4031, -0.1226, -0.0768]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-1.2185e-01,  6.1146e-03, -2.0991e-03,  ..., -5.9223e-01,\n",
      "           2.7877e-01, -1.9512e-03],\n",
      "         [-2.2320e-01, -1.8990e-01,  2.5268e-01,  ..., -5.0821e-01,\n",
      "          -1.6087e-01,  1.0863e-01],\n",
      "         [-1.4256e-01,  2.3381e-01, -1.4161e-01,  ...,  1.7343e-01,\n",
      "          -2.5711e-01, -1.6399e-01],\n",
      "         ...,\n",
      "         [ 1.0542e+00,  4.8060e-05,  3.6982e-01,  ...,  2.6432e-01,\n",
      "          -3.1553e-01,  9.9773e-02],\n",
      "         [ 2.4888e-01,  1.7164e-01, -2.0123e-01,  ..., -3.8663e-02,\n",
      "           1.9841e-01,  1.0416e-01],\n",
      "         [ 2.9805e-01, -9.4715e-02,  2.3048e-01,  ...,  5.5665e-01,\n",
      "          -5.0430e-02,  1.1386e-01]]], device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-1.2185e-01,  6.1146e-03, -2.0991e-03,  ..., -5.9223e-01,\n",
      "           2.7877e-01, -1.9512e-03],\n",
      "         [-2.2320e-01, -1.8990e-01,  2.5268e-01,  ..., -5.0821e-01,\n",
      "          -1.6087e-01,  1.0863e-01],\n",
      "         [-1.4256e-01,  2.3381e-01, -1.4161e-01,  ...,  1.7343e-01,\n",
      "          -2.5711e-01, -1.6399e-01],\n",
      "         ...,\n",
      "         [ 1.0542e+00,  4.8060e-05,  3.6982e-01,  ...,  2.6432e-01,\n",
      "          -3.1553e-01,  9.9773e-02],\n",
      "         [ 2.4888e-01,  1.7164e-01, -2.0123e-01,  ..., -3.8663e-02,\n",
      "           1.9841e-01,  1.0416e-01],\n",
      "         [ 2.9805e-01, -9.4715e-02,  2.3048e-01,  ...,  5.5665e-01,\n",
      "          -5.0430e-02,  1.1386e-01]]], device='cuda:0'),) and output (tensor([[[-0.1067,  0.0272,  0.0495,  ..., -0.5550,  0.3449,  0.0266],\n",
      "         [-0.2810, -0.2540,  0.3511,  ..., -0.3212, -0.1531,  0.0849],\n",
      "         [-0.0918,  0.1632, -0.0953,  ...,  0.3118, -0.2543, -0.3889],\n",
      "         ...,\n",
      "         [ 1.2057,  0.2388,  0.7313,  ..., -0.0107, -0.2662, -0.0601],\n",
      "         [ 0.0868, -0.0030, -0.3566,  ..., -0.0203,  0.0927,  0.4072],\n",
      "         [ 0.4237,  0.1495,  0.1140,  ...,  0.4520, -0.2587,  0.0969]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.1067,  0.0272,  0.0495,  ..., -0.5550,  0.3449,  0.0266],\n",
      "         [-0.2810, -0.2540,  0.3511,  ..., -0.3212, -0.1531,  0.0849],\n",
      "         [-0.0918,  0.1632, -0.0953,  ...,  0.3118, -0.2543, -0.3889],\n",
      "         ...,\n",
      "         [ 1.2057,  0.2388,  0.7313,  ..., -0.0107, -0.2662, -0.0601],\n",
      "         [ 0.0868, -0.0030, -0.3566,  ..., -0.0203,  0.0927,  0.4072],\n",
      "         [ 0.4237,  0.1495,  0.1140,  ...,  0.4520, -0.2587,  0.0969]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.0592,  0.0731,  0.0500,  ..., -0.5517,  0.3541,  0.0595],\n",
      "         [-0.4079, -0.1713,  0.4739,  ..., -0.3808, -0.0206,  0.1279],\n",
      "         [-0.3272,  0.1859, -0.0968,  ...,  0.3286, -0.0792, -0.5001],\n",
      "         ...,\n",
      "         [ 1.1926,  0.1908,  0.7188,  ...,  0.0911, -0.3199,  0.2670],\n",
      "         [-0.0680, -0.2643, -0.0790,  ...,  0.1783, -0.0159,  0.3461],\n",
      "         [ 0.5280,  0.1291,  0.2934,  ...,  0.3097,  0.0041,  0.1398]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.0592,  0.0731,  0.0500,  ..., -0.5517,  0.3541,  0.0595],\n",
      "         [-0.4079, -0.1713,  0.4739,  ..., -0.3808, -0.0206,  0.1279],\n",
      "         [-0.3272,  0.1859, -0.0968,  ...,  0.3286, -0.0792, -0.5001],\n",
      "         ...,\n",
      "         [ 1.1926,  0.1908,  0.7188,  ...,  0.0911, -0.3199,  0.2670],\n",
      "         [-0.0680, -0.2643, -0.0790,  ...,  0.1783, -0.0159,  0.3461],\n",
      "         [ 0.5280,  0.1291,  0.2934,  ...,  0.3097,  0.0041,  0.1398]]],\n",
      "       device='cuda:0'),) and output (tensor([[[ 0.1143,  0.2237,  0.3292,  ..., -0.7706,  0.2608,  0.0269],\n",
      "         [-0.3157, -0.3818,  0.5091,  ..., -0.1029, -0.0855,  0.0829],\n",
      "         [-0.1259,  0.0724, -0.0782,  ...,  0.1688, -0.3216, -0.9696],\n",
      "         ...,\n",
      "         [ 1.2780,  0.1048,  0.4299,  ..., -0.0605, -0.6046,  0.1986],\n",
      "         [ 0.1556, -0.4153, -0.5052,  ...,  0.2656, -0.4159,  0.3938],\n",
      "         [ 0.7336,  0.1130,  0.4287,  ...,  0.5395, -0.1553,  0.0888]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[ 0.1143,  0.2237,  0.3292,  ..., -0.7706,  0.2608,  0.0269],\n",
      "         [-0.3157, -0.3818,  0.5091,  ..., -0.1029, -0.0855,  0.0829],\n",
      "         [-0.1259,  0.0724, -0.0782,  ...,  0.1688, -0.3216, -0.9696],\n",
      "         ...,\n",
      "         [ 1.2780,  0.1048,  0.4299,  ..., -0.0605, -0.6046,  0.1986],\n",
      "         [ 0.1556, -0.4153, -0.5052,  ...,  0.2656, -0.4159,  0.3938],\n",
      "         [ 0.7336,  0.1130,  0.4287,  ...,  0.5395, -0.1553,  0.0888]]],\n",
      "       device='cuda:0'),) and output (tensor([[[ 3.7106e-01,  5.4886e-01,  6.1002e-01,  ..., -7.4991e-01,\n",
      "           2.0802e-01,  4.1948e-01],\n",
      "         [-4.2719e-01, -8.1186e-01,  3.1214e-01,  ..., -1.9185e-01,\n",
      "           2.1823e-01, -4.4610e-02],\n",
      "         [-3.6207e-01,  9.0653e-02, -2.2264e-01,  ...,  8.8551e-02,\n",
      "          -1.8944e-01, -1.2557e+00],\n",
      "         ...,\n",
      "         [ 1.3846e+00, -3.3100e-02,  2.5388e-01,  ..., -1.2499e-01,\n",
      "          -6.4583e-01,  3.4565e-02],\n",
      "         [ 5.7149e-01, -3.8845e-01, -5.5261e-01,  ...,  2.6886e-01,\n",
      "          -6.1933e-02,  2.2690e-01],\n",
      "         [ 4.4359e-01, -5.2075e-01,  3.1875e-01,  ...,  6.1126e-01,\n",
      "          -7.4312e-04, -1.6454e-03]]], device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[ 3.7106e-01,  5.4886e-01,  6.1002e-01,  ..., -7.4991e-01,\n",
      "           2.0802e-01,  4.1948e-01],\n",
      "         [-4.2719e-01, -8.1186e-01,  3.1214e-01,  ..., -1.9185e-01,\n",
      "           2.1823e-01, -4.4610e-02],\n",
      "         [-3.6207e-01,  9.0653e-02, -2.2264e-01,  ...,  8.8551e-02,\n",
      "          -1.8944e-01, -1.2557e+00],\n",
      "         ...,\n",
      "         [ 1.3846e+00, -3.3100e-02,  2.5388e-01,  ..., -1.2499e-01,\n",
      "          -6.4583e-01,  3.4565e-02],\n",
      "         [ 5.7149e-01, -3.8845e-01, -5.5261e-01,  ...,  2.6886e-01,\n",
      "          -6.1933e-02,  2.2690e-01],\n",
      "         [ 4.4359e-01, -5.2075e-01,  3.1875e-01,  ...,  6.1126e-01,\n",
      "          -7.4312e-04, -1.6454e-03]]], device='cuda:0'),) and output (tensor([[[ 1.1544,  3.2965,  1.6534,  ..., -2.7009,  2.6553,  2.2700],\n",
      "         [-0.5646,  0.2101,  0.2321,  ...,  0.3202,  0.5989,  0.9006],\n",
      "         [-0.0796,  0.8279, -0.2558,  ..., -0.2976, -0.8222, -1.0227],\n",
      "         ...,\n",
      "         [ 1.3525,  0.2213, -0.0850,  ..., -0.1790, -0.8484,  0.0159],\n",
      "         [ 1.2468,  0.0259, -0.8980,  ...,  0.5971, -1.0295,  0.9572],\n",
      "         [ 0.2934, -1.3119,  0.6361,  ...,  1.5715, -0.3171,  0.7818]]],\n",
      "       device='cuda:0'),)\n",
      "[Debug] Layer names being passed: ['model.layers.0', 'model.layers.1', 'model.layers.2', 'model.layers.3', 'model.layers.4', 'model.layers.5', 'model.layers.6', 'model.layers.7', 'model.layers.8', 'model.layers.9', 'model.layers.10', 'model.layers.11', 'model.layers.12', 'model.layers.13', 'model.layers.14', 'model.layers.15', 'model.layers.16', 'model.layers.17', 'model.layers.18', 'model.layers.19', 'model.layers.20', 'model.layers.21', 'model.layers.22', 'model.layers.23', 'model.layers.24', 'model.layers.25', 'model.layers.26', 'model.layers.27', 'model.layers.28', 'model.layers.29', 'model.layers.30', 'model.layers.31', 'model.embed_tokens']\n",
      "[Debug] Trying to access layer: model.layers.0\n",
      "[Debug] Successfully found layer: model.layers.0\n",
      "[Debug] Trying to access layer: model.layers.1\n",
      "[Debug] Successfully found layer: model.layers.1\n",
      "[Debug] Trying to access layer: model.layers.2\n",
      "[Debug] Successfully found layer: model.layers.2\n",
      "[Debug] Trying to access layer: model.layers.3\n",
      "[Debug] Successfully found layer: model.layers.3\n",
      "[Debug] Trying to access layer: model.layers.4\n",
      "[Debug] Successfully found layer: model.layers.4\n",
      "[Debug] Trying to access layer: model.layers.5\n",
      "[Debug] Successfully found layer: model.layers.5\n",
      "[Debug] Trying to access layer: model.layers.6\n",
      "[Debug] Successfully found layer: model.layers.6\n",
      "[Debug] Trying to access layer: model.layers.7\n",
      "[Debug] Successfully found layer: model.layers.7\n",
      "[Debug] Trying to access layer: model.layers.8\n",
      "[Debug] Successfully found layer: model.layers.8\n",
      "[Debug] Trying to access layer: model.layers.9\n",
      "[Debug] Successfully found layer: model.layers.9\n",
      "[Debug] Trying to access layer: model.layers.10\n",
      "[Debug] Successfully found layer: model.layers.10\n",
      "[Debug] Trying to access layer: model.layers.11\n",
      "[Debug] Successfully found layer: model.layers.11\n",
      "[Debug] Trying to access layer: model.layers.12\n",
      "[Debug] Successfully found layer: model.layers.12\n",
      "[Debug] Trying to access layer: model.layers.13\n",
      "[Debug] Successfully found layer: model.layers.13\n",
      "[Debug] Trying to access layer: model.layers.14\n",
      "[Debug] Successfully found layer: model.layers.14\n",
      "[Debug] Trying to access layer: model.layers.15\n",
      "[Debug] Successfully found layer: model.layers.15\n",
      "[Debug] Trying to access layer: model.layers.16\n",
      "[Debug] Successfully found layer: model.layers.16\n",
      "[Debug] Trying to access layer: model.layers.17\n",
      "[Debug] Successfully found layer: model.layers.17\n",
      "[Debug] Trying to access layer: model.layers.18\n",
      "[Debug] Successfully found layer: model.layers.18\n",
      "[Debug] Trying to access layer: model.layers.19\n",
      "[Debug] Successfully found layer: model.layers.19\n",
      "[Debug] Trying to access layer: model.layers.20\n",
      "[Debug] Successfully found layer: model.layers.20\n",
      "[Debug] Trying to access layer: model.layers.21\n",
      "[Debug] Successfully found layer: model.layers.21\n",
      "[Debug] Trying to access layer: model.layers.22\n",
      "[Debug] Successfully found layer: model.layers.22\n",
      "[Debug] Trying to access layer: model.layers.23\n",
      "[Debug] Successfully found layer: model.layers.23\n",
      "[Debug] Trying to access layer: model.layers.24\n",
      "[Debug] Successfully found layer: model.layers.24\n",
      "[Debug] Trying to access layer: model.layers.25\n",
      "[Debug] Successfully found layer: model.layers.25\n",
      "[Debug] Trying to access layer: model.layers.26\n",
      "[Debug] Successfully found layer: model.layers.26\n",
      "[Debug] Trying to access layer: model.layers.27\n",
      "[Debug] Successfully found layer: model.layers.27\n",
      "[Debug] Trying to access layer: model.layers.28\n",
      "[Debug] Successfully found layer: model.layers.28\n",
      "[Debug] Trying to access layer: model.layers.29\n",
      "[Debug] Successfully found layer: model.layers.29\n",
      "[Debug] Trying to access layer: model.layers.30\n",
      "[Debug] Successfully found layer: model.layers.30\n",
      "[Debug] Trying to access layer: model.layers.31\n",
      "[Debug] Successfully found layer: model.layers.31\n",
      "[Debug] Trying to access layer: model.embed_tokens\n",
      "[Debug] Successfully found layer: model.embed_tokens\n",
      "[Hook] Layer Embedding(128256, 4096) received input (tensor([[128000,    845,    339,  35769,    315,    279,  11842,  65473,   1571,\n",
      "          29098,    578,  30048,  44660,    315,   1403,  38846,   4455,     25,\n",
      "            264,    330,  48195,  64239,  30155,      1,    389,    220,   1627,\n",
      "            323,    220,   1544,   6287,    220,    679,     17,     11,    323,\n",
      "            264,    330,   6349,   1601,    532,  30155,      1,    389,    220,\n",
      "           1591,    323,    220,   1682,   6287,    220,    679,     17,     13,\n",
      "            578,   6164,  30048,   3952,   2035,    389,    220,    966,    323,\n",
      "            220,   2148,   6287,   8032,     20,   1483,     22,     60,  33589,\n",
      "           4900,  65552,    386,  74187,  19073,  23415,    279,  32858,    315,\n",
      "            279,  11842,  65473,   1571,  29098,    320,     45,   1428,      8,\n",
      "            311,  28501,   4900,  94286,  55417,    483,  63186,     11,   2391,\n",
      "            279,  54559,  22260,    315,  28986,      6,  30155,   8032,     23,\n",
      "             60,  10471,    690,   3412,    279,    452,   1428,  32858,    369,\n",
      "           3116,   1667,   3156,    279,    220,   1114,    339,  30048,    304,\n",
      "          37003,    304,    220,    679,     21,     13]], device='cuda:0'),) and output tensor([[[ 2.6512e-04, -4.9973e-04, -5.8365e-04,  ...,  3.8147e-03,\n",
      "           6.3419e-05,  1.1902e-03],\n",
      "         [-3.9978e-03,  2.5024e-03,  6.7139e-03,  ...,  1.1353e-02,\n",
      "           5.8899e-03, -1.8921e-03],\n",
      "         [-1.6251e-03, -4.5471e-03,  6.6223e-03,  ...,  3.2654e-03,\n",
      "          -4.2114e-03, -8.4229e-03],\n",
      "         ...,\n",
      "         [ 2.4872e-03, -1.5411e-03, -4.8218e-03,  ...,  4.9438e-03,\n",
      "           5.5542e-03, -4.2725e-03],\n",
      "         [ 4.9133e-03,  4.5776e-03,  1.6251e-03,  ...,  1.2573e-02,\n",
      "           1.4343e-03,  2.9755e-03],\n",
      "         [-6.7520e-04, -5.7602e-04,  4.7684e-04,  ...,  3.3264e-03,\n",
      "           3.9101e-04,  4.7493e-04]]], device='cuda:0')\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[ 2.6512e-04, -4.9973e-04, -5.8365e-04,  ...,  3.8147e-03,\n",
      "           6.3419e-05,  1.1902e-03],\n",
      "         [-3.9978e-03,  2.5024e-03,  6.7139e-03,  ...,  1.1353e-02,\n",
      "           5.8899e-03, -1.8921e-03],\n",
      "         [-1.6251e-03, -4.5471e-03,  6.6223e-03,  ...,  3.2654e-03,\n",
      "          -4.2114e-03, -8.4229e-03],\n",
      "         ...,\n",
      "         [ 2.4872e-03, -1.5411e-03, -4.8218e-03,  ...,  4.9438e-03,\n",
      "           5.5542e-03, -4.2725e-03],\n",
      "         [ 4.9133e-03,  4.5776e-03,  1.6251e-03,  ...,  1.2573e-02,\n",
      "           1.4343e-03,  2.9755e-03],\n",
      "         [-6.7520e-04, -5.7602e-04,  4.7684e-04,  ...,  3.3264e-03,\n",
      "           3.9101e-04,  4.7493e-04]]], device='cuda:0'),) and output (tensor([[[-4.3565e-04,  1.4559e-02, -2.0488e-03,  ...,  6.5782e-03,\n",
      "          -1.9362e-02,  1.9588e-03],\n",
      "         [ 1.1635e-03,  8.4424e-03,  4.0573e-03,  ..., -1.7606e-02,\n",
      "          -8.6813e-03, -1.0300e-02],\n",
      "         [-1.9096e-02, -5.5479e-03,  2.3948e-04,  ..., -4.0118e-02,\n",
      "           1.7579e-02,  2.8774e-03],\n",
      "         ...,\n",
      "         [ 3.1204e-03, -8.9022e-03, -1.0858e-02,  ..., -3.1138e-02,\n",
      "           8.8437e-03,  5.2561e-03],\n",
      "         [ 1.6766e-02,  1.4342e-02,  5.2227e-04,  ..., -2.4985e-02,\n",
      "           3.2598e-02,  2.1022e-02],\n",
      "         [ 1.4282e-03,  3.5781e-03,  9.0919e-05,  ..., -5.1442e-03,\n",
      "           2.2099e-04,  4.8044e-03]]], device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-4.3565e-04,  1.4559e-02, -2.0488e-03,  ...,  6.5782e-03,\n",
      "          -1.9362e-02,  1.9588e-03],\n",
      "         [ 1.1635e-03,  8.4424e-03,  4.0573e-03,  ..., -1.7606e-02,\n",
      "          -8.6813e-03, -1.0300e-02],\n",
      "         [-1.9096e-02, -5.5479e-03,  2.3948e-04,  ..., -4.0118e-02,\n",
      "           1.7579e-02,  2.8774e-03],\n",
      "         ...,\n",
      "         [ 3.1204e-03, -8.9022e-03, -1.0858e-02,  ..., -3.1138e-02,\n",
      "           8.8437e-03,  5.2561e-03],\n",
      "         [ 1.6766e-02,  1.4342e-02,  5.2227e-04,  ..., -2.4985e-02,\n",
      "           3.2598e-02,  2.1022e-02],\n",
      "         [ 1.4282e-03,  3.5781e-03,  9.0919e-05,  ..., -5.1442e-03,\n",
      "           2.2099e-04,  4.8044e-03]]], device='cuda:0'),) and output (tensor([[[-0.0972,  0.0787, -0.0419,  ...,  0.0071,  0.0869,  0.0218],\n",
      "         [-0.0232, -0.0157,  0.0195,  ..., -0.0582, -0.0073, -0.0200],\n",
      "         [-0.0157,  0.0041,  0.0142,  ..., -0.0883, -0.0235,  0.0101],\n",
      "         ...,\n",
      "         [-0.0066, -0.0109, -0.0001,  ..., -0.0308,  0.0034, -0.0041],\n",
      "         [ 0.0209,  0.0155,  0.0092,  ..., -0.0484,  0.0291,  0.0155],\n",
      "         [ 0.0009,  0.0049, -0.0063,  ...,  0.0211, -0.0173,  0.0003]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.0972,  0.0787, -0.0419,  ...,  0.0071,  0.0869,  0.0218],\n",
      "         [-0.0232, -0.0157,  0.0195,  ..., -0.0582, -0.0073, -0.0200],\n",
      "         [-0.0157,  0.0041,  0.0142,  ..., -0.0883, -0.0235,  0.0101],\n",
      "         ...,\n",
      "         [-0.0066, -0.0109, -0.0001,  ..., -0.0308,  0.0034, -0.0041],\n",
      "         [ 0.0209,  0.0155,  0.0092,  ..., -0.0484,  0.0291,  0.0155],\n",
      "         [ 0.0009,  0.0049, -0.0063,  ...,  0.0211, -0.0173,  0.0003]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.1040,  0.0875, -0.0360,  ...,  0.0205,  0.0997,  0.0184],\n",
      "         [-0.0185,  0.0171,  0.0548,  ..., -0.0685,  0.0281, -0.0503],\n",
      "         [-0.0179,  0.0207,  0.0114,  ..., -0.0406, -0.0394, -0.0426],\n",
      "         ...,\n",
      "         [-0.0050, -0.0387, -0.0084,  ..., -0.0823, -0.0115,  0.0129],\n",
      "         [ 0.0406, -0.0109,  0.0076,  ..., -0.0528,  0.0209,  0.0128],\n",
      "         [-0.0237,  0.0046, -0.0004,  ...,  0.0282, -0.0343,  0.0008]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.1040,  0.0875, -0.0360,  ...,  0.0205,  0.0997,  0.0184],\n",
      "         [-0.0185,  0.0171,  0.0548,  ..., -0.0685,  0.0281, -0.0503],\n",
      "         [-0.0179,  0.0207,  0.0114,  ..., -0.0406, -0.0394, -0.0426],\n",
      "         ...,\n",
      "         [-0.0050, -0.0387, -0.0084,  ..., -0.0823, -0.0115,  0.0129],\n",
      "         [ 0.0406, -0.0109,  0.0076,  ..., -0.0528,  0.0209,  0.0128],\n",
      "         [-0.0237,  0.0046, -0.0004,  ...,  0.0282, -0.0343,  0.0008]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.0870,  0.1049, -0.0173,  ...,  0.0700,  0.0989,  0.0181],\n",
      "         [-0.0332,  0.0760,  0.1044,  ..., -0.0596,  0.0267, -0.0620],\n",
      "         [-0.0267,  0.0596,  0.0336,  ..., -0.0552, -0.0240, -0.0479],\n",
      "         ...,\n",
      "         [-0.0122, -0.0167, -0.0269,  ..., -0.1256, -0.0186, -0.0294],\n",
      "         [ 0.0289, -0.0049, -0.0367,  ..., -0.0724,  0.0130,  0.0174],\n",
      "         [-0.0177,  0.0065, -0.0069,  ...,  0.0444, -0.0622,  0.0228]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.0870,  0.1049, -0.0173,  ...,  0.0700,  0.0989,  0.0181],\n",
      "         [-0.0332,  0.0760,  0.1044,  ..., -0.0596,  0.0267, -0.0620],\n",
      "         [-0.0267,  0.0596,  0.0336,  ..., -0.0552, -0.0240, -0.0479],\n",
      "         ...,\n",
      "         [-0.0122, -0.0167, -0.0269,  ..., -0.1256, -0.0186, -0.0294],\n",
      "         [ 0.0289, -0.0049, -0.0367,  ..., -0.0724,  0.0130,  0.0174],\n",
      "         [-0.0177,  0.0065, -0.0069,  ...,  0.0444, -0.0622,  0.0228]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.1113,  0.1022, -0.0102,  ...,  0.0436,  0.1283,  0.0321],\n",
      "         [ 0.0084,  0.0644,  0.0991,  ..., -0.0295,  0.0097,  0.0255],\n",
      "         [-0.0683,  0.0798,  0.0925,  ...,  0.0161, -0.0465,  0.0107],\n",
      "         ...,\n",
      "         [ 0.0231, -0.0200, -0.0110,  ..., -0.0917,  0.0603, -0.0154],\n",
      "         [ 0.0679,  0.0325, -0.0306,  ..., -0.0573,  0.0397, -0.0099],\n",
      "         [ 0.0118, -0.0029, -0.0310,  ...,  0.1117, -0.0288, -0.0046]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.1113,  0.1022, -0.0102,  ...,  0.0436,  0.1283,  0.0321],\n",
      "         [ 0.0084,  0.0644,  0.0991,  ..., -0.0295,  0.0097,  0.0255],\n",
      "         [-0.0683,  0.0798,  0.0925,  ...,  0.0161, -0.0465,  0.0107],\n",
      "         ...,\n",
      "         [ 0.0231, -0.0200, -0.0110,  ..., -0.0917,  0.0603, -0.0154],\n",
      "         [ 0.0679,  0.0325, -0.0306,  ..., -0.0573,  0.0397, -0.0099],\n",
      "         [ 0.0118, -0.0029, -0.0310,  ...,  0.1117, -0.0288, -0.0046]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.1034,  0.1132,  0.0096,  ...,  0.0453,  0.1477,  0.0301],\n",
      "         [ 0.0215,  0.0767,  0.0611,  ..., -0.0436, -0.0023,  0.1123],\n",
      "         [-0.0148,  0.0856,  0.0401,  ...,  0.0006, -0.0584,  0.0348],\n",
      "         ...,\n",
      "         [ 0.0223, -0.0123,  0.0713,  ..., -0.0968,  0.0068, -0.0116],\n",
      "         [ 0.0759,  0.1029,  0.0317,  ..., -0.0600,  0.0102,  0.0590],\n",
      "         [ 0.0094, -0.0048,  0.0227,  ...,  0.0794, -0.0517, -0.0057]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.1034,  0.1132,  0.0096,  ...,  0.0453,  0.1477,  0.0301],\n",
      "         [ 0.0215,  0.0767,  0.0611,  ..., -0.0436, -0.0023,  0.1123],\n",
      "         [-0.0148,  0.0856,  0.0401,  ...,  0.0006, -0.0584,  0.0348],\n",
      "         ...,\n",
      "         [ 0.0223, -0.0123,  0.0713,  ..., -0.0968,  0.0068, -0.0116],\n",
      "         [ 0.0759,  0.1029,  0.0317,  ..., -0.0600,  0.0102,  0.0590],\n",
      "         [ 0.0094, -0.0048,  0.0227,  ...,  0.0794, -0.0517, -0.0057]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.1071,  0.1253,  0.0211,  ...,  0.0126,  0.1586,  0.0278],\n",
      "         [-0.0849, -0.0135,  0.0039,  ..., -0.0778,  0.0189,  0.0606],\n",
      "         [-0.0950,  0.1287,  0.0078,  ..., -0.0706, -0.0143, -0.0513],\n",
      "         ...,\n",
      "         [ 0.0214, -0.0085,  0.0626,  ..., -0.0991, -0.0188, -0.0101],\n",
      "         [ 0.1139,  0.0329,  0.0367,  ...,  0.0218, -0.0380,  0.0207],\n",
      "         [ 0.0914,  0.0253,  0.0075,  ...,  0.0757, -0.0049, -0.0160]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.1071,  0.1253,  0.0211,  ...,  0.0126,  0.1586,  0.0278],\n",
      "         [-0.0849, -0.0135,  0.0039,  ..., -0.0778,  0.0189,  0.0606],\n",
      "         [-0.0950,  0.1287,  0.0078,  ..., -0.0706, -0.0143, -0.0513],\n",
      "         ...,\n",
      "         [ 0.0214, -0.0085,  0.0626,  ..., -0.0991, -0.0188, -0.0101],\n",
      "         [ 0.1139,  0.0329,  0.0367,  ...,  0.0218, -0.0380,  0.0207],\n",
      "         [ 0.0914,  0.0253,  0.0075,  ...,  0.0757, -0.0049, -0.0160]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.0945,  0.1848,  0.0036,  ..., -0.0473,  0.1489,  0.0536],\n",
      "         [-0.0011, -0.0747,  0.0233,  ..., -0.1189, -0.1403,  0.0592],\n",
      "         [-0.0698,  0.0803,  0.0316,  ..., -0.1224, -0.0749, -0.1203],\n",
      "         ...,\n",
      "         [ 0.0054, -0.0590,  0.1219,  ..., -0.0354,  0.0010, -0.0463],\n",
      "         [ 0.1008,  0.0120,  0.0646,  ...,  0.1019, -0.0061, -0.0048],\n",
      "         [ 0.0609, -0.0654, -0.0190,  ...,  0.1053, -0.0616, -0.0478]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.0945,  0.1848,  0.0036,  ..., -0.0473,  0.1489,  0.0536],\n",
      "         [-0.0011, -0.0747,  0.0233,  ..., -0.1189, -0.1403,  0.0592],\n",
      "         [-0.0698,  0.0803,  0.0316,  ..., -0.1224, -0.0749, -0.1203],\n",
      "         ...,\n",
      "         [ 0.0054, -0.0590,  0.1219,  ..., -0.0354,  0.0010, -0.0463],\n",
      "         [ 0.1008,  0.0120,  0.0646,  ...,  0.1019, -0.0061, -0.0048],\n",
      "         [ 0.0609, -0.0654, -0.0190,  ...,  0.1053, -0.0616, -0.0478]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.1060,  0.2113,  0.0270,  ..., -0.1141,  0.1610,  0.0528],\n",
      "         [ 0.0049, -0.0306,  0.0225,  ..., -0.1730, -0.1589,  0.0505],\n",
      "         [-0.0368,  0.1073,  0.0495,  ..., -0.0404, -0.0925, -0.1594],\n",
      "         ...,\n",
      "         [-0.0003,  0.0566,  0.1000,  ...,  0.0600,  0.0148, -0.0257],\n",
      "         [ 0.0709,  0.0714,  0.0149,  ...,  0.1184,  0.0565, -0.0602],\n",
      "         [ 0.0757, -0.0045, -0.0069,  ...,  0.0513, -0.0077, -0.0295]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.1060,  0.2113,  0.0270,  ..., -0.1141,  0.1610,  0.0528],\n",
      "         [ 0.0049, -0.0306,  0.0225,  ..., -0.1730, -0.1589,  0.0505],\n",
      "         [-0.0368,  0.1073,  0.0495,  ..., -0.0404, -0.0925, -0.1594],\n",
      "         ...,\n",
      "         [-0.0003,  0.0566,  0.1000,  ...,  0.0600,  0.0148, -0.0257],\n",
      "         [ 0.0709,  0.0714,  0.0149,  ...,  0.1184,  0.0565, -0.0602],\n",
      "         [ 0.0757, -0.0045, -0.0069,  ...,  0.0513, -0.0077, -0.0295]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.0954,  0.2493,  0.0073,  ..., -0.1568,  0.1818,  0.0855],\n",
      "         [-0.0403, -0.0340,  0.0431,  ..., -0.1582, -0.0793,  0.0991],\n",
      "         [-0.0170,  0.0733,  0.0587,  ..., -0.0529, -0.0709, -0.1771],\n",
      "         ...,\n",
      "         [-0.0885,  0.0521,  0.0944,  ...,  0.0705,  0.0228, -0.0385],\n",
      "         [-0.0410,  0.0646,  0.0797,  ...,  0.0921,  0.0931, -0.0064],\n",
      "         [ 0.0547,  0.0966,  0.0157,  ...,  0.0036, -0.0214, -0.0709]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.0954,  0.2493,  0.0073,  ..., -0.1568,  0.1818,  0.0855],\n",
      "         [-0.0403, -0.0340,  0.0431,  ..., -0.1582, -0.0793,  0.0991],\n",
      "         [-0.0170,  0.0733,  0.0587,  ..., -0.0529, -0.0709, -0.1771],\n",
      "         ...,\n",
      "         [-0.0885,  0.0521,  0.0944,  ...,  0.0705,  0.0228, -0.0385],\n",
      "         [-0.0410,  0.0646,  0.0797,  ...,  0.0921,  0.0931, -0.0064],\n",
      "         [ 0.0547,  0.0966,  0.0157,  ...,  0.0036, -0.0214, -0.0709]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-1.5196e-02,  3.3404e-01,  1.0956e-03,  ..., -2.7502e-01,\n",
      "           1.9330e-01,  1.0341e-01],\n",
      "         [ 5.3274e-03, -3.6416e-02,  4.3333e-02,  ..., -1.5034e-01,\n",
      "          -9.9045e-02,  1.4836e-01],\n",
      "         [-7.0727e-02, -1.0097e-02,  8.0068e-02,  ..., -1.3536e-01,\n",
      "          -8.3614e-02, -1.5552e-01],\n",
      "         ...,\n",
      "         [-1.3446e-01, -6.0594e-02,  4.9889e-02,  ...,  1.5916e-02,\n",
      "           6.5026e-02,  4.0764e-02],\n",
      "         [-2.9840e-02,  2.7318e-02,  9.9658e-02,  ...,  5.7107e-02,\n",
      "           1.4709e-01, -2.5999e-03],\n",
      "         [ 4.6229e-02,  5.3342e-02,  3.0716e-04,  ..., -2.3934e-02,\n",
      "          -5.4940e-02,  7.1206e-02]]], device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-1.5196e-02,  3.3404e-01,  1.0956e-03,  ..., -2.7502e-01,\n",
      "           1.9330e-01,  1.0341e-01],\n",
      "         [ 5.3274e-03, -3.6416e-02,  4.3333e-02,  ..., -1.5034e-01,\n",
      "          -9.9045e-02,  1.4836e-01],\n",
      "         [-7.0727e-02, -1.0097e-02,  8.0068e-02,  ..., -1.3536e-01,\n",
      "          -8.3614e-02, -1.5552e-01],\n",
      "         ...,\n",
      "         [-1.3446e-01, -6.0594e-02,  4.9889e-02,  ...,  1.5916e-02,\n",
      "           6.5026e-02,  4.0764e-02],\n",
      "         [-2.9840e-02,  2.7318e-02,  9.9658e-02,  ...,  5.7107e-02,\n",
      "           1.4709e-01, -2.5999e-03],\n",
      "         [ 4.6229e-02,  5.3342e-02,  3.0716e-04,  ..., -2.3934e-02,\n",
      "          -5.4940e-02,  7.1206e-02]]], device='cuda:0'),) and output (tensor([[[-0.0356,  0.3483, -0.0366,  ..., -0.2962,  0.2088,  0.1065],\n",
      "         [ 0.0050, -0.0427,  0.0450,  ..., -0.1105, -0.0323,  0.1209],\n",
      "         [-0.0928,  0.0215,  0.1093,  ..., -0.1585,  0.0315, -0.2167],\n",
      "         ...,\n",
      "         [-0.0261, -0.0704,  0.0302,  ...,  0.0169,  0.1339,  0.1694],\n",
      "         [ 0.1132, -0.0272,  0.1052,  ...,  0.1027,  0.1827,  0.0088],\n",
      "         [ 0.0589,  0.0101,  0.0789,  ..., -0.0886, -0.0355,  0.1190]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.0356,  0.3483, -0.0366,  ..., -0.2962,  0.2088,  0.1065],\n",
      "         [ 0.0050, -0.0427,  0.0450,  ..., -0.1105, -0.0323,  0.1209],\n",
      "         [-0.0928,  0.0215,  0.1093,  ..., -0.1585,  0.0315, -0.2167],\n",
      "         ...,\n",
      "         [-0.0261, -0.0704,  0.0302,  ...,  0.0169,  0.1339,  0.1694],\n",
      "         [ 0.1132, -0.0272,  0.1052,  ...,  0.1027,  0.1827,  0.0088],\n",
      "         [ 0.0589,  0.0101,  0.0789,  ..., -0.0886, -0.0355,  0.1190]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.0127,  0.3661, -0.0316,  ..., -0.3678,  0.1973,  0.1019],\n",
      "         [-0.0341, -0.1205,  0.0580,  ..., -0.1899, -0.1128,  0.1017],\n",
      "         [-0.0812,  0.0151,  0.1285,  ..., -0.1841,  0.0107, -0.2500],\n",
      "         ...,\n",
      "         [-0.1034, -0.0943, -0.0117,  ...,  0.0441,  0.0978,  0.1793],\n",
      "         [ 0.0525, -0.1099,  0.1704,  ...,  0.2604,  0.0591, -0.0059],\n",
      "         [ 0.0313,  0.0130,  0.0615,  ..., -0.0334, -0.1021,  0.0355]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.0127,  0.3661, -0.0316,  ..., -0.3678,  0.1973,  0.1019],\n",
      "         [-0.0341, -0.1205,  0.0580,  ..., -0.1899, -0.1128,  0.1017],\n",
      "         [-0.0812,  0.0151,  0.1285,  ..., -0.1841,  0.0107, -0.2500],\n",
      "         ...,\n",
      "         [-0.1034, -0.0943, -0.0117,  ...,  0.0441,  0.0978,  0.1793],\n",
      "         [ 0.0525, -0.1099,  0.1704,  ...,  0.2604,  0.0591, -0.0059],\n",
      "         [ 0.0313,  0.0130,  0.0615,  ..., -0.0334, -0.1021,  0.0355]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.0089,  0.3233, -0.0779,  ..., -0.4491,  0.1845,  0.0951],\n",
      "         [-0.0274, -0.0729,  0.0333,  ..., -0.1921, -0.1005,  0.0500],\n",
      "         [-0.0303,  0.0239,  0.1400,  ..., -0.1590, -0.0296, -0.2620],\n",
      "         ...,\n",
      "         [-0.0676, -0.1166, -0.1287,  ..., -0.0011,  0.0569,  0.1804],\n",
      "         [-0.1259, -0.2855,  0.0514,  ...,  0.1057, -0.1755,  0.0751],\n",
      "         [-0.0757,  0.0042,  0.0345,  ..., -0.1757, -0.2136,  0.1070]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.0089,  0.3233, -0.0779,  ..., -0.4491,  0.1845,  0.0951],\n",
      "         [-0.0274, -0.0729,  0.0333,  ..., -0.1921, -0.1005,  0.0500],\n",
      "         [-0.0303,  0.0239,  0.1400,  ..., -0.1590, -0.0296, -0.2620],\n",
      "         ...,\n",
      "         [-0.0676, -0.1166, -0.1287,  ..., -0.0011,  0.0569,  0.1804],\n",
      "         [-0.1259, -0.2855,  0.0514,  ...,  0.1057, -0.1755,  0.0751],\n",
      "         [-0.0757,  0.0042,  0.0345,  ..., -0.1757, -0.2136,  0.1070]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.0244,  0.2828, -0.0532,  ..., -0.4592,  0.2300,  0.0142],\n",
      "         [-0.0550, -0.0666,  0.0117,  ..., -0.0902, -0.1231, -0.0560],\n",
      "         [ 0.0509,  0.0193, -0.1025,  ..., -0.1350, -0.0319, -0.1977],\n",
      "         ...,\n",
      "         [-0.0646, -0.0192, -0.0535,  ...,  0.0448,  0.0807,  0.2053],\n",
      "         [-0.0992, -0.1777,  0.0858,  ...,  0.1098, -0.1855,  0.0803],\n",
      "         [-0.1087, -0.0381,  0.0628,  ..., -0.0692, -0.2698,  0.1698]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.0244,  0.2828, -0.0532,  ..., -0.4592,  0.2300,  0.0142],\n",
      "         [-0.0550, -0.0666,  0.0117,  ..., -0.0902, -0.1231, -0.0560],\n",
      "         [ 0.0509,  0.0193, -0.1025,  ..., -0.1350, -0.0319, -0.1977],\n",
      "         ...,\n",
      "         [-0.0646, -0.0192, -0.0535,  ...,  0.0448,  0.0807,  0.2053],\n",
      "         [-0.0992, -0.1777,  0.0858,  ...,  0.1098, -0.1855,  0.0803],\n",
      "         [-0.1087, -0.0381,  0.0628,  ..., -0.0692, -0.2698,  0.1698]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.0928,  0.1485,  0.0158,  ..., -0.5380,  0.2664, -0.0333],\n",
      "         [-0.0479, -0.0427,  0.0393,  ..., -0.1219, -0.0533, -0.0977],\n",
      "         [ 0.0732, -0.0558, -0.1322,  ..., -0.1065, -0.0156, -0.0959],\n",
      "         ...,\n",
      "         [-0.0290, -0.0994,  0.0984,  ...,  0.0850,  0.0920,  0.1351],\n",
      "         [-0.1284, -0.1583,  0.1888,  ...,  0.0851, -0.0693,  0.1390],\n",
      "         [-0.0510, -0.0799, -0.0251,  ..., -0.0103, -0.2867,  0.0133]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.0928,  0.1485,  0.0158,  ..., -0.5380,  0.2664, -0.0333],\n",
      "         [-0.0479, -0.0427,  0.0393,  ..., -0.1219, -0.0533, -0.0977],\n",
      "         [ 0.0732, -0.0558, -0.1322,  ..., -0.1065, -0.0156, -0.0959],\n",
      "         ...,\n",
      "         [-0.0290, -0.0994,  0.0984,  ...,  0.0850,  0.0920,  0.1351],\n",
      "         [-0.1284, -0.1583,  0.1888,  ...,  0.0851, -0.0693,  0.1390],\n",
      "         [-0.0510, -0.0799, -0.0251,  ..., -0.0103, -0.2867,  0.0133]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-9.9983e-02,  1.3233e-01,  2.4070e-02,  ..., -5.4702e-01,\n",
      "           2.8443e-01, -4.5672e-02],\n",
      "         [-5.1853e-02, -1.4201e-01,  3.2739e-02,  ..., -1.3126e-01,\n",
      "          -6.9364e-02, -5.4011e-02],\n",
      "         [ 1.7197e-01, -8.5607e-02, -2.2084e-01,  ..., -2.2059e-02,\n",
      "          -6.9906e-02, -1.7567e-01],\n",
      "         ...,\n",
      "         [-4.8801e-04, -1.6716e-01,  4.7590e-02,  ...,  1.4525e-01,\n",
      "           1.6176e-01,  2.2391e-03],\n",
      "         [ 1.2916e-02, -1.9252e-01,  1.8943e-01,  ...,  1.0772e-01,\n",
      "          -2.4045e-02,  4.4927e-03],\n",
      "         [-3.3408e-03, -4.0461e-02,  1.1122e-01,  ..., -5.6859e-03,\n",
      "          -3.4003e-01, -1.4367e-02]]], device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-9.9983e-02,  1.3233e-01,  2.4070e-02,  ..., -5.4702e-01,\n",
      "           2.8443e-01, -4.5672e-02],\n",
      "         [-5.1853e-02, -1.4201e-01,  3.2739e-02,  ..., -1.3126e-01,\n",
      "          -6.9364e-02, -5.4011e-02],\n",
      "         [ 1.7197e-01, -8.5607e-02, -2.2084e-01,  ..., -2.2059e-02,\n",
      "          -6.9906e-02, -1.7567e-01],\n",
      "         ...,\n",
      "         [-4.8801e-04, -1.6716e-01,  4.7590e-02,  ...,  1.4525e-01,\n",
      "           1.6176e-01,  2.2391e-03],\n",
      "         [ 1.2916e-02, -1.9252e-01,  1.8943e-01,  ...,  1.0772e-01,\n",
      "          -2.4045e-02,  4.4927e-03],\n",
      "         [-3.3408e-03, -4.0461e-02,  1.1122e-01,  ..., -5.6859e-03,\n",
      "          -3.4003e-01, -1.4367e-02]]], device='cuda:0'),) and output (tensor([[[-0.1286,  0.0833,  0.0239,  ..., -0.5657,  0.3694, -0.0725],\n",
      "         [-0.1243, -0.1082,  0.0716,  ..., -0.1425, -0.0528, -0.0856],\n",
      "         [ 0.1218, -0.1575, -0.2735,  ...,  0.0634, -0.0087, -0.1107],\n",
      "         ...,\n",
      "         [-0.0505, -0.2267, -0.1110,  ...,  0.0703,  0.0929,  0.0837],\n",
      "         [-0.0052, -0.2327,  0.1070,  ...,  0.1731,  0.0262, -0.0128],\n",
      "         [ 0.1456, -0.0761,  0.0681,  ...,  0.0410, -0.4577, -0.0790]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.1286,  0.0833,  0.0239,  ..., -0.5657,  0.3694, -0.0725],\n",
      "         [-0.1243, -0.1082,  0.0716,  ..., -0.1425, -0.0528, -0.0856],\n",
      "         [ 0.1218, -0.1575, -0.2735,  ...,  0.0634, -0.0087, -0.1107],\n",
      "         ...,\n",
      "         [-0.0505, -0.2267, -0.1110,  ...,  0.0703,  0.0929,  0.0837],\n",
      "         [-0.0052, -0.2327,  0.1070,  ...,  0.1731,  0.0262, -0.0128],\n",
      "         [ 0.1456, -0.0761,  0.0681,  ...,  0.0410, -0.4577, -0.0790]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.1417,  0.0703,  0.0343,  ..., -0.5549,  0.3175, -0.0847],\n",
      "         [-0.2147, -0.1796,  0.0381,  ..., -0.2936,  0.0966, -0.1351],\n",
      "         [ 0.0347, -0.2122, -0.2028,  ...,  0.0137,  0.1361, -0.1133],\n",
      "         ...,\n",
      "         [-0.1248, -0.2478, -0.0829,  ...,  0.1515,  0.0885,  0.0670],\n",
      "         [ 0.1279, -0.2123, -0.0115,  ...,  0.1444, -0.0654,  0.0825],\n",
      "         [ 0.0844, -0.0148,  0.0426,  ...,  0.0386, -0.4080, -0.2060]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.1417,  0.0703,  0.0343,  ..., -0.5549,  0.3175, -0.0847],\n",
      "         [-0.2147, -0.1796,  0.0381,  ..., -0.2936,  0.0966, -0.1351],\n",
      "         [ 0.0347, -0.2122, -0.2028,  ...,  0.0137,  0.1361, -0.1133],\n",
      "         ...,\n",
      "         [-0.1248, -0.2478, -0.0829,  ...,  0.1515,  0.0885,  0.0670],\n",
      "         [ 0.1279, -0.2123, -0.0115,  ...,  0.1444, -0.0654,  0.0825],\n",
      "         [ 0.0844, -0.0148,  0.0426,  ...,  0.0386, -0.4080, -0.2060]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.1528,  0.0725,  0.0609,  ..., -0.5624,  0.3103, -0.0293],\n",
      "         [-0.1850, -0.1413,  0.1426,  ..., -0.3026,  0.0772, -0.1447],\n",
      "         [ 0.0493, -0.2556, -0.1326,  ...,  0.0985,  0.0767,  0.0390],\n",
      "         ...,\n",
      "         [-0.0022, -0.2728,  0.0109,  ...,  0.1206,  0.0519,  0.0738],\n",
      "         [ 0.1182, -0.3238, -0.1510,  ...,  0.1202, -0.1183,  0.2100],\n",
      "         [ 0.1153, -0.1664,  0.1000,  ...,  0.0361, -0.3661, -0.2315]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.1528,  0.0725,  0.0609,  ..., -0.5624,  0.3103, -0.0293],\n",
      "         [-0.1850, -0.1413,  0.1426,  ..., -0.3026,  0.0772, -0.1447],\n",
      "         [ 0.0493, -0.2556, -0.1326,  ...,  0.0985,  0.0767,  0.0390],\n",
      "         ...,\n",
      "         [-0.0022, -0.2728,  0.0109,  ...,  0.1206,  0.0519,  0.0738],\n",
      "         [ 0.1182, -0.3238, -0.1510,  ...,  0.1202, -0.1183,  0.2100],\n",
      "         [ 0.1153, -0.1664,  0.1000,  ...,  0.0361, -0.3661, -0.2315]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.1559,  0.0520,  0.0566,  ..., -0.5805,  0.2614, -0.0032],\n",
      "         [-0.2223, -0.2140,  0.0991,  ..., -0.2259, -0.0172, -0.1400],\n",
      "         [ 0.0999, -0.2742, -0.2389,  ...,  0.2585,  0.0135,  0.0100],\n",
      "         ...,\n",
      "         [-0.1619, -0.3540,  0.0092,  ...,  0.3562, -0.0102, -0.0016],\n",
      "         [ 0.1833, -0.4127, -0.2103,  ...,  0.1632, -0.2038,  0.1933],\n",
      "         [ 0.2001, -0.1875, -0.0506,  ...,  0.0823, -0.4011, -0.1861]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.1559,  0.0520,  0.0566,  ..., -0.5805,  0.2614, -0.0032],\n",
      "         [-0.2223, -0.2140,  0.0991,  ..., -0.2259, -0.0172, -0.1400],\n",
      "         [ 0.0999, -0.2742, -0.2389,  ...,  0.2585,  0.0135,  0.0100],\n",
      "         ...,\n",
      "         [-0.1619, -0.3540,  0.0092,  ...,  0.3562, -0.0102, -0.0016],\n",
      "         [ 0.1833, -0.4127, -0.2103,  ...,  0.1632, -0.2038,  0.1933],\n",
      "         [ 0.2001, -0.1875, -0.0506,  ...,  0.0823, -0.4011, -0.1861]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.1488,  0.0445,  0.0765,  ..., -0.5456,  0.2702,  0.0098],\n",
      "         [-0.2869, -0.1989,  0.0317,  ..., -0.0078, -0.0607, -0.0795],\n",
      "         [ 0.1468, -0.3958, -0.3412,  ...,  0.3049,  0.1499,  0.0353],\n",
      "         ...,\n",
      "         [-0.1917, -0.2658,  0.1969,  ...,  0.2896,  0.0213,  0.0202],\n",
      "         [ 0.1382, -0.2653, -0.2426,  ...,  0.2347, -0.2511,  0.1275],\n",
      "         [ 0.1152, -0.1727, -0.0318,  ...,  0.0986, -0.4220, -0.1969]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.1488,  0.0445,  0.0765,  ..., -0.5456,  0.2702,  0.0098],\n",
      "         [-0.2869, -0.1989,  0.0317,  ..., -0.0078, -0.0607, -0.0795],\n",
      "         [ 0.1468, -0.3958, -0.3412,  ...,  0.3049,  0.1499,  0.0353],\n",
      "         ...,\n",
      "         [-0.1917, -0.2658,  0.1969,  ...,  0.2896,  0.0213,  0.0202],\n",
      "         [ 0.1382, -0.2653, -0.2426,  ...,  0.2347, -0.2511,  0.1275],\n",
      "         [ 0.1152, -0.1727, -0.0318,  ...,  0.0986, -0.4220, -0.1969]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.1268,  0.0333,  0.0935,  ..., -0.5320,  0.2795,  0.0006],\n",
      "         [-0.2065, -0.1447,  0.1076,  ...,  0.0059,  0.0699, -0.2087],\n",
      "         [ 0.0853, -0.3016, -0.4116,  ...,  0.1782,  0.3551,  0.0675],\n",
      "         ...,\n",
      "         [-0.2292, -0.1396,  0.2954,  ...,  0.1900, -0.1858,  0.0100],\n",
      "         [ 0.2076, -0.3020, -0.3762,  ...,  0.2398, -0.1293,  0.0945],\n",
      "         [ 0.2072, -0.2110, -0.1007,  ...,  0.2225, -0.3286, -0.1040]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.1268,  0.0333,  0.0935,  ..., -0.5320,  0.2795,  0.0006],\n",
      "         [-0.2065, -0.1447,  0.1076,  ...,  0.0059,  0.0699, -0.2087],\n",
      "         [ 0.0853, -0.3016, -0.4116,  ...,  0.1782,  0.3551,  0.0675],\n",
      "         ...,\n",
      "         [-0.2292, -0.1396,  0.2954,  ...,  0.1900, -0.1858,  0.0100],\n",
      "         [ 0.2076, -0.3020, -0.3762,  ...,  0.2398, -0.1293,  0.0945],\n",
      "         [ 0.2072, -0.2110, -0.1007,  ...,  0.2225, -0.3286, -0.1040]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.1355,  0.0227,  0.0554,  ..., -0.5473,  0.2470, -0.0229],\n",
      "         [-0.2117,  0.0224,  0.0628,  ...,  0.1427,  0.0531, -0.2544],\n",
      "         [ 0.1072, -0.1972, -0.3919,  ...,  0.4610,  0.6023,  0.1164],\n",
      "         ...,\n",
      "         [-0.2132, -0.0133,  0.1339,  ..., -0.0067, -0.3759, -0.0484],\n",
      "         [ 0.2807, -0.2979, -0.2844,  ...,  0.2608, -0.1431, -0.0704],\n",
      "         [ 0.1988, -0.1454,  0.0369,  ...,  0.3223, -0.2541, -0.0758]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.1355,  0.0227,  0.0554,  ..., -0.5473,  0.2470, -0.0229],\n",
      "         [-0.2117,  0.0224,  0.0628,  ...,  0.1427,  0.0531, -0.2544],\n",
      "         [ 0.1072, -0.1972, -0.3919,  ...,  0.4610,  0.6023,  0.1164],\n",
      "         ...,\n",
      "         [-0.2132, -0.0133,  0.1339,  ..., -0.0067, -0.3759, -0.0484],\n",
      "         [ 0.2807, -0.2979, -0.2844,  ...,  0.2608, -0.1431, -0.0704],\n",
      "         [ 0.1988, -0.1454,  0.0369,  ...,  0.3223, -0.2541, -0.0758]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.1350,  0.0163,  0.0576,  ..., -0.5555,  0.2435,  0.0051],\n",
      "         [-0.2857,  0.0201,  0.0374,  ...,  0.2918, -0.0793, -0.1817],\n",
      "         [ 0.0676, -0.0413, -0.3909,  ...,  0.5682,  0.7454,  0.1225],\n",
      "         ...,\n",
      "         [-0.2746, -0.1707,  0.0793,  ..., -0.0063, -0.2777,  0.0343],\n",
      "         [ 0.2698, -0.3203, -0.2299,  ...,  0.3126,  0.0158,  0.2043],\n",
      "         [ 0.2671, -0.1906, -0.1991,  ...,  0.4499, -0.0285, -0.1219]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.1350,  0.0163,  0.0576,  ..., -0.5555,  0.2435,  0.0051],\n",
      "         [-0.2857,  0.0201,  0.0374,  ...,  0.2918, -0.0793, -0.1817],\n",
      "         [ 0.0676, -0.0413, -0.3909,  ...,  0.5682,  0.7454,  0.1225],\n",
      "         ...,\n",
      "         [-0.2746, -0.1707,  0.0793,  ..., -0.0063, -0.2777,  0.0343],\n",
      "         [ 0.2698, -0.3203, -0.2299,  ...,  0.3126,  0.0158,  0.2043],\n",
      "         [ 0.2671, -0.1906, -0.1991,  ...,  0.4499, -0.0285, -0.1219]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.1301, -0.0029,  0.0279,  ..., -0.5537,  0.2277,  0.0164],\n",
      "         [-0.3716,  0.2348, -0.1142,  ...,  0.3101, -0.1222, -0.3303],\n",
      "         [ 0.1275,  0.0796, -0.3937,  ...,  0.6097,  0.9644,  0.2618],\n",
      "         ...,\n",
      "         [-0.3346,  0.1502, -0.1286,  ..., -0.0427, -0.5197, -0.0686],\n",
      "         [ 0.2866, -0.3487, -0.3430,  ...,  0.4521,  0.1935,  0.0872],\n",
      "         [ 0.2629, -0.3815, -0.0880,  ...,  0.5872,  0.0802, -0.1135]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.1301, -0.0029,  0.0279,  ..., -0.5537,  0.2277,  0.0164],\n",
      "         [-0.3716,  0.2348, -0.1142,  ...,  0.3101, -0.1222, -0.3303],\n",
      "         [ 0.1275,  0.0796, -0.3937,  ...,  0.6097,  0.9644,  0.2618],\n",
      "         ...,\n",
      "         [-0.3346,  0.1502, -0.1286,  ..., -0.0427, -0.5197, -0.0686],\n",
      "         [ 0.2866, -0.3487, -0.3430,  ...,  0.4521,  0.1935,  0.0872],\n",
      "         [ 0.2629, -0.3815, -0.0880,  ...,  0.5872,  0.0802, -0.1135]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.1219,  0.0061, -0.0021,  ..., -0.5922,  0.2788, -0.0020],\n",
      "         [-0.4066,  0.1578, -0.3644,  ...,  0.2719, -0.1309, -0.3285],\n",
      "         [ 0.2164,  0.0852, -0.4821,  ...,  0.7558,  1.0506,  0.2703],\n",
      "         ...,\n",
      "         [-0.2478,  0.2306, -0.2873,  ..., -0.1833, -0.4956,  0.0111],\n",
      "         [ 0.3773, -0.1802, -0.3950,  ...,  0.6968,  0.2309,  0.1566],\n",
      "         [ 0.2439, -0.5669, -0.1163,  ...,  0.7002,  0.2025, -0.1258]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.1219,  0.0061, -0.0021,  ..., -0.5922,  0.2788, -0.0020],\n",
      "         [-0.4066,  0.1578, -0.3644,  ...,  0.2719, -0.1309, -0.3285],\n",
      "         [ 0.2164,  0.0852, -0.4821,  ...,  0.7558,  1.0506,  0.2703],\n",
      "         ...,\n",
      "         [-0.2478,  0.2306, -0.2873,  ..., -0.1833, -0.4956,  0.0111],\n",
      "         [ 0.3773, -0.1802, -0.3950,  ...,  0.6968,  0.2309,  0.1566],\n",
      "         [ 0.2439, -0.5669, -0.1163,  ...,  0.7002,  0.2025, -0.1258]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.1067,  0.0272,  0.0495,  ..., -0.5550,  0.3449,  0.0266],\n",
      "         [-0.2457,  0.2012, -0.3508,  ...,  0.5533, -0.0294, -0.2936],\n",
      "         [ 0.2407, -0.2440, -0.4190,  ...,  0.8718,  0.8686,  0.4074],\n",
      "         ...,\n",
      "         [ 0.0144,  0.3584, -0.4229,  ..., -0.1169, -0.4806,  0.2663],\n",
      "         [ 0.3885, -0.1889, -0.5789,  ...,  0.8922,  0.3446,  0.3988],\n",
      "         [ 0.4730, -0.4142, -0.3746,  ...,  0.7819,  0.0311, -0.1895]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.1067,  0.0272,  0.0495,  ..., -0.5550,  0.3449,  0.0266],\n",
      "         [-0.2457,  0.2012, -0.3508,  ...,  0.5533, -0.0294, -0.2936],\n",
      "         [ 0.2407, -0.2440, -0.4190,  ...,  0.8718,  0.8686,  0.4074],\n",
      "         ...,\n",
      "         [ 0.0144,  0.3584, -0.4229,  ..., -0.1169, -0.4806,  0.2663],\n",
      "         [ 0.3885, -0.1889, -0.5789,  ...,  0.8922,  0.3446,  0.3988],\n",
      "         [ 0.4730, -0.4142, -0.3746,  ...,  0.7819,  0.0311, -0.1895]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.0592,  0.0731,  0.0500,  ..., -0.5517,  0.3541,  0.0595],\n",
      "         [-0.1491,  0.2906, -0.3473,  ...,  0.4304, -0.3498, -0.2285],\n",
      "         [ 0.2240, -0.5075, -0.3081,  ...,  0.7190,  0.7699,  0.4732],\n",
      "         ...,\n",
      "         [ 0.2419,  0.0755, -0.0893,  ..., -0.4113, -0.4185,  0.1114],\n",
      "         [ 0.2751, -0.7452, -0.5078,  ...,  0.9115,  0.4344,  0.0919],\n",
      "         [ 0.3379, -0.6071, -0.1954,  ...,  0.6804, -0.0035, -0.3947]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.0592,  0.0731,  0.0500,  ..., -0.5517,  0.3541,  0.0595],\n",
      "         [-0.1491,  0.2906, -0.3473,  ...,  0.4304, -0.3498, -0.2285],\n",
      "         [ 0.2240, -0.5075, -0.3081,  ...,  0.7190,  0.7699,  0.4732],\n",
      "         ...,\n",
      "         [ 0.2419,  0.0755, -0.0893,  ..., -0.4113, -0.4185,  0.1114],\n",
      "         [ 0.2751, -0.7452, -0.5078,  ...,  0.9115,  0.4344,  0.0919],\n",
      "         [ 0.3379, -0.6071, -0.1954,  ...,  0.6804, -0.0035, -0.3947]]],\n",
      "       device='cuda:0'),) and output (tensor([[[ 0.1143,  0.2237,  0.3292,  ..., -0.7706,  0.2608,  0.0269],\n",
      "         [ 0.3181,  0.2667, -0.3304,  ...,  0.2294, -0.3617, -0.4728],\n",
      "         [ 0.4612, -0.7076, -0.3262,  ...,  0.9085,  0.6731,  0.3346],\n",
      "         ...,\n",
      "         [ 0.3157, -0.2935, -0.3448,  ..., -0.0664, -0.8725,  0.0266],\n",
      "         [ 0.3707, -1.4412, -0.7335,  ...,  1.3002,  0.3079,  0.2248],\n",
      "         [ 0.3762, -1.1335, -0.4732,  ...,  0.8322, -0.1411, -0.6868]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[ 0.1143,  0.2237,  0.3292,  ..., -0.7706,  0.2608,  0.0269],\n",
      "         [ 0.3181,  0.2667, -0.3304,  ...,  0.2294, -0.3617, -0.4728],\n",
      "         [ 0.4612, -0.7076, -0.3262,  ...,  0.9085,  0.6731,  0.3346],\n",
      "         ...,\n",
      "         [ 0.3157, -0.2935, -0.3448,  ..., -0.0664, -0.8725,  0.0266],\n",
      "         [ 0.3707, -1.4412, -0.7335,  ...,  1.3002,  0.3079,  0.2248],\n",
      "         [ 0.3762, -1.1335, -0.4732,  ...,  0.8322, -0.1411, -0.6868]]],\n",
      "       device='cuda:0'),) and output (tensor([[[ 0.3711,  0.5489,  0.6100,  ..., -0.7499,  0.2080,  0.4195],\n",
      "         [ 0.6806, -0.2820, -0.8240,  ...,  0.0875, -0.0084, -1.0710],\n",
      "         [ 0.8947, -0.8220, -0.5250,  ...,  0.7518,  1.0155, -0.1589],\n",
      "         ...,\n",
      "         [ 0.2956, -0.3086, -0.4034,  ..., -0.5619, -0.9656, -0.0088],\n",
      "         [ 0.3080, -1.8236, -1.0771,  ...,  1.5790, -0.1248, -0.1863],\n",
      "         [-0.0127, -1.7071, -0.7022,  ...,  0.9683, -0.2851, -1.0891]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[ 0.3711,  0.5489,  0.6100,  ..., -0.7499,  0.2080,  0.4195],\n",
      "         [ 0.6806, -0.2820, -0.8240,  ...,  0.0875, -0.0084, -1.0710],\n",
      "         [ 0.8947, -0.8220, -0.5250,  ...,  0.7518,  1.0155, -0.1589],\n",
      "         ...,\n",
      "         [ 0.2956, -0.3086, -0.4034,  ..., -0.5619, -0.9656, -0.0088],\n",
      "         [ 0.3080, -1.8236, -1.0771,  ...,  1.5790, -0.1248, -0.1863],\n",
      "         [-0.0127, -1.7071, -0.7022,  ...,  0.9683, -0.2851, -1.0891]]],\n",
      "       device='cuda:0'),) and output (tensor([[[ 1.1544,  3.2965,  1.6534,  ..., -2.7009,  2.6553,  2.2700],\n",
      "         [ 1.3914, -0.3825, -1.4336,  ..., -0.5576,  0.2289, -1.3504],\n",
      "         [ 0.5423,  0.2028, -0.7454,  ...,  0.8412,  1.1451, -0.5720],\n",
      "         ...,\n",
      "         [-0.2723, -0.9695,  0.0807,  ..., -0.3314, -0.7611,  0.0987],\n",
      "         [ 0.2264, -2.1466, -1.6674,  ...,  3.3153, -2.3512,  1.0315],\n",
      "         [ 0.3983, -2.0386, -0.9145,  ...,  2.0270, -1.4769, -0.2283]]],\n",
      "       device='cuda:0'),)\n",
      "[Debug] Layer names being passed: ['model.layers.0', 'model.layers.1', 'model.layers.2', 'model.layers.3', 'model.layers.4', 'model.layers.5', 'model.layers.6', 'model.layers.7', 'model.layers.8', 'model.layers.9', 'model.layers.10', 'model.layers.11', 'model.layers.12', 'model.layers.13', 'model.layers.14', 'model.layers.15', 'model.layers.16', 'model.layers.17', 'model.layers.18', 'model.layers.19', 'model.layers.20', 'model.layers.21', 'model.layers.22', 'model.layers.23', 'model.layers.24', 'model.layers.25', 'model.layers.26', 'model.layers.27', 'model.layers.28', 'model.layers.29', 'model.layers.30', 'model.layers.31', 'model.embed_tokens']\n",
      "[Debug] Trying to access layer: model.layers.0\n",
      "[Debug] Successfully found layer: model.layers.0\n",
      "[Debug] Trying to access layer: model.layers.1\n",
      "[Debug] Successfully found layer: model.layers.1\n",
      "[Debug] Trying to access layer: model.layers.2\n",
      "[Debug] Successfully found layer: model.layers.2\n",
      "[Debug] Trying to access layer: model.layers.3\n",
      "[Debug] Successfully found layer: model.layers.3\n",
      "[Debug] Trying to access layer: model.layers.4\n",
      "[Debug] Successfully found layer: model.layers.4\n",
      "[Debug] Trying to access layer: model.layers.5\n",
      "[Debug] Successfully found layer: model.layers.5\n",
      "[Debug] Trying to access layer: model.layers.6\n",
      "[Debug] Successfully found layer: model.layers.6\n",
      "[Debug] Trying to access layer: model.layers.7\n",
      "[Debug] Successfully found layer: model.layers.7\n",
      "[Debug] Trying to access layer: model.layers.8\n",
      "[Debug] Successfully found layer: model.layers.8\n",
      "[Debug] Trying to access layer: model.layers.9\n",
      "[Debug] Successfully found layer: model.layers.9\n",
      "[Debug] Trying to access layer: model.layers.10\n",
      "[Debug] Successfully found layer: model.layers.10\n",
      "[Debug] Trying to access layer: model.layers.11\n",
      "[Debug] Successfully found layer: model.layers.11\n",
      "[Debug] Trying to access layer: model.layers.12\n",
      "[Debug] Successfully found layer: model.layers.12\n",
      "[Debug] Trying to access layer: model.layers.13\n",
      "[Debug] Successfully found layer: model.layers.13\n",
      "[Debug] Trying to access layer: model.layers.14\n",
      "[Debug] Successfully found layer: model.layers.14\n",
      "[Debug] Trying to access layer: model.layers.15\n",
      "[Debug] Successfully found layer: model.layers.15\n",
      "[Debug] Trying to access layer: model.layers.16\n",
      "[Debug] Successfully found layer: model.layers.16\n",
      "[Debug] Trying to access layer: model.layers.17\n",
      "[Debug] Successfully found layer: model.layers.17\n",
      "[Debug] Trying to access layer: model.layers.18\n",
      "[Debug] Successfully found layer: model.layers.18\n",
      "[Debug] Trying to access layer: model.layers.19\n",
      "[Debug] Successfully found layer: model.layers.19\n",
      "[Debug] Trying to access layer: model.layers.20\n",
      "[Debug] Successfully found layer: model.layers.20\n",
      "[Debug] Trying to access layer: model.layers.21\n",
      "[Debug] Successfully found layer: model.layers.21\n",
      "[Debug] Trying to access layer: model.layers.22\n",
      "[Debug] Successfully found layer: model.layers.22\n",
      "[Debug] Trying to access layer: model.layers.23\n",
      "[Debug] Successfully found layer: model.layers.23\n",
      "[Debug] Trying to access layer: model.layers.24\n",
      "[Debug] Successfully found layer: model.layers.24\n",
      "[Debug] Trying to access layer: model.layers.25\n",
      "[Debug] Successfully found layer: model.layers.25\n",
      "[Debug] Trying to access layer: model.layers.26\n",
      "[Debug] Successfully found layer: model.layers.26\n",
      "[Debug] Trying to access layer: model.layers.27\n",
      "[Debug] Successfully found layer: model.layers.27\n",
      "[Debug] Trying to access layer: model.layers.28\n",
      "[Debug] Successfully found layer: model.layers.28\n",
      "[Debug] Trying to access layer: model.layers.29\n",
      "[Debug] Successfully found layer: model.layers.29\n",
      "[Debug] Trying to access layer: model.layers.30\n",
      "[Debug] Successfully found layer: model.layers.30\n",
      "[Debug] Trying to access layer: model.layers.31\n",
      "[Debug] Successfully found layer: model.layers.31\n",
      "[Debug] Trying to access layer: model.embed_tokens\n",
      "[Debug] Successfully found layer: model.embed_tokens\n",
      "[Hook] Layer Embedding(128256, 4096) received input (tensor([[128000,  33731,    323,    279,  34823,  90849,  10771,    311,  12074,\n",
      "            520,    279,  23978,    304,  58814,    323,  81801,     11,    279,\n",
      "           3446,  44853,    810,   1109,    220,     20,     11,    931,   1667,\n",
      "           4227,     11,   3196,    389,    264,  24716,   5438,  62488,   3446,\n",
      "           1376,    902,    374,   1457,  21771,    555,  29036,   9761,   1705,\n",
      "            439,   7520,     52,    220,  16884,    578,  16576,  10699,    800,\n",
      "           1286,  64922,    596,  58248,   8032,     22,     60]],\n",
      "       device='cuda:0'),) and output tensor([[[ 2.6512e-04, -4.9973e-04, -5.8365e-04,  ...,  3.8147e-03,\n",
      "           6.3419e-05,  1.1902e-03],\n",
      "         [-6.4392e-03,  2.1820e-03,  5.8899e-03,  ..., -3.5400e-03,\n",
      "          -4.8828e-03,  1.5335e-03],\n",
      "         [-2.0218e-04, -3.8605e-03,  8.3160e-04,  ...,  5.1880e-04,\n",
      "           1.7853e-03,  5.4016e-03],\n",
      "         ...,\n",
      "         [ 2.1667e-03,  7.7515e-03,  1.2817e-02,  ...,  9.7656e-03,\n",
      "          -8.9111e-03, -1.9836e-03],\n",
      "         [-2.6093e-03,  4.0894e-03, -1.2493e-04,  ...,  1.5869e-02,\n",
      "           7.5989e-03,  7.7820e-03],\n",
      "         [ 3.2616e-04,  7.5531e-04,  3.9368e-03,  ...,  1.0132e-02,\n",
      "           6.0425e-03, -1.3542e-04]]], device='cuda:0')\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[ 2.6512e-04, -4.9973e-04, -5.8365e-04,  ...,  3.8147e-03,\n",
      "           6.3419e-05,  1.1902e-03],\n",
      "         [-6.4392e-03,  2.1820e-03,  5.8899e-03,  ..., -3.5400e-03,\n",
      "          -4.8828e-03,  1.5335e-03],\n",
      "         [-2.0218e-04, -3.8605e-03,  8.3160e-04,  ...,  5.1880e-04,\n",
      "           1.7853e-03,  5.4016e-03],\n",
      "         ...,\n",
      "         [ 2.1667e-03,  7.7515e-03,  1.2817e-02,  ...,  9.7656e-03,\n",
      "          -8.9111e-03, -1.9836e-03],\n",
      "         [-2.6093e-03,  4.0894e-03, -1.2493e-04,  ...,  1.5869e-02,\n",
      "           7.5989e-03,  7.7820e-03],\n",
      "         [ 3.2616e-04,  7.5531e-04,  3.9368e-03,  ...,  1.0132e-02,\n",
      "           6.0425e-03, -1.3542e-04]]], device='cuda:0'),) and output (tensor([[[-0.0004,  0.0146, -0.0020,  ...,  0.0066, -0.0194,  0.0020],\n",
      "         [-0.0114,  0.0114,  0.0048,  ..., -0.0059, -0.0299, -0.0098],\n",
      "         [-0.0041, -0.0013,  0.0173,  ..., -0.0076,  0.0058,  0.0131],\n",
      "         ...,\n",
      "         [-0.0120, -0.0005,  0.0145,  ...,  0.0137,  0.0079,  0.0023],\n",
      "         [-0.0145, -0.0104, -0.0016,  ...,  0.0196,  0.0076,  0.0127],\n",
      "         [-0.0107,  0.0116, -0.0087,  ..., -0.0023, -0.0023,  0.0113]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.0004,  0.0146, -0.0020,  ...,  0.0066, -0.0194,  0.0020],\n",
      "         [-0.0114,  0.0114,  0.0048,  ..., -0.0059, -0.0299, -0.0098],\n",
      "         [-0.0041, -0.0013,  0.0173,  ..., -0.0076,  0.0058,  0.0131],\n",
      "         ...,\n",
      "         [-0.0120, -0.0005,  0.0145,  ...,  0.0137,  0.0079,  0.0023],\n",
      "         [-0.0145, -0.0104, -0.0016,  ...,  0.0196,  0.0076,  0.0127],\n",
      "         [-0.0107,  0.0116, -0.0087,  ..., -0.0023, -0.0023,  0.0113]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.0972,  0.0787, -0.0419,  ...,  0.0071,  0.0869,  0.0218],\n",
      "         [-0.0057,  0.0148, -0.0119,  ..., -0.0330, -0.0324, -0.0157],\n",
      "         [-0.0247, -0.0037,  0.0318,  ...,  0.0004, -0.0088,  0.0426],\n",
      "         ...,\n",
      "         [-0.0209,  0.0048, -0.0010,  ..., -0.0156, -0.0135,  0.0153],\n",
      "         [-0.0333, -0.0189, -0.0198,  ...,  0.0054, -0.0082,  0.0023],\n",
      "         [-0.0171, -0.0218, -0.0112,  ..., -0.0241, -0.0151,  0.0179]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.0972,  0.0787, -0.0419,  ...,  0.0071,  0.0869,  0.0218],\n",
      "         [-0.0057,  0.0148, -0.0119,  ..., -0.0330, -0.0324, -0.0157],\n",
      "         [-0.0247, -0.0037,  0.0318,  ...,  0.0004, -0.0088,  0.0426],\n",
      "         ...,\n",
      "         [-0.0209,  0.0048, -0.0010,  ..., -0.0156, -0.0135,  0.0153],\n",
      "         [-0.0333, -0.0189, -0.0198,  ...,  0.0054, -0.0082,  0.0023],\n",
      "         [-0.0171, -0.0218, -0.0112,  ..., -0.0241, -0.0151,  0.0179]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.1040,  0.0875, -0.0360,  ...,  0.0205,  0.0997,  0.0184],\n",
      "         [ 0.0325,  0.0205, -0.0052,  ..., -0.0066, -0.0628, -0.0052],\n",
      "         [ 0.0054,  0.0243,  0.0184,  ...,  0.0503, -0.0109,  0.0485],\n",
      "         ...,\n",
      "         [-0.0121, -0.0041,  0.0213,  ...,  0.0183,  0.0081,  0.0297],\n",
      "         [-0.0430, -0.0053, -0.0406,  ..., -0.0441, -0.0211,  0.0216],\n",
      "         [-0.0348, -0.0171,  0.0009,  ..., -0.0165, -0.0398,  0.0201]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.1040,  0.0875, -0.0360,  ...,  0.0205,  0.0997,  0.0184],\n",
      "         [ 0.0325,  0.0205, -0.0052,  ..., -0.0066, -0.0628, -0.0052],\n",
      "         [ 0.0054,  0.0243,  0.0184,  ...,  0.0503, -0.0109,  0.0485],\n",
      "         ...,\n",
      "         [-0.0121, -0.0041,  0.0213,  ...,  0.0183,  0.0081,  0.0297],\n",
      "         [-0.0430, -0.0053, -0.0406,  ..., -0.0441, -0.0211,  0.0216],\n",
      "         [-0.0348, -0.0171,  0.0009,  ..., -0.0165, -0.0398,  0.0201]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.0870,  0.1049, -0.0173,  ...,  0.0700,  0.0989,  0.0181],\n",
      "         [ 0.0028,  0.0630, -0.0347,  ..., -0.0333, -0.0941, -0.0442],\n",
      "         [ 0.0075,  0.0544,  0.0628,  ...,  0.0840, -0.0231,  0.0591],\n",
      "         ...,\n",
      "         [-0.0478, -0.0283,  0.0133,  ...,  0.0282, -0.0419,  0.0251],\n",
      "         [-0.0407, -0.0269, -0.0253,  ..., -0.0465, -0.0191,  0.0517],\n",
      "         [-0.0676, -0.0072, -0.0141,  ..., -0.0203, -0.0700,  0.0647]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.0870,  0.1049, -0.0173,  ...,  0.0700,  0.0989,  0.0181],\n",
      "         [ 0.0028,  0.0630, -0.0347,  ..., -0.0333, -0.0941, -0.0442],\n",
      "         [ 0.0075,  0.0544,  0.0628,  ...,  0.0840, -0.0231,  0.0591],\n",
      "         ...,\n",
      "         [-0.0478, -0.0283,  0.0133,  ...,  0.0282, -0.0419,  0.0251],\n",
      "         [-0.0407, -0.0269, -0.0253,  ..., -0.0465, -0.0191,  0.0517],\n",
      "         [-0.0676, -0.0072, -0.0141,  ..., -0.0203, -0.0700,  0.0647]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.1113,  0.1022, -0.0102,  ...,  0.0436,  0.1283,  0.0321],\n",
      "         [-0.0439,  0.0840,  0.0259,  ..., -0.0137, -0.1078, -0.0067],\n",
      "         [-0.0335,  0.0395,  0.0721,  ...,  0.1120, -0.0313,  0.1021],\n",
      "         ...,\n",
      "         [-0.0258, -0.0212,  0.0042,  ...,  0.0687, -0.0820,  0.0768],\n",
      "         [-0.0667,  0.0058, -0.0164,  ..., -0.0129, -0.0651,  0.0931],\n",
      "         [-0.0310, -0.0361,  0.0188,  ...,  0.0053, -0.0183,  0.0503]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.1113,  0.1022, -0.0102,  ...,  0.0436,  0.1283,  0.0321],\n",
      "         [-0.0439,  0.0840,  0.0259,  ..., -0.0137, -0.1078, -0.0067],\n",
      "         [-0.0335,  0.0395,  0.0721,  ...,  0.1120, -0.0313,  0.1021],\n",
      "         ...,\n",
      "         [-0.0258, -0.0212,  0.0042,  ...,  0.0687, -0.0820,  0.0768],\n",
      "         [-0.0667,  0.0058, -0.0164,  ..., -0.0129, -0.0651,  0.0931],\n",
      "         [-0.0310, -0.0361,  0.0188,  ...,  0.0053, -0.0183,  0.0503]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.1034,  0.1132,  0.0096,  ...,  0.0453,  0.1477,  0.0301],\n",
      "         [ 0.0286,  0.0389,  0.0113,  ..., -0.0610, -0.0768,  0.0174],\n",
      "         [ 0.0106,  0.0312,  0.0907,  ...,  0.0574, -0.0168,  0.1151],\n",
      "         ...,\n",
      "         [-0.0466,  0.0389,  0.1216,  ...,  0.0745, -0.0932,  0.0438],\n",
      "         [-0.0793, -0.0236, -0.0396,  ..., -0.0203, -0.0029,  0.0902],\n",
      "         [-0.0753, -0.0781,  0.0370,  ...,  0.0596, -0.0292,  0.0343]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.1034,  0.1132,  0.0096,  ...,  0.0453,  0.1477,  0.0301],\n",
      "         [ 0.0286,  0.0389,  0.0113,  ..., -0.0610, -0.0768,  0.0174],\n",
      "         [ 0.0106,  0.0312,  0.0907,  ...,  0.0574, -0.0168,  0.1151],\n",
      "         ...,\n",
      "         [-0.0466,  0.0389,  0.1216,  ...,  0.0745, -0.0932,  0.0438],\n",
      "         [-0.0793, -0.0236, -0.0396,  ..., -0.0203, -0.0029,  0.0902],\n",
      "         [-0.0753, -0.0781,  0.0370,  ...,  0.0596, -0.0292,  0.0343]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.1071,  0.1253,  0.0211,  ...,  0.0126,  0.1586,  0.0278],\n",
      "         [ 0.0254,  0.0138, -0.0056,  ..., -0.0460, -0.0385, -0.0067],\n",
      "         [ 0.0140,  0.0422,  0.0276,  ...,  0.0670,  0.0242,  0.0970],\n",
      "         ...,\n",
      "         [-0.1205,  0.0877,  0.1191,  ...,  0.1595, -0.0927,  0.0374],\n",
      "         [-0.0903,  0.0690, -0.0274,  ..., -0.0815, -0.0389,  0.1043],\n",
      "         [-0.1166, -0.0697,  0.0736,  ...,  0.0775, -0.0451,  0.0216]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.1071,  0.1253,  0.0211,  ...,  0.0126,  0.1586,  0.0278],\n",
      "         [ 0.0254,  0.0138, -0.0056,  ..., -0.0460, -0.0385, -0.0067],\n",
      "         [ 0.0140,  0.0422,  0.0276,  ...,  0.0670,  0.0242,  0.0970],\n",
      "         ...,\n",
      "         [-0.1205,  0.0877,  0.1191,  ...,  0.1595, -0.0927,  0.0374],\n",
      "         [-0.0903,  0.0690, -0.0274,  ..., -0.0815, -0.0389,  0.1043],\n",
      "         [-0.1166, -0.0697,  0.0736,  ...,  0.0775, -0.0451,  0.0216]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.0945,  0.1848,  0.0036,  ..., -0.0473,  0.1489,  0.0536],\n",
      "         [ 0.0708,  0.0055, -0.0248,  ..., -0.1380, -0.1420,  0.0132],\n",
      "         [ 0.0287, -0.0266, -0.0300,  ..., -0.0286, -0.0718,  0.1367],\n",
      "         ...,\n",
      "         [-0.1097, -0.0117,  0.1240,  ...,  0.1440, -0.0342,  0.0093],\n",
      "         [-0.1590,  0.0076, -0.0259,  ...,  0.0070,  0.0088,  0.0971],\n",
      "         [-0.1280, -0.1090,  0.0104,  ...,  0.1014, -0.0602,  0.0329]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.0945,  0.1848,  0.0036,  ..., -0.0473,  0.1489,  0.0536],\n",
      "         [ 0.0708,  0.0055, -0.0248,  ..., -0.1380, -0.1420,  0.0132],\n",
      "         [ 0.0287, -0.0266, -0.0300,  ..., -0.0286, -0.0718,  0.1367],\n",
      "         ...,\n",
      "         [-0.1097, -0.0117,  0.1240,  ...,  0.1440, -0.0342,  0.0093],\n",
      "         [-0.1590,  0.0076, -0.0259,  ...,  0.0070,  0.0088,  0.0971],\n",
      "         [-0.1280, -0.1090,  0.0104,  ...,  0.1014, -0.0602,  0.0329]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.1060,  0.2113,  0.0270,  ..., -0.1141,  0.1610,  0.0528],\n",
      "         [ 0.0891,  0.0464,  0.0150,  ..., -0.1351, -0.1431, -0.0111],\n",
      "         [ 0.0775,  0.0387,  0.0323,  ..., -0.0299, -0.1450,  0.1292],\n",
      "         ...,\n",
      "         [-0.1375, -0.0495,  0.0633,  ...,  0.1731, -0.0167, -0.0986],\n",
      "         [-0.2742, -0.0506, -0.0417,  ..., -0.0236,  0.0903,  0.0885],\n",
      "         [-0.1331, -0.1703,  0.0073,  ...,  0.0349, -0.0380, -0.0168]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.1060,  0.2113,  0.0270,  ..., -0.1141,  0.1610,  0.0528],\n",
      "         [ 0.0891,  0.0464,  0.0150,  ..., -0.1351, -0.1431, -0.0111],\n",
      "         [ 0.0775,  0.0387,  0.0323,  ..., -0.0299, -0.1450,  0.1292],\n",
      "         ...,\n",
      "         [-0.1375, -0.0495,  0.0633,  ...,  0.1731, -0.0167, -0.0986],\n",
      "         [-0.2742, -0.0506, -0.0417,  ..., -0.0236,  0.0903,  0.0885],\n",
      "         [-0.1331, -0.1703,  0.0073,  ...,  0.0349, -0.0380, -0.0168]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.0954,  0.2493,  0.0073,  ..., -0.1568,  0.1818,  0.0855],\n",
      "         [ 0.0247,  0.0353,  0.0174,  ..., -0.1185, -0.1029, -0.0125],\n",
      "         [ 0.0435,  0.0314,  0.0048,  ..., -0.0146, -0.0881,  0.1430],\n",
      "         ...,\n",
      "         [-0.0529, -0.0343, -0.0603,  ...,  0.0604, -0.0111, -0.2280],\n",
      "         [-0.0942, -0.1981, -0.0253,  ..., -0.0769,  0.2136,  0.0582],\n",
      "         [-0.0273, -0.2063, -0.0129,  ..., -0.0326, -0.0223, -0.0626]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.0954,  0.2493,  0.0073,  ..., -0.1568,  0.1818,  0.0855],\n",
      "         [ 0.0247,  0.0353,  0.0174,  ..., -0.1185, -0.1029, -0.0125],\n",
      "         [ 0.0435,  0.0314,  0.0048,  ..., -0.0146, -0.0881,  0.1430],\n",
      "         ...,\n",
      "         [-0.0529, -0.0343, -0.0603,  ...,  0.0604, -0.0111, -0.2280],\n",
      "         [-0.0942, -0.1981, -0.0253,  ..., -0.0769,  0.2136,  0.0582],\n",
      "         [-0.0273, -0.2063, -0.0129,  ..., -0.0326, -0.0223, -0.0626]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.0152,  0.3340,  0.0011,  ..., -0.2750,  0.1933,  0.1034],\n",
      "         [ 0.1112,  0.0284,  0.0888,  ..., -0.1266, -0.1327,  0.0091],\n",
      "         [ 0.1119, -0.0331,  0.0282,  ..., -0.0177, -0.0641,  0.1553],\n",
      "         ...,\n",
      "         [-0.0650, -0.0722, -0.0034,  ...,  0.0068, -0.0079, -0.1394],\n",
      "         [-0.0947, -0.1055, -0.0570,  ..., -0.0358,  0.1123,  0.1464],\n",
      "         [ 0.0039, -0.1619, -0.0478,  ..., -0.0490, -0.0936,  0.0297]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.0152,  0.3340,  0.0011,  ..., -0.2750,  0.1933,  0.1034],\n",
      "         [ 0.1112,  0.0284,  0.0888,  ..., -0.1266, -0.1327,  0.0091],\n",
      "         [ 0.1119, -0.0331,  0.0282,  ..., -0.0177, -0.0641,  0.1553],\n",
      "         ...,\n",
      "         [-0.0650, -0.0722, -0.0034,  ...,  0.0068, -0.0079, -0.1394],\n",
      "         [-0.0947, -0.1055, -0.0570,  ..., -0.0358,  0.1123,  0.1464],\n",
      "         [ 0.0039, -0.1619, -0.0478,  ..., -0.0490, -0.0936,  0.0297]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.0356,  0.3483, -0.0366,  ..., -0.2962,  0.2088,  0.1065],\n",
      "         [ 0.0570, -0.0095,  0.0588,  ..., -0.1291, -0.0475,  0.0199],\n",
      "         [ 0.0531, -0.0386, -0.0229,  ..., -0.0248, -0.0052,  0.1940],\n",
      "         ...,\n",
      "         [-0.0693, -0.0243, -0.0703,  ...,  0.0572,  0.0393, -0.1185],\n",
      "         [-0.1061, -0.1205, -0.1362,  ..., -0.1397,  0.1551, -0.0258],\n",
      "         [-0.0115, -0.0697, -0.0184,  ..., -0.0107, -0.0703, -0.0279]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.0356,  0.3483, -0.0366,  ..., -0.2962,  0.2088,  0.1065],\n",
      "         [ 0.0570, -0.0095,  0.0588,  ..., -0.1291, -0.0475,  0.0199],\n",
      "         [ 0.0531, -0.0386, -0.0229,  ..., -0.0248, -0.0052,  0.1940],\n",
      "         ...,\n",
      "         [-0.0693, -0.0243, -0.0703,  ...,  0.0572,  0.0393, -0.1185],\n",
      "         [-0.1061, -0.1205, -0.1362,  ..., -0.1397,  0.1551, -0.0258],\n",
      "         [-0.0115, -0.0697, -0.0184,  ..., -0.0107, -0.0703, -0.0279]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.0127,  0.3661, -0.0316,  ..., -0.3678,  0.1973,  0.1019],\n",
      "         [ 0.0589, -0.0548,  0.1048,  ..., -0.1313, -0.0421,  0.0594],\n",
      "         [ 0.0200, -0.0937, -0.0241,  ...,  0.0123, -0.0453,  0.1546],\n",
      "         ...,\n",
      "         [-0.1415, -0.1666, -0.0607,  ...,  0.0840, -0.0193, -0.0497],\n",
      "         [-0.1168, -0.1303, -0.0135,  ..., -0.0942,  0.0323,  0.0201],\n",
      "         [-0.0924, -0.1302, -0.1002,  ...,  0.0276, -0.1466, -0.0936]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.0127,  0.3661, -0.0316,  ..., -0.3678,  0.1973,  0.1019],\n",
      "         [ 0.0589, -0.0548,  0.1048,  ..., -0.1313, -0.0421,  0.0594],\n",
      "         [ 0.0200, -0.0937, -0.0241,  ...,  0.0123, -0.0453,  0.1546],\n",
      "         ...,\n",
      "         [-0.1415, -0.1666, -0.0607,  ...,  0.0840, -0.0193, -0.0497],\n",
      "         [-0.1168, -0.1303, -0.0135,  ..., -0.0942,  0.0323,  0.0201],\n",
      "         [-0.0924, -0.1302, -0.1002,  ...,  0.0276, -0.1466, -0.0936]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.0089,  0.3233, -0.0779,  ..., -0.4491,  0.1845,  0.0951],\n",
      "         [ 0.0398, -0.0933,  0.0544,  ..., -0.1363,  0.0038,  0.0334],\n",
      "         [-0.0356, -0.0503, -0.0352,  ..., -0.0178,  0.0554,  0.2327],\n",
      "         ...,\n",
      "         [-0.1498, -0.1549,  0.0287,  ..., -0.0460, -0.0407,  0.0989],\n",
      "         [-0.1234, -0.0962,  0.0521,  ..., -0.2471, -0.0360,  0.0535],\n",
      "         [-0.0701, -0.1306,  0.0022,  ...,  0.0154, -0.2661, -0.0079]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.0089,  0.3233, -0.0779,  ..., -0.4491,  0.1845,  0.0951],\n",
      "         [ 0.0398, -0.0933,  0.0544,  ..., -0.1363,  0.0038,  0.0334],\n",
      "         [-0.0356, -0.0503, -0.0352,  ..., -0.0178,  0.0554,  0.2327],\n",
      "         ...,\n",
      "         [-0.1498, -0.1549,  0.0287,  ..., -0.0460, -0.0407,  0.0989],\n",
      "         [-0.1234, -0.0962,  0.0521,  ..., -0.2471, -0.0360,  0.0535],\n",
      "         [-0.0701, -0.1306,  0.0022,  ...,  0.0154, -0.2661, -0.0079]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.0244,  0.2828, -0.0532,  ..., -0.4592,  0.2300,  0.0142],\n",
      "         [ 0.0482, -0.1366, -0.0849,  ..., -0.0976, -0.0585,  0.0084],\n",
      "         [ 0.0036, -0.0747,  0.0538,  ...,  0.0021,  0.0749,  0.1723],\n",
      "         ...,\n",
      "         [-0.0799, -0.2652,  0.0684,  ..., -0.0908, -0.0155,  0.0883],\n",
      "         [-0.1997, -0.1905, -0.0327,  ..., -0.2684, -0.1281, -0.0590],\n",
      "         [-0.0630, -0.1737, -0.0825,  ..., -0.0125, -0.3690,  0.0937]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.0244,  0.2828, -0.0532,  ..., -0.4592,  0.2300,  0.0142],\n",
      "         [ 0.0482, -0.1366, -0.0849,  ..., -0.0976, -0.0585,  0.0084],\n",
      "         [ 0.0036, -0.0747,  0.0538,  ...,  0.0021,  0.0749,  0.1723],\n",
      "         ...,\n",
      "         [-0.0799, -0.2652,  0.0684,  ..., -0.0908, -0.0155,  0.0883],\n",
      "         [-0.1997, -0.1905, -0.0327,  ..., -0.2684, -0.1281, -0.0590],\n",
      "         [-0.0630, -0.1737, -0.0825,  ..., -0.0125, -0.3690,  0.0937]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-9.2775e-02,  1.4845e-01,  1.5798e-02,  ..., -5.3800e-01,\n",
      "           2.6636e-01, -3.3290e-02],\n",
      "         [-5.6649e-02, -2.0874e-01, -1.1564e-02,  ..., -8.7492e-02,\n",
      "           5.3820e-02,  1.5227e-02],\n",
      "         [-3.2511e-02, -9.6567e-02, -4.1099e-03,  ..., -3.1324e-04,\n",
      "           7.7354e-02,  1.2439e-01],\n",
      "         ...,\n",
      "         [ 1.1527e-01, -2.9506e-01,  1.6303e-01,  ..., -7.7215e-02,\n",
      "          -7.6886e-02,  3.1301e-02],\n",
      "         [-1.0489e-01, -2.1695e-01, -4.9370e-02,  ..., -2.6902e-01,\n",
      "          -1.2936e-01, -1.0763e-01],\n",
      "         [ 6.9056e-02, -1.3043e-01,  1.0863e-02,  ...,  3.4398e-02,\n",
      "          -4.8878e-01,  5.3811e-02]]], device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-9.2775e-02,  1.4845e-01,  1.5798e-02,  ..., -5.3800e-01,\n",
      "           2.6636e-01, -3.3290e-02],\n",
      "         [-5.6649e-02, -2.0874e-01, -1.1564e-02,  ..., -8.7492e-02,\n",
      "           5.3820e-02,  1.5227e-02],\n",
      "         [-3.2511e-02, -9.6567e-02, -4.1099e-03,  ..., -3.1324e-04,\n",
      "           7.7354e-02,  1.2439e-01],\n",
      "         ...,\n",
      "         [ 1.1527e-01, -2.9506e-01,  1.6303e-01,  ..., -7.7215e-02,\n",
      "          -7.6886e-02,  3.1301e-02],\n",
      "         [-1.0489e-01, -2.1695e-01, -4.9370e-02,  ..., -2.6902e-01,\n",
      "          -1.2936e-01, -1.0763e-01],\n",
      "         [ 6.9056e-02, -1.3043e-01,  1.0863e-02,  ...,  3.4398e-02,\n",
      "          -4.8878e-01,  5.3811e-02]]], device='cuda:0'),) and output (tensor([[[-0.1000,  0.1323,  0.0241,  ..., -0.5470,  0.2844, -0.0457],\n",
      "         [-0.1309, -0.2241, -0.0490,  ...,  0.0622, -0.0486,  0.0798],\n",
      "         [-0.1135, -0.0514, -0.0763,  ...,  0.1114,  0.0117,  0.1878],\n",
      "         ...,\n",
      "         [ 0.2577, -0.2160,  0.0630,  ..., -0.0389, -0.0379,  0.2201],\n",
      "         [-0.1430, -0.1522, -0.1664,  ..., -0.2032, -0.0703,  0.0212],\n",
      "         [ 0.2740, -0.1108,  0.0379,  ...,  0.2493, -0.4019, -0.0042]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.1000,  0.1323,  0.0241,  ..., -0.5470,  0.2844, -0.0457],\n",
      "         [-0.1309, -0.2241, -0.0490,  ...,  0.0622, -0.0486,  0.0798],\n",
      "         [-0.1135, -0.0514, -0.0763,  ...,  0.1114,  0.0117,  0.1878],\n",
      "         ...,\n",
      "         [ 0.2577, -0.2160,  0.0630,  ..., -0.0389, -0.0379,  0.2201],\n",
      "         [-0.1430, -0.1522, -0.1664,  ..., -0.2032, -0.0703,  0.0212],\n",
      "         [ 0.2740, -0.1108,  0.0379,  ...,  0.2493, -0.4019, -0.0042]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.1286,  0.0833,  0.0239,  ..., -0.5657,  0.3694, -0.0725],\n",
      "         [-0.2528, -0.0341,  0.0986,  ..., -0.0898, -0.0587,  0.0939],\n",
      "         [-0.0487, -0.0428, -0.1104,  ...,  0.0534,  0.0322,  0.1656],\n",
      "         ...,\n",
      "         [ 0.5293, -0.3046,  0.0494,  ..., -0.1340, -0.1053,  0.1180],\n",
      "         [-0.0486, -0.1742, -0.1647,  ..., -0.2410, -0.1863, -0.0226],\n",
      "         [ 0.2219, -0.1640,  0.0587,  ...,  0.1681, -0.5758, -0.0789]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.1286,  0.0833,  0.0239,  ..., -0.5657,  0.3694, -0.0725],\n",
      "         [-0.2528, -0.0341,  0.0986,  ..., -0.0898, -0.0587,  0.0939],\n",
      "         [-0.0487, -0.0428, -0.1104,  ...,  0.0534,  0.0322,  0.1656],\n",
      "         ...,\n",
      "         [ 0.5293, -0.3046,  0.0494,  ..., -0.1340, -0.1053,  0.1180],\n",
      "         [-0.0486, -0.1742, -0.1647,  ..., -0.2410, -0.1863, -0.0226],\n",
      "         [ 0.2219, -0.1640,  0.0587,  ...,  0.1681, -0.5758, -0.0789]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.1417,  0.0703,  0.0343,  ..., -0.5549,  0.3175, -0.0847],\n",
      "         [-0.3476, -0.0937,  0.1124,  ..., -0.0447, -0.1078,  0.1821],\n",
      "         [-0.0221,  0.0286, -0.2782,  ...,  0.0609, -0.1302,  0.2448],\n",
      "         ...,\n",
      "         [ 0.3815, -0.2336,  0.0381,  ..., -0.2035,  0.0899,  0.1228],\n",
      "         [-0.1721, -0.1221, -0.2228,  ..., -0.4223, -0.0922, -0.1832],\n",
      "         [ 0.1701, -0.0941, -0.0143,  ...,  0.1360, -0.5268, -0.2005]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.1417,  0.0703,  0.0343,  ..., -0.5549,  0.3175, -0.0847],\n",
      "         [-0.3476, -0.0937,  0.1124,  ..., -0.0447, -0.1078,  0.1821],\n",
      "         [-0.0221,  0.0286, -0.2782,  ...,  0.0609, -0.1302,  0.2448],\n",
      "         ...,\n",
      "         [ 0.3815, -0.2336,  0.0381,  ..., -0.2035,  0.0899,  0.1228],\n",
      "         [-0.1721, -0.1221, -0.2228,  ..., -0.4223, -0.0922, -0.1832],\n",
      "         [ 0.1701, -0.0941, -0.0143,  ...,  0.1360, -0.5268, -0.2005]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.1528,  0.0725,  0.0609,  ..., -0.5624,  0.3103, -0.0293],\n",
      "         [-0.3468, -0.1552,  0.2314,  ..., -0.0407, -0.0664,  0.3190],\n",
      "         [ 0.0597, -0.0380, -0.1851,  ...,  0.0809, -0.1507,  0.2311],\n",
      "         ...,\n",
      "         [ 0.3769, -0.2940,  0.0096,  ..., -0.1861,  0.2429, -0.0519],\n",
      "         [-0.1392, -0.1094, -0.2228,  ..., -0.5264,  0.0231, -0.1949],\n",
      "         [ 0.0949, -0.1667, -0.0570,  ...,  0.0743, -0.4794, -0.2262]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.1528,  0.0725,  0.0609,  ..., -0.5624,  0.3103, -0.0293],\n",
      "         [-0.3468, -0.1552,  0.2314,  ..., -0.0407, -0.0664,  0.3190],\n",
      "         [ 0.0597, -0.0380, -0.1851,  ...,  0.0809, -0.1507,  0.2311],\n",
      "         ...,\n",
      "         [ 0.3769, -0.2940,  0.0096,  ..., -0.1861,  0.2429, -0.0519],\n",
      "         [-0.1392, -0.1094, -0.2228,  ..., -0.5264,  0.0231, -0.1949],\n",
      "         [ 0.0949, -0.1667, -0.0570,  ...,  0.0743, -0.4794, -0.2262]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.1559,  0.0520,  0.0566,  ..., -0.5805,  0.2614, -0.0032],\n",
      "         [-0.3409, -0.1812,  0.2162,  ..., -0.0630, -0.0938,  0.4737],\n",
      "         [-0.0008, -0.0057, -0.2167,  ..., -0.1046, -0.0899,  0.4806],\n",
      "         ...,\n",
      "         [ 0.4121, -0.3301, -0.0821,  ..., -0.0209,  0.3206, -0.1690],\n",
      "         [-0.1873,  0.0080, -0.4639,  ..., -0.4682,  0.0169, -0.2189],\n",
      "         [ 0.1271, -0.3067, -0.1438,  ...,  0.1752, -0.4898, -0.1782]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.1559,  0.0520,  0.0566,  ..., -0.5805,  0.2614, -0.0032],\n",
      "         [-0.3409, -0.1812,  0.2162,  ..., -0.0630, -0.0938,  0.4737],\n",
      "         [-0.0008, -0.0057, -0.2167,  ..., -0.1046, -0.0899,  0.4806],\n",
      "         ...,\n",
      "         [ 0.4121, -0.3301, -0.0821,  ..., -0.0209,  0.3206, -0.1690],\n",
      "         [-0.1873,  0.0080, -0.4639,  ..., -0.4682,  0.0169, -0.2189],\n",
      "         [ 0.1271, -0.3067, -0.1438,  ...,  0.1752, -0.4898, -0.1782]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.1488,  0.0445,  0.0765,  ..., -0.5456,  0.2702,  0.0098],\n",
      "         [-0.3340, -0.1390,  0.2591,  ..., -0.0094, -0.2378,  0.6264],\n",
      "         [-0.0264,  0.1246, -0.1871,  ..., -0.1057, -0.1459,  0.4017],\n",
      "         ...,\n",
      "         [ 0.3085, -0.3824, -0.1274,  ...,  0.1155,  0.2494, -0.0608],\n",
      "         [-0.3497,  0.1222, -0.3719,  ..., -0.5053, -0.0398, -0.1384],\n",
      "         [ 0.1051, -0.2249,  0.0042,  ...,  0.1668, -0.5290, -0.1637]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.1488,  0.0445,  0.0765,  ..., -0.5456,  0.2702,  0.0098],\n",
      "         [-0.3340, -0.1390,  0.2591,  ..., -0.0094, -0.2378,  0.6264],\n",
      "         [-0.0264,  0.1246, -0.1871,  ..., -0.1057, -0.1459,  0.4017],\n",
      "         ...,\n",
      "         [ 0.3085, -0.3824, -0.1274,  ...,  0.1155,  0.2494, -0.0608],\n",
      "         [-0.3497,  0.1222, -0.3719,  ..., -0.5053, -0.0398, -0.1384],\n",
      "         [ 0.1051, -0.2249,  0.0042,  ...,  0.1668, -0.5290, -0.1637]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-1.2677e-01,  3.3335e-02,  9.3478e-02,  ..., -5.3204e-01,\n",
      "           2.7953e-01,  5.9797e-04],\n",
      "         [-3.8235e-01, -1.4862e-01,  2.7782e-01,  ..., -3.6675e-02,\n",
      "          -3.2798e-01,  6.4951e-01],\n",
      "         [ 6.5553e-02,  1.1509e-01, -1.2514e-01,  ..., -2.1509e-01,\n",
      "          -9.9851e-02,  3.9839e-01],\n",
      "         ...,\n",
      "         [ 3.4181e-01, -3.7999e-01, -1.3191e-01,  ...,  1.8753e-01,\n",
      "           1.7647e-01, -6.6395e-02],\n",
      "         [-2.5695e-01,  1.9184e-01, -2.8686e-01,  ..., -4.8380e-01,\n",
      "          -4.6380e-02, -2.5360e-01],\n",
      "         [ 1.5102e-01, -7.8654e-02,  1.6351e-01,  ...,  1.2423e-01,\n",
      "          -3.9035e-01, -1.1388e-01]]], device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-1.2677e-01,  3.3335e-02,  9.3478e-02,  ..., -5.3204e-01,\n",
      "           2.7953e-01,  5.9797e-04],\n",
      "         [-3.8235e-01, -1.4862e-01,  2.7782e-01,  ..., -3.6675e-02,\n",
      "          -3.2798e-01,  6.4951e-01],\n",
      "         [ 6.5553e-02,  1.1509e-01, -1.2514e-01,  ..., -2.1509e-01,\n",
      "          -9.9851e-02,  3.9839e-01],\n",
      "         ...,\n",
      "         [ 3.4181e-01, -3.7999e-01, -1.3191e-01,  ...,  1.8753e-01,\n",
      "           1.7647e-01, -6.6395e-02],\n",
      "         [-2.5695e-01,  1.9184e-01, -2.8686e-01,  ..., -4.8380e-01,\n",
      "          -4.6380e-02, -2.5360e-01],\n",
      "         [ 1.5102e-01, -7.8654e-02,  1.6351e-01,  ...,  1.2423e-01,\n",
      "          -3.9035e-01, -1.1388e-01]]], device='cuda:0'),) and output (tensor([[[-0.1355,  0.0227,  0.0554,  ..., -0.5473,  0.2470, -0.0229],\n",
      "         [-0.3956, -0.1122,  0.3374,  ..., -0.1388, -0.2907,  0.5349],\n",
      "         [ 0.1060,  0.2558, -0.1204,  ..., -0.3737,  0.0878,  0.3147],\n",
      "         ...,\n",
      "         [ 0.2785, -0.2675, -0.1476,  ...,  0.2119,  0.2262, -0.0592],\n",
      "         [-0.3195,  0.0877, -0.2838,  ..., -0.5359,  0.0512, -0.1994],\n",
      "         [ 0.2757, -0.0395,  0.1697,  ...,  0.1969, -0.3733, -0.0639]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.1355,  0.0227,  0.0554,  ..., -0.5473,  0.2470, -0.0229],\n",
      "         [-0.3956, -0.1122,  0.3374,  ..., -0.1388, -0.2907,  0.5349],\n",
      "         [ 0.1060,  0.2558, -0.1204,  ..., -0.3737,  0.0878,  0.3147],\n",
      "         ...,\n",
      "         [ 0.2785, -0.2675, -0.1476,  ...,  0.2119,  0.2262, -0.0592],\n",
      "         [-0.3195,  0.0877, -0.2838,  ..., -0.5359,  0.0512, -0.1994],\n",
      "         [ 0.2757, -0.0395,  0.1697,  ...,  0.1969, -0.3733, -0.0639]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.1350,  0.0163,  0.0576,  ..., -0.5555,  0.2435,  0.0051],\n",
      "         [-0.3819, -0.0926,  0.2660,  ..., -0.1231, -0.2469,  0.5790],\n",
      "         [ 0.1095,  0.2018, -0.2706,  ..., -0.3730,  0.1479,  0.5800],\n",
      "         ...,\n",
      "         [ 0.3390, -0.4073, -0.2863,  ...,  0.2302,  0.2991, -0.1429],\n",
      "         [-0.3749,  0.2236, -0.2374,  ..., -0.5348, -0.1018, -0.3128],\n",
      "         [ 0.3606, -0.0797,  0.1057,  ...,  0.3374, -0.4276,  0.0100]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.1350,  0.0163,  0.0576,  ..., -0.5555,  0.2435,  0.0051],\n",
      "         [-0.3819, -0.0926,  0.2660,  ..., -0.1231, -0.2469,  0.5790],\n",
      "         [ 0.1095,  0.2018, -0.2706,  ..., -0.3730,  0.1479,  0.5800],\n",
      "         ...,\n",
      "         [ 0.3390, -0.4073, -0.2863,  ...,  0.2302,  0.2991, -0.1429],\n",
      "         [-0.3749,  0.2236, -0.2374,  ..., -0.5348, -0.1018, -0.3128],\n",
      "         [ 0.3606, -0.0797,  0.1057,  ...,  0.3374, -0.4276,  0.0100]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-1.3006e-01, -2.8984e-03,  2.7912e-02,  ..., -5.5374e-01,\n",
      "           2.2766e-01,  1.6436e-02],\n",
      "         [-3.4164e-01, -1.9288e-01,  2.2457e-01,  ..., -9.4415e-02,\n",
      "          -1.7347e-01,  3.6338e-01],\n",
      "         [ 2.0363e-01,  7.4130e-02, -4.5921e-01,  ..., -3.6011e-01,\n",
      "           7.9223e-02,  3.9045e-01],\n",
      "         ...,\n",
      "         [ 3.1232e-01, -3.4047e-01,  7.6062e-03,  ...,  1.1794e-01,\n",
      "           2.2909e-01, -7.7561e-02],\n",
      "         [-4.4662e-01,  7.0402e-02, -1.6611e-01,  ..., -6.6410e-01,\n",
      "           6.0926e-04, -4.2065e-01],\n",
      "         [ 3.1550e-01, -5.1893e-02,  1.7541e-01,  ...,  2.7411e-01,\n",
      "          -4.6230e-01,  5.6500e-02]]], device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-1.3006e-01, -2.8984e-03,  2.7912e-02,  ..., -5.5374e-01,\n",
      "           2.2766e-01,  1.6436e-02],\n",
      "         [-3.4164e-01, -1.9288e-01,  2.2457e-01,  ..., -9.4415e-02,\n",
      "          -1.7347e-01,  3.6338e-01],\n",
      "         [ 2.0363e-01,  7.4130e-02, -4.5921e-01,  ..., -3.6011e-01,\n",
      "           7.9223e-02,  3.9045e-01],\n",
      "         ...,\n",
      "         [ 3.1232e-01, -3.4047e-01,  7.6062e-03,  ...,  1.1794e-01,\n",
      "           2.2909e-01, -7.7561e-02],\n",
      "         [-4.4662e-01,  7.0402e-02, -1.6611e-01,  ..., -6.6410e-01,\n",
      "           6.0926e-04, -4.2065e-01],\n",
      "         [ 3.1550e-01, -5.1893e-02,  1.7541e-01,  ...,  2.7411e-01,\n",
      "          -4.6230e-01,  5.6500e-02]]], device='cuda:0'),) and output (tensor([[[-1.2185e-01,  6.1148e-03, -2.0994e-03,  ..., -5.9223e-01,\n",
      "           2.7877e-01, -1.9514e-03],\n",
      "         [-3.2836e-01, -6.4895e-05,  1.6677e-01,  ...,  7.2438e-02,\n",
      "          -1.0604e-01,  4.2332e-01],\n",
      "         [ 3.7011e-01,  4.0326e-03, -3.4196e-01,  ..., -5.9383e-01,\n",
      "           1.5829e-01,  5.3514e-01],\n",
      "         ...,\n",
      "         [ 3.7124e-01, -2.6795e-01,  4.4698e-02,  ...,  2.7006e-01,\n",
      "           2.2638e-01, -5.4006e-02],\n",
      "         [-4.6977e-01,  2.7011e-02, -1.7372e-01,  ..., -6.2543e-01,\n",
      "           1.5754e-01, -3.3782e-01],\n",
      "         [ 3.2001e-01, -1.3806e-02,  2.1769e-01,  ...,  3.9993e-01,\n",
      "          -3.4366e-01,  5.8253e-02]]], device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-1.2185e-01,  6.1148e-03, -2.0994e-03,  ..., -5.9223e-01,\n",
      "           2.7877e-01, -1.9514e-03],\n",
      "         [-3.2836e-01, -6.4895e-05,  1.6677e-01,  ...,  7.2438e-02,\n",
      "          -1.0604e-01,  4.2332e-01],\n",
      "         [ 3.7011e-01,  4.0326e-03, -3.4196e-01,  ..., -5.9383e-01,\n",
      "           1.5829e-01,  5.3514e-01],\n",
      "         ...,\n",
      "         [ 3.7124e-01, -2.6795e-01,  4.4698e-02,  ...,  2.7006e-01,\n",
      "           2.2638e-01, -5.4006e-02],\n",
      "         [-4.6977e-01,  2.7011e-02, -1.7372e-01,  ..., -6.2543e-01,\n",
      "           1.5754e-01, -3.3782e-01],\n",
      "         [ 3.2001e-01, -1.3806e-02,  2.1769e-01,  ...,  3.9993e-01,\n",
      "          -3.4366e-01,  5.8253e-02]]], device='cuda:0'),) and output (tensor([[[-0.1067,  0.0272,  0.0495,  ..., -0.5550,  0.3449,  0.0266],\n",
      "         [-0.2263, -0.0910,  0.2343,  ...,  0.1674,  0.0279,  0.2381],\n",
      "         [ 0.2662, -0.0813, -0.4020,  ..., -0.4626, -0.0832,  0.5188],\n",
      "         ...,\n",
      "         [ 0.2017, -0.1560,  0.1062,  ...,  0.2935, -0.1521,  0.0840],\n",
      "         [-0.4621,  0.2136, -0.0880,  ..., -0.5714, -0.0393, -0.2429],\n",
      "         [ 0.1685, -0.1782,  0.1810,  ...,  0.3437, -0.6092,  0.0228]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.1067,  0.0272,  0.0495,  ..., -0.5550,  0.3449,  0.0266],\n",
      "         [-0.2263, -0.0910,  0.2343,  ...,  0.1674,  0.0279,  0.2381],\n",
      "         [ 0.2662, -0.0813, -0.4020,  ..., -0.4626, -0.0832,  0.5188],\n",
      "         ...,\n",
      "         [ 0.2017, -0.1560,  0.1062,  ...,  0.2935, -0.1521,  0.0840],\n",
      "         [-0.4621,  0.2136, -0.0880,  ..., -0.5714, -0.0393, -0.2429],\n",
      "         [ 0.1685, -0.1782,  0.1810,  ...,  0.3437, -0.6092,  0.0228]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.0592,  0.0731,  0.0500,  ..., -0.5517,  0.3541,  0.0595],\n",
      "         [-0.5974,  0.0706,  0.3566,  ...,  0.0952,  0.0330,  0.1969],\n",
      "         [ 0.1440, -0.1378, -0.4561,  ..., -0.2230, -0.0463,  0.1784],\n",
      "         ...,\n",
      "         [ 0.1245, -0.2655,  0.4232,  ...,  0.3674, -0.0991, -0.1428],\n",
      "         [-0.5114,  0.2577, -0.1506,  ..., -0.9402,  0.1123, -0.3704],\n",
      "         [-0.1422, -0.3161,  0.3742,  ...,  0.2786, -0.6390,  0.0384]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.0592,  0.0731,  0.0500,  ..., -0.5517,  0.3541,  0.0595],\n",
      "         [-0.5974,  0.0706,  0.3566,  ...,  0.0952,  0.0330,  0.1969],\n",
      "         [ 0.1440, -0.1378, -0.4561,  ..., -0.2230, -0.0463,  0.1784],\n",
      "         ...,\n",
      "         [ 0.1245, -0.2655,  0.4232,  ...,  0.3674, -0.0991, -0.1428],\n",
      "         [-0.5114,  0.2577, -0.1506,  ..., -0.9402,  0.1123, -0.3704],\n",
      "         [-0.1422, -0.3161,  0.3742,  ...,  0.2786, -0.6390,  0.0384]]],\n",
      "       device='cuda:0'),) and output (tensor([[[ 0.1143,  0.2237,  0.3292,  ..., -0.7706,  0.2608,  0.0269],\n",
      "         [-0.4173, -0.0953,  0.2047,  ..., -0.0104, -0.0170,  0.0384],\n",
      "         [ 0.4673, -0.5926, -0.4324,  ..., -0.1688, -0.0480,  0.0061],\n",
      "         ...,\n",
      "         [ 0.2107, -0.5953,  0.3518,  ...,  0.1729,  0.0882,  0.1315],\n",
      "         [-0.2447,  0.0539, -0.3146,  ..., -0.9800, -0.1587, -0.2004],\n",
      "         [ 0.0047, -0.3466,  0.3821,  ...,  0.3978, -0.7811,  0.1538]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[ 0.1143,  0.2237,  0.3292,  ..., -0.7706,  0.2608,  0.0269],\n",
      "         [-0.4173, -0.0953,  0.2047,  ..., -0.0104, -0.0170,  0.0384],\n",
      "         [ 0.4673, -0.5926, -0.4324,  ..., -0.1688, -0.0480,  0.0061],\n",
      "         ...,\n",
      "         [ 0.2107, -0.5953,  0.3518,  ...,  0.1729,  0.0882,  0.1315],\n",
      "         [-0.2447,  0.0539, -0.3146,  ..., -0.9800, -0.1587, -0.2004],\n",
      "         [ 0.0047, -0.3466,  0.3821,  ...,  0.3978, -0.7811,  0.1538]]],\n",
      "       device='cuda:0'),) and output (tensor([[[ 0.3711,  0.5489,  0.6100,  ..., -0.7499,  0.2080,  0.4195],\n",
      "         [-0.2543,  0.1464, -0.0604,  ..., -0.2533,  0.0780, -0.3696],\n",
      "         [ 0.3988, -0.6972, -1.1289,  ..., -0.4356,  0.0793, -0.6891],\n",
      "         ...,\n",
      "         [ 0.0038, -0.6282, -0.0846,  ...,  0.1215, -0.2365, -0.2902],\n",
      "         [-0.7776, -0.0743, -0.5076,  ..., -0.6932, -0.1646, -0.9592],\n",
      "         [-0.0724, -0.5981,  0.0809,  ...,  0.7413, -0.8021, -0.1242]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[ 0.3711,  0.5489,  0.6100,  ..., -0.7499,  0.2080,  0.4195],\n",
      "         [-0.2543,  0.1464, -0.0604,  ..., -0.2533,  0.0780, -0.3696],\n",
      "         [ 0.3988, -0.6972, -1.1289,  ..., -0.4356,  0.0793, -0.6891],\n",
      "         ...,\n",
      "         [ 0.0038, -0.6282, -0.0846,  ...,  0.1215, -0.2365, -0.2902],\n",
      "         [-0.7776, -0.0743, -0.5076,  ..., -0.6932, -0.1646, -0.9592],\n",
      "         [-0.0724, -0.5981,  0.0809,  ...,  0.7413, -0.8021, -0.1242]]],\n",
      "       device='cuda:0'),) and output (tensor([[[ 1.1544,  3.2965,  1.6534,  ..., -2.7009,  2.6553,  2.2700],\n",
      "         [-0.4047,  1.1024,  0.2272,  ..., -0.3268, -0.2346, -0.1542],\n",
      "         [ 0.1545, -0.4113, -1.6692,  ..., -0.0726, -0.3063, -0.6527],\n",
      "         ...,\n",
      "         [-0.6999, -1.6752, -0.0947,  ...,  0.0656,  0.1249,  0.6203],\n",
      "         [-1.0586, -0.7295, -0.7574,  ...,  0.0738, -1.2828, -0.1915],\n",
      "         [-0.6995, -1.4611,  0.3477,  ...,  1.9621, -0.5023,  0.7619]]],\n",
      "       device='cuda:0'),)\n",
      "[Debug] Layer names being passed: ['model.layers.0', 'model.layers.1', 'model.layers.2', 'model.layers.3', 'model.layers.4', 'model.layers.5', 'model.layers.6', 'model.layers.7', 'model.layers.8', 'model.layers.9', 'model.layers.10', 'model.layers.11', 'model.layers.12', 'model.layers.13', 'model.layers.14', 'model.layers.15', 'model.layers.16', 'model.layers.17', 'model.layers.18', 'model.layers.19', 'model.layers.20', 'model.layers.21', 'model.layers.22', 'model.layers.23', 'model.layers.24', 'model.layers.25', 'model.layers.26', 'model.layers.27', 'model.layers.28', 'model.layers.29', 'model.layers.30', 'model.layers.31', 'model.embed_tokens']\n",
      "[Debug] Trying to access layer: model.layers.0\n",
      "[Debug] Successfully found layer: model.layers.0\n",
      "[Debug] Trying to access layer: model.layers.1\n",
      "[Debug] Successfully found layer: model.layers.1\n",
      "[Debug] Trying to access layer: model.layers.2\n",
      "[Debug] Successfully found layer: model.layers.2\n",
      "[Debug] Trying to access layer: model.layers.3\n",
      "[Debug] Successfully found layer: model.layers.3\n",
      "[Debug] Trying to access layer: model.layers.4\n",
      "[Debug] Successfully found layer: model.layers.4\n",
      "[Debug] Trying to access layer: model.layers.5\n",
      "[Debug] Successfully found layer: model.layers.5\n",
      "[Debug] Trying to access layer: model.layers.6\n",
      "[Debug] Successfully found layer: model.layers.6\n",
      "[Debug] Trying to access layer: model.layers.7\n",
      "[Debug] Successfully found layer: model.layers.7\n",
      "[Debug] Trying to access layer: model.layers.8\n",
      "[Debug] Successfully found layer: model.layers.8\n",
      "[Debug] Trying to access layer: model.layers.9\n",
      "[Debug] Successfully found layer: model.layers.9\n",
      "[Debug] Trying to access layer: model.layers.10\n",
      "[Debug] Successfully found layer: model.layers.10\n",
      "[Debug] Trying to access layer: model.layers.11\n",
      "[Debug] Successfully found layer: model.layers.11\n",
      "[Debug] Trying to access layer: model.layers.12\n",
      "[Debug] Successfully found layer: model.layers.12\n",
      "[Debug] Trying to access layer: model.layers.13\n",
      "[Debug] Successfully found layer: model.layers.13\n",
      "[Debug] Trying to access layer: model.layers.14\n",
      "[Debug] Successfully found layer: model.layers.14\n",
      "[Debug] Trying to access layer: model.layers.15\n",
      "[Debug] Successfully found layer: model.layers.15\n",
      "[Debug] Trying to access layer: model.layers.16\n",
      "[Debug] Successfully found layer: model.layers.16\n",
      "[Debug] Trying to access layer: model.layers.17\n",
      "[Debug] Successfully found layer: model.layers.17\n",
      "[Debug] Trying to access layer: model.layers.18\n",
      "[Debug] Successfully found layer: model.layers.18\n",
      "[Debug] Trying to access layer: model.layers.19\n",
      "[Debug] Successfully found layer: model.layers.19\n",
      "[Debug] Trying to access layer: model.layers.20\n",
      "[Debug] Successfully found layer: model.layers.20\n",
      "[Debug] Trying to access layer: model.layers.21\n",
      "[Debug] Successfully found layer: model.layers.21\n",
      "[Debug] Trying to access layer: model.layers.22\n",
      "[Debug] Successfully found layer: model.layers.22\n",
      "[Debug] Trying to access layer: model.layers.23\n",
      "[Debug] Successfully found layer: model.layers.23\n",
      "[Debug] Trying to access layer: model.layers.24\n",
      "[Debug] Successfully found layer: model.layers.24\n",
      "[Debug] Trying to access layer: model.layers.25\n",
      "[Debug] Successfully found layer: model.layers.25\n",
      "[Debug] Trying to access layer: model.layers.26\n",
      "[Debug] Successfully found layer: model.layers.26\n",
      "[Debug] Trying to access layer: model.layers.27\n",
      "[Debug] Successfully found layer: model.layers.27\n",
      "[Debug] Trying to access layer: model.layers.28\n",
      "[Debug] Successfully found layer: model.layers.28\n",
      "[Debug] Trying to access layer: model.layers.29\n",
      "[Debug] Successfully found layer: model.layers.29\n",
      "[Debug] Trying to access layer: model.layers.30\n",
      "[Debug] Successfully found layer: model.layers.30\n",
      "[Debug] Trying to access layer: model.layers.31\n",
      "[Debug] Successfully found layer: model.layers.31\n",
      "[Debug] Trying to access layer: model.embed_tokens\n",
      "[Debug] Successfully found layer: model.embed_tokens\n",
      "[Hook] Layer Embedding(128256, 4096) received input (tensor([[128000,  43824,    315,  19152,  22425,   2740,     11,  13673,   6593,\n",
      "           8853,    617,   8272,    279,   8013,  14135,    555,   2389,  14782,\n",
      "            279,  12628,    315,  42090,    315,  19152,    323,  42090,    315,\n",
      "          48190,    320,   8578,   7497,      8,    311,   1202,  38581,  24797,\n",
      "            594,  20073,    279,   2316,    315,  19150,    315,  19152,    320,\n",
      "           6204,      8,    369,    872,   3495,   4967,   8547,     11,  79283,\n",
      "            311,    279,  30661,     11,    477,    369,    872,  99119,  10896,\n",
      "            988,     13,  10541,    279,   8857,    315,  13673,  13642,   7497,\n",
      "          12628,    617,   1027,  19560,   7620,   2533,    279,    220,   2550,\n",
      "             15,     82,     11,   1234,    279,   3766,  13673,  20143,   7174,\n",
      "          24686,    320,  93173,     37,      8,    814,  14958,  71974,    439,\n",
      "           9580,    220,     22,  42090,    596,  12628,   3871,    449,   1023,\n",
      "          41534,   7620,     13]], device='cuda:0'),) and output tensor([[[ 2.6512e-04, -4.9973e-04, -5.8365e-04,  ...,  3.8147e-03,\n",
      "           6.3419e-05,  1.1902e-03],\n",
      "         [-2.6611e-02, -1.4832e-02, -1.2329e-02,  ...,  6.3477e-03,\n",
      "          -1.2451e-02, -7.2327e-03],\n",
      "         [ 1.2817e-03,  9.1171e-04,  2.0905e-03,  ...,  1.6251e-03,\n",
      "           4.0894e-03, -4.0283e-03],\n",
      "         ...,\n",
      "         [ 6.4697e-03, -1.3062e-02,  2.6855e-03,  ...,  7.5073e-03,\n",
      "          -1.1841e-02,  1.6785e-03],\n",
      "         [ 1.0376e-03,  8.0566e-03, -8.6670e-03,  ...,  1.2634e-02,\n",
      "          -2.3956e-03,  1.5015e-02],\n",
      "         [-6.7520e-04, -5.7602e-04,  4.7684e-04,  ...,  3.3264e-03,\n",
      "           3.9101e-04,  4.7493e-04]]], device='cuda:0')\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[ 2.6512e-04, -4.9973e-04, -5.8365e-04,  ...,  3.8147e-03,\n",
      "           6.3419e-05,  1.1902e-03],\n",
      "         [-2.6611e-02, -1.4832e-02, -1.2329e-02,  ...,  6.3477e-03,\n",
      "          -1.2451e-02, -7.2327e-03],\n",
      "         [ 1.2817e-03,  9.1171e-04,  2.0905e-03,  ...,  1.6251e-03,\n",
      "           4.0894e-03, -4.0283e-03],\n",
      "         ...,\n",
      "         [ 6.4697e-03, -1.3062e-02,  2.6855e-03,  ...,  7.5073e-03,\n",
      "          -1.1841e-02,  1.6785e-03],\n",
      "         [ 1.0376e-03,  8.0566e-03, -8.6670e-03,  ...,  1.2634e-02,\n",
      "          -2.3956e-03,  1.5015e-02],\n",
      "         [-6.7520e-04, -5.7602e-04,  4.7684e-04,  ...,  3.3264e-03,\n",
      "           3.9101e-04,  4.7493e-04]]], device='cuda:0'),) and output (tensor([[[-4.3565e-04,  1.4559e-02, -2.0488e-03,  ...,  6.5782e-03,\n",
      "          -1.9362e-02,  1.9588e-03],\n",
      "         [-3.1056e-02, -8.7182e-03,  5.2548e-03,  ...,  1.8787e-02,\n",
      "          -2.4009e-02, -9.6664e-03],\n",
      "         [-6.3112e-03,  1.6263e-03,  1.4939e-02,  ..., -1.7951e-02,\n",
      "          -2.0311e-02,  2.1546e-02],\n",
      "         ...,\n",
      "         [ 7.9445e-03, -1.5119e-02,  1.8725e-02,  ..., -8.3949e-03,\n",
      "          -1.8097e-03,  1.3061e-02],\n",
      "         [-7.4576e-03,  3.6672e-02, -8.8698e-03,  ..., -2.3593e-02,\n",
      "           5.6582e-03,  1.5289e-02],\n",
      "         [ 5.6966e-04, -5.1608e-04,  6.0480e-03,  ...,  1.0109e-02,\n",
      "           7.9549e-03,  9.0700e-05]]], device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-4.3565e-04,  1.4559e-02, -2.0488e-03,  ...,  6.5782e-03,\n",
      "          -1.9362e-02,  1.9588e-03],\n",
      "         [-3.1056e-02, -8.7182e-03,  5.2548e-03,  ...,  1.8787e-02,\n",
      "          -2.4009e-02, -9.6664e-03],\n",
      "         [-6.3112e-03,  1.6263e-03,  1.4939e-02,  ..., -1.7951e-02,\n",
      "          -2.0311e-02,  2.1546e-02],\n",
      "         ...,\n",
      "         [ 7.9445e-03, -1.5119e-02,  1.8725e-02,  ..., -8.3949e-03,\n",
      "          -1.8097e-03,  1.3061e-02],\n",
      "         [-7.4576e-03,  3.6672e-02, -8.8698e-03,  ..., -2.3593e-02,\n",
      "           5.6582e-03,  1.5289e-02],\n",
      "         [ 5.6966e-04, -5.1608e-04,  6.0480e-03,  ...,  1.0109e-02,\n",
      "           7.9549e-03,  9.0700e-05]]], device='cuda:0'),) and output (tensor([[[-0.0972,  0.0787, -0.0419,  ...,  0.0071,  0.0869,  0.0218],\n",
      "         [-0.0384,  0.0046,  0.0027,  ...,  0.0268, -0.0204, -0.0098],\n",
      "         [-0.0403,  0.0030,  0.0444,  ..., -0.0075, -0.0031, -0.0070],\n",
      "         ...,\n",
      "         [ 0.0242, -0.0319,  0.0320,  ..., -0.0347, -0.0262,  0.0088],\n",
      "         [-0.0129,  0.0356, -0.0310,  ..., -0.1390, -0.0035,  0.0324],\n",
      "         [-0.0070,  0.0060, -0.0058,  ...,  0.0199, -0.0041,  0.0019]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.0972,  0.0787, -0.0419,  ...,  0.0071,  0.0869,  0.0218],\n",
      "         [-0.0384,  0.0046,  0.0027,  ...,  0.0268, -0.0204, -0.0098],\n",
      "         [-0.0403,  0.0030,  0.0444,  ..., -0.0075, -0.0031, -0.0070],\n",
      "         ...,\n",
      "         [ 0.0242, -0.0319,  0.0320,  ..., -0.0347, -0.0262,  0.0088],\n",
      "         [-0.0129,  0.0356, -0.0310,  ..., -0.1390, -0.0035,  0.0324],\n",
      "         [-0.0070,  0.0060, -0.0058,  ...,  0.0199, -0.0041,  0.0019]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.1040,  0.0875, -0.0360,  ...,  0.0205,  0.0997,  0.0184],\n",
      "         [-0.0505,  0.0215,  0.0053,  ...,  0.1235, -0.0311, -0.0171],\n",
      "         [-0.0233,  0.0461,  0.0541,  ...,  0.0744, -0.0296, -0.0250],\n",
      "         ...,\n",
      "         [ 0.0324, -0.0321,  0.0141,  ..., -0.0069, -0.0751,  0.0246],\n",
      "         [-0.0287,  0.0444, -0.0337,  ..., -0.0793, -0.0111,  0.0212],\n",
      "         [-0.0180,  0.0013,  0.0131,  ...,  0.0174, -0.0109,  0.0033]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.1040,  0.0875, -0.0360,  ...,  0.0205,  0.0997,  0.0184],\n",
      "         [-0.0505,  0.0215,  0.0053,  ...,  0.1235, -0.0311, -0.0171],\n",
      "         [-0.0233,  0.0461,  0.0541,  ...,  0.0744, -0.0296, -0.0250],\n",
      "         ...,\n",
      "         [ 0.0324, -0.0321,  0.0141,  ..., -0.0069, -0.0751,  0.0246],\n",
      "         [-0.0287,  0.0444, -0.0337,  ..., -0.0793, -0.0111,  0.0212],\n",
      "         [-0.0180,  0.0013,  0.0131,  ...,  0.0174, -0.0109,  0.0033]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.0870,  0.1049, -0.0173,  ...,  0.0700,  0.0989,  0.0181],\n",
      "         [-0.0509,  0.0546, -0.0274,  ...,  0.1569, -0.0159, -0.0343],\n",
      "         [ 0.0022,  0.0311,  0.0548,  ...,  0.1590,  0.0056, -0.0008],\n",
      "         ...,\n",
      "         [ 0.0358, -0.0731,  0.0077,  ..., -0.0510, -0.0413,  0.0447],\n",
      "         [-0.0364,  0.0676, -0.0203,  ..., -0.1212,  0.0066,  0.0802],\n",
      "         [-0.0357,  0.0156, -0.0131,  ...,  0.0261, -0.0406, -0.0093]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.0870,  0.1049, -0.0173,  ...,  0.0700,  0.0989,  0.0181],\n",
      "         [-0.0509,  0.0546, -0.0274,  ...,  0.1569, -0.0159, -0.0343],\n",
      "         [ 0.0022,  0.0311,  0.0548,  ...,  0.1590,  0.0056, -0.0008],\n",
      "         ...,\n",
      "         [ 0.0358, -0.0731,  0.0077,  ..., -0.0510, -0.0413,  0.0447],\n",
      "         [-0.0364,  0.0676, -0.0203,  ..., -0.1212,  0.0066,  0.0802],\n",
      "         [-0.0357,  0.0156, -0.0131,  ...,  0.0261, -0.0406, -0.0093]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.1113,  0.1022, -0.0102,  ...,  0.0436,  0.1283,  0.0321],\n",
      "         [-0.0374,  0.0407, -0.0212,  ...,  0.2103, -0.0579,  0.0573],\n",
      "         [-0.0047,  0.0126,  0.0848,  ...,  0.2130, -0.0484,  0.0885],\n",
      "         ...,\n",
      "         [ 0.0291, -0.0675, -0.0023,  ..., -0.0388, -0.0865,  0.0539],\n",
      "         [-0.0011,  0.0553,  0.0374,  ..., -0.1314, -0.0467,  0.0325],\n",
      "         [-0.0337, -0.0154,  0.0031,  ...,  0.0228, -0.0228, -0.0236]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.1113,  0.1022, -0.0102,  ...,  0.0436,  0.1283,  0.0321],\n",
      "         [-0.0374,  0.0407, -0.0212,  ...,  0.2103, -0.0579,  0.0573],\n",
      "         [-0.0047,  0.0126,  0.0848,  ...,  0.2130, -0.0484,  0.0885],\n",
      "         ...,\n",
      "         [ 0.0291, -0.0675, -0.0023,  ..., -0.0388, -0.0865,  0.0539],\n",
      "         [-0.0011,  0.0553,  0.0374,  ..., -0.1314, -0.0467,  0.0325],\n",
      "         [-0.0337, -0.0154,  0.0031,  ...,  0.0228, -0.0228, -0.0236]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.1034,  0.1132,  0.0096,  ...,  0.0453,  0.1477,  0.0301],\n",
      "         [-0.0136,  0.0878, -0.0430,  ...,  0.2082, -0.0345,  0.0925],\n",
      "         [ 0.0893,  0.0272, -0.0165,  ...,  0.2469, -0.0702,  0.0505],\n",
      "         ...,\n",
      "         [ 0.0556, -0.1430, -0.0542,  ..., -0.1250, -0.0689,  0.0607],\n",
      "         [ 0.0843,  0.0501, -0.0586,  ..., -0.0590, -0.0501,  0.0740],\n",
      "         [-0.0228, -0.0446, -0.0034,  ...,  0.0574, -0.0530, -0.0791]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.1034,  0.1132,  0.0096,  ...,  0.0453,  0.1477,  0.0301],\n",
      "         [-0.0136,  0.0878, -0.0430,  ...,  0.2082, -0.0345,  0.0925],\n",
      "         [ 0.0893,  0.0272, -0.0165,  ...,  0.2469, -0.0702,  0.0505],\n",
      "         ...,\n",
      "         [ 0.0556, -0.1430, -0.0542,  ..., -0.1250, -0.0689,  0.0607],\n",
      "         [ 0.0843,  0.0501, -0.0586,  ..., -0.0590, -0.0501,  0.0740],\n",
      "         [-0.0228, -0.0446, -0.0034,  ...,  0.0574, -0.0530, -0.0791]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.1071,  0.1253,  0.0211,  ...,  0.0126,  0.1586,  0.0278],\n",
      "         [-0.0824,  0.0560, -0.1140,  ...,  0.2213, -0.0005,  0.0607],\n",
      "         [ 0.0616,  0.0769, -0.0433,  ...,  0.2127,  0.0004, -0.0039],\n",
      "         ...,\n",
      "         [-0.0959, -0.0979, -0.0822,  ..., -0.0487, -0.0342,  0.0819],\n",
      "         [-0.0385,  0.0072, -0.0216,  ...,  0.0144, -0.0702,  0.0923],\n",
      "         [ 0.0895, -0.0846,  0.0026,  ...,  0.0488, -0.0682, -0.0378]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.1071,  0.1253,  0.0211,  ...,  0.0126,  0.1586,  0.0278],\n",
      "         [-0.0824,  0.0560, -0.1140,  ...,  0.2213, -0.0005,  0.0607],\n",
      "         [ 0.0616,  0.0769, -0.0433,  ...,  0.2127,  0.0004, -0.0039],\n",
      "         ...,\n",
      "         [-0.0959, -0.0979, -0.0822,  ..., -0.0487, -0.0342,  0.0819],\n",
      "         [-0.0385,  0.0072, -0.0216,  ...,  0.0144, -0.0702,  0.0923],\n",
      "         [ 0.0895, -0.0846,  0.0026,  ...,  0.0488, -0.0682, -0.0378]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.0945,  0.1848,  0.0036,  ..., -0.0473,  0.1489,  0.0536],\n",
      "         [-0.0801,  0.0412, -0.1079,  ...,  0.1884, -0.1360,  0.0501],\n",
      "         [ 0.1037, -0.0251, -0.0719,  ...,  0.1394, -0.0040, -0.0402],\n",
      "         ...,\n",
      "         [-0.0660, -0.1512, -0.0040,  ..., -0.1407,  0.0333,  0.1276],\n",
      "         [ 0.0214, -0.0905, -0.1242,  ..., -0.0189, -0.0455,  0.0746],\n",
      "         [-0.0023, -0.1201, -0.0875,  ...,  0.0682, -0.0332, -0.0449]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.0945,  0.1848,  0.0036,  ..., -0.0473,  0.1489,  0.0536],\n",
      "         [-0.0801,  0.0412, -0.1079,  ...,  0.1884, -0.1360,  0.0501],\n",
      "         [ 0.1037, -0.0251, -0.0719,  ...,  0.1394, -0.0040, -0.0402],\n",
      "         ...,\n",
      "         [-0.0660, -0.1512, -0.0040,  ..., -0.1407,  0.0333,  0.1276],\n",
      "         [ 0.0214, -0.0905, -0.1242,  ..., -0.0189, -0.0455,  0.0746],\n",
      "         [-0.0023, -0.1201, -0.0875,  ...,  0.0682, -0.0332, -0.0449]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-1.0599e-01,  2.1130e-01,  2.7006e-02,  ..., -1.1406e-01,\n",
      "           1.6103e-01,  5.2809e-02],\n",
      "         [-1.6937e-02,  3.9836e-02, -4.2399e-02,  ...,  1.4700e-01,\n",
      "          -1.6060e-01,  3.5388e-02],\n",
      "         [ 6.3120e-02, -1.0018e-01, -1.3407e-02,  ...,  1.5683e-01,\n",
      "          -8.8950e-02, -7.9101e-02],\n",
      "         ...,\n",
      "         [-8.6178e-02, -1.6546e-01,  1.1897e-05,  ..., -1.1196e-01,\n",
      "           1.1991e-01,  5.6263e-02],\n",
      "         [-2.7279e-02, -9.1214e-02, -1.0895e-02,  ..., -5.0980e-02,\n",
      "          -5.6318e-02, -8.9544e-03],\n",
      "         [ 8.0307e-02, -1.1037e-01, -8.1334e-02,  ...,  3.4793e-02,\n",
      "           7.3502e-04, -5.6649e-02]]], device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-1.0599e-01,  2.1130e-01,  2.7006e-02,  ..., -1.1406e-01,\n",
      "           1.6103e-01,  5.2809e-02],\n",
      "         [-1.6937e-02,  3.9836e-02, -4.2399e-02,  ...,  1.4700e-01,\n",
      "          -1.6060e-01,  3.5388e-02],\n",
      "         [ 6.3120e-02, -1.0018e-01, -1.3407e-02,  ...,  1.5683e-01,\n",
      "          -8.8950e-02, -7.9101e-02],\n",
      "         ...,\n",
      "         [-8.6178e-02, -1.6546e-01,  1.1897e-05,  ..., -1.1196e-01,\n",
      "           1.1991e-01,  5.6263e-02],\n",
      "         [-2.7279e-02, -9.1214e-02, -1.0895e-02,  ..., -5.0980e-02,\n",
      "          -5.6318e-02, -8.9544e-03],\n",
      "         [ 8.0307e-02, -1.1037e-01, -8.1334e-02,  ...,  3.4793e-02,\n",
      "           7.3502e-04, -5.6649e-02]]], device='cuda:0'),) and output (tensor([[[-0.0954,  0.2493,  0.0073,  ..., -0.1568,  0.1818,  0.0855],\n",
      "         [-0.1316,  0.0479, -0.0586,  ...,  0.1444, -0.1196,  0.0452],\n",
      "         [-0.1243, -0.1093,  0.0316,  ...,  0.1755, -0.0532, -0.1008],\n",
      "         ...,\n",
      "         [-0.0984, -0.1780,  0.0926,  ..., -0.0790,  0.1694,  0.0499],\n",
      "         [-0.0256, -0.0434, -0.0012,  ..., -0.1201,  0.0141, -0.0455],\n",
      "         [-0.0093, -0.0018, -0.0843,  ...,  0.0423, -0.0110, -0.0543]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.0954,  0.2493,  0.0073,  ..., -0.1568,  0.1818,  0.0855],\n",
      "         [-0.1316,  0.0479, -0.0586,  ...,  0.1444, -0.1196,  0.0452],\n",
      "         [-0.1243, -0.1093,  0.0316,  ...,  0.1755, -0.0532, -0.1008],\n",
      "         ...,\n",
      "         [-0.0984, -0.1780,  0.0926,  ..., -0.0790,  0.1694,  0.0499],\n",
      "         [-0.0256, -0.0434, -0.0012,  ..., -0.1201,  0.0141, -0.0455],\n",
      "         [-0.0093, -0.0018, -0.0843,  ...,  0.0423, -0.0110, -0.0543]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.0152,  0.3340,  0.0011,  ..., -0.2750,  0.1933,  0.1034],\n",
      "         [-0.0433,  0.0365, -0.0877,  ...,  0.1284, -0.0814,  0.0274],\n",
      "         [-0.1331, -0.1001,  0.0575,  ...,  0.1463, -0.0917, -0.1428],\n",
      "         ...,\n",
      "         [ 0.0082, -0.2135,  0.1169,  ..., -0.0864,  0.1225,  0.0817],\n",
      "         [-0.0852, -0.0699,  0.0828,  ..., -0.1069, -0.0729, -0.0650],\n",
      "         [-0.0331,  0.0622, -0.0829,  ...,  0.0465, -0.0109, -0.0036]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.0152,  0.3340,  0.0011,  ..., -0.2750,  0.1933,  0.1034],\n",
      "         [-0.0433,  0.0365, -0.0877,  ...,  0.1284, -0.0814,  0.0274],\n",
      "         [-0.1331, -0.1001,  0.0575,  ...,  0.1463, -0.0917, -0.1428],\n",
      "         ...,\n",
      "         [ 0.0082, -0.2135,  0.1169,  ..., -0.0864,  0.1225,  0.0817],\n",
      "         [-0.0852, -0.0699,  0.0828,  ..., -0.1069, -0.0729, -0.0650],\n",
      "         [-0.0331,  0.0622, -0.0829,  ...,  0.0465, -0.0109, -0.0036]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.0356,  0.3483, -0.0366,  ..., -0.2962,  0.2088,  0.1065],\n",
      "         [-0.0113, -0.0099, -0.0772,  ...,  0.0144, -0.0517,  0.0257],\n",
      "         [-0.1643, -0.0392,  0.0634,  ...,  0.0830, -0.0605, -0.2062],\n",
      "         ...,\n",
      "         [ 0.0871, -0.1420,  0.0922,  ..., -0.1455,  0.1591,  0.1089],\n",
      "         [-0.0564, -0.0627,  0.0545,  ..., -0.1797, -0.0105, -0.0205],\n",
      "         [-0.0528, -0.0556, -0.1060,  ..., -0.0259,  0.0466, -0.0654]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.0356,  0.3483, -0.0366,  ..., -0.2962,  0.2088,  0.1065],\n",
      "         [-0.0113, -0.0099, -0.0772,  ...,  0.0144, -0.0517,  0.0257],\n",
      "         [-0.1643, -0.0392,  0.0634,  ...,  0.0830, -0.0605, -0.2062],\n",
      "         ...,\n",
      "         [ 0.0871, -0.1420,  0.0922,  ..., -0.1455,  0.1591,  0.1089],\n",
      "         [-0.0564, -0.0627,  0.0545,  ..., -0.1797, -0.0105, -0.0205],\n",
      "         [-0.0528, -0.0556, -0.1060,  ..., -0.0259,  0.0466, -0.0654]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.0127,  0.3661, -0.0316,  ..., -0.3678,  0.1973,  0.1019],\n",
      "         [ 0.0241, -0.0878, -0.0090,  ...,  0.0391,  0.0088,  0.0862],\n",
      "         [-0.1336, -0.0854,  0.0725,  ...,  0.0520, -0.0693, -0.2326],\n",
      "         ...,\n",
      "         [ 0.0988,  0.0584,  0.1727,  ...,  0.0162,  0.0169,  0.1015],\n",
      "         [-0.0267,  0.1586,  0.2051,  ...,  0.1131, -0.0251, -0.0164],\n",
      "         [-0.0219,  0.0198,  0.0481,  ...,  0.0920,  0.0453, -0.1878]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.0127,  0.3661, -0.0316,  ..., -0.3678,  0.1973,  0.1019],\n",
      "         [ 0.0241, -0.0878, -0.0090,  ...,  0.0391,  0.0088,  0.0862],\n",
      "         [-0.1336, -0.0854,  0.0725,  ...,  0.0520, -0.0693, -0.2326],\n",
      "         ...,\n",
      "         [ 0.0988,  0.0584,  0.1727,  ...,  0.0162,  0.0169,  0.1015],\n",
      "         [-0.0267,  0.1586,  0.2051,  ...,  0.1131, -0.0251, -0.0164],\n",
      "         [-0.0219,  0.0198,  0.0481,  ...,  0.0920,  0.0453, -0.1878]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.0089,  0.3233, -0.0779,  ..., -0.4491,  0.1845,  0.0951],\n",
      "         [-0.0049, -0.1268, -0.0940,  ..., -0.0131,  0.0088,  0.1124],\n",
      "         [-0.1426, -0.0674,  0.0047,  ...,  0.0657,  0.0217, -0.1762],\n",
      "         ...,\n",
      "         [-0.1169, -0.0389,  0.0939,  ..., -0.0549, -0.0468,  0.0845],\n",
      "         [-0.2368,  0.0592,  0.3018,  ...,  0.0222, -0.1082, -0.0801],\n",
      "         [-0.0867,  0.1360,  0.0803,  ..., -0.0699, -0.0269, -0.1322]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.0089,  0.3233, -0.0779,  ..., -0.4491,  0.1845,  0.0951],\n",
      "         [-0.0049, -0.1268, -0.0940,  ..., -0.0131,  0.0088,  0.1124],\n",
      "         [-0.1426, -0.0674,  0.0047,  ...,  0.0657,  0.0217, -0.1762],\n",
      "         ...,\n",
      "         [-0.1169, -0.0389,  0.0939,  ..., -0.0549, -0.0468,  0.0845],\n",
      "         [-0.2368,  0.0592,  0.3018,  ...,  0.0222, -0.1082, -0.0801],\n",
      "         [-0.0867,  0.1360,  0.0803,  ..., -0.0699, -0.0269, -0.1322]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.0244,  0.2828, -0.0532,  ..., -0.4592,  0.2300,  0.0142],\n",
      "         [ 0.0675, -0.2118, -0.1229,  ...,  0.0063, -0.0210,  0.0149],\n",
      "         [-0.1291, -0.0619,  0.0334,  ...,  0.0484,  0.1174, -0.2044],\n",
      "         ...,\n",
      "         [-0.1966, -0.0740,  0.0495,  ..., -0.1800, -0.0363, -0.0364],\n",
      "         [-0.1612,  0.0708,  0.1518,  ..., -0.1915, -0.0937, -0.1070],\n",
      "         [-0.0760,  0.1094, -0.0351,  ..., -0.0515, -0.1727, -0.0974]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.0244,  0.2828, -0.0532,  ..., -0.4592,  0.2300,  0.0142],\n",
      "         [ 0.0675, -0.2118, -0.1229,  ...,  0.0063, -0.0210,  0.0149],\n",
      "         [-0.1291, -0.0619,  0.0334,  ...,  0.0484,  0.1174, -0.2044],\n",
      "         ...,\n",
      "         [-0.1966, -0.0740,  0.0495,  ..., -0.1800, -0.0363, -0.0364],\n",
      "         [-0.1612,  0.0708,  0.1518,  ..., -0.1915, -0.0937, -0.1070],\n",
      "         [-0.0760,  0.1094, -0.0351,  ..., -0.0515, -0.1727, -0.0974]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.0928,  0.1485,  0.0158,  ..., -0.5380,  0.2664, -0.0333],\n",
      "         [-0.0213, -0.2680, -0.0503,  ..., -0.0298,  0.0872, -0.0313],\n",
      "         [-0.1408, -0.0515, -0.1130,  ...,  0.0356,  0.0980, -0.1877],\n",
      "         ...,\n",
      "         [-0.1653, -0.2118, -0.0548,  ..., -0.1813,  0.0969,  0.0883],\n",
      "         [-0.1227,  0.0122,  0.1528,  ..., -0.0861,  0.0341, -0.0068],\n",
      "         [ 0.0925,  0.1407,  0.0269,  ...,  0.0092, -0.1946, -0.0887]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.0928,  0.1485,  0.0158,  ..., -0.5380,  0.2664, -0.0333],\n",
      "         [-0.0213, -0.2680, -0.0503,  ..., -0.0298,  0.0872, -0.0313],\n",
      "         [-0.1408, -0.0515, -0.1130,  ...,  0.0356,  0.0980, -0.1877],\n",
      "         ...,\n",
      "         [-0.1653, -0.2118, -0.0548,  ..., -0.1813,  0.0969,  0.0883],\n",
      "         [-0.1227,  0.0122,  0.1528,  ..., -0.0861,  0.0341, -0.0068],\n",
      "         [ 0.0925,  0.1407,  0.0269,  ...,  0.0092, -0.1946, -0.0887]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.1000,  0.1323,  0.0241,  ..., -0.5470,  0.2844, -0.0457],\n",
      "         [ 0.0532, -0.3198,  0.0812,  ...,  0.0498, -0.0745, -0.1783],\n",
      "         [-0.1705, -0.1248, -0.1991,  ...,  0.0719, -0.0155, -0.1754],\n",
      "         ...,\n",
      "         [-0.1905, -0.2421, -0.1423,  ..., -0.1074,  0.0963,  0.1126],\n",
      "         [-0.1145, -0.0548,  0.0450,  ..., -0.0045,  0.0315,  0.0708],\n",
      "         [ 0.0938,  0.2701,  0.0909,  ...,  0.0687, -0.1714, -0.2274]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.1000,  0.1323,  0.0241,  ..., -0.5470,  0.2844, -0.0457],\n",
      "         [ 0.0532, -0.3198,  0.0812,  ...,  0.0498, -0.0745, -0.1783],\n",
      "         [-0.1705, -0.1248, -0.1991,  ...,  0.0719, -0.0155, -0.1754],\n",
      "         ...,\n",
      "         [-0.1905, -0.2421, -0.1423,  ..., -0.1074,  0.0963,  0.1126],\n",
      "         [-0.1145, -0.0548,  0.0450,  ..., -0.0045,  0.0315,  0.0708],\n",
      "         [ 0.0938,  0.2701,  0.0909,  ...,  0.0687, -0.1714, -0.2274]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-1.2863e-01,  8.3288e-02,  2.3927e-02,  ..., -5.6571e-01,\n",
      "           3.6938e-01, -7.2482e-02],\n",
      "         [-4.3202e-02, -3.7638e-01,  1.2078e-01,  ..., -9.2329e-02,\n",
      "          -9.4799e-02, -2.6277e-01],\n",
      "         [-1.7088e-01, -1.1800e-01, -3.0322e-01,  ..., -2.2943e-02,\n",
      "           3.6755e-02, -2.7755e-01],\n",
      "         ...,\n",
      "         [-2.4567e-01, -2.0520e-01, -3.1447e-01,  ..., -1.0606e-01,\n",
      "           2.8214e-01,  8.2999e-02],\n",
      "         [-5.9912e-02,  1.6409e-04, -1.7709e-01,  ..., -3.5807e-02,\n",
      "           1.9481e-01, -2.0750e-02],\n",
      "         [ 9.7664e-02,  1.5374e-01, -9.2137e-02,  ..., -3.2442e-02,\n",
      "          -3.0604e-01, -4.2980e-01]]], device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-1.2863e-01,  8.3288e-02,  2.3927e-02,  ..., -5.6571e-01,\n",
      "           3.6938e-01, -7.2482e-02],\n",
      "         [-4.3202e-02, -3.7638e-01,  1.2078e-01,  ..., -9.2329e-02,\n",
      "          -9.4799e-02, -2.6277e-01],\n",
      "         [-1.7088e-01, -1.1800e-01, -3.0322e-01,  ..., -2.2943e-02,\n",
      "           3.6755e-02, -2.7755e-01],\n",
      "         ...,\n",
      "         [-2.4567e-01, -2.0520e-01, -3.1447e-01,  ..., -1.0606e-01,\n",
      "           2.8214e-01,  8.2999e-02],\n",
      "         [-5.9912e-02,  1.6409e-04, -1.7709e-01,  ..., -3.5807e-02,\n",
      "           1.9481e-01, -2.0750e-02],\n",
      "         [ 9.7664e-02,  1.5374e-01, -9.2137e-02,  ..., -3.2442e-02,\n",
      "          -3.0604e-01, -4.2980e-01]]], device='cuda:0'),) and output (tensor([[[-0.1417,  0.0703,  0.0343,  ..., -0.5549,  0.3175, -0.0847],\n",
      "         [-0.0966, -0.3707,  0.1335,  ..., -0.0604,  0.0023, -0.2344],\n",
      "         [-0.1593, -0.3615, -0.2513,  ..., -0.1206,  0.0078, -0.2982],\n",
      "         ...,\n",
      "         [-0.4128, -0.2945, -0.3202,  ..., -0.0992,  0.2334,  0.0102],\n",
      "         [-0.0429,  0.1004, -0.2245,  ...,  0.0239,  0.1554, -0.0595],\n",
      "         [ 0.0251,  0.2226, -0.0736,  ..., -0.0128, -0.2803, -0.5557]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.1417,  0.0703,  0.0343,  ..., -0.5549,  0.3175, -0.0847],\n",
      "         [-0.0966, -0.3707,  0.1335,  ..., -0.0604,  0.0023, -0.2344],\n",
      "         [-0.1593, -0.3615, -0.2513,  ..., -0.1206,  0.0078, -0.2982],\n",
      "         ...,\n",
      "         [-0.4128, -0.2945, -0.3202,  ..., -0.0992,  0.2334,  0.0102],\n",
      "         [-0.0429,  0.1004, -0.2245,  ...,  0.0239,  0.1554, -0.0595],\n",
      "         [ 0.0251,  0.2226, -0.0736,  ..., -0.0128, -0.2803, -0.5557]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.1528,  0.0725,  0.0609,  ..., -0.5624,  0.3103, -0.0293],\n",
      "         [-0.0068, -0.2760,  0.0715,  ..., -0.0323, -0.0103, -0.1341],\n",
      "         [-0.0562, -0.2811, -0.2783,  ..., -0.1634,  0.0883, -0.3397],\n",
      "         ...,\n",
      "         [-0.2809, -0.1214, -0.2035,  ..., -0.1393,  0.1795, -0.1016],\n",
      "         [-0.0361,  0.0938, -0.2119,  ..., -0.0440,  0.1700, -0.0925],\n",
      "         [-0.0054,  0.2141,  0.0084,  ..., -0.0683, -0.1629, -0.5878]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.1528,  0.0725,  0.0609,  ..., -0.5624,  0.3103, -0.0293],\n",
      "         [-0.0068, -0.2760,  0.0715,  ..., -0.0323, -0.0103, -0.1341],\n",
      "         [-0.0562, -0.2811, -0.2783,  ..., -0.1634,  0.0883, -0.3397],\n",
      "         ...,\n",
      "         [-0.2809, -0.1214, -0.2035,  ..., -0.1393,  0.1795, -0.1016],\n",
      "         [-0.0361,  0.0938, -0.2119,  ..., -0.0440,  0.1700, -0.0925],\n",
      "         [-0.0054,  0.2141,  0.0084,  ..., -0.0683, -0.1629, -0.5878]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.1559,  0.0520,  0.0566,  ..., -0.5805,  0.2614, -0.0032],\n",
      "         [ 0.0203, -0.3056,  0.0487,  ..., -0.0420, -0.0219, -0.0766],\n",
      "         [-0.1485, -0.2414,  0.0007,  ..., -0.0677, -0.0114, -0.3405],\n",
      "         ...,\n",
      "         [-0.3761, -0.1626, -0.2724,  ..., -0.2697,  0.1550, -0.1630],\n",
      "         [-0.0051,  0.1075, -0.2365,  ..., -0.1105,  0.1346, -0.1179],\n",
      "         [ 0.0775,  0.1043,  0.0364,  ..., -0.0392, -0.1943, -0.5728]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.1559,  0.0520,  0.0566,  ..., -0.5805,  0.2614, -0.0032],\n",
      "         [ 0.0203, -0.3056,  0.0487,  ..., -0.0420, -0.0219, -0.0766],\n",
      "         [-0.1485, -0.2414,  0.0007,  ..., -0.0677, -0.0114, -0.3405],\n",
      "         ...,\n",
      "         [-0.3761, -0.1626, -0.2724,  ..., -0.2697,  0.1550, -0.1630],\n",
      "         [-0.0051,  0.1075, -0.2365,  ..., -0.1105,  0.1346, -0.1179],\n",
      "         [ 0.0775,  0.1043,  0.0364,  ..., -0.0392, -0.1943, -0.5728]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.1488,  0.0445,  0.0765,  ..., -0.5456,  0.2702,  0.0098],\n",
      "         [ 0.0273, -0.3545, -0.0304,  ..., -0.0121,  0.1310, -0.0746],\n",
      "         [-0.1391, -0.2644, -0.2392,  ..., -0.1234,  0.1104, -0.2771],\n",
      "         ...,\n",
      "         [-0.5732,  0.0127, -0.3119,  ..., -0.2817,  0.0983, -0.0643],\n",
      "         [-0.1973,  0.2103, -0.2926,  ..., -0.0689, -0.0259,  0.0022],\n",
      "         [ 0.0660,  0.1398,  0.0961,  ..., -0.0118, -0.1811, -0.6505]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.1488,  0.0445,  0.0765,  ..., -0.5456,  0.2702,  0.0098],\n",
      "         [ 0.0273, -0.3545, -0.0304,  ..., -0.0121,  0.1310, -0.0746],\n",
      "         [-0.1391, -0.2644, -0.2392,  ..., -0.1234,  0.1104, -0.2771],\n",
      "         ...,\n",
      "         [-0.5732,  0.0127, -0.3119,  ..., -0.2817,  0.0983, -0.0643],\n",
      "         [-0.1973,  0.2103, -0.2926,  ..., -0.0689, -0.0259,  0.0022],\n",
      "         [ 0.0660,  0.1398,  0.0961,  ..., -0.0118, -0.1811, -0.6505]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.1268,  0.0333,  0.0935,  ..., -0.5320,  0.2795,  0.0006],\n",
      "         [ 0.0798, -0.3126,  0.0637,  ...,  0.0017,  0.3143, -0.1617],\n",
      "         [-0.2487, -0.1314, -0.2693,  ..., -0.0029,  0.1335, -0.3443],\n",
      "         ...,\n",
      "         [-0.3992, -0.0329, -0.4220,  ..., -0.3832,  0.1382,  0.0615],\n",
      "         [-0.2158,  0.0047, -0.2519,  ..., -0.0381,  0.0967,  0.0139],\n",
      "         [ 0.0771,  0.1187,  0.0980,  ..., -0.1251, -0.1506, -0.4360]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.1268,  0.0333,  0.0935,  ..., -0.5320,  0.2795,  0.0006],\n",
      "         [ 0.0798, -0.3126,  0.0637,  ...,  0.0017,  0.3143, -0.1617],\n",
      "         [-0.2487, -0.1314, -0.2693,  ..., -0.0029,  0.1335, -0.3443],\n",
      "         ...,\n",
      "         [-0.3992, -0.0329, -0.4220,  ..., -0.3832,  0.1382,  0.0615],\n",
      "         [-0.2158,  0.0047, -0.2519,  ..., -0.0381,  0.0967,  0.0139],\n",
      "         [ 0.0771,  0.1187,  0.0980,  ..., -0.1251, -0.1506, -0.4360]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.1355,  0.0227,  0.0554,  ..., -0.5473,  0.2470, -0.0229],\n",
      "         [ 0.0394, -0.2621,  0.1004,  ...,  0.0727,  0.2956, -0.0585],\n",
      "         [-0.1917, -0.1080, -0.2471,  ...,  0.1858, -0.0018, -0.1665],\n",
      "         ...,\n",
      "         [-0.3116, -0.2259, -0.2720,  ..., -0.3729,  0.0640,  0.1050],\n",
      "         [-0.2456, -0.1339, -0.3263,  ..., -0.2285, -0.0034, -0.2368],\n",
      "         [ 0.1036,  0.1942,  0.1831,  ..., -0.1628, -0.2201, -0.4761]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.1355,  0.0227,  0.0554,  ..., -0.5473,  0.2470, -0.0229],\n",
      "         [ 0.0394, -0.2621,  0.1004,  ...,  0.0727,  0.2956, -0.0585],\n",
      "         [-0.1917, -0.1080, -0.2471,  ...,  0.1858, -0.0018, -0.1665],\n",
      "         ...,\n",
      "         [-0.3116, -0.2259, -0.2720,  ..., -0.3729,  0.0640,  0.1050],\n",
      "         [-0.2456, -0.1339, -0.3263,  ..., -0.2285, -0.0034, -0.2368],\n",
      "         [ 0.1036,  0.1942,  0.1831,  ..., -0.1628, -0.2201, -0.4761]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.1350,  0.0163,  0.0576,  ..., -0.5555,  0.2435,  0.0051],\n",
      "         [ 0.1293, -0.1159,  0.1245,  ...,  0.2165,  0.4189,  0.0848],\n",
      "         [-0.3147,  0.0154, -0.1991,  ...,  0.0528,  0.1466, -0.1876],\n",
      "         ...,\n",
      "         [-0.2555,  0.0033, -0.1460,  ..., -0.3057, -0.0063,  0.0028],\n",
      "         [-0.2195, -0.2487, -0.3029,  ..., -0.0832,  0.1267, -0.1791],\n",
      "         [ 0.1152,  0.2765,  0.2520,  ...,  0.0449, -0.1384, -0.4291]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.1350,  0.0163,  0.0576,  ..., -0.5555,  0.2435,  0.0051],\n",
      "         [ 0.1293, -0.1159,  0.1245,  ...,  0.2165,  0.4189,  0.0848],\n",
      "         [-0.3147,  0.0154, -0.1991,  ...,  0.0528,  0.1466, -0.1876],\n",
      "         ...,\n",
      "         [-0.2555,  0.0033, -0.1460,  ..., -0.3057, -0.0063,  0.0028],\n",
      "         [-0.2195, -0.2487, -0.3029,  ..., -0.0832,  0.1267, -0.1791],\n",
      "         [ 0.1152,  0.2765,  0.2520,  ...,  0.0449, -0.1384, -0.4291]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.1301, -0.0029,  0.0279,  ..., -0.5537,  0.2277,  0.0164],\n",
      "         [-0.0012, -0.0952,  0.2106,  ...,  0.3727,  0.3033,  0.1629],\n",
      "         [-0.4942,  0.1333, -0.1162,  ...,  0.0126, -0.0428, -0.4490],\n",
      "         ...,\n",
      "         [-0.0494,  0.0884, -0.0940,  ..., -0.4594, -0.1638,  0.1242],\n",
      "         [-0.3177, -0.1443, -0.1878,  ..., -0.0111,  0.2694, -0.2778],\n",
      "         [ 0.0440,  0.4505,  0.4136,  ...,  0.1462, -0.0483, -0.4583]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.1301, -0.0029,  0.0279,  ..., -0.5537,  0.2277,  0.0164],\n",
      "         [-0.0012, -0.0952,  0.2106,  ...,  0.3727,  0.3033,  0.1629],\n",
      "         [-0.4942,  0.1333, -0.1162,  ...,  0.0126, -0.0428, -0.4490],\n",
      "         ...,\n",
      "         [-0.0494,  0.0884, -0.0940,  ..., -0.4594, -0.1638,  0.1242],\n",
      "         [-0.3177, -0.1443, -0.1878,  ..., -0.0111,  0.2694, -0.2778],\n",
      "         [ 0.0440,  0.4505,  0.4136,  ...,  0.1462, -0.0483, -0.4583]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.1219,  0.0061, -0.0021,  ..., -0.5922,  0.2788, -0.0020],\n",
      "         [ 0.0985, -0.0185, -0.0034,  ...,  0.4764,  0.3710,  0.1742],\n",
      "         [-0.5042,  0.0658,  0.0234,  ..., -0.2291,  0.0299, -0.3968],\n",
      "         ...,\n",
      "         [-0.2008,  0.1997, -0.2301,  ..., -0.5347, -0.1570,  0.0542],\n",
      "         [-0.3512, -0.2052, -0.0899,  ...,  0.0742,  0.3468, -0.2501],\n",
      "         [ 0.1217,  0.4012,  0.4591,  ...,  0.1781, -0.0483, -0.3576]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.1219,  0.0061, -0.0021,  ..., -0.5922,  0.2788, -0.0020],\n",
      "         [ 0.0985, -0.0185, -0.0034,  ...,  0.4764,  0.3710,  0.1742],\n",
      "         [-0.5042,  0.0658,  0.0234,  ..., -0.2291,  0.0299, -0.3968],\n",
      "         ...,\n",
      "         [-0.2008,  0.1997, -0.2301,  ..., -0.5347, -0.1570,  0.0542],\n",
      "         [-0.3512, -0.2052, -0.0899,  ...,  0.0742,  0.3468, -0.2501],\n",
      "         [ 0.1217,  0.4012,  0.4591,  ...,  0.1781, -0.0483, -0.3576]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.1067,  0.0272,  0.0495,  ..., -0.5550,  0.3449,  0.0266],\n",
      "         [ 0.1521, -0.0408,  0.1034,  ...,  0.6584,  0.2987,  0.1843],\n",
      "         [-0.5157,  0.0925, -0.1288,  ..., -0.4263, -0.1266, -0.4681],\n",
      "         ...,\n",
      "         [-0.1967,  0.2208, -0.2037,  ..., -0.7468,  0.2386, -0.0654],\n",
      "         [-0.4120, -0.4673,  0.0827,  ..., -0.0118,  0.3710, -0.3265],\n",
      "         [ 0.0122,  0.1867,  0.5100,  ...,  0.2997, -0.0513, -0.4867]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.1067,  0.0272,  0.0495,  ..., -0.5550,  0.3449,  0.0266],\n",
      "         [ 0.1521, -0.0408,  0.1034,  ...,  0.6584,  0.2987,  0.1843],\n",
      "         [-0.5157,  0.0925, -0.1288,  ..., -0.4263, -0.1266, -0.4681],\n",
      "         ...,\n",
      "         [-0.1967,  0.2208, -0.2037,  ..., -0.7468,  0.2386, -0.0654],\n",
      "         [-0.4120, -0.4673,  0.0827,  ..., -0.0118,  0.3710, -0.3265],\n",
      "         [ 0.0122,  0.1867,  0.5100,  ...,  0.2997, -0.0513, -0.4867]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.0592,  0.0731,  0.0500,  ..., -0.5517,  0.3541,  0.0595],\n",
      "         [-0.0221,  0.0068,  0.2556,  ...,  0.5886,  0.3857,  0.3758],\n",
      "         [-0.8070,  0.2266, -0.1184,  ..., -0.4883, -0.1820, -0.4341],\n",
      "         ...,\n",
      "         [-0.1982,  0.1572, -0.0817,  ..., -0.7013,  0.3575, -0.0394],\n",
      "         [-0.7433, -0.6105,  0.3793,  ...,  0.0262,  0.5183, -0.2107],\n",
      "         [-0.0853,  0.0990,  0.4634,  ...,  0.1273,  0.0728, -0.5460]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.0592,  0.0731,  0.0500,  ..., -0.5517,  0.3541,  0.0595],\n",
      "         [-0.0221,  0.0068,  0.2556,  ...,  0.5886,  0.3857,  0.3758],\n",
      "         [-0.8070,  0.2266, -0.1184,  ..., -0.4883, -0.1820, -0.4341],\n",
      "         ...,\n",
      "         [-0.1982,  0.1572, -0.0817,  ..., -0.7013,  0.3575, -0.0394],\n",
      "         [-0.7433, -0.6105,  0.3793,  ...,  0.0262,  0.5183, -0.2107],\n",
      "         [-0.0853,  0.0990,  0.4634,  ...,  0.1273,  0.0728, -0.5460]]],\n",
      "       device='cuda:0'),) and output (tensor([[[ 0.1143,  0.2237,  0.3292,  ..., -0.7706,  0.2608,  0.0269],\n",
      "         [ 0.2110, -0.3148,  0.2018,  ...,  0.5906,  0.4778,  0.2661],\n",
      "         [-0.8185, -0.0535, -0.3198,  ..., -0.3387, -0.5673, -0.8881],\n",
      "         ...,\n",
      "         [ 0.0344, -0.2456,  0.1602,  ..., -0.5255,  0.3081,  0.0680],\n",
      "         [-0.7577, -0.8651,  0.3659,  ...,  0.0448,  0.3829, -0.1839],\n",
      "         [ 0.1312,  0.2641,  0.2931,  ...,  0.0730,  0.0065, -0.7443]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[ 0.1143,  0.2237,  0.3292,  ..., -0.7706,  0.2608,  0.0269],\n",
      "         [ 0.2110, -0.3148,  0.2018,  ...,  0.5906,  0.4778,  0.2661],\n",
      "         [-0.8185, -0.0535, -0.3198,  ..., -0.3387, -0.5673, -0.8881],\n",
      "         ...,\n",
      "         [ 0.0344, -0.2456,  0.1602,  ..., -0.5255,  0.3081,  0.0680],\n",
      "         [-0.7577, -0.8651,  0.3659,  ...,  0.0448,  0.3829, -0.1839],\n",
      "         [ 0.1312,  0.2641,  0.2931,  ...,  0.0730,  0.0065, -0.7443]]],\n",
      "       device='cuda:0'),) and output (tensor([[[ 0.3711,  0.5489,  0.6100,  ..., -0.7499,  0.2080,  0.4195],\n",
      "         [ 0.4777, -0.8573, -0.1860,  ...,  0.5172,  0.3424,  0.4901],\n",
      "         [-0.8291, -0.3770, -0.5317,  ..., -0.3576, -0.4553, -1.2699],\n",
      "         ...,\n",
      "         [ 0.0262, -0.3380,  0.2186,  ..., -0.6645,  0.1015, -0.4337],\n",
      "         [-0.8603, -0.6574,  0.4586,  ...,  0.3986,  0.3323, -0.7024],\n",
      "         [-0.0293,  0.0474, -0.0074,  ..., -0.0259,  0.1092, -1.2988]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[ 0.3711,  0.5489,  0.6100,  ..., -0.7499,  0.2080,  0.4195],\n",
      "         [ 0.4777, -0.8573, -0.1860,  ...,  0.5172,  0.3424,  0.4901],\n",
      "         [-0.8291, -0.3770, -0.5317,  ..., -0.3576, -0.4553, -1.2699],\n",
      "         ...,\n",
      "         [ 0.0262, -0.3380,  0.2186,  ..., -0.6645,  0.1015, -0.4337],\n",
      "         [-0.8603, -0.6574,  0.4586,  ...,  0.3986,  0.3323, -0.7024],\n",
      "         [-0.0293,  0.0474, -0.0074,  ..., -0.0259,  0.1092, -1.2988]]],\n",
      "       device='cuda:0'),) and output (tensor([[[ 1.1544,  3.2965,  1.6534,  ..., -2.7009,  2.6553,  2.2700],\n",
      "         [ 0.7733, -0.4470, -0.5573,  ...,  0.9518, -0.1398,  1.0986],\n",
      "         [-1.2911,  0.0695, -0.8982,  ..., -0.6811, -1.3059, -0.7743],\n",
      "         ...,\n",
      "         [ 0.2172,  0.0351,  0.5455,  ...,  0.1383, -0.2061, -0.1128],\n",
      "         [-0.8170, -0.3994,  0.6139,  ...,  0.5256, -1.4762,  0.2991],\n",
      "         [ 0.6559, -1.2880,  0.7118,  ...,  1.2669, -0.9056, -0.9622]]],\n",
      "       device='cuda:0'),)\n",
      "[Debug] Layer names being passed: ['model.layers.0', 'model.layers.1', 'model.layers.2', 'model.layers.3', 'model.layers.4', 'model.layers.5', 'model.layers.6', 'model.layers.7', 'model.layers.8', 'model.layers.9', 'model.layers.10', 'model.layers.11', 'model.layers.12', 'model.layers.13', 'model.layers.14', 'model.layers.15', 'model.layers.16', 'model.layers.17', 'model.layers.18', 'model.layers.19', 'model.layers.20', 'model.layers.21', 'model.layers.22', 'model.layers.23', 'model.layers.24', 'model.layers.25', 'model.layers.26', 'model.layers.27', 'model.layers.28', 'model.layers.29', 'model.layers.30', 'model.layers.31', 'model.embed_tokens']\n",
      "[Debug] Trying to access layer: model.layers.0\n",
      "[Debug] Successfully found layer: model.layers.0\n",
      "[Debug] Trying to access layer: model.layers.1\n",
      "[Debug] Successfully found layer: model.layers.1\n",
      "[Debug] Trying to access layer: model.layers.2\n",
      "[Debug] Successfully found layer: model.layers.2\n",
      "[Debug] Trying to access layer: model.layers.3\n",
      "[Debug] Successfully found layer: model.layers.3\n",
      "[Debug] Trying to access layer: model.layers.4\n",
      "[Debug] Successfully found layer: model.layers.4\n",
      "[Debug] Trying to access layer: model.layers.5\n",
      "[Debug] Successfully found layer: model.layers.5\n",
      "[Debug] Trying to access layer: model.layers.6\n",
      "[Debug] Successfully found layer: model.layers.6\n",
      "[Debug] Trying to access layer: model.layers.7\n",
      "[Debug] Successfully found layer: model.layers.7\n",
      "[Debug] Trying to access layer: model.layers.8\n",
      "[Debug] Successfully found layer: model.layers.8\n",
      "[Debug] Trying to access layer: model.layers.9\n",
      "[Debug] Successfully found layer: model.layers.9\n",
      "[Debug] Trying to access layer: model.layers.10\n",
      "[Debug] Successfully found layer: model.layers.10\n",
      "[Debug] Trying to access layer: model.layers.11\n",
      "[Debug] Successfully found layer: model.layers.11\n",
      "[Debug] Trying to access layer: model.layers.12\n",
      "[Debug] Successfully found layer: model.layers.12\n",
      "[Debug] Trying to access layer: model.layers.13\n",
      "[Debug] Successfully found layer: model.layers.13\n",
      "[Debug] Trying to access layer: model.layers.14\n",
      "[Debug] Successfully found layer: model.layers.14\n",
      "[Debug] Trying to access layer: model.layers.15\n",
      "[Debug] Successfully found layer: model.layers.15\n",
      "[Debug] Trying to access layer: model.layers.16\n",
      "[Debug] Successfully found layer: model.layers.16\n",
      "[Debug] Trying to access layer: model.layers.17\n",
      "[Debug] Successfully found layer: model.layers.17\n",
      "[Debug] Trying to access layer: model.layers.18\n",
      "[Debug] Successfully found layer: model.layers.18\n",
      "[Debug] Trying to access layer: model.layers.19\n",
      "[Debug] Successfully found layer: model.layers.19\n",
      "[Debug] Trying to access layer: model.layers.20\n",
      "[Debug] Successfully found layer: model.layers.20\n",
      "[Debug] Trying to access layer: model.layers.21\n",
      "[Debug] Successfully found layer: model.layers.21\n",
      "[Debug] Trying to access layer: model.layers.22\n",
      "[Debug] Successfully found layer: model.layers.22\n",
      "[Debug] Trying to access layer: model.layers.23\n",
      "[Debug] Successfully found layer: model.layers.23\n",
      "[Debug] Trying to access layer: model.layers.24\n",
      "[Debug] Successfully found layer: model.layers.24\n",
      "[Debug] Trying to access layer: model.layers.25\n",
      "[Debug] Successfully found layer: model.layers.25\n",
      "[Debug] Trying to access layer: model.layers.26\n",
      "[Debug] Successfully found layer: model.layers.26\n",
      "[Debug] Trying to access layer: model.layers.27\n",
      "[Debug] Successfully found layer: model.layers.27\n",
      "[Debug] Trying to access layer: model.layers.28\n",
      "[Debug] Successfully found layer: model.layers.28\n",
      "[Debug] Trying to access layer: model.layers.29\n",
      "[Debug] Successfully found layer: model.layers.29\n",
      "[Debug] Trying to access layer: model.layers.30\n",
      "[Debug] Successfully found layer: model.layers.30\n",
      "[Debug] Trying to access layer: model.layers.31\n",
      "[Debug] Successfully found layer: model.layers.31\n",
      "[Debug] Trying to access layer: model.embed_tokens\n",
      "[Debug] Successfully found layer: model.embed_tokens\n",
      "[Hook] Layer Embedding(128256, 4096) received input (tensor([[128000,  69032,   9777,   1169,  19912,   9777,   1169,    374,    279,\n",
      "           2316,   3752,    323,  46684,    315,  12656,  42482,    596,  31926,\n",
      "           9777,   1169,     13,   1283,    374,    279,  19912,    315,  35440,\n",
      "             11,  63904,    311,    279,  96188,  10194,  62412,   9334,     11,\n",
      "            323,   4538,    315,   6342,   9777,   1169,     11,    279,   3766,\n",
      "           6342,    315,  35440,     13,   2468,    279,   7314,    315,    279,\n",
      "           1514,     11,    568,  28970,    449,   3508,     11,    323,   1268,\n",
      "             11,    311,    264,  53305,    279,  10102,    315,    813,   7126,\n",
      "             11,    323,  28970,    449,    813,   1866,  47942,   3235,    279,\n",
      "           1648,     13,   3296,    279,    842,    315,    279,  31926,     11,\n",
      "           9777,   1169,    706,   9057,    279,  16779,    315,   3735,    263,\n",
      "           9334,     11,   5034,    531,    288,     11,  62412,   9334,     11,\n",
      "            323,   1403,  54627,   3095,    315,    813,    505,    279,   3907,\n",
      "            315,    468,  23257,   7881,  16870,    967,  35534,     89,    323,\n",
      "          33592,    268,    267,    944,     13,   1283,    374,   1101,  46345,\n",
      "           6532,    304,    279,  16779,    315,    813,   3021,    507,    764,\n",
      "          37029,    320,     67,  51520,      8,    323,    315,    813,   6691,\n",
      "          20524,    376,    799,    320,   5481,   3416,    291,    555,  62412,\n",
      "           9334,    555,  16930,    570]], device='cuda:0'),) and output tensor([[[ 2.6512e-04, -4.9973e-04, -5.8365e-04,  ...,  3.8147e-03,\n",
      "           6.3419e-05,  1.1902e-03],\n",
      "         [-1.6113e-02,  8.6670e-03, -5.5542e-03,  ...,  1.7578e-02,\n",
      "           6.5308e-03,  1.2283e-03],\n",
      "         [-7.9346e-03,  1.6602e-02, -4.1504e-03,  ..., -1.3611e-02,\n",
      "           1.0803e-02, -1.0437e-02],\n",
      "         ...,\n",
      "         [-4.8828e-03, -3.0823e-03, -4.6387e-03,  ...,  4.8523e-03,\n",
      "          -7.0095e-05,  1.1673e-03],\n",
      "         [-2.7313e-03,  6.0654e-04,  3.6812e-04,  ..., -2.7313e-03,\n",
      "          -3.6469e-03,  3.5248e-03],\n",
      "         [-6.6528e-03,  7.6294e-04,  2.1973e-03,  ...,  1.2360e-03,\n",
      "           6.8359e-03,  5.3406e-03]]], device='cuda:0')\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[ 2.6512e-04, -4.9973e-04, -5.8365e-04,  ...,  3.8147e-03,\n",
      "           6.3419e-05,  1.1902e-03],\n",
      "         [-1.6113e-02,  8.6670e-03, -5.5542e-03,  ...,  1.7578e-02,\n",
      "           6.5308e-03,  1.2283e-03],\n",
      "         [-7.9346e-03,  1.6602e-02, -4.1504e-03,  ..., -1.3611e-02,\n",
      "           1.0803e-02, -1.0437e-02],\n",
      "         ...,\n",
      "         [-4.8828e-03, -3.0823e-03, -4.6387e-03,  ...,  4.8523e-03,\n",
      "          -7.0095e-05,  1.1673e-03],\n",
      "         [-2.7313e-03,  6.0654e-04,  3.6812e-04,  ..., -2.7313e-03,\n",
      "          -3.6469e-03,  3.5248e-03],\n",
      "         [-6.6528e-03,  7.6294e-04,  2.1973e-03,  ...,  1.2360e-03,\n",
      "           6.8359e-03,  5.3406e-03]]], device='cuda:0'),) and output (tensor([[[-0.0004,  0.0146, -0.0020,  ...,  0.0066, -0.0194,  0.0020],\n",
      "         [-0.0401,  0.0300,  0.0135,  ...,  0.0258, -0.0238,  0.0026],\n",
      "         [-0.0146,  0.0183,  0.0058,  ..., -0.0478, -0.0070, -0.0244],\n",
      "         ...,\n",
      "         [-0.0042, -0.0054, -0.0005,  ..., -0.0167, -0.0022, -0.0075],\n",
      "         [-0.0003, -0.0196, -0.0174,  ...,  0.0047, -0.0156,  0.0471],\n",
      "         [ 0.0041,  0.0040,  0.0091,  ...,  0.0166,  0.0085,  0.0091]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.0004,  0.0146, -0.0020,  ...,  0.0066, -0.0194,  0.0020],\n",
      "         [-0.0401,  0.0300,  0.0135,  ...,  0.0258, -0.0238,  0.0026],\n",
      "         [-0.0146,  0.0183,  0.0058,  ..., -0.0478, -0.0070, -0.0244],\n",
      "         ...,\n",
      "         [-0.0042, -0.0054, -0.0005,  ..., -0.0167, -0.0022, -0.0075],\n",
      "         [-0.0003, -0.0196, -0.0174,  ...,  0.0047, -0.0156,  0.0471],\n",
      "         [ 0.0041,  0.0040,  0.0091,  ...,  0.0166,  0.0085,  0.0091]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-9.7233e-02,  7.8729e-02, -4.1909e-02,  ...,  7.0540e-03,\n",
      "           8.6910e-02,  2.1819e-02],\n",
      "         [-4.4247e-02,  3.6295e-02,  1.8516e-02,  ...,  6.2567e-03,\n",
      "          -3.7343e-02,  9.0176e-03],\n",
      "         [-5.2621e-02,  9.0507e-03,  2.5289e-02,  ..., -1.0750e-01,\n",
      "           1.1599e-02, -1.7797e-02],\n",
      "         ...,\n",
      "         [-1.4066e-02,  4.3199e-03, -1.3870e-02,  ..., -5.0315e-03,\n",
      "          -2.8504e-02,  2.1528e-04],\n",
      "         [ 9.5155e-03, -8.4052e-03, -3.1333e-02,  ...,  1.8046e-02,\n",
      "          -4.6628e-02,  2.7142e-02],\n",
      "         [ 4.8237e-03, -6.1684e-03, -9.7863e-05,  ...,  5.7285e-03,\n",
      "          -4.2772e-03,  1.4995e-02]]], device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-9.7233e-02,  7.8729e-02, -4.1909e-02,  ...,  7.0540e-03,\n",
      "           8.6910e-02,  2.1819e-02],\n",
      "         [-4.4247e-02,  3.6295e-02,  1.8516e-02,  ...,  6.2567e-03,\n",
      "          -3.7343e-02,  9.0176e-03],\n",
      "         [-5.2621e-02,  9.0507e-03,  2.5289e-02,  ..., -1.0750e-01,\n",
      "           1.1599e-02, -1.7797e-02],\n",
      "         ...,\n",
      "         [-1.4066e-02,  4.3199e-03, -1.3870e-02,  ..., -5.0315e-03,\n",
      "          -2.8504e-02,  2.1528e-04],\n",
      "         [ 9.5155e-03, -8.4052e-03, -3.1333e-02,  ...,  1.8046e-02,\n",
      "          -4.6628e-02,  2.7142e-02],\n",
      "         [ 4.8237e-03, -6.1684e-03, -9.7863e-05,  ...,  5.7285e-03,\n",
      "          -4.2772e-03,  1.4995e-02]]], device='cuda:0'),) and output (tensor([[[-0.1040,  0.0875, -0.0360,  ...,  0.0205,  0.0997,  0.0184],\n",
      "         [-0.1058,  0.0228,  0.0257,  ...,  0.0664, -0.0585,  0.0202],\n",
      "         [-0.0306,  0.0094,  0.0160,  ..., -0.0627,  0.0037, -0.0196],\n",
      "         ...,\n",
      "         [-0.0446,  0.0083,  0.0186,  ..., -0.0252, -0.0500, -0.0241],\n",
      "         [-0.0187, -0.0349,  0.0112,  ..., -0.0034, -0.0584, -0.0034],\n",
      "         [-0.0094, -0.0001,  0.0188,  ...,  0.0030, -0.0051,  0.0385]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.1040,  0.0875, -0.0360,  ...,  0.0205,  0.0997,  0.0184],\n",
      "         [-0.1058,  0.0228,  0.0257,  ...,  0.0664, -0.0585,  0.0202],\n",
      "         [-0.0306,  0.0094,  0.0160,  ..., -0.0627,  0.0037, -0.0196],\n",
      "         ...,\n",
      "         [-0.0446,  0.0083,  0.0186,  ..., -0.0252, -0.0500, -0.0241],\n",
      "         [-0.0187, -0.0349,  0.0112,  ..., -0.0034, -0.0584, -0.0034],\n",
      "         [-0.0094, -0.0001,  0.0188,  ...,  0.0030, -0.0051,  0.0385]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.0870,  0.1049, -0.0173,  ...,  0.0700,  0.0989,  0.0181],\n",
      "         [-0.1037,  0.0624,  0.0278,  ...,  0.0364, -0.0458, -0.0381],\n",
      "         [-0.0285, -0.0014,  0.0431,  ..., -0.0343,  0.0229,  0.0136],\n",
      "         ...,\n",
      "         [-0.0539,  0.0276, -0.0087,  ..., -0.0364, -0.0988, -0.0619],\n",
      "         [ 0.0102, -0.0505, -0.0534,  ..., -0.0008, -0.0718, -0.0245],\n",
      "         [-0.0171, -0.0141,  0.0231,  ...,  0.0046, -0.0473,  0.0318]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.0870,  0.1049, -0.0173,  ...,  0.0700,  0.0989,  0.0181],\n",
      "         [-0.1037,  0.0624,  0.0278,  ...,  0.0364, -0.0458, -0.0381],\n",
      "         [-0.0285, -0.0014,  0.0431,  ..., -0.0343,  0.0229,  0.0136],\n",
      "         ...,\n",
      "         [-0.0539,  0.0276, -0.0087,  ..., -0.0364, -0.0988, -0.0619],\n",
      "         [ 0.0102, -0.0505, -0.0534,  ..., -0.0008, -0.0718, -0.0245],\n",
      "         [-0.0171, -0.0141,  0.0231,  ...,  0.0046, -0.0473,  0.0318]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.1113,  0.1022, -0.0102,  ...,  0.0436,  0.1283,  0.0321],\n",
      "         [-0.1543,  0.0744,  0.0554,  ...,  0.0953, -0.0112, -0.0051],\n",
      "         [-0.0498,  0.0205,  0.0245,  ..., -0.0164, -0.0794,  0.0040],\n",
      "         ...,\n",
      "         [-0.0602, -0.0102, -0.0388,  ..., -0.0727, -0.1031, -0.0495],\n",
      "         [ 0.0432, -0.0133, -0.0803,  ...,  0.0152, -0.0396,  0.0170],\n",
      "         [-0.0179, -0.0461,  0.0285,  ...,  0.0441, -0.0601,  0.0362]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.1113,  0.1022, -0.0102,  ...,  0.0436,  0.1283,  0.0321],\n",
      "         [-0.1543,  0.0744,  0.0554,  ...,  0.0953, -0.0112, -0.0051],\n",
      "         [-0.0498,  0.0205,  0.0245,  ..., -0.0164, -0.0794,  0.0040],\n",
      "         ...,\n",
      "         [-0.0602, -0.0102, -0.0388,  ..., -0.0727, -0.1031, -0.0495],\n",
      "         [ 0.0432, -0.0133, -0.0803,  ...,  0.0152, -0.0396,  0.0170],\n",
      "         [-0.0179, -0.0461,  0.0285,  ...,  0.0441, -0.0601,  0.0362]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.1034,  0.1132,  0.0096,  ...,  0.0453,  0.1477,  0.0301],\n",
      "         [-0.0219,  0.0696,  0.0377,  ...,  0.0581, -0.0357,  0.0267],\n",
      "         [-0.0367,  0.0611,  0.0411,  ..., -0.0979, -0.0466,  0.0880],\n",
      "         ...,\n",
      "         [-0.0986, -0.0554,  0.0462,  ..., -0.1109, -0.1157, -0.0665],\n",
      "         [ 0.0033, -0.0353, -0.0697,  ..., -0.0888, -0.0337,  0.1025],\n",
      "         [-0.0374, -0.0136,  0.0138,  ...,  0.0597, -0.0384,  0.0633]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.1034,  0.1132,  0.0096,  ...,  0.0453,  0.1477,  0.0301],\n",
      "         [-0.0219,  0.0696,  0.0377,  ...,  0.0581, -0.0357,  0.0267],\n",
      "         [-0.0367,  0.0611,  0.0411,  ..., -0.0979, -0.0466,  0.0880],\n",
      "         ...,\n",
      "         [-0.0986, -0.0554,  0.0462,  ..., -0.1109, -0.1157, -0.0665],\n",
      "         [ 0.0033, -0.0353, -0.0697,  ..., -0.0888, -0.0337,  0.1025],\n",
      "         [-0.0374, -0.0136,  0.0138,  ...,  0.0597, -0.0384,  0.0633]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-1.0708e-01,  1.2526e-01,  2.1099e-02,  ...,  1.2627e-02,\n",
      "           1.5859e-01,  2.7839e-02],\n",
      "         [-2.0141e-03,  1.0025e-01, -7.1702e-02,  ...,  8.5174e-02,\n",
      "          -5.1911e-04, -1.3886e-03],\n",
      "         [-8.9453e-02,  1.0419e-02, -2.9185e-02,  ...,  1.1688e-02,\n",
      "          -8.2499e-03,  1.8132e-01],\n",
      "         ...,\n",
      "         [-7.7071e-02, -2.8589e-02,  2.1594e-02,  ...,  1.3242e-02,\n",
      "          -8.6551e-02, -1.3491e-01],\n",
      "         [ 4.5659e-02, -5.6253e-02, -5.6480e-02,  ...,  3.5589e-02,\n",
      "          -2.0118e-02,  1.5016e-01],\n",
      "         [-3.3125e-02, -8.1450e-02, -1.5729e-04,  ...,  6.1026e-02,\n",
      "          -4.9963e-02,  5.9180e-02]]], device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-1.0708e-01,  1.2526e-01,  2.1099e-02,  ...,  1.2627e-02,\n",
      "           1.5859e-01,  2.7839e-02],\n",
      "         [-2.0141e-03,  1.0025e-01, -7.1702e-02,  ...,  8.5174e-02,\n",
      "          -5.1911e-04, -1.3886e-03],\n",
      "         [-8.9453e-02,  1.0419e-02, -2.9185e-02,  ...,  1.1688e-02,\n",
      "          -8.2499e-03,  1.8132e-01],\n",
      "         ...,\n",
      "         [-7.7071e-02, -2.8589e-02,  2.1594e-02,  ...,  1.3242e-02,\n",
      "          -8.6551e-02, -1.3491e-01],\n",
      "         [ 4.5659e-02, -5.6253e-02, -5.6480e-02,  ...,  3.5589e-02,\n",
      "          -2.0118e-02,  1.5016e-01],\n",
      "         [-3.3125e-02, -8.1450e-02, -1.5729e-04,  ...,  6.1026e-02,\n",
      "          -4.9963e-02,  5.9180e-02]]], device='cuda:0'),) and output (tensor([[[-0.0945,  0.1848,  0.0036,  ..., -0.0473,  0.1489,  0.0536],\n",
      "         [-0.0662,  0.1391, -0.1392,  ...,  0.0379, -0.0861, -0.0227],\n",
      "         [-0.1210, -0.0036, -0.0942,  ..., -0.1214, -0.0582,  0.1339],\n",
      "         ...,\n",
      "         [-0.1147, -0.0354,  0.0935,  ..., -0.0375, -0.1381, -0.1575],\n",
      "         [ 0.0461, -0.0712, -0.0130,  ...,  0.1112, -0.0640,  0.0743],\n",
      "         [-0.0376, -0.0911, -0.0180,  ...,  0.0869, -0.0674,  0.0292]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.0945,  0.1848,  0.0036,  ..., -0.0473,  0.1489,  0.0536],\n",
      "         [-0.0662,  0.1391, -0.1392,  ...,  0.0379, -0.0861, -0.0227],\n",
      "         [-0.1210, -0.0036, -0.0942,  ..., -0.1214, -0.0582,  0.1339],\n",
      "         ...,\n",
      "         [-0.1147, -0.0354,  0.0935,  ..., -0.0375, -0.1381, -0.1575],\n",
      "         [ 0.0461, -0.0712, -0.0130,  ...,  0.1112, -0.0640,  0.0743],\n",
      "         [-0.0376, -0.0911, -0.0180,  ...,  0.0869, -0.0674,  0.0292]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.1060,  0.2113,  0.0270,  ..., -0.1141,  0.1610,  0.0528],\n",
      "         [-0.0660,  0.1528, -0.0806,  ..., -0.0202, -0.0906,  0.0252],\n",
      "         [-0.0835, -0.0521, -0.1501,  ..., -0.1917, -0.0380,  0.1264],\n",
      "         ...,\n",
      "         [-0.0454, -0.0555,  0.0140,  ..., -0.0321, -0.1321, -0.1764],\n",
      "         [-0.0157, -0.1489, -0.0462,  ...,  0.1665, -0.0491,  0.1697],\n",
      "         [-0.1262, -0.1153, -0.0150,  ...,  0.0171,  0.0456,  0.0255]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.1060,  0.2113,  0.0270,  ..., -0.1141,  0.1610,  0.0528],\n",
      "         [-0.0660,  0.1528, -0.0806,  ..., -0.0202, -0.0906,  0.0252],\n",
      "         [-0.0835, -0.0521, -0.1501,  ..., -0.1917, -0.0380,  0.1264],\n",
      "         ...,\n",
      "         [-0.0454, -0.0555,  0.0140,  ..., -0.0321, -0.1321, -0.1764],\n",
      "         [-0.0157, -0.1489, -0.0462,  ...,  0.1665, -0.0491,  0.1697],\n",
      "         [-0.1262, -0.1153, -0.0150,  ...,  0.0171,  0.0456,  0.0255]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.0954,  0.2493,  0.0073,  ..., -0.1568,  0.1818,  0.0855],\n",
      "         [-0.0827,  0.1179, -0.0856,  ...,  0.0247, -0.0573,  0.0281],\n",
      "         [-0.1934, -0.0943, -0.1765,  ..., -0.2278,  0.0866,  0.0855],\n",
      "         ...,\n",
      "         [-0.0688, -0.0017,  0.0171,  ..., -0.0663, -0.0323, -0.0656],\n",
      "         [-0.0609, -0.0109, -0.1513,  ...,  0.0929,  0.0198,  0.2031],\n",
      "         [-0.1140, -0.0363,  0.0336,  ...,  0.0156,  0.0185,  0.0373]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.0954,  0.2493,  0.0073,  ..., -0.1568,  0.1818,  0.0855],\n",
      "         [-0.0827,  0.1179, -0.0856,  ...,  0.0247, -0.0573,  0.0281],\n",
      "         [-0.1934, -0.0943, -0.1765,  ..., -0.2278,  0.0866,  0.0855],\n",
      "         ...,\n",
      "         [-0.0688, -0.0017,  0.0171,  ..., -0.0663, -0.0323, -0.0656],\n",
      "         [-0.0609, -0.0109, -0.1513,  ...,  0.0929,  0.0198,  0.2031],\n",
      "         [-0.1140, -0.0363,  0.0336,  ...,  0.0156,  0.0185,  0.0373]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.0152,  0.3340,  0.0011,  ..., -0.2750,  0.1933,  0.1034],\n",
      "         [-0.0018,  0.1956, -0.0621,  ..., -0.0064, -0.0779, -0.0067],\n",
      "         [-0.2164, -0.1243, -0.1492,  ..., -0.1697,  0.0743,  0.0438],\n",
      "         ...,\n",
      "         [-0.0471, -0.0651,  0.0424,  ..., -0.0950, -0.0959, -0.1392],\n",
      "         [ 0.0264,  0.0070, -0.0958,  ...,  0.0664, -0.0190,  0.0945],\n",
      "         [-0.0673, -0.0876,  0.0272,  ..., -0.0074, -0.0414,  0.0577]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.0152,  0.3340,  0.0011,  ..., -0.2750,  0.1933,  0.1034],\n",
      "         [-0.0018,  0.1956, -0.0621,  ..., -0.0064, -0.0779, -0.0067],\n",
      "         [-0.2164, -0.1243, -0.1492,  ..., -0.1697,  0.0743,  0.0438],\n",
      "         ...,\n",
      "         [-0.0471, -0.0651,  0.0424,  ..., -0.0950, -0.0959, -0.1392],\n",
      "         [ 0.0264,  0.0070, -0.0958,  ...,  0.0664, -0.0190,  0.0945],\n",
      "         [-0.0673, -0.0876,  0.0272,  ..., -0.0074, -0.0414,  0.0577]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.0356,  0.3483, -0.0366,  ..., -0.2962,  0.2088,  0.1065],\n",
      "         [-0.0022,  0.1488, -0.1166,  ..., -0.0475, -0.0232, -0.0640],\n",
      "         [-0.2596, -0.1750, -0.1343,  ..., -0.0808,  0.1431,  0.0062],\n",
      "         ...,\n",
      "         [-0.0023,  0.0161,  0.0546,  ..., -0.1027, -0.1339, -0.0741],\n",
      "         [ 0.0310, -0.0366, -0.1156,  ..., -0.0009,  0.0893,  0.0642],\n",
      "         [-0.0561, -0.1042,  0.0708,  ..., -0.0199, -0.0347,  0.0385]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.0356,  0.3483, -0.0366,  ..., -0.2962,  0.2088,  0.1065],\n",
      "         [-0.0022,  0.1488, -0.1166,  ..., -0.0475, -0.0232, -0.0640],\n",
      "         [-0.2596, -0.1750, -0.1343,  ..., -0.0808,  0.1431,  0.0062],\n",
      "         ...,\n",
      "         [-0.0023,  0.0161,  0.0546,  ..., -0.1027, -0.1339, -0.0741],\n",
      "         [ 0.0310, -0.0366, -0.1156,  ..., -0.0009,  0.0893,  0.0642],\n",
      "         [-0.0561, -0.1042,  0.0708,  ..., -0.0199, -0.0347,  0.0385]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.0127,  0.3661, -0.0316,  ..., -0.3678,  0.1973,  0.1019],\n",
      "         [ 0.0439,  0.0085, -0.0299,  ..., -0.0030,  0.0133,  0.0015],\n",
      "         [-0.2262, -0.1609, -0.1210,  ..., -0.0888,  0.0322, -0.0339],\n",
      "         ...,\n",
      "         [ 0.0396,  0.0704,  0.1617,  ..., -0.0982, -0.0377,  0.0602],\n",
      "         [ 0.0034,  0.0388,  0.0189,  ...,  0.0623,  0.1242,  0.1498],\n",
      "         [-0.0466, -0.1327,  0.1353,  ...,  0.0852,  0.0094,  0.0297]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.0127,  0.3661, -0.0316,  ..., -0.3678,  0.1973,  0.1019],\n",
      "         [ 0.0439,  0.0085, -0.0299,  ..., -0.0030,  0.0133,  0.0015],\n",
      "         [-0.2262, -0.1609, -0.1210,  ..., -0.0888,  0.0322, -0.0339],\n",
      "         ...,\n",
      "         [ 0.0396,  0.0704,  0.1617,  ..., -0.0982, -0.0377,  0.0602],\n",
      "         [ 0.0034,  0.0388,  0.0189,  ...,  0.0623,  0.1242,  0.1498],\n",
      "         [-0.0466, -0.1327,  0.1353,  ...,  0.0852,  0.0094,  0.0297]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.0089,  0.3233, -0.0779,  ..., -0.4491,  0.1845,  0.0951],\n",
      "         [-0.0322, -0.0072, -0.1343,  ..., -0.0150, -0.0120, -0.1274],\n",
      "         [-0.1883, -0.1487, -0.1697,  ..., -0.1554,  0.0138, -0.0876],\n",
      "         ...,\n",
      "         [-0.1619,  0.1006,  0.1637,  ..., -0.0870, -0.0627,  0.1038],\n",
      "         [-0.0883, -0.0310,  0.0172,  ...,  0.0520,  0.0700,  0.1935],\n",
      "         [-0.1467, -0.1595,  0.1993,  ..., -0.0313, -0.1154,  0.1760]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.0089,  0.3233, -0.0779,  ..., -0.4491,  0.1845,  0.0951],\n",
      "         [-0.0322, -0.0072, -0.1343,  ..., -0.0150, -0.0120, -0.1274],\n",
      "         [-0.1883, -0.1487, -0.1697,  ..., -0.1554,  0.0138, -0.0876],\n",
      "         ...,\n",
      "         [-0.1619,  0.1006,  0.1637,  ..., -0.0870, -0.0627,  0.1038],\n",
      "         [-0.0883, -0.0310,  0.0172,  ...,  0.0520,  0.0700,  0.1935],\n",
      "         [-0.1467, -0.1595,  0.1993,  ..., -0.0313, -0.1154,  0.1760]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.0244,  0.2828, -0.0532,  ..., -0.4592,  0.2300,  0.0142],\n",
      "         [-0.0095, -0.0460, -0.1093,  ..., -0.0129, -0.0872, -0.1192],\n",
      "         [-0.2507, -0.1869, -0.1435,  ..., -0.2304, -0.0418, -0.1008],\n",
      "         ...,\n",
      "         [-0.0319,  0.1683,  0.1116,  ..., -0.0977, -0.2270,  0.1654],\n",
      "         [-0.0085,  0.0464,  0.0405,  ..., -0.0100, -0.0295,  0.1061],\n",
      "         [-0.1738, -0.1266,  0.1766,  ...,  0.0671, -0.1140,  0.1470]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.0244,  0.2828, -0.0532,  ..., -0.4592,  0.2300,  0.0142],\n",
      "         [-0.0095, -0.0460, -0.1093,  ..., -0.0129, -0.0872, -0.1192],\n",
      "         [-0.2507, -0.1869, -0.1435,  ..., -0.2304, -0.0418, -0.1008],\n",
      "         ...,\n",
      "         [-0.0319,  0.1683,  0.1116,  ..., -0.0977, -0.2270,  0.1654],\n",
      "         [-0.0085,  0.0464,  0.0405,  ..., -0.0100, -0.0295,  0.1061],\n",
      "         [-0.1738, -0.1266,  0.1766,  ...,  0.0671, -0.1140,  0.1470]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.0928,  0.1485,  0.0158,  ..., -0.5380,  0.2664, -0.0333],\n",
      "         [-0.1237, -0.1032, -0.0058,  ..., -0.0634, -0.0057, -0.0929],\n",
      "         [-0.4226, -0.2741, -0.0080,  ..., -0.1446,  0.0191, -0.3139],\n",
      "         ...,\n",
      "         [-0.1347,  0.0490,  0.0552,  ..., -0.0301, -0.1994,  0.0776],\n",
      "         [-0.2341, -0.0785, -0.0620,  ...,  0.0680, -0.0996,  0.1335],\n",
      "         [-0.1340, -0.1426,  0.1748,  ...,  0.0216, -0.0879,  0.2384]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.0928,  0.1485,  0.0158,  ..., -0.5380,  0.2664, -0.0333],\n",
      "         [-0.1237, -0.1032, -0.0058,  ..., -0.0634, -0.0057, -0.0929],\n",
      "         [-0.4226, -0.2741, -0.0080,  ..., -0.1446,  0.0191, -0.3139],\n",
      "         ...,\n",
      "         [-0.1347,  0.0490,  0.0552,  ..., -0.0301, -0.1994,  0.0776],\n",
      "         [-0.2341, -0.0785, -0.0620,  ...,  0.0680, -0.0996,  0.1335],\n",
      "         [-0.1340, -0.1426,  0.1748,  ...,  0.0216, -0.0879,  0.2384]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.1000,  0.1323,  0.0241,  ..., -0.5470,  0.2844, -0.0457],\n",
      "         [-0.0798,  0.0107,  0.0645,  ..., -0.1736, -0.0118, -0.0830],\n",
      "         [-0.3323, -0.3777,  0.0403,  ..., -0.0489, -0.0680, -0.2907],\n",
      "         ...,\n",
      "         [ 0.1597, -0.0123,  0.0053,  ..., -0.0039, -0.0805, -0.0225],\n",
      "         [ 0.0662, -0.1629, -0.1396,  ...,  0.1571, -0.0395,  0.0626],\n",
      "         [ 0.0085, -0.1195,  0.2216,  ...,  0.0943, -0.1921,  0.1894]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.1000,  0.1323,  0.0241,  ..., -0.5470,  0.2844, -0.0457],\n",
      "         [-0.0798,  0.0107,  0.0645,  ..., -0.1736, -0.0118, -0.0830],\n",
      "         [-0.3323, -0.3777,  0.0403,  ..., -0.0489, -0.0680, -0.2907],\n",
      "         ...,\n",
      "         [ 0.1597, -0.0123,  0.0053,  ..., -0.0039, -0.0805, -0.0225],\n",
      "         [ 0.0662, -0.1629, -0.1396,  ...,  0.1571, -0.0395,  0.0626],\n",
      "         [ 0.0085, -0.1195,  0.2216,  ...,  0.0943, -0.1921,  0.1894]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.1286,  0.0833,  0.0239,  ..., -0.5657,  0.3694, -0.0725],\n",
      "         [-0.1845,  0.0147,  0.0548,  ..., -0.2247, -0.0071, -0.1707],\n",
      "         [-0.2477, -0.4082, -0.1570,  ..., -0.0454, -0.1787, -0.3669],\n",
      "         ...,\n",
      "         [ 0.0277, -0.0733,  0.0354,  ..., -0.1219, -0.1669,  0.0861],\n",
      "         [ 0.0111, -0.0595, -0.1526,  ...,  0.0572, -0.0352,  0.1148],\n",
      "         [-0.0790, -0.1084,  0.1758,  ...,  0.1046, -0.3697,  0.1387]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.1286,  0.0833,  0.0239,  ..., -0.5657,  0.3694, -0.0725],\n",
      "         [-0.1845,  0.0147,  0.0548,  ..., -0.2247, -0.0071, -0.1707],\n",
      "         [-0.2477, -0.4082, -0.1570,  ..., -0.0454, -0.1787, -0.3669],\n",
      "         ...,\n",
      "         [ 0.0277, -0.0733,  0.0354,  ..., -0.1219, -0.1669,  0.0861],\n",
      "         [ 0.0111, -0.0595, -0.1526,  ...,  0.0572, -0.0352,  0.1148],\n",
      "         [-0.0790, -0.1084,  0.1758,  ...,  0.1046, -0.3697,  0.1387]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-1.4171e-01,  7.0286e-02,  3.4279e-02,  ..., -5.5491e-01,\n",
      "           3.1749e-01, -8.4653e-02],\n",
      "         [-2.9720e-01,  5.3854e-02,  3.5240e-04,  ..., -1.5780e-01,\n",
      "          -9.2154e-02, -2.5074e-01],\n",
      "         [-3.2321e-01, -4.5305e-01, -2.3923e-01,  ..., -3.7755e-02,\n",
      "          -1.4293e-01, -4.7712e-01],\n",
      "         ...,\n",
      "         [-1.2228e-01, -3.3176e-01,  2.9933e-02,  ..., -2.7429e-01,\n",
      "          -1.0566e-01,  1.9763e-01],\n",
      "         [-7.1389e-03, -1.3466e-01, -2.7907e-01,  ...,  1.2051e-02,\n",
      "           6.8151e-03,  1.9032e-01],\n",
      "         [-1.6418e-01, -2.0116e-01,  1.9920e-01,  ...,  9.5484e-02,\n",
      "          -4.2509e-01,  1.5960e-01]]], device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-1.4171e-01,  7.0286e-02,  3.4279e-02,  ..., -5.5491e-01,\n",
      "           3.1749e-01, -8.4653e-02],\n",
      "         [-2.9720e-01,  5.3854e-02,  3.5240e-04,  ..., -1.5780e-01,\n",
      "          -9.2154e-02, -2.5074e-01],\n",
      "         [-3.2321e-01, -4.5305e-01, -2.3923e-01,  ..., -3.7755e-02,\n",
      "          -1.4293e-01, -4.7712e-01],\n",
      "         ...,\n",
      "         [-1.2228e-01, -3.3176e-01,  2.9933e-02,  ..., -2.7429e-01,\n",
      "          -1.0566e-01,  1.9763e-01],\n",
      "         [-7.1389e-03, -1.3466e-01, -2.7907e-01,  ...,  1.2051e-02,\n",
      "           6.8151e-03,  1.9032e-01],\n",
      "         [-1.6418e-01, -2.0116e-01,  1.9920e-01,  ...,  9.5484e-02,\n",
      "          -4.2509e-01,  1.5960e-01]]], device='cuda:0'),) and output (tensor([[[-0.1528,  0.0725,  0.0609,  ..., -0.5624,  0.3103, -0.0293],\n",
      "         [-0.3908, -0.0573,  0.0649,  ..., -0.2805, -0.0396, -0.1368],\n",
      "         [-0.4620, -0.4409, -0.2494,  ..., -0.1959, -0.0710, -0.4080],\n",
      "         ...,\n",
      "         [-0.2051, -0.4303,  0.1105,  ..., -0.2716, -0.2865,  0.1692],\n",
      "         [-0.0581, -0.2198, -0.2525,  ...,  0.0300, -0.0499,  0.1694],\n",
      "         [-0.2552, -0.1765,  0.1704,  ...,  0.0979, -0.4386,  0.0828]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.1528,  0.0725,  0.0609,  ..., -0.5624,  0.3103, -0.0293],\n",
      "         [-0.3908, -0.0573,  0.0649,  ..., -0.2805, -0.0396, -0.1368],\n",
      "         [-0.4620, -0.4409, -0.2494,  ..., -0.1959, -0.0710, -0.4080],\n",
      "         ...,\n",
      "         [-0.2051, -0.4303,  0.1105,  ..., -0.2716, -0.2865,  0.1692],\n",
      "         [-0.0581, -0.2198, -0.2525,  ...,  0.0300, -0.0499,  0.1694],\n",
      "         [-0.2552, -0.1765,  0.1704,  ...,  0.0979, -0.4386,  0.0828]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.1559,  0.0520,  0.0566,  ..., -0.5805,  0.2614, -0.0032],\n",
      "         [-0.4862, -0.1525,  0.1492,  ..., -0.1361, -0.1307, -0.1420],\n",
      "         [-0.4711, -0.6555, -0.1286,  ..., -0.1772, -0.3081, -0.4329],\n",
      "         ...,\n",
      "         [-0.1629, -0.5037,  0.1011,  ..., -0.2082, -0.2734,  0.1132],\n",
      "         [-0.1418, -0.2083, -0.2198,  ...,  0.0575, -0.0642,  0.1453],\n",
      "         [-0.1774, -0.3745,  0.1111,  ...,  0.0902, -0.4055,  0.0619]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.1559,  0.0520,  0.0566,  ..., -0.5805,  0.2614, -0.0032],\n",
      "         [-0.4862, -0.1525,  0.1492,  ..., -0.1361, -0.1307, -0.1420],\n",
      "         [-0.4711, -0.6555, -0.1286,  ..., -0.1772, -0.3081, -0.4329],\n",
      "         ...,\n",
      "         [-0.1629, -0.5037,  0.1011,  ..., -0.2082, -0.2734,  0.1132],\n",
      "         [-0.1418, -0.2083, -0.2198,  ...,  0.0575, -0.0642,  0.1453],\n",
      "         [-0.1774, -0.3745,  0.1111,  ...,  0.0902, -0.4055,  0.0619]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.1488,  0.0445,  0.0765,  ..., -0.5456,  0.2702,  0.0098],\n",
      "         [-0.5877, -0.1936,  0.1361,  ..., -0.0183,  0.0057, -0.1792],\n",
      "         [-0.3510, -0.6635, -0.2669,  ..., -0.1931, -0.3364, -0.3954],\n",
      "         ...,\n",
      "         [-0.3460, -0.4682,  0.3364,  ..., -0.1487, -0.1965,  0.0612],\n",
      "         [-0.2828, -0.1220, -0.1798,  ...,  0.0263, -0.0258,  0.1389],\n",
      "         [-0.2730, -0.2933,  0.0855,  ..., -0.0323, -0.3723,  0.0660]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.1488,  0.0445,  0.0765,  ..., -0.5456,  0.2702,  0.0098],\n",
      "         [-0.5877, -0.1936,  0.1361,  ..., -0.0183,  0.0057, -0.1792],\n",
      "         [-0.3510, -0.6635, -0.2669,  ..., -0.1931, -0.3364, -0.3954],\n",
      "         ...,\n",
      "         [-0.3460, -0.4682,  0.3364,  ..., -0.1487, -0.1965,  0.0612],\n",
      "         [-0.2828, -0.1220, -0.1798,  ...,  0.0263, -0.0258,  0.1389],\n",
      "         [-0.2730, -0.2933,  0.0855,  ..., -0.0323, -0.3723,  0.0660]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-1.2677e-01,  3.3335e-02,  9.3478e-02,  ..., -5.3204e-01,\n",
      "           2.7953e-01,  5.9818e-04],\n",
      "         [-5.4254e-01, -2.9416e-01,  1.1099e-01,  ..., -7.4366e-02,\n",
      "           2.0368e-01, -9.1497e-02],\n",
      "         [-4.2142e-01, -7.3107e-01, -3.3358e-01,  ..., -2.9636e-01,\n",
      "          -3.6018e-01, -3.6818e-01],\n",
      "         ...,\n",
      "         [-5.0444e-01, -6.2386e-01,  3.9570e-01,  ..., -4.6631e-01,\n",
      "          -1.1860e-01, -1.3605e-01],\n",
      "         [-3.2316e-01, -1.3654e-01, -6.0990e-02,  ..., -1.6203e-01,\n",
      "           2.4304e-02,  9.1309e-02],\n",
      "         [-2.4250e-01, -1.9507e-01,  1.6713e-01,  ..., -1.0591e-01,\n",
      "          -3.4730e-01,  1.7590e-01]]], device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-1.2677e-01,  3.3335e-02,  9.3478e-02,  ..., -5.3204e-01,\n",
      "           2.7953e-01,  5.9818e-04],\n",
      "         [-5.4254e-01, -2.9416e-01,  1.1099e-01,  ..., -7.4366e-02,\n",
      "           2.0368e-01, -9.1497e-02],\n",
      "         [-4.2142e-01, -7.3107e-01, -3.3358e-01,  ..., -2.9636e-01,\n",
      "          -3.6018e-01, -3.6818e-01],\n",
      "         ...,\n",
      "         [-5.0444e-01, -6.2386e-01,  3.9570e-01,  ..., -4.6631e-01,\n",
      "          -1.1860e-01, -1.3605e-01],\n",
      "         [-3.2316e-01, -1.3654e-01, -6.0990e-02,  ..., -1.6203e-01,\n",
      "           2.4304e-02,  9.1309e-02],\n",
      "         [-2.4250e-01, -1.9507e-01,  1.6713e-01,  ..., -1.0591e-01,\n",
      "          -3.4730e-01,  1.7590e-01]]], device='cuda:0'),) and output (tensor([[[-0.1355,  0.0227,  0.0554,  ..., -0.5473,  0.2470, -0.0229],\n",
      "         [-0.4298, -0.3588,  0.1629,  ..., -0.2425,  0.0962, -0.1345],\n",
      "         [-0.5646, -0.9280, -0.4146,  ..., -0.3824, -0.3839, -0.4171],\n",
      "         ...,\n",
      "         [-0.6980, -0.9249,  0.3433,  ..., -0.3652, -0.2885, -0.2138],\n",
      "         [-0.3441, -0.2363, -0.0716,  ..., -0.3388, -0.0369,  0.0124],\n",
      "         [-0.2501, -0.3183,  0.0976,  ..., -0.0087, -0.4020,  0.1623]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.1355,  0.0227,  0.0554,  ..., -0.5473,  0.2470, -0.0229],\n",
      "         [-0.4298, -0.3588,  0.1629,  ..., -0.2425,  0.0962, -0.1345],\n",
      "         [-0.5646, -0.9280, -0.4146,  ..., -0.3824, -0.3839, -0.4171],\n",
      "         ...,\n",
      "         [-0.6980, -0.9249,  0.3433,  ..., -0.3652, -0.2885, -0.2138],\n",
      "         [-0.3441, -0.2363, -0.0716,  ..., -0.3388, -0.0369,  0.0124],\n",
      "         [-0.2501, -0.3183,  0.0976,  ..., -0.0087, -0.4020,  0.1623]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.1350,  0.0163,  0.0576,  ..., -0.5555,  0.2435,  0.0051],\n",
      "         [-0.5245, -0.4302,  0.2398,  ..., -0.1639,  0.1147, -0.1599],\n",
      "         [-0.4848, -0.8088, -0.3904,  ..., -0.3394, -0.5574, -0.3670],\n",
      "         ...,\n",
      "         [-0.4423, -0.7977,  0.3800,  ..., -0.4054, -0.3578, -0.1447],\n",
      "         [-0.3048, -0.3650,  0.0344,  ..., -0.3261,  0.1367,  0.2957],\n",
      "         [-0.1324, -0.5740,  0.1102,  ...,  0.0481, -0.4424,  0.0907]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.1350,  0.0163,  0.0576,  ..., -0.5555,  0.2435,  0.0051],\n",
      "         [-0.5245, -0.4302,  0.2398,  ..., -0.1639,  0.1147, -0.1599],\n",
      "         [-0.4848, -0.8088, -0.3904,  ..., -0.3394, -0.5574, -0.3670],\n",
      "         ...,\n",
      "         [-0.4423, -0.7977,  0.3800,  ..., -0.4054, -0.3578, -0.1447],\n",
      "         [-0.3048, -0.3650,  0.0344,  ..., -0.3261,  0.1367,  0.2957],\n",
      "         [-0.1324, -0.5740,  0.1102,  ...,  0.0481, -0.4424,  0.0907]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.1301, -0.0029,  0.0279,  ..., -0.5537,  0.2277,  0.0164],\n",
      "         [-0.6334, -0.4102,  0.3207,  ..., -0.0684,  0.0655, -0.2914],\n",
      "         [-0.4002, -0.8030, -0.4040,  ..., -0.3144, -0.4401, -0.2834],\n",
      "         ...,\n",
      "         [-0.4542, -0.5936,  0.3607,  ..., -0.5523, -0.2994, -0.1731],\n",
      "         [-0.4064, -0.4148,  0.1989,  ..., -0.3464,  0.1152,  0.2560],\n",
      "         [-0.1844, -0.4795,  0.1811,  ...,  0.0554, -0.5732,  0.0976]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.1301, -0.0029,  0.0279,  ..., -0.5537,  0.2277,  0.0164],\n",
      "         [-0.6334, -0.4102,  0.3207,  ..., -0.0684,  0.0655, -0.2914],\n",
      "         [-0.4002, -0.8030, -0.4040,  ..., -0.3144, -0.4401, -0.2834],\n",
      "         ...,\n",
      "         [-0.4542, -0.5936,  0.3607,  ..., -0.5523, -0.2994, -0.1731],\n",
      "         [-0.4064, -0.4148,  0.1989,  ..., -0.3464,  0.1152,  0.2560],\n",
      "         [-0.1844, -0.4795,  0.1811,  ...,  0.0554, -0.5732,  0.0976]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.1219,  0.0061, -0.0021,  ..., -0.5922,  0.2788, -0.0020],\n",
      "         [-0.5137, -0.3447,  0.3415,  ..., -0.1962,  0.0743, -0.3774],\n",
      "         [-0.3766, -0.8112, -0.2222,  ..., -0.4428, -0.3349, -0.0686],\n",
      "         ...,\n",
      "         [-0.5481, -0.5650,  0.2828,  ..., -0.4753, -0.2903, -0.0987],\n",
      "         [-0.5385, -0.3037,  0.2832,  ...,  0.0149,  0.2616,  0.4922],\n",
      "         [-0.2228, -0.5315,  0.3059,  ...,  0.2005, -0.6030,  0.2468]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.1219,  0.0061, -0.0021,  ..., -0.5922,  0.2788, -0.0020],\n",
      "         [-0.5137, -0.3447,  0.3415,  ..., -0.1962,  0.0743, -0.3774],\n",
      "         [-0.3766, -0.8112, -0.2222,  ..., -0.4428, -0.3349, -0.0686],\n",
      "         ...,\n",
      "         [-0.5481, -0.5650,  0.2828,  ..., -0.4753, -0.2903, -0.0987],\n",
      "         [-0.5385, -0.3037,  0.2832,  ...,  0.0149,  0.2616,  0.4922],\n",
      "         [-0.2228, -0.5315,  0.3059,  ...,  0.2005, -0.6030,  0.2468]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.1067,  0.0272,  0.0495,  ..., -0.5550,  0.3449,  0.0266],\n",
      "         [-0.3596, -0.4166,  0.4309,  ..., -0.2844,  0.1600, -0.2812],\n",
      "         [-0.4348, -0.9205, -0.2234,  ..., -0.4733, -0.3113,  0.0674],\n",
      "         ...,\n",
      "         [-0.7819, -0.4781,  0.1027,  ..., -0.2704, -0.3318, -0.2643],\n",
      "         [-0.6514, -0.3074,  0.3356,  ...,  0.1984,  0.1176,  0.4801],\n",
      "         [-0.2314, -0.5405,  0.2581,  ...,  0.3218, -0.7622,  0.2914]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.1067,  0.0272,  0.0495,  ..., -0.5550,  0.3449,  0.0266],\n",
      "         [-0.3596, -0.4166,  0.4309,  ..., -0.2844,  0.1600, -0.2812],\n",
      "         [-0.4348, -0.9205, -0.2234,  ..., -0.4733, -0.3113,  0.0674],\n",
      "         ...,\n",
      "         [-0.7819, -0.4781,  0.1027,  ..., -0.2704, -0.3318, -0.2643],\n",
      "         [-0.6514, -0.3074,  0.3356,  ...,  0.1984,  0.1176,  0.4801],\n",
      "         [-0.2314, -0.5405,  0.2581,  ...,  0.3218, -0.7622,  0.2914]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.0592,  0.0731,  0.0500,  ..., -0.5517,  0.3541,  0.0595],\n",
      "         [-0.2750, -0.4862,  0.5325,  ..., -0.3929,  0.2905, -0.2240],\n",
      "         [-0.4268, -0.9137, -0.2252,  ..., -0.4621, -0.0831, -0.0841],\n",
      "         ...,\n",
      "         [-0.9784, -0.4491,  0.0915,  ..., -0.1665, -0.2609, -0.4295],\n",
      "         [-0.6975, -0.5728,  0.5031,  ...,  0.1724,  0.5742,  0.3418],\n",
      "         [-0.3007, -0.6068,  0.6229,  ..., -0.0351, -0.5201,  0.2733]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.0592,  0.0731,  0.0500,  ..., -0.5517,  0.3541,  0.0595],\n",
      "         [-0.2750, -0.4862,  0.5325,  ..., -0.3929,  0.2905, -0.2240],\n",
      "         [-0.4268, -0.9137, -0.2252,  ..., -0.4621, -0.0831, -0.0841],\n",
      "         ...,\n",
      "         [-0.9784, -0.4491,  0.0915,  ..., -0.1665, -0.2609, -0.4295],\n",
      "         [-0.6975, -0.5728,  0.5031,  ...,  0.1724,  0.5742,  0.3418],\n",
      "         [-0.3007, -0.6068,  0.6229,  ..., -0.0351, -0.5201,  0.2733]]],\n",
      "       device='cuda:0'),) and output (tensor([[[ 0.1143,  0.2237,  0.3292,  ..., -0.7706,  0.2608,  0.0269],\n",
      "         [-0.0571, -0.7245,  0.8273,  ..., -0.4477,  0.1004, -0.5636],\n",
      "         [-0.2279, -1.1624, -0.1400,  ..., -0.3923, -0.3665, -0.0475],\n",
      "         ...,\n",
      "         [-1.0889, -0.5198,  0.1088,  ..., -0.2137, -0.3404, -0.4297],\n",
      "         [-0.7149, -1.0243,  0.7069,  ...,  0.2859,  0.3975,  0.0575],\n",
      "         [-0.4150, -0.9159,  0.6687,  ...,  0.0106, -0.7312,  0.2254]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[ 0.1143,  0.2237,  0.3292,  ..., -0.7706,  0.2608,  0.0269],\n",
      "         [-0.0571, -0.7245,  0.8273,  ..., -0.4477,  0.1004, -0.5636],\n",
      "         [-0.2279, -1.1624, -0.1400,  ..., -0.3923, -0.3665, -0.0475],\n",
      "         ...,\n",
      "         [-1.0889, -0.5198,  0.1088,  ..., -0.2137, -0.3404, -0.4297],\n",
      "         [-0.7149, -1.0243,  0.7069,  ...,  0.2859,  0.3975,  0.0575],\n",
      "         [-0.4150, -0.9159,  0.6687,  ...,  0.0106, -0.7312,  0.2254]]],\n",
      "       device='cuda:0'),) and output (tensor([[[ 0.3711,  0.5489,  0.6100,  ..., -0.7499,  0.2080,  0.4195],\n",
      "         [ 0.1820, -0.8959,  0.7963,  ..., -0.3602, -0.1109, -0.5925],\n",
      "         [-0.1525, -1.3434, -0.3477,  ..., -0.1254, -0.5361, -0.2844],\n",
      "         ...,\n",
      "         [-1.5880, -0.7005,  0.1080,  ..., -0.2050, -0.4037, -0.7183],\n",
      "         [-0.9593, -0.8525,  0.9962,  ...,  0.2043,  0.1580, -0.4483],\n",
      "         [-0.8108, -1.3704,  0.5319,  ...,  0.0224, -0.8992, -0.0386]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[ 0.3711,  0.5489,  0.6100,  ..., -0.7499,  0.2080,  0.4195],\n",
      "         [ 0.1820, -0.8959,  0.7963,  ..., -0.3602, -0.1109, -0.5925],\n",
      "         [-0.1525, -1.3434, -0.3477,  ..., -0.1254, -0.5361, -0.2844],\n",
      "         ...,\n",
      "         [-1.5880, -0.7005,  0.1080,  ..., -0.2050, -0.4037, -0.7183],\n",
      "         [-0.9593, -0.8525,  0.9962,  ...,  0.2043,  0.1580, -0.4483],\n",
      "         [-0.8108, -1.3704,  0.5319,  ...,  0.0224, -0.8992, -0.0386]]],\n",
      "       device='cuda:0'),) and output (tensor([[[ 1.1544,  3.2965,  1.6534,  ..., -2.7009,  2.6553,  2.2700],\n",
      "         [-0.4784, -0.4922,  0.6369,  ..., -0.3695, -0.1523, -0.6018],\n",
      "         [-0.1925, -0.8501,  0.3959,  ..., -0.3445, -0.9934,  0.1144],\n",
      "         ...,\n",
      "         [-1.6506, -0.1603, -1.3725,  ..., -0.6694, -0.1337, -0.9454],\n",
      "         [-1.7308, -0.7187,  0.2921,  ..., -0.0050, -0.0784,  0.3718],\n",
      "         [-1.7173, -2.1999,  0.7290,  ...,  0.6766, -0.1468,  0.6942]]],\n",
      "       device='cuda:0'),)\n",
      "[Debug] Layer names being passed: ['model.layers.0', 'model.layers.1', 'model.layers.2', 'model.layers.3', 'model.layers.4', 'model.layers.5', 'model.layers.6', 'model.layers.7', 'model.layers.8', 'model.layers.9', 'model.layers.10', 'model.layers.11', 'model.layers.12', 'model.layers.13', 'model.layers.14', 'model.layers.15', 'model.layers.16', 'model.layers.17', 'model.layers.18', 'model.layers.19', 'model.layers.20', 'model.layers.21', 'model.layers.22', 'model.layers.23', 'model.layers.24', 'model.layers.25', 'model.layers.26', 'model.layers.27', 'model.layers.28', 'model.layers.29', 'model.layers.30', 'model.layers.31', 'model.embed_tokens']\n",
      "[Debug] Trying to access layer: model.layers.0\n",
      "[Debug] Successfully found layer: model.layers.0\n",
      "[Debug] Trying to access layer: model.layers.1\n",
      "[Debug] Successfully found layer: model.layers.1\n",
      "[Debug] Trying to access layer: model.layers.2\n",
      "[Debug] Successfully found layer: model.layers.2\n",
      "[Debug] Trying to access layer: model.layers.3\n",
      "[Debug] Successfully found layer: model.layers.3\n",
      "[Debug] Trying to access layer: model.layers.4\n",
      "[Debug] Successfully found layer: model.layers.4\n",
      "[Debug] Trying to access layer: model.layers.5\n",
      "[Debug] Successfully found layer: model.layers.5\n",
      "[Debug] Trying to access layer: model.layers.6\n",
      "[Debug] Successfully found layer: model.layers.6\n",
      "[Debug] Trying to access layer: model.layers.7\n",
      "[Debug] Successfully found layer: model.layers.7\n",
      "[Debug] Trying to access layer: model.layers.8\n",
      "[Debug] Successfully found layer: model.layers.8\n",
      "[Debug] Trying to access layer: model.layers.9\n",
      "[Debug] Successfully found layer: model.layers.9\n",
      "[Debug] Trying to access layer: model.layers.10\n",
      "[Debug] Successfully found layer: model.layers.10\n",
      "[Debug] Trying to access layer: model.layers.11\n",
      "[Debug] Successfully found layer: model.layers.11\n",
      "[Debug] Trying to access layer: model.layers.12\n",
      "[Debug] Successfully found layer: model.layers.12\n",
      "[Debug] Trying to access layer: model.layers.13\n",
      "[Debug] Successfully found layer: model.layers.13\n",
      "[Debug] Trying to access layer: model.layers.14\n",
      "[Debug] Successfully found layer: model.layers.14\n",
      "[Debug] Trying to access layer: model.layers.15\n",
      "[Debug] Successfully found layer: model.layers.15\n",
      "[Debug] Trying to access layer: model.layers.16\n",
      "[Debug] Successfully found layer: model.layers.16\n",
      "[Debug] Trying to access layer: model.layers.17\n",
      "[Debug] Successfully found layer: model.layers.17\n",
      "[Debug] Trying to access layer: model.layers.18\n",
      "[Debug] Successfully found layer: model.layers.18\n",
      "[Debug] Trying to access layer: model.layers.19\n",
      "[Debug] Successfully found layer: model.layers.19\n",
      "[Debug] Trying to access layer: model.layers.20\n",
      "[Debug] Successfully found layer: model.layers.20\n",
      "[Debug] Trying to access layer: model.layers.21\n",
      "[Debug] Successfully found layer: model.layers.21\n",
      "[Debug] Trying to access layer: model.layers.22\n",
      "[Debug] Successfully found layer: model.layers.22\n",
      "[Debug] Trying to access layer: model.layers.23\n",
      "[Debug] Successfully found layer: model.layers.23\n",
      "[Debug] Trying to access layer: model.layers.24\n",
      "[Debug] Successfully found layer: model.layers.24\n",
      "[Debug] Trying to access layer: model.layers.25\n",
      "[Debug] Successfully found layer: model.layers.25\n",
      "[Debug] Trying to access layer: model.layers.26\n",
      "[Debug] Successfully found layer: model.layers.26\n",
      "[Debug] Trying to access layer: model.layers.27\n",
      "[Debug] Successfully found layer: model.layers.27\n",
      "[Debug] Trying to access layer: model.layers.28\n",
      "[Debug] Successfully found layer: model.layers.28\n",
      "[Debug] Trying to access layer: model.layers.29\n",
      "[Debug] Successfully found layer: model.layers.29\n",
      "[Debug] Trying to access layer: model.layers.30\n",
      "[Debug] Successfully found layer: model.layers.30\n",
      "[Debug] Trying to access layer: model.layers.31\n",
      "[Debug] Successfully found layer: model.layers.31\n",
      "[Debug] Trying to access layer: model.embed_tokens\n",
      "[Debug] Successfully found layer: model.embed_tokens\n",
      "[Hook] Layer Embedding(128256, 4096) received input (tensor([[128000,     32,  59064,  15996,     88,  18449,    330,     32,  59064,\n",
      "          15996,     88,  18449,      1,    374,    264,   3224,   5609,   5439,\n",
      "            555,   4418,  43179,    350,    676,     13,  25842,  25891,    330,\n",
      "           8161,    956,  25672,   3092,  18449,      1,    323,  10887,    555,\n",
      "            578,   2947,  11377,  34179,    304,    220,   2550,     16,     11,\n",
      "           1202,    836,    574,   3010,   5614,    311,    330,     32,  59064,\n",
      "          15996,     88,  18449,      1,    323,  10887,    555,  33919,  13558,\n",
      "          71924,    389,    813,    220,   2550,     17,   8176,   4427,    480,\n",
      "            525,   2052,     13,    578,   5609,    374,  71924,      6,  17755,\n",
      "           3254,    323,  12223,   5609,     11,    433,   1903,   1461,  11495,\n",
      "            323,    706,   1027,    813,   1455,   6992,   5609,     13,   1102,\n",
      "           6244,    279,   1176,   3254,   3596,    311,  11322,  24657,  45092,\n",
      "           2704,    304,   8494,     58,     16,     60,    323,   1101,    220,\n",
      "           2550,     17,    596,   1888,  48724,   3254,    304,    279,   1890,\n",
      "           3224,   8032,     17,   1483,     18,     60,    763,    279,   3723,\n",
      "           4273,    433,   6244,    264,  49480,   4295,    389,   2477,    323,\n",
      "           3224,   9063,     11,   1069,   1802,    520,   1396,    220,     19,\n",
      "            389,    279,  67293,   8166,    220,   1041,    323,  61376,    279,\n",
      "           8166,  14438,  40200,   9676,     11,  10671,    279,   1176,   3224,\n",
      "           3254,    311,    387,  23759,  45092,   2533,  49419,  34467,    323,\n",
      "            423,   8788,   3744,    263,    596,    330,   3957,   8329,    304,\n",
      "            279,   9384,      1,    304,    220,   3753,     18,   8032,     19,\n",
      "             60,    578,   3254,  40901,    304,   3892,   5961,     11,    323,\n",
      "           1306,   1694,  15109,    389,   7054,    315,    279,    393,   3806,\n",
      "            304,    279,   3723,  15422,     11,  78292,    520,   1396,    220,\n",
      "             18,    389,    279,   6560,  47422,  21964,     13,   1102,   8625,\n",
      "          71924,    596,   8706,   4295,   3254,    304,    279,    549,    815,\n",
      "             13,    311,   2457,     11,    323,    813,   1193,    832,    311,\n",
      "           5662,    279,   1948,    220,    605,    315,    279,  67293,   8166,\n",
      "            220,   1041,     13,  11361,    311,    279,   2835,    315,    420,\n",
      "           4295,     11,   1070,    574,    279,  25176,    315,    279,   1584,\n",
      "          15612,   1139,    279,  21391,     11,  10671,    264,  46141,   3059,\n",
      "           8032,     20,   1483,     21,   1483,     22,   1483,     23,     60,\n",
      "            578,   5609,    374,   6646,    555,   1063,    439,    832,    315,\n",
      "            279,  12047,  11936,    315,    682,    892,     11,  16850,    520,\n",
      "           1396,   1403,    304,  99122,     16,    323,  88668,    596,   1160,\n",
      "            315,    279,    330,   1135,   7648,  18371,    288,    316,    989,\n",
      "          11717,  40200,  18374,  61046,     24,     60,  11458,    433,    374,\n",
      "          15324,    439,    264,  66743,   4261,    304,   3224,   4731,   1405,\n",
      "          71924,   7263,  36646,   2802,    304,    264,  23069,  28875,    315,\n",
      "           4731,  24059,  14992,  24475,     13]], device='cuda:0'),) and output tensor([[[ 2.6512e-04, -4.9973e-04, -5.8365e-04,  ...,  3.8147e-03,\n",
      "           6.3419e-05,  1.1902e-03],\n",
      "         [ 9.5215e-03, -2.8534e-03, -6.0120e-03,  ...,  6.1417e-04,\n",
      "           1.5625e-02,  9.9945e-04],\n",
      "         [-1.5564e-02, -2.4109e-03,  9.5215e-03,  ..., -1.3000e-02,\n",
      "          -8.3008e-03,  2.1210e-03],\n",
      "         ...,\n",
      "         [ 2.3937e-04,  3.2654e-03,  9.0942e-03,  ...,  5.7678e-03,\n",
      "          -4.8523e-03, -1.1719e-02],\n",
      "         [ 5.2185e-03,  3.2196e-03,  3.0670e-03,  ..., -1.0254e-02,\n",
      "           8.5449e-03,  1.6403e-04],\n",
      "         [-6.7520e-04, -5.7602e-04,  4.7684e-04,  ...,  3.3264e-03,\n",
      "           3.9101e-04,  4.7493e-04]]], device='cuda:0')\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[ 2.6512e-04, -4.9973e-04, -5.8365e-04,  ...,  3.8147e-03,\n",
      "           6.3419e-05,  1.1902e-03],\n",
      "         [ 9.5215e-03, -2.8534e-03, -6.0120e-03,  ...,  6.1417e-04,\n",
      "           1.5625e-02,  9.9945e-04],\n",
      "         [-1.5564e-02, -2.4109e-03,  9.5215e-03,  ..., -1.3000e-02,\n",
      "          -8.3008e-03,  2.1210e-03],\n",
      "         ...,\n",
      "         [ 2.3937e-04,  3.2654e-03,  9.0942e-03,  ...,  5.7678e-03,\n",
      "          -4.8523e-03, -1.1719e-02],\n",
      "         [ 5.2185e-03,  3.2196e-03,  3.0670e-03,  ..., -1.0254e-02,\n",
      "           8.5449e-03,  1.6403e-04],\n",
      "         [-6.7520e-04, -5.7602e-04,  4.7684e-04,  ...,  3.3264e-03,\n",
      "           3.9101e-04,  4.7493e-04]]], device='cuda:0'),) and output (tensor([[[-0.0004,  0.0146, -0.0020,  ...,  0.0066, -0.0194,  0.0020],\n",
      "         [ 0.0075, -0.0063,  0.0006,  ..., -0.0231,  0.0009, -0.0162],\n",
      "         [ 0.0034, -0.0097,  0.0103,  ..., -0.0262, -0.0024,  0.0111],\n",
      "         ...,\n",
      "         [ 0.0074,  0.0005,  0.0119,  ..., -0.0181, -0.0064, -0.0362],\n",
      "         [ 0.0191,  0.0108,  0.0044,  ..., -0.0081,  0.0320,  0.0128],\n",
      "         [ 0.0044, -0.0027,  0.0114,  ...,  0.0060,  0.0068, -0.0030]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.0004,  0.0146, -0.0020,  ...,  0.0066, -0.0194,  0.0020],\n",
      "         [ 0.0075, -0.0063,  0.0006,  ..., -0.0231,  0.0009, -0.0162],\n",
      "         [ 0.0034, -0.0097,  0.0103,  ..., -0.0262, -0.0024,  0.0111],\n",
      "         ...,\n",
      "         [ 0.0074,  0.0005,  0.0119,  ..., -0.0181, -0.0064, -0.0362],\n",
      "         [ 0.0191,  0.0108,  0.0044,  ..., -0.0081,  0.0320,  0.0128],\n",
      "         [ 0.0044, -0.0027,  0.0114,  ...,  0.0060,  0.0068, -0.0030]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.0972,  0.0787, -0.0419,  ...,  0.0071,  0.0869,  0.0218],\n",
      "         [ 0.0063, -0.0185, -0.0013,  ..., -0.0363, -0.0078, -0.0294],\n",
      "         [-0.0138,  0.0052,  0.0091,  ..., -0.0812, -0.0249,  0.0460],\n",
      "         ...,\n",
      "         [ 0.0236,  0.0099,  0.0005,  ..., -0.0576, -0.0214, -0.0432],\n",
      "         [ 0.0232, -0.0120, -0.0066,  ..., -0.0719,  0.0172,  0.0349],\n",
      "         [ 0.0034, -0.0005, -0.0026,  ...,  0.0186, -0.0034,  0.0046]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.0972,  0.0787, -0.0419,  ...,  0.0071,  0.0869,  0.0218],\n",
      "         [ 0.0063, -0.0185, -0.0013,  ..., -0.0363, -0.0078, -0.0294],\n",
      "         [-0.0138,  0.0052,  0.0091,  ..., -0.0812, -0.0249,  0.0460],\n",
      "         ...,\n",
      "         [ 0.0236,  0.0099,  0.0005,  ..., -0.0576, -0.0214, -0.0432],\n",
      "         [ 0.0232, -0.0120, -0.0066,  ..., -0.0719,  0.0172,  0.0349],\n",
      "         [ 0.0034, -0.0005, -0.0026,  ...,  0.0186, -0.0034,  0.0046]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.1040,  0.0875, -0.0360,  ...,  0.0205,  0.0997,  0.0184],\n",
      "         [ 0.0074, -0.0332, -0.0117,  ..., -0.0213, -0.0109, -0.0280],\n",
      "         [-0.0108,  0.0179, -0.0013,  ..., -0.0455, -0.0571,  0.0201],\n",
      "         ...,\n",
      "         [ 0.0630,  0.0024,  0.0551,  ..., -0.0483, -0.0171, -0.0360],\n",
      "         [ 0.0380,  0.0275,  0.0095,  ...,  0.0139,  0.0532,  0.0254],\n",
      "         [-0.0195, -0.0058,  0.0098,  ...,  0.0268, -0.0107,  0.0158]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.1040,  0.0875, -0.0360,  ...,  0.0205,  0.0997,  0.0184],\n",
      "         [ 0.0074, -0.0332, -0.0117,  ..., -0.0213, -0.0109, -0.0280],\n",
      "         [-0.0108,  0.0179, -0.0013,  ..., -0.0455, -0.0571,  0.0201],\n",
      "         ...,\n",
      "         [ 0.0630,  0.0024,  0.0551,  ..., -0.0483, -0.0171, -0.0360],\n",
      "         [ 0.0380,  0.0275,  0.0095,  ...,  0.0139,  0.0532,  0.0254],\n",
      "         [-0.0195, -0.0058,  0.0098,  ...,  0.0268, -0.0107,  0.0158]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.0870,  0.1049, -0.0173,  ...,  0.0700,  0.0989,  0.0181],\n",
      "         [-0.0080,  0.0209, -0.0068,  ..., -0.0590, -0.0062, -0.0849],\n",
      "         [-0.0361,  0.0058, -0.0183,  ..., -0.0588, -0.0462,  0.0292],\n",
      "         ...,\n",
      "         [ 0.0914, -0.0247,  0.0254,  ..., -0.0552, -0.0015,  0.0228],\n",
      "         [ 0.0179,  0.0211,  0.0255,  ..., -0.0259,  0.0666,  0.0759],\n",
      "         [-0.0219, -0.0064,  0.0133,  ...,  0.0393, -0.0348,  0.0248]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.0870,  0.1049, -0.0173,  ...,  0.0700,  0.0989,  0.0181],\n",
      "         [-0.0080,  0.0209, -0.0068,  ..., -0.0590, -0.0062, -0.0849],\n",
      "         [-0.0361,  0.0058, -0.0183,  ..., -0.0588, -0.0462,  0.0292],\n",
      "         ...,\n",
      "         [ 0.0914, -0.0247,  0.0254,  ..., -0.0552, -0.0015,  0.0228],\n",
      "         [ 0.0179,  0.0211,  0.0255,  ..., -0.0259,  0.0666,  0.0759],\n",
      "         [-0.0219, -0.0064,  0.0133,  ...,  0.0393, -0.0348,  0.0248]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.1113,  0.1022, -0.0102,  ...,  0.0436,  0.1283,  0.0321],\n",
      "         [-0.0295,  0.0313,  0.0064,  ...,  0.0008, -0.0299, -0.0233],\n",
      "         [ 0.0013, -0.0666, -0.0612,  ..., -0.0337, -0.0453,  0.0626],\n",
      "         ...,\n",
      "         [ 0.1672, -0.0652,  0.0287,  ..., -0.0014,  0.0653,  0.0033],\n",
      "         [ 0.0873, -0.0138,  0.0389,  ..., -0.0119,  0.0755,  0.0860],\n",
      "         [-0.0185, -0.0193,  0.0184,  ...,  0.0534, -0.0323,  0.0128]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.1113,  0.1022, -0.0102,  ...,  0.0436,  0.1283,  0.0321],\n",
      "         [-0.0295,  0.0313,  0.0064,  ...,  0.0008, -0.0299, -0.0233],\n",
      "         [ 0.0013, -0.0666, -0.0612,  ..., -0.0337, -0.0453,  0.0626],\n",
      "         ...,\n",
      "         [ 0.1672, -0.0652,  0.0287,  ..., -0.0014,  0.0653,  0.0033],\n",
      "         [ 0.0873, -0.0138,  0.0389,  ..., -0.0119,  0.0755,  0.0860],\n",
      "         [-0.0185, -0.0193,  0.0184,  ...,  0.0534, -0.0323,  0.0128]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.1034,  0.1132,  0.0096,  ...,  0.0453,  0.1477,  0.0301],\n",
      "         [ 0.0259,  0.0060,  0.0273,  ..., -0.0171, -0.0224, -0.0084],\n",
      "         [-0.0698, -0.0578, -0.0298,  ..., -0.0741, -0.0179,  0.1106],\n",
      "         ...,\n",
      "         [ 0.1907, -0.0595,  0.0741,  ..., -0.0819,  0.0382,  0.0392],\n",
      "         [ 0.0462,  0.0014,  0.0051,  ..., -0.0366,  0.0701,  0.1053],\n",
      "         [-0.0128, -0.0088,  0.0257,  ...,  0.0758, -0.0434,  0.0250]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.1034,  0.1132,  0.0096,  ...,  0.0453,  0.1477,  0.0301],\n",
      "         [ 0.0259,  0.0060,  0.0273,  ..., -0.0171, -0.0224, -0.0084],\n",
      "         [-0.0698, -0.0578, -0.0298,  ..., -0.0741, -0.0179,  0.1106],\n",
      "         ...,\n",
      "         [ 0.1907, -0.0595,  0.0741,  ..., -0.0819,  0.0382,  0.0392],\n",
      "         [ 0.0462,  0.0014,  0.0051,  ..., -0.0366,  0.0701,  0.1053],\n",
      "         [-0.0128, -0.0088,  0.0257,  ...,  0.0758, -0.0434,  0.0250]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.1071,  0.1253,  0.0211,  ...,  0.0126,  0.1586,  0.0278],\n",
      "         [ 0.0614,  0.0329, -0.0089,  ...,  0.0164, -0.0148,  0.0340],\n",
      "         [-0.0159, -0.0290,  0.0772,  ..., -0.0875, -0.0615,  0.0332],\n",
      "         ...,\n",
      "         [ 0.0911, -0.0634,  0.0485,  ...,  0.0120,  0.0298,  0.0561],\n",
      "         [-0.0163,  0.0071, -0.0251,  ...,  0.0400,  0.1020,  0.1151],\n",
      "         [-0.0460,  0.0039, -0.0143,  ...,  0.0646, -0.0370, -0.0032]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.1071,  0.1253,  0.0211,  ...,  0.0126,  0.1586,  0.0278],\n",
      "         [ 0.0614,  0.0329, -0.0089,  ...,  0.0164, -0.0148,  0.0340],\n",
      "         [-0.0159, -0.0290,  0.0772,  ..., -0.0875, -0.0615,  0.0332],\n",
      "         ...,\n",
      "         [ 0.0911, -0.0634,  0.0485,  ...,  0.0120,  0.0298,  0.0561],\n",
      "         [-0.0163,  0.0071, -0.0251,  ...,  0.0400,  0.1020,  0.1151],\n",
      "         [-0.0460,  0.0039, -0.0143,  ...,  0.0646, -0.0370, -0.0032]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.0945,  0.1848,  0.0036,  ..., -0.0473,  0.1489,  0.0535],\n",
      "         [ 0.1515,  0.0021, -0.0444,  ..., -0.0487, -0.0953,  0.0250],\n",
      "         [-0.0830, -0.0196,  0.0691,  ..., -0.1400, -0.0834,  0.0704],\n",
      "         ...,\n",
      "         [ 0.0528,  0.0007,  0.1075,  ..., -0.0243,  0.0544,  0.0503],\n",
      "         [-0.0427,  0.0383,  0.0280,  ...,  0.0394,  0.1157,  0.0703],\n",
      "         [-0.0915, -0.1396, -0.0294,  ...,  0.0611, -0.0638,  0.0020]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.0945,  0.1848,  0.0036,  ..., -0.0473,  0.1489,  0.0535],\n",
      "         [ 0.1515,  0.0021, -0.0444,  ..., -0.0487, -0.0953,  0.0250],\n",
      "         [-0.0830, -0.0196,  0.0691,  ..., -0.1400, -0.0834,  0.0704],\n",
      "         ...,\n",
      "         [ 0.0528,  0.0007,  0.1075,  ..., -0.0243,  0.0544,  0.0503],\n",
      "         [-0.0427,  0.0383,  0.0280,  ...,  0.0394,  0.1157,  0.0703],\n",
      "         [-0.0915, -0.1396, -0.0294,  ...,  0.0611, -0.0638,  0.0020]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.1060,  0.2113,  0.0270,  ..., -0.1141,  0.1610,  0.0528],\n",
      "         [ 0.1546,  0.0720, -0.0650,  ..., -0.0536, -0.1254, -0.0091],\n",
      "         [-0.0450, -0.0027, -0.0152,  ..., -0.1558, -0.0758,  0.0300],\n",
      "         ...,\n",
      "         [ 0.1446,  0.1380,  0.0114,  ...,  0.0407, -0.0029, -0.0178],\n",
      "         [ 0.0961,  0.1210, -0.0345,  ...,  0.1610,  0.0245,  0.0512],\n",
      "         [ 0.0400, -0.0904, -0.0090,  ...,  0.0107, -0.0711, -0.0119]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.1060,  0.2113,  0.0270,  ..., -0.1141,  0.1610,  0.0528],\n",
      "         [ 0.1546,  0.0720, -0.0650,  ..., -0.0536, -0.1254, -0.0091],\n",
      "         [-0.0450, -0.0027, -0.0152,  ..., -0.1558, -0.0758,  0.0300],\n",
      "         ...,\n",
      "         [ 0.1446,  0.1380,  0.0114,  ...,  0.0407, -0.0029, -0.0178],\n",
      "         [ 0.0961,  0.1210, -0.0345,  ...,  0.1610,  0.0245,  0.0512],\n",
      "         [ 0.0400, -0.0904, -0.0090,  ...,  0.0107, -0.0711, -0.0119]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.0954,  0.2493,  0.0073,  ..., -0.1568,  0.1818,  0.0855],\n",
      "         [ 0.0596,  0.0586, -0.0296,  ..., -0.0490, -0.0521,  0.0321],\n",
      "         [-0.1375,  0.0148, -0.0200,  ..., -0.1458, -0.0030,  0.0179],\n",
      "         ...,\n",
      "         [ 0.1098, -0.0677,  0.0462,  ..., -0.0166,  0.0357, -0.0037],\n",
      "         [ 0.0960, -0.0180,  0.0383,  ...,  0.0852,  0.0604,  0.0562],\n",
      "         [ 0.1431, -0.0336,  0.0069,  ...,  0.0383, -0.0961,  0.0466]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.0954,  0.2493,  0.0073,  ..., -0.1568,  0.1818,  0.0855],\n",
      "         [ 0.0596,  0.0586, -0.0296,  ..., -0.0490, -0.0521,  0.0321],\n",
      "         [-0.1375,  0.0148, -0.0200,  ..., -0.1458, -0.0030,  0.0179],\n",
      "         ...,\n",
      "         [ 0.1098, -0.0677,  0.0462,  ..., -0.0166,  0.0357, -0.0037],\n",
      "         [ 0.0960, -0.0180,  0.0383,  ...,  0.0852,  0.0604,  0.0562],\n",
      "         [ 0.1431, -0.0336,  0.0069,  ...,  0.0383, -0.0961,  0.0466]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.0152,  0.3340,  0.0011,  ..., -0.2750,  0.1933,  0.1034],\n",
      "         [ 0.0816,  0.0589,  0.0093,  ..., -0.0358, -0.0618,  0.0454],\n",
      "         [-0.0797, -0.0237,  0.0196,  ..., -0.1922, -0.0580,  0.0200],\n",
      "         ...,\n",
      "         [ 0.1330, -0.0050,  0.0834,  ...,  0.0043,  0.0148,  0.0409],\n",
      "         [ 0.1118, -0.0241,  0.0365,  ...,  0.0911,  0.0216,  0.1044],\n",
      "         [ 0.1306, -0.0733, -0.0074,  ...,  0.0162, -0.1546, -0.0080]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.0152,  0.3340,  0.0011,  ..., -0.2750,  0.1933,  0.1034],\n",
      "         [ 0.0816,  0.0589,  0.0093,  ..., -0.0358, -0.0618,  0.0454],\n",
      "         [-0.0797, -0.0237,  0.0196,  ..., -0.1922, -0.0580,  0.0200],\n",
      "         ...,\n",
      "         [ 0.1330, -0.0050,  0.0834,  ...,  0.0043,  0.0148,  0.0409],\n",
      "         [ 0.1118, -0.0241,  0.0365,  ...,  0.0911,  0.0216,  0.1044],\n",
      "         [ 0.1306, -0.0733, -0.0074,  ...,  0.0162, -0.1546, -0.0080]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.0356,  0.3483, -0.0366,  ..., -0.2962,  0.2088,  0.1065],\n",
      "         [ 0.0905,  0.0637, -0.0029,  ..., -0.0807, -0.0057,  0.0723],\n",
      "         [-0.0885, -0.0290,  0.0473,  ..., -0.1870,  0.0205,  0.0072],\n",
      "         ...,\n",
      "         [ 0.1712, -0.0474,  0.1029,  ...,  0.0613,  0.1176,  0.0906],\n",
      "         [ 0.1610, -0.1277,  0.0509,  ...,  0.0599,  0.0143,  0.1534],\n",
      "         [ 0.0448, -0.1467, -0.0382,  ..., -0.0308, -0.0371, -0.0480]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.0356,  0.3483, -0.0366,  ..., -0.2962,  0.2088,  0.1065],\n",
      "         [ 0.0905,  0.0637, -0.0029,  ..., -0.0807, -0.0057,  0.0723],\n",
      "         [-0.0885, -0.0290,  0.0473,  ..., -0.1870,  0.0205,  0.0072],\n",
      "         ...,\n",
      "         [ 0.1712, -0.0474,  0.1029,  ...,  0.0613,  0.1176,  0.0906],\n",
      "         [ 0.1610, -0.1277,  0.0509,  ...,  0.0599,  0.0143,  0.1534],\n",
      "         [ 0.0448, -0.1467, -0.0382,  ..., -0.0308, -0.0371, -0.0480]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.0127,  0.3661, -0.0316,  ..., -0.3678,  0.1973,  0.1019],\n",
      "         [ 0.0986,  0.0296,  0.0290,  ..., -0.0896, -0.1103,  0.0989],\n",
      "         [-0.0591, -0.0528,  0.0571,  ..., -0.1916, -0.0759,  0.0296],\n",
      "         ...,\n",
      "         [ 0.0929, -0.1268,  0.1173,  ...,  0.0450,  0.0983,  0.0459],\n",
      "         [ 0.1083, -0.0635,  0.0540,  ...,  0.0277,  0.0715,  0.1143],\n",
      "         [-0.0027, -0.1226, -0.0437,  ..., -0.1035, -0.0501, -0.0868]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.0127,  0.3661, -0.0316,  ..., -0.3678,  0.1973,  0.1019],\n",
      "         [ 0.0986,  0.0296,  0.0290,  ..., -0.0896, -0.1103,  0.0989],\n",
      "         [-0.0591, -0.0528,  0.0571,  ..., -0.1916, -0.0759,  0.0296],\n",
      "         ...,\n",
      "         [ 0.0929, -0.1268,  0.1173,  ...,  0.0450,  0.0983,  0.0459],\n",
      "         [ 0.1083, -0.0635,  0.0540,  ...,  0.0277,  0.0715,  0.1143],\n",
      "         [-0.0027, -0.1226, -0.0437,  ..., -0.1035, -0.0501, -0.0868]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.0089,  0.3233, -0.0779,  ..., -0.4491,  0.1845,  0.0951],\n",
      "         [-0.0364,  0.0119,  0.0376,  ..., -0.1608, -0.0963,  0.0276],\n",
      "         [-0.0041, -0.0826,  0.0949,  ..., -0.1581, -0.0163,  0.0129],\n",
      "         ...,\n",
      "         [ 0.1122, -0.1368,  0.0857,  ...,  0.0042,  0.0129,  0.0212],\n",
      "         [-0.0134, -0.1338,  0.0742,  ..., -0.0353, -0.0284,  0.1244],\n",
      "         [-0.0111, -0.1618,  0.0316,  ..., -0.2358, -0.1120,  0.0791]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.0089,  0.3233, -0.0779,  ..., -0.4491,  0.1845,  0.0951],\n",
      "         [-0.0364,  0.0119,  0.0376,  ..., -0.1608, -0.0963,  0.0276],\n",
      "         [-0.0041, -0.0826,  0.0949,  ..., -0.1581, -0.0163,  0.0129],\n",
      "         ...,\n",
      "         [ 0.1122, -0.1368,  0.0857,  ...,  0.0042,  0.0129,  0.0212],\n",
      "         [-0.0134, -0.1338,  0.0742,  ..., -0.0353, -0.0284,  0.1244],\n",
      "         [-0.0111, -0.1618,  0.0316,  ..., -0.2358, -0.1120,  0.0791]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.0244,  0.2828, -0.0532,  ..., -0.4592,  0.2300,  0.0142],\n",
      "         [-0.0444, -0.1131, -0.0133,  ..., -0.1917, -0.0740, -0.0870],\n",
      "         [-0.1074, -0.1282,  0.0370,  ..., -0.1346, -0.0844,  0.0769],\n",
      "         ...,\n",
      "         [ 0.0007, -0.1758,  0.1900,  ..., -0.0562,  0.0775, -0.0842],\n",
      "         [-0.0524, -0.1238,  0.1335,  ..., -0.0274,  0.0804,  0.1072],\n",
      "         [ 0.0460, -0.1001, -0.0455,  ..., -0.1529, -0.0886,  0.0032]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.0244,  0.2828, -0.0532,  ..., -0.4592,  0.2300,  0.0142],\n",
      "         [-0.0444, -0.1131, -0.0133,  ..., -0.1917, -0.0740, -0.0870],\n",
      "         [-0.1074, -0.1282,  0.0370,  ..., -0.1346, -0.0844,  0.0769],\n",
      "         ...,\n",
      "         [ 0.0007, -0.1758,  0.1900,  ..., -0.0562,  0.0775, -0.0842],\n",
      "         [-0.0524, -0.1238,  0.1335,  ..., -0.0274,  0.0804,  0.1072],\n",
      "         [ 0.0460, -0.1001, -0.0455,  ..., -0.1529, -0.0886,  0.0032]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.0928,  0.1485,  0.0158,  ..., -0.5380,  0.2664, -0.0333],\n",
      "         [-0.0397, -0.2309, -0.0998,  ..., -0.2479, -0.0745, -0.1689],\n",
      "         [ 0.0519, -0.2818, -0.0159,  ..., -0.0954, -0.1209,  0.0175],\n",
      "         ...,\n",
      "         [-0.0661, -0.1597,  0.0378,  ..., -0.0505, -0.0817, -0.1530],\n",
      "         [-0.0549, -0.0370,  0.1212,  ...,  0.0110, -0.0152,  0.0765],\n",
      "         [ 0.1051, -0.1169,  0.0523,  ..., -0.0811, -0.2014,  0.0605]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.0928,  0.1485,  0.0158,  ..., -0.5380,  0.2664, -0.0333],\n",
      "         [-0.0397, -0.2309, -0.0998,  ..., -0.2479, -0.0745, -0.1689],\n",
      "         [ 0.0519, -0.2818, -0.0159,  ..., -0.0954, -0.1209,  0.0175],\n",
      "         ...,\n",
      "         [-0.0661, -0.1597,  0.0378,  ..., -0.0505, -0.0817, -0.1530],\n",
      "         [-0.0549, -0.0370,  0.1212,  ...,  0.0110, -0.0152,  0.0765],\n",
      "         [ 0.1051, -0.1169,  0.0523,  ..., -0.0811, -0.2014,  0.0605]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.1000,  0.1323,  0.0241,  ..., -0.5470,  0.2844, -0.0457],\n",
      "         [ 0.0226, -0.2869, -0.1186,  ..., -0.2290, -0.1965, -0.3376],\n",
      "         [ 0.1387, -0.1678,  0.0103,  ..., -0.0700, -0.2619, -0.1001],\n",
      "         ...,\n",
      "         [-0.0096, -0.1538, -0.0123,  ..., -0.1148, -0.1771, -0.1044],\n",
      "         [-0.1570, -0.1501,  0.0900,  ...,  0.0387, -0.0469,  0.0697],\n",
      "         [ 0.1525, -0.0825,  0.0819,  ..., -0.0041, -0.2587,  0.0537]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.1000,  0.1323,  0.0241,  ..., -0.5470,  0.2844, -0.0457],\n",
      "         [ 0.0226, -0.2869, -0.1186,  ..., -0.2290, -0.1965, -0.3376],\n",
      "         [ 0.1387, -0.1678,  0.0103,  ..., -0.0700, -0.2619, -0.1001],\n",
      "         ...,\n",
      "         [-0.0096, -0.1538, -0.0123,  ..., -0.1148, -0.1771, -0.1044],\n",
      "         [-0.1570, -0.1501,  0.0900,  ...,  0.0387, -0.0469,  0.0697],\n",
      "         [ 0.1525, -0.0825,  0.0819,  ..., -0.0041, -0.2587,  0.0537]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.1286,  0.0833,  0.0239,  ..., -0.5657,  0.3694, -0.0725],\n",
      "         [ 0.0272, -0.2544, -0.1524,  ..., -0.2670, -0.2126, -0.3332],\n",
      "         [ 0.1314, -0.2702,  0.0336,  ..., -0.0919, -0.2301, -0.2218],\n",
      "         ...,\n",
      "         [ 0.0551, -0.1611,  0.0582,  ..., -0.1979,  0.0114, -0.0566],\n",
      "         [-0.1453, -0.1116,  0.1075,  ..., -0.0547, -0.1138, -0.0069],\n",
      "         [ 0.1260, -0.1531,  0.0374,  ..., -0.0251, -0.3633, -0.0101]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.1286,  0.0833,  0.0239,  ..., -0.5657,  0.3694, -0.0725],\n",
      "         [ 0.0272, -0.2544, -0.1524,  ..., -0.2670, -0.2126, -0.3332],\n",
      "         [ 0.1314, -0.2702,  0.0336,  ..., -0.0919, -0.2301, -0.2218],\n",
      "         ...,\n",
      "         [ 0.0551, -0.1611,  0.0582,  ..., -0.1979,  0.0114, -0.0566],\n",
      "         [-0.1453, -0.1116,  0.1075,  ..., -0.0547, -0.1138, -0.0069],\n",
      "         [ 0.1260, -0.1531,  0.0374,  ..., -0.0251, -0.3633, -0.0101]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.1417,  0.0703,  0.0343,  ..., -0.5549,  0.3175, -0.0847],\n",
      "         [ 0.0548, -0.2938, -0.1678,  ..., -0.2741, -0.2020, -0.3334],\n",
      "         [-0.0418, -0.3389,  0.0167,  ...,  0.0753, -0.1309, -0.1940],\n",
      "         ...,\n",
      "         [-0.0498, -0.2885,  0.0841,  ..., -0.1877, -0.0413,  0.0075],\n",
      "         [-0.2066, -0.1338,  0.0442,  ...,  0.0496, -0.1571,  0.0065],\n",
      "         [ 0.1314, -0.1762,  0.0254,  ..., -0.0122, -0.4574, -0.0276]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.1417,  0.0703,  0.0343,  ..., -0.5549,  0.3175, -0.0847],\n",
      "         [ 0.0548, -0.2938, -0.1678,  ..., -0.2741, -0.2020, -0.3334],\n",
      "         [-0.0418, -0.3389,  0.0167,  ...,  0.0753, -0.1309, -0.1940],\n",
      "         ...,\n",
      "         [-0.0498, -0.2885,  0.0841,  ..., -0.1877, -0.0413,  0.0075],\n",
      "         [-0.2066, -0.1338,  0.0442,  ...,  0.0496, -0.1571,  0.0065],\n",
      "         [ 0.1314, -0.1762,  0.0254,  ..., -0.0122, -0.4574, -0.0276]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.1528,  0.0725,  0.0609,  ..., -0.5624,  0.3103, -0.0293],\n",
      "         [ 0.1174, -0.3897, -0.1668,  ..., -0.2701, -0.1509, -0.5044],\n",
      "         [-0.0401, -0.0997, -0.0086,  ...,  0.0371, -0.1824, -0.3883],\n",
      "         ...,\n",
      "         [-0.0109, -0.4195,  0.1383,  ..., -0.2404, -0.0924, -0.0641],\n",
      "         [-0.2323, -0.3750,  0.1208,  ...,  0.0484, -0.1287,  0.0583],\n",
      "         [ 0.0182, -0.2227,  0.1053,  ..., -0.0273, -0.3854, -0.0368]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.1528,  0.0725,  0.0609,  ..., -0.5624,  0.3103, -0.0293],\n",
      "         [ 0.1174, -0.3897, -0.1668,  ..., -0.2701, -0.1509, -0.5044],\n",
      "         [-0.0401, -0.0997, -0.0086,  ...,  0.0371, -0.1824, -0.3883],\n",
      "         ...,\n",
      "         [-0.0109, -0.4195,  0.1383,  ..., -0.2404, -0.0924, -0.0641],\n",
      "         [-0.2323, -0.3750,  0.1208,  ...,  0.0484, -0.1287,  0.0583],\n",
      "         [ 0.0182, -0.2227,  0.1053,  ..., -0.0273, -0.3854, -0.0368]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.1559,  0.0520,  0.0566,  ..., -0.5805,  0.2614, -0.0032],\n",
      "         [ 0.1583, -0.5433, -0.2263,  ..., -0.2247, -0.1306, -0.5301],\n",
      "         [ 0.0708, -0.1543, -0.0658,  ...,  0.1073, -0.0855, -0.3059],\n",
      "         ...,\n",
      "         [-0.1277, -0.4677,  0.2800,  ..., -0.1468, -0.0172, -0.0503],\n",
      "         [-0.1856, -0.4962,  0.1230,  ...,  0.0847, -0.1357,  0.0945],\n",
      "         [ 0.1166, -0.2597,  0.0382,  ...,  0.0181, -0.4168, -0.0352]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.1559,  0.0520,  0.0566,  ..., -0.5805,  0.2614, -0.0032],\n",
      "         [ 0.1583, -0.5433, -0.2263,  ..., -0.2247, -0.1306, -0.5301],\n",
      "         [ 0.0708, -0.1543, -0.0658,  ...,  0.1073, -0.0855, -0.3059],\n",
      "         ...,\n",
      "         [-0.1277, -0.4677,  0.2800,  ..., -0.1468, -0.0172, -0.0503],\n",
      "         [-0.1856, -0.4962,  0.1230,  ...,  0.0847, -0.1357,  0.0945],\n",
      "         [ 0.1166, -0.2597,  0.0382,  ...,  0.0181, -0.4168, -0.0352]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.1488,  0.0445,  0.0765,  ..., -0.5456,  0.2702,  0.0098],\n",
      "         [ 0.2638, -0.4766, -0.2296,  ..., -0.3424, -0.1554, -0.6561],\n",
      "         [ 0.0364, -0.1148, -0.0351,  ...,  0.0324, -0.1738, -0.2226],\n",
      "         ...,\n",
      "         [-0.0134, -0.2679,  0.2911,  ..., -0.0312, -0.2275, -0.0015],\n",
      "         [-0.1538, -0.5036,  0.0840,  ...,  0.2611, -0.2994,  0.0568],\n",
      "         [ 0.1282, -0.2465, -0.0374,  ..., -0.0945, -0.4416, -0.0628]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.1488,  0.0445,  0.0765,  ..., -0.5456,  0.2702,  0.0098],\n",
      "         [ 0.2638, -0.4766, -0.2296,  ..., -0.3424, -0.1554, -0.6561],\n",
      "         [ 0.0364, -0.1148, -0.0351,  ...,  0.0324, -0.1738, -0.2226],\n",
      "         ...,\n",
      "         [-0.0134, -0.2679,  0.2911,  ..., -0.0312, -0.2275, -0.0015],\n",
      "         [-0.1538, -0.5036,  0.0840,  ...,  0.2611, -0.2994,  0.0568],\n",
      "         [ 0.1282, -0.2465, -0.0374,  ..., -0.0945, -0.4416, -0.0628]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-1.2677e-01,  3.3336e-02,  9.3478e-02,  ..., -5.3204e-01,\n",
      "           2.7953e-01,  5.9784e-04],\n",
      "         [ 3.8208e-01, -4.5856e-01, -2.3745e-01,  ..., -3.9980e-01,\n",
      "          -1.4827e-01, -7.0184e-01],\n",
      "         [-2.7754e-02, -1.6025e-01, -1.1063e-02,  ...,  4.5489e-02,\n",
      "          -2.0305e-01, -2.3760e-01],\n",
      "         ...,\n",
      "         [ 2.1150e-01, -1.5954e-01,  3.9167e-01,  ..., -3.5704e-02,\n",
      "          -1.8800e-01,  1.1496e-01],\n",
      "         [-1.6817e-01, -3.0498e-01,  1.4168e-01,  ...,  3.2671e-01,\n",
      "          -2.0207e-01,  1.6360e-01],\n",
      "         [ 1.6327e-01, -2.3119e-01, -3.0695e-02,  ...,  3.0849e-02,\n",
      "          -3.6836e-01,  8.7626e-02]]], device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-1.2677e-01,  3.3336e-02,  9.3478e-02,  ..., -5.3204e-01,\n",
      "           2.7953e-01,  5.9784e-04],\n",
      "         [ 3.8208e-01, -4.5856e-01, -2.3745e-01,  ..., -3.9980e-01,\n",
      "          -1.4827e-01, -7.0184e-01],\n",
      "         [-2.7754e-02, -1.6025e-01, -1.1063e-02,  ...,  4.5489e-02,\n",
      "          -2.0305e-01, -2.3760e-01],\n",
      "         ...,\n",
      "         [ 2.1150e-01, -1.5954e-01,  3.9167e-01,  ..., -3.5704e-02,\n",
      "          -1.8800e-01,  1.1496e-01],\n",
      "         [-1.6817e-01, -3.0498e-01,  1.4168e-01,  ...,  3.2671e-01,\n",
      "          -2.0207e-01,  1.6360e-01],\n",
      "         [ 1.6327e-01, -2.3119e-01, -3.0695e-02,  ...,  3.0849e-02,\n",
      "          -3.6836e-01,  8.7626e-02]]], device='cuda:0'),) and output (tensor([[[-0.1355,  0.0227,  0.0554,  ..., -0.5473,  0.2470, -0.0229],\n",
      "         [ 0.4855, -0.4358, -0.2227,  ..., -0.4391, -0.2909, -0.7967],\n",
      "         [ 0.1184, -0.3620,  0.0974,  ...,  0.0497, -0.4094, -0.1474],\n",
      "         ...,\n",
      "         [ 0.4179, -0.1387,  0.1043,  ..., -0.0116, -0.3947,  0.2243],\n",
      "         [-0.0670, -0.2672,  0.0792,  ...,  0.2449, -0.2695,  0.1946],\n",
      "         [ 0.1943, -0.1918, -0.0288,  ...,  0.1649, -0.4097,  0.0168]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.1355,  0.0227,  0.0554,  ..., -0.5473,  0.2470, -0.0229],\n",
      "         [ 0.4855, -0.4358, -0.2227,  ..., -0.4391, -0.2909, -0.7967],\n",
      "         [ 0.1184, -0.3620,  0.0974,  ...,  0.0497, -0.4094, -0.1474],\n",
      "         ...,\n",
      "         [ 0.4179, -0.1387,  0.1043,  ..., -0.0116, -0.3947,  0.2243],\n",
      "         [-0.0670, -0.2672,  0.0792,  ...,  0.2449, -0.2695,  0.1946],\n",
      "         [ 0.1943, -0.1918, -0.0288,  ...,  0.1649, -0.4097,  0.0168]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.1350,  0.0163,  0.0576,  ..., -0.5555,  0.2435,  0.0051],\n",
      "         [ 0.4149, -0.3438, -0.3105,  ..., -0.3788, -0.4150, -0.7457],\n",
      "         [-0.0248, -0.4285,  0.2591,  ..., -0.1817, -0.4129, -0.1111],\n",
      "         ...,\n",
      "         [ 0.4319, -0.2442,  0.1520,  ..., -0.1788, -0.4639,  0.3200],\n",
      "         [-0.0769, -0.3872,  0.1054,  ...,  0.3658, -0.0806,  0.4388],\n",
      "         [ 0.2909, -0.3475,  0.1305,  ...,  0.4495, -0.2178,  0.1970]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.1350,  0.0163,  0.0576,  ..., -0.5555,  0.2435,  0.0051],\n",
      "         [ 0.4149, -0.3438, -0.3105,  ..., -0.3788, -0.4150, -0.7457],\n",
      "         [-0.0248, -0.4285,  0.2591,  ..., -0.1817, -0.4129, -0.1111],\n",
      "         ...,\n",
      "         [ 0.4319, -0.2442,  0.1520,  ..., -0.1788, -0.4639,  0.3200],\n",
      "         [-0.0769, -0.3872,  0.1054,  ...,  0.3658, -0.0806,  0.4388],\n",
      "         [ 0.2909, -0.3475,  0.1305,  ...,  0.4495, -0.2178,  0.1970]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.1301, -0.0029,  0.0279,  ..., -0.5537,  0.2277,  0.0164],\n",
      "         [ 0.4361, -0.3682, -0.3629,  ..., -0.3841, -0.3892, -0.8256],\n",
      "         [-0.2475, -0.5268,  0.2410,  ...,  0.0294, -0.3133, -0.2449],\n",
      "         ...,\n",
      "         [ 0.3755, -0.2800,  0.3867,  ..., -0.1761, -0.5324,  0.2541],\n",
      "         [-0.0116, -0.3126,  0.0655,  ...,  0.5443, -0.0009,  0.3773],\n",
      "         [ 0.2386, -0.2063,  0.3159,  ...,  0.5156, -0.0149,  0.0490]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.1301, -0.0029,  0.0279,  ..., -0.5537,  0.2277,  0.0164],\n",
      "         [ 0.4361, -0.3682, -0.3629,  ..., -0.3841, -0.3892, -0.8256],\n",
      "         [-0.2475, -0.5268,  0.2410,  ...,  0.0294, -0.3133, -0.2449],\n",
      "         ...,\n",
      "         [ 0.3755, -0.2800,  0.3867,  ..., -0.1761, -0.5324,  0.2541],\n",
      "         [-0.0116, -0.3126,  0.0655,  ...,  0.5443, -0.0009,  0.3773],\n",
      "         [ 0.2386, -0.2063,  0.3159,  ...,  0.5156, -0.0149,  0.0490]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.1219,  0.0061, -0.0021,  ..., -0.5922,  0.2788, -0.0020],\n",
      "         [ 0.3768, -0.2940, -0.4567,  ..., -0.3322, -0.4018, -0.7275],\n",
      "         [-0.2452, -0.4251,  0.2449,  ...,  0.0706, -0.2960, -0.1961],\n",
      "         ...,\n",
      "         [ 0.1091, -0.2816,  0.5192,  ..., -0.2380, -0.6443,  0.1892],\n",
      "         [-0.0720, -0.4210,  0.1724,  ...,  0.6145,  0.0949,  0.3459],\n",
      "         [ 0.3432, -0.2330,  0.4324,  ...,  0.7539,  0.1247,  0.0018]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.1219,  0.0061, -0.0021,  ..., -0.5922,  0.2788, -0.0020],\n",
      "         [ 0.3768, -0.2940, -0.4567,  ..., -0.3322, -0.4018, -0.7275],\n",
      "         [-0.2452, -0.4251,  0.2449,  ...,  0.0706, -0.2960, -0.1961],\n",
      "         ...,\n",
      "         [ 0.1091, -0.2816,  0.5192,  ..., -0.2380, -0.6443,  0.1892],\n",
      "         [-0.0720, -0.4210,  0.1724,  ...,  0.6145,  0.0949,  0.3459],\n",
      "         [ 0.3432, -0.2330,  0.4324,  ...,  0.7539,  0.1247,  0.0018]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-1.0667e-01,  2.7209e-02,  4.9523e-02,  ..., -5.5496e-01,\n",
      "           3.4494e-01,  2.6647e-02],\n",
      "         [ 3.7994e-01, -4.1569e-01, -4.7811e-01,  ..., -3.3544e-01,\n",
      "          -3.4557e-01, -7.0172e-01],\n",
      "         [-3.3378e-01, -2.7369e-01,  4.7915e-02,  ...,  2.1941e-01,\n",
      "          -2.5460e-01, -3.1357e-01],\n",
      "         ...,\n",
      "         [-6.6480e-03, -3.1390e-01,  6.4239e-01,  ...,  1.6008e-02,\n",
      "          -7.9012e-01,  4.4817e-01],\n",
      "         [-2.8256e-01, -4.4239e-01,  2.3196e-01,  ...,  8.4178e-01,\n",
      "           2.2649e-01,  5.7000e-01],\n",
      "         [ 5.0632e-01, -1.2654e-01,  5.1798e-01,  ...,  9.9662e-01,\n",
      "          -9.4587e-02,  9.5365e-04]]], device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-1.0667e-01,  2.7209e-02,  4.9523e-02,  ..., -5.5496e-01,\n",
      "           3.4494e-01,  2.6647e-02],\n",
      "         [ 3.7994e-01, -4.1569e-01, -4.7811e-01,  ..., -3.3544e-01,\n",
      "          -3.4557e-01, -7.0172e-01],\n",
      "         [-3.3378e-01, -2.7369e-01,  4.7915e-02,  ...,  2.1941e-01,\n",
      "          -2.5460e-01, -3.1357e-01],\n",
      "         ...,\n",
      "         [-6.6480e-03, -3.1390e-01,  6.4239e-01,  ...,  1.6008e-02,\n",
      "          -7.9012e-01,  4.4817e-01],\n",
      "         [-2.8256e-01, -4.4239e-01,  2.3196e-01,  ...,  8.4178e-01,\n",
      "           2.2649e-01,  5.7000e-01],\n",
      "         [ 5.0632e-01, -1.2654e-01,  5.1798e-01,  ...,  9.9662e-01,\n",
      "          -9.4587e-02,  9.5365e-04]]], device='cuda:0'),) and output (tensor([[[-0.0592,  0.0731,  0.0500,  ..., -0.5517,  0.3541,  0.0595],\n",
      "         [ 0.1075, -0.5540, -0.4087,  ..., -0.5272, -0.2561, -0.5925],\n",
      "         [-0.2970, -0.4582,  0.1679,  ...,  0.2746, -0.0826, -0.5327],\n",
      "         ...,\n",
      "         [-0.4455, -0.5758,  0.9887,  ..., -0.0696, -1.0175,  0.4482],\n",
      "         [-0.4987, -1.0185,  0.5017,  ...,  0.7859,  0.4168,  0.4333],\n",
      "         [ 0.4939, -0.4518,  0.7134,  ...,  0.8737, -0.1466, -0.2261]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.0592,  0.0731,  0.0500,  ..., -0.5517,  0.3541,  0.0595],\n",
      "         [ 0.1075, -0.5540, -0.4087,  ..., -0.5272, -0.2561, -0.5925],\n",
      "         [-0.2970, -0.4582,  0.1679,  ...,  0.2746, -0.0826, -0.5327],\n",
      "         ...,\n",
      "         [-0.4455, -0.5758,  0.9887,  ..., -0.0696, -1.0175,  0.4482],\n",
      "         [-0.4987, -1.0185,  0.5017,  ...,  0.7859,  0.4168,  0.4333],\n",
      "         [ 0.4939, -0.4518,  0.7134,  ...,  0.8737, -0.1466, -0.2261]]],\n",
      "       device='cuda:0'),) and output (tensor([[[ 0.1143,  0.2237,  0.3292,  ..., -0.7706,  0.2608,  0.0269],\n",
      "         [ 0.2277, -0.6213, -0.4816,  ..., -0.4835, -0.2967, -0.6243],\n",
      "         [-0.6707, -0.6815, -0.1203,  ...,  0.2391, -0.1351, -0.4887],\n",
      "         ...,\n",
      "         [-0.1081, -0.6825,  1.0234,  ..., -0.1154, -1.4197,  0.6475],\n",
      "         [-0.3294, -1.1419,  0.1581,  ...,  1.0760,  0.2277,  0.8325],\n",
      "         [ 0.4066, -0.7241,  0.8159,  ...,  1.2632, -0.3010,  0.0068]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[ 0.1143,  0.2237,  0.3292,  ..., -0.7706,  0.2608,  0.0269],\n",
      "         [ 0.2277, -0.6213, -0.4816,  ..., -0.4835, -0.2967, -0.6243],\n",
      "         [-0.6707, -0.6815, -0.1203,  ...,  0.2391, -0.1351, -0.4887],\n",
      "         ...,\n",
      "         [-0.1081, -0.6825,  1.0234,  ..., -0.1154, -1.4197,  0.6475],\n",
      "         [-0.3294, -1.1419,  0.1581,  ...,  1.0760,  0.2277,  0.8325],\n",
      "         [ 0.4066, -0.7241,  0.8159,  ...,  1.2632, -0.3010,  0.0068]]],\n",
      "       device='cuda:0'),) and output (tensor([[[ 0.3711,  0.5489,  0.6100,  ..., -0.7499,  0.2080,  0.4195],\n",
      "         [ 0.2964, -1.4950, -0.9199,  ..., -0.8054, -0.3623, -1.0354],\n",
      "         [-0.5516, -0.8930, -0.4495,  ...,  0.0493, -0.1172, -0.9305],\n",
      "         ...,\n",
      "         [-0.1701, -0.7540,  0.7233,  ..., -0.4267, -1.6203,  0.1835],\n",
      "         [-0.2602, -1.5205,  0.1772,  ...,  0.9877, -0.1273,  0.5192],\n",
      "         [-0.1258, -1.6806,  0.5762,  ...,  1.2788, -0.4289, -0.5511]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[ 0.3711,  0.5489,  0.6100,  ..., -0.7499,  0.2080,  0.4195],\n",
      "         [ 0.2964, -1.4950, -0.9199,  ..., -0.8054, -0.3623, -1.0354],\n",
      "         [-0.5516, -0.8930, -0.4495,  ...,  0.0493, -0.1172, -0.9305],\n",
      "         ...,\n",
      "         [-0.1701, -0.7540,  0.7233,  ..., -0.4267, -1.6203,  0.1835],\n",
      "         [-0.2602, -1.5205,  0.1772,  ...,  0.9877, -0.1273,  0.5192],\n",
      "         [-0.1258, -1.6806,  0.5762,  ...,  1.2788, -0.4289, -0.5511]]],\n",
      "       device='cuda:0'),) and output (tensor([[[ 1.1544,  3.2965,  1.6534,  ..., -2.7009,  2.6553,  2.2700],\n",
      "         [ 1.2285, -1.1859, -2.2483,  ..., -1.4310, -0.0345, -1.0306],\n",
      "         [-0.7751, -0.3063,  0.3257,  ...,  0.1356,  0.1642, -0.5978],\n",
      "         ...,\n",
      "         [ 0.5497, -2.1672,  1.3655,  ...,  0.7600, -1.2180,  1.0634],\n",
      "         [ 0.5247, -2.3694,  0.4743,  ...,  2.8505, -0.9071,  1.8700],\n",
      "         [-0.0793, -2.4915,  1.3799,  ...,  2.1372, -0.7193, -0.0305]]],\n",
      "       device='cuda:0'),)\n",
      "[Debug] Layer names being passed: ['model.layers.0', 'model.layers.1', 'model.layers.2', 'model.layers.3', 'model.layers.4', 'model.layers.5', 'model.layers.6', 'model.layers.7', 'model.layers.8', 'model.layers.9', 'model.layers.10', 'model.layers.11', 'model.layers.12', 'model.layers.13', 'model.layers.14', 'model.layers.15', 'model.layers.16', 'model.layers.17', 'model.layers.18', 'model.layers.19', 'model.layers.20', 'model.layers.21', 'model.layers.22', 'model.layers.23', 'model.layers.24', 'model.layers.25', 'model.layers.26', 'model.layers.27', 'model.layers.28', 'model.layers.29', 'model.layers.30', 'model.layers.31', 'model.embed_tokens']\n",
      "[Debug] Trying to access layer: model.layers.0\n",
      "[Debug] Successfully found layer: model.layers.0\n",
      "[Debug] Trying to access layer: model.layers.1\n",
      "[Debug] Successfully found layer: model.layers.1\n",
      "[Debug] Trying to access layer: model.layers.2\n",
      "[Debug] Successfully found layer: model.layers.2\n",
      "[Debug] Trying to access layer: model.layers.3\n",
      "[Debug] Successfully found layer: model.layers.3\n",
      "[Debug] Trying to access layer: model.layers.4\n",
      "[Debug] Successfully found layer: model.layers.4\n",
      "[Debug] Trying to access layer: model.layers.5\n",
      "[Debug] Successfully found layer: model.layers.5\n",
      "[Debug] Trying to access layer: model.layers.6\n",
      "[Debug] Successfully found layer: model.layers.6\n",
      "[Debug] Trying to access layer: model.layers.7\n",
      "[Debug] Successfully found layer: model.layers.7\n",
      "[Debug] Trying to access layer: model.layers.8\n",
      "[Debug] Successfully found layer: model.layers.8\n",
      "[Debug] Trying to access layer: model.layers.9\n",
      "[Debug] Successfully found layer: model.layers.9\n",
      "[Debug] Trying to access layer: model.layers.10\n",
      "[Debug] Successfully found layer: model.layers.10\n",
      "[Debug] Trying to access layer: model.layers.11\n",
      "[Debug] Successfully found layer: model.layers.11\n",
      "[Debug] Trying to access layer: model.layers.12\n",
      "[Debug] Successfully found layer: model.layers.12\n",
      "[Debug] Trying to access layer: model.layers.13\n",
      "[Debug] Successfully found layer: model.layers.13\n",
      "[Debug] Trying to access layer: model.layers.14\n",
      "[Debug] Successfully found layer: model.layers.14\n",
      "[Debug] Trying to access layer: model.layers.15\n",
      "[Debug] Successfully found layer: model.layers.15\n",
      "[Debug] Trying to access layer: model.layers.16\n",
      "[Debug] Successfully found layer: model.layers.16\n",
      "[Debug] Trying to access layer: model.layers.17\n",
      "[Debug] Successfully found layer: model.layers.17\n",
      "[Debug] Trying to access layer: model.layers.18\n",
      "[Debug] Successfully found layer: model.layers.18\n",
      "[Debug] Trying to access layer: model.layers.19\n",
      "[Debug] Successfully found layer: model.layers.19\n",
      "[Debug] Trying to access layer: model.layers.20\n",
      "[Debug] Successfully found layer: model.layers.20\n",
      "[Debug] Trying to access layer: model.layers.21\n",
      "[Debug] Successfully found layer: model.layers.21\n",
      "[Debug] Trying to access layer: model.layers.22\n",
      "[Debug] Successfully found layer: model.layers.22\n",
      "[Debug] Trying to access layer: model.layers.23\n",
      "[Debug] Successfully found layer: model.layers.23\n",
      "[Debug] Trying to access layer: model.layers.24\n",
      "[Debug] Successfully found layer: model.layers.24\n",
      "[Debug] Trying to access layer: model.layers.25\n",
      "[Debug] Successfully found layer: model.layers.25\n",
      "[Debug] Trying to access layer: model.layers.26\n",
      "[Debug] Successfully found layer: model.layers.26\n",
      "[Debug] Trying to access layer: model.layers.27\n",
      "[Debug] Successfully found layer: model.layers.27\n",
      "[Debug] Trying to access layer: model.layers.28\n",
      "[Debug] Successfully found layer: model.layers.28\n",
      "[Debug] Trying to access layer: model.layers.29\n",
      "[Debug] Successfully found layer: model.layers.29\n",
      "[Debug] Trying to access layer: model.layers.30\n",
      "[Debug] Successfully found layer: model.layers.30\n",
      "[Debug] Trying to access layer: model.layers.31\n",
      "[Debug] Successfully found layer: model.layers.31\n",
      "[Debug] Trying to access layer: model.embed_tokens\n",
      "[Debug] Successfully found layer: model.embed_tokens\n",
      "[Hook] Layer Embedding(128256, 4096) received input (tensor([[128000,    791,  99671,   2617,    315,  15754,  43761,  25187,    580,\n",
      "             25,   3778,  24845,  15457,    578,  99671,   2617,    315,  15754,\n",
      "          43761,  25187,    580,     25,   3778,  24845,  15457,    374,    279,\n",
      "           2132,   3280,    315,    279,  30001,    837,   9977,  84108,  12707,\n",
      "           4101,   3778,  24845,  15457,     13,    578,   3280,  85170,    389,\n",
      "           6186,    220,   1114,     11,    220,    679,     23,  17706,     16,\n",
      "           1483,     17,     60,    323,  20536,    389,   5587,    220,   1691,\n",
      "             11,    220,    679,     23,     13,   1102,  17610,    315,    264,\n",
      "           2860,    315,    220,     24,  18243,  17706,     18,     60,    323,\n",
      "          41424,    279,  10102,    315,  15034,  15754,  43761,  25187,    580,\n",
      "            555,   6275,  25534,  13929,    356,    359,  29718,     11,   3196,\n",
      "            389,  11583,  96672,  32210,    596,   2363,  38506,  12440,    435,\n",
      "          51585,     25,  13929,    356,    359,  29718,     11,  15754,  43761,\n",
      "          25187,    580,     11,    323,    279,  84419,  22092,   2418,  81175,\n",
      "            304,    549,    815,     13,  11346,   8032,     17,   1483,     19,\n",
      "             60]], device='cuda:0'),) and output tensor([[[ 2.6512e-04, -4.9973e-04, -5.8365e-04,  ...,  3.8147e-03,\n",
      "           6.3419e-05,  1.1902e-03],\n",
      "         [ 2.7466e-04, -3.4027e-03, -6.4087e-04,  ...,  8.4229e-03,\n",
      "          -4.2114e-03,  1.5335e-03],\n",
      "         [-7.4158e-03,  2.1744e-04, -5.3101e-03,  ...,  1.4893e-02,\n",
      "          -2.6855e-02,  5.4626e-03],\n",
      "         ...,\n",
      "         [ 5.4626e-03, -9.2773e-03,  1.6846e-02,  ...,  5.3024e-04,\n",
      "           1.5503e-02, -2.4567e-03],\n",
      "         [ 3.5706e-03,  3.3569e-03,  6.8970e-03,  ...,  1.3550e-02,\n",
      "          -4.5471e-03, -4.9744e-03],\n",
      "         [ 3.2616e-04,  7.5531e-04,  3.9368e-03,  ...,  1.0132e-02,\n",
      "           6.0425e-03, -1.3542e-04]]], device='cuda:0')\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[ 2.6512e-04, -4.9973e-04, -5.8365e-04,  ...,  3.8147e-03,\n",
      "           6.3419e-05,  1.1902e-03],\n",
      "         [ 2.7466e-04, -3.4027e-03, -6.4087e-04,  ...,  8.4229e-03,\n",
      "          -4.2114e-03,  1.5335e-03],\n",
      "         [-7.4158e-03,  2.1744e-04, -5.3101e-03,  ...,  1.4893e-02,\n",
      "          -2.6855e-02,  5.4626e-03],\n",
      "         ...,\n",
      "         [ 5.4626e-03, -9.2773e-03,  1.6846e-02,  ...,  5.3024e-04,\n",
      "           1.5503e-02, -2.4567e-03],\n",
      "         [ 3.5706e-03,  3.3569e-03,  6.8970e-03,  ...,  1.3550e-02,\n",
      "          -4.5471e-03, -4.9744e-03],\n",
      "         [ 3.2616e-04,  7.5531e-04,  3.9368e-03,  ...,  1.0132e-02,\n",
      "           6.0425e-03, -1.3542e-04]]], device='cuda:0'),) and output (tensor([[[-4.3565e-04,  1.4559e-02, -2.0488e-03,  ...,  6.5782e-03,\n",
      "          -1.9362e-02,  1.9588e-03],\n",
      "         [-1.5181e-02, -7.0963e-03, -3.6262e-03,  ...,  1.5371e-03,\n",
      "          -6.2427e-03, -5.1726e-03],\n",
      "         [-7.3374e-05, -1.4127e-02, -3.5515e-04,  ..., -1.2922e-02,\n",
      "          -4.3560e-02, -1.9163e-02],\n",
      "         ...,\n",
      "         [ 8.3952e-03,  8.7306e-03,  2.4588e-02,  ..., -4.4600e-03,\n",
      "           2.6868e-02,  1.6858e-02],\n",
      "         [ 4.4588e-03, -2.4717e-02, -3.7520e-03,  ...,  5.2865e-03,\n",
      "          -9.3477e-03, -4.6639e-03],\n",
      "         [-9.8662e-03,  7.0202e-03, -4.8459e-03,  ..., -4.0995e-03,\n",
      "          -1.1197e-03,  9.6511e-03]]], device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-4.3565e-04,  1.4559e-02, -2.0488e-03,  ...,  6.5782e-03,\n",
      "          -1.9362e-02,  1.9588e-03],\n",
      "         [-1.5181e-02, -7.0963e-03, -3.6262e-03,  ...,  1.5371e-03,\n",
      "          -6.2427e-03, -5.1726e-03],\n",
      "         [-7.3374e-05, -1.4127e-02, -3.5515e-04,  ..., -1.2922e-02,\n",
      "          -4.3560e-02, -1.9163e-02],\n",
      "         ...,\n",
      "         [ 8.3952e-03,  8.7306e-03,  2.4588e-02,  ..., -4.4600e-03,\n",
      "           2.6868e-02,  1.6858e-02],\n",
      "         [ 4.4588e-03, -2.4717e-02, -3.7520e-03,  ...,  5.2865e-03,\n",
      "          -9.3477e-03, -4.6639e-03],\n",
      "         [-9.8662e-03,  7.0202e-03, -4.8459e-03,  ..., -4.0995e-03,\n",
      "          -1.1197e-03,  9.6511e-03]]], device='cuda:0'),) and output (tensor([[[-0.0972,  0.0787, -0.0419,  ...,  0.0071,  0.0869,  0.0218],\n",
      "         [-0.0250, -0.0023, -0.0025,  ..., -0.0171, -0.0123, -0.0091],\n",
      "         [ 0.0306, -0.0258,  0.0125,  ..., -0.0404, -0.0449, -0.0250],\n",
      "         ...,\n",
      "         [-0.0351, -0.0088,  0.0018,  ..., -0.0049,  0.0124,  0.0492],\n",
      "         [ 0.0066, -0.0362, -0.0227,  ...,  0.0076, -0.0357,  0.0154],\n",
      "         [-0.0188, -0.0212, -0.0178,  ...,  0.0059, -0.0073,  0.0194]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.0972,  0.0787, -0.0419,  ...,  0.0071,  0.0869,  0.0218],\n",
      "         [-0.0250, -0.0023, -0.0025,  ..., -0.0171, -0.0123, -0.0091],\n",
      "         [ 0.0306, -0.0258,  0.0125,  ..., -0.0404, -0.0449, -0.0250],\n",
      "         ...,\n",
      "         [-0.0351, -0.0088,  0.0018,  ..., -0.0049,  0.0124,  0.0492],\n",
      "         [ 0.0066, -0.0362, -0.0227,  ...,  0.0076, -0.0357,  0.0154],\n",
      "         [-0.0188, -0.0212, -0.0178,  ...,  0.0059, -0.0073,  0.0194]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.1040,  0.0875, -0.0360,  ...,  0.0205,  0.0997,  0.0184],\n",
      "         [-0.0416,  0.0026,  0.0068,  ...,  0.0168, -0.0236, -0.0086],\n",
      "         [ 0.0361, -0.0458,  0.0456,  ...,  0.0058, -0.0654, -0.0252],\n",
      "         ...,\n",
      "         [-0.0182, -0.0396,  0.0183,  ..., -0.0220, -0.0021,  0.1004],\n",
      "         [-0.0074, -0.0881, -0.0487,  ..., -0.0615, -0.0508,  0.0141],\n",
      "         [-0.0506, -0.0241,  0.0086,  ...,  0.0217, -0.0174,  0.0321]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.1040,  0.0875, -0.0360,  ...,  0.0205,  0.0997,  0.0184],\n",
      "         [-0.0416,  0.0026,  0.0068,  ...,  0.0168, -0.0236, -0.0086],\n",
      "         [ 0.0361, -0.0458,  0.0456,  ...,  0.0058, -0.0654, -0.0252],\n",
      "         ...,\n",
      "         [-0.0182, -0.0396,  0.0183,  ..., -0.0220, -0.0021,  0.1004],\n",
      "         [-0.0074, -0.0881, -0.0487,  ..., -0.0615, -0.0508,  0.0141],\n",
      "         [-0.0506, -0.0241,  0.0086,  ...,  0.0217, -0.0174,  0.0321]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.0870,  0.1049, -0.0173,  ...,  0.0700,  0.0989,  0.0181],\n",
      "         [-0.0527,  0.0377, -0.0005,  ..., -0.0223, -0.0112, -0.0610],\n",
      "         [ 0.0267, -0.0363,  0.0167,  ...,  0.0340, -0.0944, -0.0361],\n",
      "         ...,\n",
      "         [-0.0562, -0.0553, -0.0185,  ..., -0.0578, -0.0067,  0.0683],\n",
      "         [-0.0464, -0.0999, -0.0368,  ..., -0.0332, -0.0790,  0.0310],\n",
      "         [-0.0900, -0.0219, -0.0296,  ..., -0.0105, -0.0664,  0.0377]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.0870,  0.1049, -0.0173,  ...,  0.0700,  0.0989,  0.0181],\n",
      "         [-0.0527,  0.0377, -0.0005,  ..., -0.0223, -0.0112, -0.0610],\n",
      "         [ 0.0267, -0.0363,  0.0167,  ...,  0.0340, -0.0944, -0.0361],\n",
      "         ...,\n",
      "         [-0.0562, -0.0553, -0.0185,  ..., -0.0578, -0.0067,  0.0683],\n",
      "         [-0.0464, -0.0999, -0.0368,  ..., -0.0332, -0.0790,  0.0310],\n",
      "         [-0.0900, -0.0219, -0.0296,  ..., -0.0105, -0.0664,  0.0377]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-1.1125e-01,  1.0220e-01, -1.0192e-02,  ...,  4.3573e-02,\n",
      "           1.2825e-01,  3.2118e-02],\n",
      "         [-5.9656e-02,  4.1294e-02,  3.8638e-02,  ...,  3.4347e-02,\n",
      "          -1.4551e-02, -2.1487e-05],\n",
      "         [ 4.7793e-02, -5.5158e-02,  2.0172e-02,  ...,  9.2105e-02,\n",
      "          -6.9670e-02, -7.9244e-02],\n",
      "         ...,\n",
      "         [-7.3321e-02, -2.4983e-02, -1.6198e-02,  ..., -6.5192e-02,\n",
      "          -2.7062e-02,  1.2484e-01],\n",
      "         [-4.8934e-02, -6.8651e-02, -2.4252e-03,  ..., -9.9368e-02,\n",
      "          -8.4661e-02,  8.2085e-02],\n",
      "         [-8.0088e-02, -3.0292e-02, -1.8894e-02,  ...,  6.2893e-02,\n",
      "          -3.7486e-02,  2.6106e-02]]], device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-1.1125e-01,  1.0220e-01, -1.0192e-02,  ...,  4.3573e-02,\n",
      "           1.2825e-01,  3.2118e-02],\n",
      "         [-5.9656e-02,  4.1294e-02,  3.8638e-02,  ...,  3.4347e-02,\n",
      "          -1.4551e-02, -2.1487e-05],\n",
      "         [ 4.7793e-02, -5.5158e-02,  2.0172e-02,  ...,  9.2105e-02,\n",
      "          -6.9670e-02, -7.9244e-02],\n",
      "         ...,\n",
      "         [-7.3321e-02, -2.4983e-02, -1.6198e-02,  ..., -6.5192e-02,\n",
      "          -2.7062e-02,  1.2484e-01],\n",
      "         [-4.8934e-02, -6.8651e-02, -2.4252e-03,  ..., -9.9368e-02,\n",
      "          -8.4661e-02,  8.2085e-02],\n",
      "         [-8.0088e-02, -3.0292e-02, -1.8894e-02,  ...,  6.2893e-02,\n",
      "          -3.7486e-02,  2.6106e-02]]], device='cuda:0'),) and output (tensor([[[-0.1034,  0.1132,  0.0096,  ...,  0.0453,  0.1477,  0.0301],\n",
      "         [-0.0344,  0.0188,  0.0766,  ..., -0.0083,  0.0013, -0.0027],\n",
      "         [ 0.0179, -0.0117,  0.0279,  ...,  0.1457, -0.0623, -0.0684],\n",
      "         ...,\n",
      "         [-0.1070, -0.0686,  0.0309,  ..., -0.0418, -0.0361,  0.1356],\n",
      "         [-0.0536, -0.0706,  0.0655,  ..., -0.0541, -0.0985,  0.1033],\n",
      "         [-0.0401, -0.0231,  0.0326,  ...,  0.0753, -0.0433,  0.0318]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.1034,  0.1132,  0.0096,  ...,  0.0453,  0.1477,  0.0301],\n",
      "         [-0.0344,  0.0188,  0.0766,  ..., -0.0083,  0.0013, -0.0027],\n",
      "         [ 0.0179, -0.0117,  0.0279,  ...,  0.1457, -0.0623, -0.0684],\n",
      "         ...,\n",
      "         [-0.1070, -0.0686,  0.0309,  ..., -0.0418, -0.0361,  0.1356],\n",
      "         [-0.0536, -0.0706,  0.0655,  ..., -0.0541, -0.0985,  0.1033],\n",
      "         [-0.0401, -0.0231,  0.0326,  ...,  0.0753, -0.0433,  0.0318]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.1071,  0.1253,  0.0211,  ...,  0.0126,  0.1586,  0.0278],\n",
      "         [ 0.0101,  0.0627,  0.0499,  ...,  0.0054,  0.0088,  0.0445],\n",
      "         [ 0.0017, -0.0164,  0.0617,  ...,  0.1481, -0.0572,  0.0550],\n",
      "         ...,\n",
      "         [-0.0819, -0.0837,  0.0829,  ..., -0.0139, -0.0536,  0.1452],\n",
      "         [-0.0424, -0.0715,  0.0560,  ..., -0.0170, -0.1200,  0.1247],\n",
      "         [-0.0683, -0.0562,  0.0373,  ...,  0.0710, -0.0121, -0.0180]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.1071,  0.1253,  0.0211,  ...,  0.0126,  0.1586,  0.0278],\n",
      "         [ 0.0101,  0.0627,  0.0499,  ...,  0.0054,  0.0088,  0.0445],\n",
      "         [ 0.0017, -0.0164,  0.0617,  ...,  0.1481, -0.0572,  0.0550],\n",
      "         ...,\n",
      "         [-0.0819, -0.0837,  0.0829,  ..., -0.0139, -0.0536,  0.1452],\n",
      "         [-0.0424, -0.0715,  0.0560,  ..., -0.0170, -0.1200,  0.1247],\n",
      "         [-0.0683, -0.0562,  0.0373,  ...,  0.0710, -0.0121, -0.0180]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.0945,  0.1848,  0.0036,  ..., -0.0473,  0.1489,  0.0536],\n",
      "         [ 0.1059,  0.0064,  0.0021,  ..., -0.0513, -0.1022,  0.0099],\n",
      "         [ 0.0738, -0.0419,  0.0272,  ...,  0.0646, -0.1219,  0.0800],\n",
      "         ...,\n",
      "         [-0.1173, -0.0381,  0.0940,  ...,  0.0383, -0.1008,  0.1531],\n",
      "         [-0.0478, -0.0492,  0.0308,  ...,  0.0065, -0.1561,  0.0915],\n",
      "         [-0.0945, -0.0689,  0.0329,  ...,  0.0984, -0.1280,  0.0126]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.0945,  0.1848,  0.0036,  ..., -0.0473,  0.1489,  0.0536],\n",
      "         [ 0.1059,  0.0064,  0.0021,  ..., -0.0513, -0.1022,  0.0099],\n",
      "         [ 0.0738, -0.0419,  0.0272,  ...,  0.0646, -0.1219,  0.0800],\n",
      "         ...,\n",
      "         [-0.1173, -0.0381,  0.0940,  ...,  0.0383, -0.1008,  0.1531],\n",
      "         [-0.0478, -0.0492,  0.0308,  ...,  0.0065, -0.1561,  0.0915],\n",
      "         [-0.0945, -0.0689,  0.0329,  ...,  0.0984, -0.1280,  0.0126]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.1060,  0.2113,  0.0270,  ..., -0.1141,  0.1610,  0.0528],\n",
      "         [ 0.1145,  0.0790, -0.0139,  ..., -0.0473, -0.1167,  0.0026],\n",
      "         [ 0.0615, -0.0024,  0.0130,  ...,  0.0737, -0.1436,  0.0148],\n",
      "         ...,\n",
      "         [-0.0719, -0.0562,  0.0151,  ...,  0.1184, -0.0537,  0.1146],\n",
      "         [-0.0618, -0.0912,  0.0191,  ...,  0.1515, -0.1669,  0.0800],\n",
      "         [-0.1571, -0.1907,  0.0237,  ...,  0.0828, -0.1209,  0.0074]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.1060,  0.2113,  0.0270,  ..., -0.1141,  0.1610,  0.0528],\n",
      "         [ 0.1145,  0.0790, -0.0139,  ..., -0.0473, -0.1167,  0.0026],\n",
      "         [ 0.0615, -0.0024,  0.0130,  ...,  0.0737, -0.1436,  0.0148],\n",
      "         ...,\n",
      "         [-0.0719, -0.0562,  0.0151,  ...,  0.1184, -0.0537,  0.1146],\n",
      "         [-0.0618, -0.0912,  0.0191,  ...,  0.1515, -0.1669,  0.0800],\n",
      "         [-0.1571, -0.1907,  0.0237,  ...,  0.0828, -0.1209,  0.0074]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.0954,  0.2493,  0.0073,  ..., -0.1568,  0.1818,  0.0855],\n",
      "         [ 0.0329,  0.0794,  0.0036,  ..., -0.0334, -0.0492,  0.0487],\n",
      "         [-0.0531, -0.0448,  0.0251,  ...,  0.0423, -0.1172,  0.0550],\n",
      "         ...,\n",
      "         [-0.1189, -0.0958,  0.0342,  ...,  0.0613,  0.0413,  0.1080],\n",
      "         [-0.0655, -0.1728,  0.0946,  ...,  0.1123, -0.0217,  0.0610],\n",
      "         [-0.0374, -0.0421, -0.0215,  ..., -0.0147, -0.1092, -0.0073]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.0954,  0.2493,  0.0073,  ..., -0.1568,  0.1818,  0.0855],\n",
      "         [ 0.0329,  0.0794,  0.0036,  ..., -0.0334, -0.0492,  0.0487],\n",
      "         [-0.0531, -0.0448,  0.0251,  ...,  0.0423, -0.1172,  0.0550],\n",
      "         ...,\n",
      "         [-0.1189, -0.0958,  0.0342,  ...,  0.0613,  0.0413,  0.1080],\n",
      "         [-0.0655, -0.1728,  0.0946,  ...,  0.1123, -0.0217,  0.0610],\n",
      "         [-0.0374, -0.0421, -0.0215,  ..., -0.0147, -0.1092, -0.0073]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.0152,  0.3340,  0.0011,  ..., -0.2750,  0.1933,  0.1034],\n",
      "         [ 0.0674,  0.0644,  0.0446,  ..., -0.0611, -0.0590,  0.0768],\n",
      "         [-0.0245, -0.0710, -0.0335,  ..., -0.0004, -0.0897,  0.0201],\n",
      "         ...,\n",
      "         [-0.1138, -0.0898,  0.0515,  ..., -0.0056,  0.0708,  0.0758],\n",
      "         [-0.0500, -0.1979,  0.0324,  ...,  0.1109, -0.0808,  0.0618],\n",
      "         [-0.0311, -0.0613,  0.0277,  ..., -0.0068, -0.1438,  0.1134]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.0152,  0.3340,  0.0011,  ..., -0.2750,  0.1933,  0.1034],\n",
      "         [ 0.0674,  0.0644,  0.0446,  ..., -0.0611, -0.0590,  0.0768],\n",
      "         [-0.0245, -0.0710, -0.0335,  ..., -0.0004, -0.0897,  0.0201],\n",
      "         ...,\n",
      "         [-0.1138, -0.0898,  0.0515,  ..., -0.0056,  0.0708,  0.0758],\n",
      "         [-0.0500, -0.1979,  0.0324,  ...,  0.1109, -0.0808,  0.0618],\n",
      "         [-0.0311, -0.0613,  0.0277,  ..., -0.0068, -0.1438,  0.1134]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.0356,  0.3483, -0.0366,  ..., -0.2962,  0.2088,  0.1065],\n",
      "         [ 0.0575,  0.0667,  0.0356,  ..., -0.1157, -0.0330,  0.0805],\n",
      "         [-0.0762, -0.0879, -0.0706,  ..., -0.0569, -0.0340, -0.0023],\n",
      "         ...,\n",
      "         [-0.1005, -0.0947,  0.0397,  ...,  0.0353,  0.0532, -0.0512],\n",
      "         [-0.1225, -0.2111, -0.0411,  ...,  0.0221,  0.0660, -0.1251],\n",
      "         [ 0.0068, -0.0898,  0.0177,  ..., -0.0645,  0.0432,  0.0969]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.0356,  0.3483, -0.0366,  ..., -0.2962,  0.2088,  0.1065],\n",
      "         [ 0.0575,  0.0667,  0.0356,  ..., -0.1157, -0.0330,  0.0805],\n",
      "         [-0.0762, -0.0879, -0.0706,  ..., -0.0569, -0.0340, -0.0023],\n",
      "         ...,\n",
      "         [-0.1005, -0.0947,  0.0397,  ...,  0.0353,  0.0532, -0.0512],\n",
      "         [-0.1225, -0.2111, -0.0411,  ...,  0.0221,  0.0660, -0.1251],\n",
      "         [ 0.0068, -0.0898,  0.0177,  ..., -0.0645,  0.0432,  0.0969]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.0127,  0.3661, -0.0316,  ..., -0.3678,  0.1973,  0.1019],\n",
      "         [ 0.0629,  0.0343,  0.0581,  ..., -0.1197, -0.0838,  0.1232],\n",
      "         [-0.0848, -0.0822, -0.0319,  ..., -0.0516, -0.1077, -0.0134],\n",
      "         ...,\n",
      "         [-0.2720, -0.1091,  0.0754,  ...,  0.1320,  0.1198,  0.0645],\n",
      "         [-0.1879, -0.1671, -0.1214,  ...,  0.1061, -0.0608, -0.0981],\n",
      "         [-0.0888, -0.1687, -0.0169,  ...,  0.0478, -0.1637,  0.0663]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.0127,  0.3661, -0.0316,  ..., -0.3678,  0.1973,  0.1019],\n",
      "         [ 0.0629,  0.0343,  0.0581,  ..., -0.1197, -0.0838,  0.1232],\n",
      "         [-0.0848, -0.0822, -0.0319,  ..., -0.0516, -0.1077, -0.0134],\n",
      "         ...,\n",
      "         [-0.2720, -0.1091,  0.0754,  ...,  0.1320,  0.1198,  0.0645],\n",
      "         [-0.1879, -0.1671, -0.1214,  ...,  0.1061, -0.0608, -0.0981],\n",
      "         [-0.0888, -0.1687, -0.0169,  ...,  0.0478, -0.1637,  0.0663]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.0089,  0.3233, -0.0779,  ..., -0.4491,  0.1845,  0.0951],\n",
      "         [-0.0656,  0.0027,  0.0496,  ..., -0.1376, -0.0422,  0.0665],\n",
      "         [-0.0596, -0.0300,  0.0546,  ..., -0.0407, -0.0931, -0.0573],\n",
      "         ...,\n",
      "         [-0.2550, -0.1992,  0.0229,  ...,  0.2390, -0.0032,  0.2114],\n",
      "         [-0.2563, -0.1843, -0.0828,  ..., -0.0107, -0.1318, -0.0274],\n",
      "         [-0.1429, -0.0273,  0.1055,  ..., -0.0962, -0.2499,  0.1304]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.0089,  0.3233, -0.0779,  ..., -0.4491,  0.1845,  0.0951],\n",
      "         [-0.0656,  0.0027,  0.0496,  ..., -0.1376, -0.0422,  0.0665],\n",
      "         [-0.0596, -0.0300,  0.0546,  ..., -0.0407, -0.0931, -0.0573],\n",
      "         ...,\n",
      "         [-0.2550, -0.1992,  0.0229,  ...,  0.2390, -0.0032,  0.2114],\n",
      "         [-0.2563, -0.1843, -0.0828,  ..., -0.0107, -0.1318, -0.0274],\n",
      "         [-0.1429, -0.0273,  0.1055,  ..., -0.0962, -0.2499,  0.1304]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.0244,  0.2828, -0.0532,  ..., -0.4592,  0.2300,  0.0142],\n",
      "         [-0.0418, -0.1385,  0.0318,  ..., -0.1957,  0.0263, -0.0267],\n",
      "         [-0.0706, -0.0258,  0.0475,  ..., -0.0095, -0.1598, -0.0349],\n",
      "         ...,\n",
      "         [-0.2857, -0.1544, -0.0748,  ...,  0.1759,  0.0849,  0.1142],\n",
      "         [-0.2616, -0.1548, -0.1113,  ..., -0.1403, -0.1151, -0.1525],\n",
      "         [-0.1568, -0.0011,  0.0512,  ..., -0.0353, -0.2777,  0.1834]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.0244,  0.2828, -0.0532,  ..., -0.4592,  0.2300,  0.0142],\n",
      "         [-0.0418, -0.1385,  0.0318,  ..., -0.1957,  0.0263, -0.0267],\n",
      "         [-0.0706, -0.0258,  0.0475,  ..., -0.0095, -0.1598, -0.0349],\n",
      "         ...,\n",
      "         [-0.2857, -0.1544, -0.0748,  ...,  0.1759,  0.0849,  0.1142],\n",
      "         [-0.2616, -0.1548, -0.1113,  ..., -0.1403, -0.1151, -0.1525],\n",
      "         [-0.1568, -0.0011,  0.0512,  ..., -0.0353, -0.2777,  0.1834]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.0928,  0.1485,  0.0158,  ..., -0.5380,  0.2664, -0.0333],\n",
      "         [-0.1055, -0.2053,  0.0479,  ..., -0.2984,  0.0673, -0.0260],\n",
      "         [-0.1339, -0.0036,  0.0268,  ...,  0.2234, -0.0664, -0.0039],\n",
      "         ...,\n",
      "         [-0.1644, -0.2202,  0.0574,  ...,  0.1129,  0.1686,  0.0370],\n",
      "         [-0.2344, -0.2038,  0.0154,  ..., -0.2479, -0.1539, -0.2207],\n",
      "         [-0.0406, -0.1518, -0.0643,  ..., -0.0487, -0.4928,  0.1721]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.0928,  0.1485,  0.0158,  ..., -0.5380,  0.2664, -0.0333],\n",
      "         [-0.1055, -0.2053,  0.0479,  ..., -0.2984,  0.0673, -0.0260],\n",
      "         [-0.1339, -0.0036,  0.0268,  ...,  0.2234, -0.0664, -0.0039],\n",
      "         ...,\n",
      "         [-0.1644, -0.2202,  0.0574,  ...,  0.1129,  0.1686,  0.0370],\n",
      "         [-0.2344, -0.2038,  0.0154,  ..., -0.2479, -0.1539, -0.2207],\n",
      "         [-0.0406, -0.1518, -0.0643,  ..., -0.0487, -0.4928,  0.1721]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.1000,  0.1323,  0.0241,  ..., -0.5470,  0.2844, -0.0457],\n",
      "         [-0.0166, -0.2701,  0.0152,  ..., -0.1550, -0.0027, -0.0524],\n",
      "         [-0.0844,  0.0620,  0.0811,  ...,  0.2851, -0.2193, -0.0621],\n",
      "         ...,\n",
      "         [-0.0529, -0.2667, -0.0713,  ...,  0.1700,  0.1226,  0.0696],\n",
      "         [-0.2445, -0.2268,  0.0074,  ..., -0.1885, -0.1573, -0.1969],\n",
      "         [ 0.0385,  0.0477, -0.0497,  ...,  0.0676, -0.4901,  0.1041]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.1000,  0.1323,  0.0241,  ..., -0.5470,  0.2844, -0.0457],\n",
      "         [-0.0166, -0.2701,  0.0152,  ..., -0.1550, -0.0027, -0.0524],\n",
      "         [-0.0844,  0.0620,  0.0811,  ...,  0.2851, -0.2193, -0.0621],\n",
      "         ...,\n",
      "         [-0.0529, -0.2667, -0.0713,  ...,  0.1700,  0.1226,  0.0696],\n",
      "         [-0.2445, -0.2268,  0.0074,  ..., -0.1885, -0.1573, -0.1969],\n",
      "         [ 0.0385,  0.0477, -0.0497,  ...,  0.0676, -0.4901,  0.1041]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.1286,  0.0833,  0.0239,  ..., -0.5657,  0.3694, -0.0725],\n",
      "         [-0.0665, -0.2103, -0.0007,  ..., -0.2566, -0.0726,  0.0518],\n",
      "         [-0.2598,  0.0570,  0.0747,  ...,  0.2629, -0.3507, -0.2084],\n",
      "         ...,\n",
      "         [ 0.1948, -0.0459, -0.0071,  ...,  0.0905, -0.0404,  0.0045],\n",
      "         [-0.1223, -0.3128, -0.0399,  ..., -0.1471, -0.3552, -0.2213],\n",
      "         [ 0.0210,  0.0716, -0.0226,  ...,  0.0520, -0.6179,  0.0240]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.1286,  0.0833,  0.0239,  ..., -0.5657,  0.3694, -0.0725],\n",
      "         [-0.0665, -0.2103, -0.0007,  ..., -0.2566, -0.0726,  0.0518],\n",
      "         [-0.2598,  0.0570,  0.0747,  ...,  0.2629, -0.3507, -0.2084],\n",
      "         ...,\n",
      "         [ 0.1948, -0.0459, -0.0071,  ...,  0.0905, -0.0404,  0.0045],\n",
      "         [-0.1223, -0.3128, -0.0399,  ..., -0.1471, -0.3552, -0.2213],\n",
      "         [ 0.0210,  0.0716, -0.0226,  ...,  0.0520, -0.6179,  0.0240]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-1.4171e-01,  7.0286e-02,  3.4279e-02,  ..., -5.5491e-01,\n",
      "           3.1749e-01, -8.4654e-02],\n",
      "         [-9.5375e-02, -2.5233e-01, -9.8082e-04,  ..., -2.5723e-01,\n",
      "           5.2690e-02,  1.5637e-02],\n",
      "         [-2.7661e-01,  2.6115e-02,  6.6899e-02,  ...,  3.2652e-01,\n",
      "          -3.5924e-01, -1.8449e-01],\n",
      "         ...,\n",
      "         [ 1.6796e-01,  4.8587e-04, -4.8112e-02,  ..., -5.1709e-03,\n",
      "          -1.3967e-02, -9.2257e-02],\n",
      "         [-2.7186e-01, -2.8749e-01, -1.4513e-01,  ..., -2.7989e-01,\n",
      "          -3.8042e-01, -2.9075e-01],\n",
      "         [-4.8696e-02,  1.0878e-01,  7.1459e-02,  ...,  1.6171e-01,\n",
      "          -5.4697e-01,  1.0639e-02]]], device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-1.4171e-01,  7.0286e-02,  3.4279e-02,  ..., -5.5491e-01,\n",
      "           3.1749e-01, -8.4654e-02],\n",
      "         [-9.5375e-02, -2.5233e-01, -9.8082e-04,  ..., -2.5723e-01,\n",
      "           5.2690e-02,  1.5637e-02],\n",
      "         [-2.7661e-01,  2.6115e-02,  6.6899e-02,  ...,  3.2652e-01,\n",
      "          -3.5924e-01, -1.8449e-01],\n",
      "         ...,\n",
      "         [ 1.6796e-01,  4.8587e-04, -4.8112e-02,  ..., -5.1709e-03,\n",
      "          -1.3967e-02, -9.2257e-02],\n",
      "         [-2.7186e-01, -2.8749e-01, -1.4513e-01,  ..., -2.7989e-01,\n",
      "          -3.8042e-01, -2.9075e-01],\n",
      "         [-4.8696e-02,  1.0878e-01,  7.1459e-02,  ...,  1.6171e-01,\n",
      "          -5.4697e-01,  1.0639e-02]]], device='cuda:0'),) and output (tensor([[[-0.1528,  0.0725,  0.0609,  ..., -0.5624,  0.3103, -0.0293],\n",
      "         [-0.0828, -0.2299, -0.0623,  ..., -0.2730,  0.0972,  0.0513],\n",
      "         [-0.2957,  0.0901,  0.0488,  ...,  0.4345, -0.3956, -0.2142],\n",
      "         ...,\n",
      "         [ 0.2223,  0.0191, -0.0140,  ...,  0.0692,  0.1159, -0.0826],\n",
      "         [-0.2996, -0.3235, -0.1456,  ..., -0.2993, -0.3519, -0.2748],\n",
      "         [-0.1013,  0.1690,  0.1310,  ...,  0.0494, -0.5254,  0.0826]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.1528,  0.0725,  0.0609,  ..., -0.5624,  0.3103, -0.0293],\n",
      "         [-0.0828, -0.2299, -0.0623,  ..., -0.2730,  0.0972,  0.0513],\n",
      "         [-0.2957,  0.0901,  0.0488,  ...,  0.4345, -0.3956, -0.2142],\n",
      "         ...,\n",
      "         [ 0.2223,  0.0191, -0.0140,  ...,  0.0692,  0.1159, -0.0826],\n",
      "         [-0.2996, -0.3235, -0.1456,  ..., -0.2993, -0.3519, -0.2748],\n",
      "         [-0.1013,  0.1690,  0.1310,  ...,  0.0494, -0.5254,  0.0826]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-1.5591e-01,  5.2012e-02,  5.6647e-02,  ..., -5.8049e-01,\n",
      "           2.6142e-01, -3.2377e-03],\n",
      "         [-2.0807e-01, -3.3983e-01, -1.9838e-01,  ..., -2.9090e-01,\n",
      "           9.9228e-02,  1.3890e-01],\n",
      "         [-3.9545e-01, -4.4465e-05,  1.4758e-01,  ...,  6.6516e-01,\n",
      "          -4.9165e-01, -2.9048e-01],\n",
      "         ...,\n",
      "         [ 3.0360e-01,  7.3114e-02, -1.5020e-01,  ...,  2.1065e-01,\n",
      "           1.6287e-01,  2.6384e-02],\n",
      "         [-2.0823e-01, -1.2270e-01, -3.4753e-01,  ..., -2.8253e-01,\n",
      "          -3.5977e-01, -2.6678e-01],\n",
      "         [-2.7292e-02,  2.1317e-01, -2.5133e-02,  ...,  4.3014e-02,\n",
      "          -6.1539e-01,  3.5818e-02]]], device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-1.5591e-01,  5.2012e-02,  5.6647e-02,  ..., -5.8049e-01,\n",
      "           2.6142e-01, -3.2377e-03],\n",
      "         [-2.0807e-01, -3.3983e-01, -1.9838e-01,  ..., -2.9090e-01,\n",
      "           9.9228e-02,  1.3890e-01],\n",
      "         [-3.9545e-01, -4.4465e-05,  1.4758e-01,  ...,  6.6516e-01,\n",
      "          -4.9165e-01, -2.9048e-01],\n",
      "         ...,\n",
      "         [ 3.0360e-01,  7.3114e-02, -1.5020e-01,  ...,  2.1065e-01,\n",
      "           1.6287e-01,  2.6384e-02],\n",
      "         [-2.0823e-01, -1.2270e-01, -3.4753e-01,  ..., -2.8253e-01,\n",
      "          -3.5977e-01, -2.6678e-01],\n",
      "         [-2.7292e-02,  2.1317e-01, -2.5133e-02,  ...,  4.3014e-02,\n",
      "          -6.1539e-01,  3.5818e-02]]], device='cuda:0'),) and output (tensor([[[-0.1488,  0.0445,  0.0765,  ..., -0.5456,  0.2702,  0.0098],\n",
      "         [-0.1915, -0.4067, -0.1452,  ..., -0.3101,  0.0884,  0.1732],\n",
      "         [-0.2131, -0.0263,  0.0018,  ...,  0.6624, -0.4119, -0.4236],\n",
      "         ...,\n",
      "         [ 0.2699,  0.1683, -0.1930,  ...,  0.2286,  0.1970,  0.2003],\n",
      "         [-0.1884, -0.0842, -0.2643,  ..., -0.3207, -0.3769, -0.1424],\n",
      "         [ 0.0440,  0.2373, -0.1041,  ...,  0.0642, -0.6709,  0.0838]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.1488,  0.0445,  0.0765,  ..., -0.5456,  0.2702,  0.0098],\n",
      "         [-0.1915, -0.4067, -0.1452,  ..., -0.3101,  0.0884,  0.1732],\n",
      "         [-0.2131, -0.0263,  0.0018,  ...,  0.6624, -0.4119, -0.4236],\n",
      "         ...,\n",
      "         [ 0.2699,  0.1683, -0.1930,  ...,  0.2286,  0.1970,  0.2003],\n",
      "         [-0.1884, -0.0842, -0.2643,  ..., -0.3207, -0.3769, -0.1424],\n",
      "         [ 0.0440,  0.2373, -0.1041,  ...,  0.0642, -0.6709,  0.0838]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.1268,  0.0333,  0.0935,  ..., -0.5320,  0.2795,  0.0006],\n",
      "         [-0.2520, -0.3090, -0.0884,  ..., -0.3552,  0.2079,  0.1885],\n",
      "         [-0.2274,  0.1151,  0.1483,  ...,  0.5862, -0.4123, -0.4589],\n",
      "         ...,\n",
      "         [ 0.4156,  0.1990, -0.1798,  ...,  0.1270,  0.1198,  0.1676],\n",
      "         [-0.1072, -0.0512, -0.2779,  ..., -0.3736, -0.3078, -0.2047],\n",
      "         [ 0.1788,  0.2512,  0.1786,  ...,  0.1856, -0.5923,  0.0511]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.1268,  0.0333,  0.0935,  ..., -0.5320,  0.2795,  0.0006],\n",
      "         [-0.2520, -0.3090, -0.0884,  ..., -0.3552,  0.2079,  0.1885],\n",
      "         [-0.2274,  0.1151,  0.1483,  ...,  0.5862, -0.4123, -0.4589],\n",
      "         ...,\n",
      "         [ 0.4156,  0.1990, -0.1798,  ...,  0.1270,  0.1198,  0.1676],\n",
      "         [-0.1072, -0.0512, -0.2779,  ..., -0.3736, -0.3078, -0.2047],\n",
      "         [ 0.1788,  0.2512,  0.1786,  ...,  0.1856, -0.5923,  0.0511]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.1355,  0.0227,  0.0554,  ..., -0.5473,  0.2470, -0.0229],\n",
      "         [-0.2753, -0.3434, -0.1208,  ..., -0.2487,  0.1757,  0.1581],\n",
      "         [-0.3392, -0.0408,  0.2485,  ...,  0.7706, -0.3827, -0.4986],\n",
      "         ...,\n",
      "         [ 0.4245,  0.1805, -0.1857,  ...,  0.0586, -0.0604,  0.1174],\n",
      "         [-0.0790, -0.0708, -0.2714,  ..., -0.4654, -0.2549, -0.1464],\n",
      "         [ 0.3399,  0.2214,  0.2737,  ...,  0.3914, -0.7091,  0.2511]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.1355,  0.0227,  0.0554,  ..., -0.5473,  0.2470, -0.0229],\n",
      "         [-0.2753, -0.3434, -0.1208,  ..., -0.2487,  0.1757,  0.1581],\n",
      "         [-0.3392, -0.0408,  0.2485,  ...,  0.7706, -0.3827, -0.4986],\n",
      "         ...,\n",
      "         [ 0.4245,  0.1805, -0.1857,  ...,  0.0586, -0.0604,  0.1174],\n",
      "         [-0.0790, -0.0708, -0.2714,  ..., -0.4654, -0.2549, -0.1464],\n",
      "         [ 0.3399,  0.2214,  0.2737,  ...,  0.3914, -0.7091,  0.2511]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.1350,  0.0163,  0.0576,  ..., -0.5555,  0.2435,  0.0051],\n",
      "         [-0.3503, -0.2899, -0.2047,  ..., -0.3031,  0.3191,  0.0919],\n",
      "         [-0.3067,  0.1185,  0.3303,  ...,  0.8379, -0.5859, -0.4985],\n",
      "         ...,\n",
      "         [ 0.3554,  0.1298, -0.2365,  ..., -0.0626,  0.1179,  0.1325],\n",
      "         [-0.0116,  0.0027, -0.2458,  ..., -0.4247, -0.2579, -0.1851],\n",
      "         [ 0.2101,  0.0854,  0.2637,  ...,  0.4938, -0.4598,  0.2789]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.1350,  0.0163,  0.0576,  ..., -0.5555,  0.2435,  0.0051],\n",
      "         [-0.3503, -0.2899, -0.2047,  ..., -0.3031,  0.3191,  0.0919],\n",
      "         [-0.3067,  0.1185,  0.3303,  ...,  0.8379, -0.5859, -0.4985],\n",
      "         ...,\n",
      "         [ 0.3554,  0.1298, -0.2365,  ..., -0.0626,  0.1179,  0.1325],\n",
      "         [-0.0116,  0.0027, -0.2458,  ..., -0.4247, -0.2579, -0.1851],\n",
      "         [ 0.2101,  0.0854,  0.2637,  ...,  0.4938, -0.4598,  0.2789]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.1301, -0.0029,  0.0279,  ..., -0.5537,  0.2277,  0.0164],\n",
      "         [-0.3928, -0.2932, -0.2480,  ..., -0.3036,  0.4593,  0.1231],\n",
      "         [-0.4890,  0.1510,  0.3040,  ...,  1.0315, -0.4792, -0.4733],\n",
      "         ...,\n",
      "         [ 0.3658,  0.1920, -0.1587,  ..., -0.1228,  0.2037,  0.2450],\n",
      "         [-0.1377, -0.1305, -0.2385,  ..., -0.7468, -0.2261, -0.2688],\n",
      "         [ 0.1461,  0.1839,  0.4153,  ...,  0.4663, -0.5542,  0.1759]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.1301, -0.0029,  0.0279,  ..., -0.5537,  0.2277,  0.0164],\n",
      "         [-0.3928, -0.2932, -0.2480,  ..., -0.3036,  0.4593,  0.1231],\n",
      "         [-0.4890,  0.1510,  0.3040,  ...,  1.0315, -0.4792, -0.4733],\n",
      "         ...,\n",
      "         [ 0.3658,  0.1920, -0.1587,  ..., -0.1228,  0.2037,  0.2450],\n",
      "         [-0.1377, -0.1305, -0.2385,  ..., -0.7468, -0.2261, -0.2688],\n",
      "         [ 0.1461,  0.1839,  0.4153,  ...,  0.4663, -0.5542,  0.1759]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.1219,  0.0061, -0.0021,  ..., -0.5922,  0.2788, -0.0020],\n",
      "         [-0.4759, -0.2114, -0.3038,  ..., -0.2573,  0.4895,  0.1874],\n",
      "         [-0.6553,  0.2659,  0.2849,  ...,  1.2630, -0.5062, -0.4169],\n",
      "         ...,\n",
      "         [ 0.5081,  0.4131, -0.0988,  ..., -0.1791,  0.2940,  0.1206],\n",
      "         [-0.1485, -0.1001, -0.0529,  ..., -0.6727, -0.0609, -0.0892],\n",
      "         [ 0.1199,  0.0028,  0.7902,  ...,  0.6465, -0.3230,  0.4343]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.1219,  0.0061, -0.0021,  ..., -0.5922,  0.2788, -0.0020],\n",
      "         [-0.4759, -0.2114, -0.3038,  ..., -0.2573,  0.4895,  0.1874],\n",
      "         [-0.6553,  0.2659,  0.2849,  ...,  1.2630, -0.5062, -0.4169],\n",
      "         ...,\n",
      "         [ 0.5081,  0.4131, -0.0988,  ..., -0.1791,  0.2940,  0.1206],\n",
      "         [-0.1485, -0.1001, -0.0529,  ..., -0.6727, -0.0609, -0.0892],\n",
      "         [ 0.1199,  0.0028,  0.7902,  ...,  0.6465, -0.3230,  0.4343]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.1067,  0.0272,  0.0495,  ..., -0.5550,  0.3449,  0.0266],\n",
      "         [-0.4536, -0.2750, -0.3888,  ..., -0.2699,  0.4878,  0.2823],\n",
      "         [-0.6003, -0.0088,  0.2748,  ...,  1.2480, -0.5644, -0.5677],\n",
      "         ...,\n",
      "         [ 0.3960,  0.6100,  0.1169,  ..., -0.2153,  0.2071,  0.3351],\n",
      "         [-0.2648,  0.1332,  0.0904,  ..., -0.7049,  0.0186,  0.1411],\n",
      "         [ 0.0178, -0.0371,  0.8603,  ...,  0.6311, -0.4529,  0.4163]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.1067,  0.0272,  0.0495,  ..., -0.5550,  0.3449,  0.0266],\n",
      "         [-0.4536, -0.2750, -0.3888,  ..., -0.2699,  0.4878,  0.2823],\n",
      "         [-0.6003, -0.0088,  0.2748,  ...,  1.2480, -0.5644, -0.5677],\n",
      "         ...,\n",
      "         [ 0.3960,  0.6100,  0.1169,  ..., -0.2153,  0.2071,  0.3351],\n",
      "         [-0.2648,  0.1332,  0.0904,  ..., -0.7049,  0.0186,  0.1411],\n",
      "         [ 0.0178, -0.0371,  0.8603,  ...,  0.6311, -0.4529,  0.4163]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.0592,  0.0731,  0.0500,  ..., -0.5517,  0.3541,  0.0595],\n",
      "         [-0.7193, -0.1890, -0.3826,  ..., -0.4359,  0.5140,  0.4503],\n",
      "         [-0.9750,  0.0524,  0.2226,  ...,  1.3926, -0.7250, -0.4369],\n",
      "         ...,\n",
      "         [ 0.1365,  0.5479,  0.5646,  ..., -0.5482,  0.2043,  0.3955],\n",
      "         [-0.4618,  0.0775,  0.1536,  ..., -1.1817, -0.0331, -0.1529],\n",
      "         [ 0.0124, -0.0624,  1.0232,  ...,  0.3864, -0.4684,  0.3827]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.0592,  0.0731,  0.0500,  ..., -0.5517,  0.3541,  0.0595],\n",
      "         [-0.7193, -0.1890, -0.3826,  ..., -0.4359,  0.5140,  0.4503],\n",
      "         [-0.9750,  0.0524,  0.2226,  ...,  1.3926, -0.7250, -0.4369],\n",
      "         ...,\n",
      "         [ 0.1365,  0.5479,  0.5646,  ..., -0.5482,  0.2043,  0.3955],\n",
      "         [-0.4618,  0.0775,  0.1536,  ..., -1.1817, -0.0331, -0.1529],\n",
      "         [ 0.0124, -0.0624,  1.0232,  ...,  0.3864, -0.4684,  0.3827]]],\n",
      "       device='cuda:0'),) and output (tensor([[[ 0.1143,  0.2237,  0.3292,  ..., -0.7706,  0.2608,  0.0269],\n",
      "         [-0.8500, -0.3440, -0.4355,  ..., -0.4002,  0.4552,  0.7503],\n",
      "         [-1.2092, -0.3225,  0.0084,  ...,  1.5779, -1.0097, -0.2559],\n",
      "         ...,\n",
      "         [ 0.2262,  0.1611,  0.3234,  ..., -0.6358,  0.0366,  0.3607],\n",
      "         [-0.3123, -0.0554, -0.1687,  ..., -1.0835, -0.0498, -0.1965],\n",
      "         [-0.0638, -0.3025,  1.2530,  ...,  0.6834, -0.1828,  0.3104]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[ 0.1143,  0.2237,  0.3292,  ..., -0.7706,  0.2608,  0.0269],\n",
      "         [-0.8500, -0.3440, -0.4355,  ..., -0.4002,  0.4552,  0.7503],\n",
      "         [-1.2092, -0.3225,  0.0084,  ...,  1.5779, -1.0097, -0.2559],\n",
      "         ...,\n",
      "         [ 0.2262,  0.1611,  0.3234,  ..., -0.6358,  0.0366,  0.3607],\n",
      "         [-0.3123, -0.0554, -0.1687,  ..., -1.0835, -0.0498, -0.1965],\n",
      "         [-0.0638, -0.3025,  1.2530,  ...,  0.6834, -0.1828,  0.3104]]],\n",
      "       device='cuda:0'),) and output (tensor([[[ 0.3711,  0.5489,  0.6100,  ..., -0.7499,  0.2080,  0.4195],\n",
      "         [-0.8181, -0.7324, -0.9711,  ..., -0.5807,  0.4651,  0.3843],\n",
      "         [-0.8491, -0.7277, -0.3598,  ...,  1.6950, -0.5421, -0.2090],\n",
      "         ...,\n",
      "         [ 0.0515,  0.1568, -0.1654,  ..., -0.8480, -0.0181, -0.1041],\n",
      "         [-0.6976, -0.4406, -0.4978,  ..., -1.0011, -0.2971, -0.7357],\n",
      "         [ 0.1061, -0.8187,  1.1304,  ...,  0.4806, -0.3646, -0.0593]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[ 0.3711,  0.5489,  0.6100,  ..., -0.7499,  0.2080,  0.4195],\n",
      "         [-0.8181, -0.7324, -0.9711,  ..., -0.5807,  0.4651,  0.3843],\n",
      "         [-0.8491, -0.7277, -0.3598,  ...,  1.6950, -0.5421, -0.2090],\n",
      "         ...,\n",
      "         [ 0.0515,  0.1568, -0.1654,  ..., -0.8480, -0.0181, -0.1041],\n",
      "         [-0.6976, -0.4406, -0.4978,  ..., -1.0011, -0.2971, -0.7357],\n",
      "         [ 0.1061, -0.8187,  1.1304,  ...,  0.4806, -0.3646, -0.0593]]],\n",
      "       device='cuda:0'),) and output (tensor([[[ 1.1544,  3.2965,  1.6534,  ..., -2.7009,  2.6553,  2.2700],\n",
      "         [-0.1324,  0.1072, -2.4778,  ..., -0.6485, -0.4276,  1.7071],\n",
      "         [-0.9823, -0.1495,  0.6527,  ...,  1.7561, -0.1239,  0.2410],\n",
      "         ...,\n",
      "         [ 0.4552, -0.7134,  0.4040,  ..., -0.5419, -0.1519,  0.7839],\n",
      "         [-0.6074, -1.4691, -0.4762,  ..., -0.4266, -1.2712, -0.0236],\n",
      "         [ 0.4386, -0.9525,  0.5477,  ...,  0.6588, -0.0894,  0.6572]]],\n",
      "       device='cuda:0'),)\n",
      "[Debug] Layer names being passed: ['model.layers.0', 'model.layers.1', 'model.layers.2', 'model.layers.3', 'model.layers.4', 'model.layers.5', 'model.layers.6', 'model.layers.7', 'model.layers.8', 'model.layers.9', 'model.layers.10', 'model.layers.11', 'model.layers.12', 'model.layers.13', 'model.layers.14', 'model.layers.15', 'model.layers.16', 'model.layers.17', 'model.layers.18', 'model.layers.19', 'model.layers.20', 'model.layers.21', 'model.layers.22', 'model.layers.23', 'model.layers.24', 'model.layers.25', 'model.layers.26', 'model.layers.27', 'model.layers.28', 'model.layers.29', 'model.layers.30', 'model.layers.31', 'model.embed_tokens']\n",
      "[Debug] Trying to access layer: model.layers.0\n",
      "[Debug] Successfully found layer: model.layers.0\n",
      "[Debug] Trying to access layer: model.layers.1\n",
      "[Debug] Successfully found layer: model.layers.1\n",
      "[Debug] Trying to access layer: model.layers.2\n",
      "[Debug] Successfully found layer: model.layers.2\n",
      "[Debug] Trying to access layer: model.layers.3\n",
      "[Debug] Successfully found layer: model.layers.3\n",
      "[Debug] Trying to access layer: model.layers.4\n",
      "[Debug] Successfully found layer: model.layers.4\n",
      "[Debug] Trying to access layer: model.layers.5\n",
      "[Debug] Successfully found layer: model.layers.5\n",
      "[Debug] Trying to access layer: model.layers.6\n",
      "[Debug] Successfully found layer: model.layers.6\n",
      "[Debug] Trying to access layer: model.layers.7\n",
      "[Debug] Successfully found layer: model.layers.7\n",
      "[Debug] Trying to access layer: model.layers.8\n",
      "[Debug] Successfully found layer: model.layers.8\n",
      "[Debug] Trying to access layer: model.layers.9\n",
      "[Debug] Successfully found layer: model.layers.9\n",
      "[Debug] Trying to access layer: model.layers.10\n",
      "[Debug] Successfully found layer: model.layers.10\n",
      "[Debug] Trying to access layer: model.layers.11\n",
      "[Debug] Successfully found layer: model.layers.11\n",
      "[Debug] Trying to access layer: model.layers.12\n",
      "[Debug] Successfully found layer: model.layers.12\n",
      "[Debug] Trying to access layer: model.layers.13\n",
      "[Debug] Successfully found layer: model.layers.13\n",
      "[Debug] Trying to access layer: model.layers.14\n",
      "[Debug] Successfully found layer: model.layers.14\n",
      "[Debug] Trying to access layer: model.layers.15\n",
      "[Debug] Successfully found layer: model.layers.15\n",
      "[Debug] Trying to access layer: model.layers.16\n",
      "[Debug] Successfully found layer: model.layers.16\n",
      "[Debug] Trying to access layer: model.layers.17\n",
      "[Debug] Successfully found layer: model.layers.17\n",
      "[Debug] Trying to access layer: model.layers.18\n",
      "[Debug] Successfully found layer: model.layers.18\n",
      "[Debug] Trying to access layer: model.layers.19\n",
      "[Debug] Successfully found layer: model.layers.19\n",
      "[Debug] Trying to access layer: model.layers.20\n",
      "[Debug] Successfully found layer: model.layers.20\n",
      "[Debug] Trying to access layer: model.layers.21\n",
      "[Debug] Successfully found layer: model.layers.21\n",
      "[Debug] Trying to access layer: model.layers.22\n",
      "[Debug] Successfully found layer: model.layers.22\n",
      "[Debug] Trying to access layer: model.layers.23\n",
      "[Debug] Successfully found layer: model.layers.23\n",
      "[Debug] Trying to access layer: model.layers.24\n",
      "[Debug] Successfully found layer: model.layers.24\n",
      "[Debug] Trying to access layer: model.layers.25\n",
      "[Debug] Successfully found layer: model.layers.25\n",
      "[Debug] Trying to access layer: model.layers.26\n",
      "[Debug] Successfully found layer: model.layers.26\n",
      "[Debug] Trying to access layer: model.layers.27\n",
      "[Debug] Successfully found layer: model.layers.27\n",
      "[Debug] Trying to access layer: model.layers.28\n",
      "[Debug] Successfully found layer: model.layers.28\n",
      "[Debug] Trying to access layer: model.layers.29\n",
      "[Debug] Successfully found layer: model.layers.29\n",
      "[Debug] Trying to access layer: model.layers.30\n",
      "[Debug] Successfully found layer: model.layers.30\n",
      "[Debug] Trying to access layer: model.layers.31\n",
      "[Debug] Successfully found layer: model.layers.31\n",
      "[Debug] Trying to access layer: model.embed_tokens\n",
      "[Debug] Successfully found layer: model.embed_tokens\n",
      "[Hook] Layer Embedding(128256, 4096) received input (tensor([[128000,  18590,   4718,    468,   6241,    320,  31255,      8,  17646,\n",
      "           4718,    468,   6241,    574,   6689,   3221,  29492,   4409,     11,\n",
      "          19313,    449,  39970,   7314,    304,   3297,    220,   5162,     23,\n",
      "            323,  13696,    304,   6664,   8032,     17,     60,   7089,  10687,\n",
      "           2997,   6295,  24941,  11940,     11,   7188,    323,   5960,  86857,\n",
      "           5165,  20585,     26,    279,  74564,   1051,  42508,    520,  82910,\n",
      "          31362,    449,  40592,  44146,  46090,     13,    578,   4632,    596,\n",
      "           2926,   8199,    574,    400,    605,   4194,  59413,     11,   1603,\n",
      "            433,   9778,  35717,    311,    400,    508,   4194,  59413,     13,\n",
      "            362,   7446,  20900,    315,    400,   1490,     11,    931,    574,\n",
      "          52872,    311,   7710,   6445,    323,  13941,    311,    279,  39970,\n",
      "           3813,     11,    439,    279,  18585,   9689,    574,   7154,    220,\n",
      "           1399,   8931,   3201,     13,    578,  37067,   3190,   1511,    304,\n",
      "            279,   4632,   2853,    400,     17,     13,     19,   4194,  59413,\n",
      "            311,   1977,   8032,     17,     60]], device='cuda:0'),) and output tensor([[[ 2.6512e-04, -4.9973e-04, -5.8365e-04,  ...,  3.8147e-03,\n",
      "           6.3419e-05,  1.1902e-03],\n",
      "         [-1.5564e-02, -2.2949e-02,  5.1880e-03,  ...,  9.8877e-03,\n",
      "           2.8534e-03,  2.6855e-02],\n",
      "         [-4.3945e-03, -1.1230e-02,  4.3640e-03,  ..., -4.2114e-03,\n",
      "          -2.6123e-02,  6.3477e-03],\n",
      "         ...,\n",
      "         [ 2.1667e-03,  7.7515e-03,  1.2817e-02,  ...,  9.7656e-03,\n",
      "          -8.9111e-03, -1.9836e-03],\n",
      "         [ 4.7913e-03,  2.4567e-03, -3.6812e-04,  ...,  1.3550e-02,\n",
      "           1.9150e-03, -3.0975e-03],\n",
      "         [ 3.2616e-04,  7.5531e-04,  3.9368e-03,  ...,  1.0132e-02,\n",
      "           6.0425e-03, -1.3542e-04]]], device='cuda:0')\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[ 2.6512e-04, -4.9973e-04, -5.8365e-04,  ...,  3.8147e-03,\n",
      "           6.3419e-05,  1.1902e-03],\n",
      "         [-1.5564e-02, -2.2949e-02,  5.1880e-03,  ...,  9.8877e-03,\n",
      "           2.8534e-03,  2.6855e-02],\n",
      "         [-4.3945e-03, -1.1230e-02,  4.3640e-03,  ..., -4.2114e-03,\n",
      "          -2.6123e-02,  6.3477e-03],\n",
      "         ...,\n",
      "         [ 2.1667e-03,  7.7515e-03,  1.2817e-02,  ...,  9.7656e-03,\n",
      "          -8.9111e-03, -1.9836e-03],\n",
      "         [ 4.7913e-03,  2.4567e-03, -3.6812e-04,  ...,  1.3550e-02,\n",
      "           1.9150e-03, -3.0975e-03],\n",
      "         [ 3.2616e-04,  7.5531e-04,  3.9368e-03,  ...,  1.0132e-02,\n",
      "           6.0425e-03, -1.3542e-04]]], device='cuda:0'),) and output (tensor([[[-0.0004,  0.0146, -0.0020,  ...,  0.0066, -0.0194,  0.0020],\n",
      "         [-0.0217,  0.0073,  0.0142,  ...,  0.0157,  0.0020,  0.0101],\n",
      "         [-0.0021, -0.0092, -0.0081,  ..., -0.0396, -0.0162,  0.0054],\n",
      "         ...,\n",
      "         [-0.0147, -0.0031,  0.0166,  ...,  0.0188,  0.0111,  0.0009],\n",
      "         [-0.0112, -0.0187,  0.0013,  ...,  0.0193,  0.0047,  0.0039],\n",
      "         [-0.0133,  0.0100, -0.0102,  ...,  0.0049, -0.0029,  0.0124]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.0004,  0.0146, -0.0020,  ...,  0.0066, -0.0194,  0.0020],\n",
      "         [-0.0217,  0.0073,  0.0142,  ...,  0.0157,  0.0020,  0.0101],\n",
      "         [-0.0021, -0.0092, -0.0081,  ..., -0.0396, -0.0162,  0.0054],\n",
      "         ...,\n",
      "         [-0.0147, -0.0031,  0.0166,  ...,  0.0188,  0.0111,  0.0009],\n",
      "         [-0.0112, -0.0187,  0.0013,  ...,  0.0193,  0.0047,  0.0039],\n",
      "         [-0.0133,  0.0100, -0.0102,  ...,  0.0049, -0.0029,  0.0124]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.0972,  0.0787, -0.0419,  ...,  0.0071,  0.0869,  0.0218],\n",
      "         [ 0.0057,  0.0148,  0.0073,  ...,  0.0067, -0.0155,  0.0097],\n",
      "         [ 0.0296, -0.0315, -0.0189,  ..., -0.0478, -0.0485,  0.0209],\n",
      "         ...,\n",
      "         [-0.0140,  0.0094, -0.0026,  ...,  0.0047, -0.0023,  0.0124],\n",
      "         [-0.0152, -0.0110, -0.0174,  ...,  0.0010, -0.0194,  0.0105],\n",
      "         [-0.0168, -0.0135, -0.0172,  ..., -0.0101, -0.0138,  0.0180]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.0972,  0.0787, -0.0419,  ...,  0.0071,  0.0869,  0.0218],\n",
      "         [ 0.0057,  0.0148,  0.0073,  ...,  0.0067, -0.0155,  0.0097],\n",
      "         [ 0.0296, -0.0315, -0.0189,  ..., -0.0478, -0.0485,  0.0209],\n",
      "         ...,\n",
      "         [-0.0140,  0.0094, -0.0026,  ...,  0.0047, -0.0023,  0.0124],\n",
      "         [-0.0152, -0.0110, -0.0174,  ...,  0.0010, -0.0194,  0.0105],\n",
      "         [-0.0168, -0.0135, -0.0172,  ..., -0.0101, -0.0138,  0.0180]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.1040,  0.0875, -0.0360,  ...,  0.0205,  0.0997,  0.0184],\n",
      "         [ 0.0342,  0.0390, -0.0141,  ...,  0.0166, -0.0484, -0.0078],\n",
      "         [ 0.0254,  0.0113, -0.0151,  ..., -0.0293, -0.0495,  0.0478],\n",
      "         ...,\n",
      "         [-0.0415,  0.0057,  0.0074,  ...,  0.0150,  0.0076,  0.0477],\n",
      "         [-0.0287, -0.0235, -0.0250,  ...,  0.0070, -0.0172,  0.0332],\n",
      "         [-0.0583, -0.0223,  0.0030,  ..., -0.0214, -0.0344,  0.0300]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.1040,  0.0875, -0.0360,  ...,  0.0205,  0.0997,  0.0184],\n",
      "         [ 0.0342,  0.0390, -0.0141,  ...,  0.0166, -0.0484, -0.0078],\n",
      "         [ 0.0254,  0.0113, -0.0151,  ..., -0.0293, -0.0495,  0.0478],\n",
      "         ...,\n",
      "         [-0.0415,  0.0057,  0.0074,  ...,  0.0150,  0.0076,  0.0477],\n",
      "         [-0.0287, -0.0235, -0.0250,  ...,  0.0070, -0.0172,  0.0332],\n",
      "         [-0.0583, -0.0223,  0.0030,  ..., -0.0214, -0.0344,  0.0300]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.0870,  0.1049, -0.0173,  ...,  0.0700,  0.0989,  0.0181],\n",
      "         [ 0.0537,  0.0650, -0.0183,  ..., -0.0009, -0.0640, -0.0429],\n",
      "         [ 0.0749, -0.0064, -0.0059,  ..., -0.0093, -0.0588,  0.0076],\n",
      "         ...,\n",
      "         [-0.1005, -0.0139, -0.0135,  ...,  0.0064, -0.0390,  0.0484],\n",
      "         [-0.0384, -0.0418,  0.0177,  ..., -0.0136, -0.0355,  0.0625],\n",
      "         [-0.0746, -0.0068, -0.0164,  ..., -0.0223, -0.0936,  0.0302]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.0870,  0.1049, -0.0173,  ...,  0.0700,  0.0989,  0.0181],\n",
      "         [ 0.0537,  0.0650, -0.0183,  ..., -0.0009, -0.0640, -0.0429],\n",
      "         [ 0.0749, -0.0064, -0.0059,  ..., -0.0093, -0.0588,  0.0076],\n",
      "         ...,\n",
      "         [-0.1005, -0.0139, -0.0135,  ...,  0.0064, -0.0390,  0.0484],\n",
      "         [-0.0384, -0.0418,  0.0177,  ..., -0.0136, -0.0355,  0.0625],\n",
      "         [-0.0746, -0.0068, -0.0164,  ..., -0.0223, -0.0936,  0.0302]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.1113,  0.1022, -0.0102,  ...,  0.0436,  0.1283,  0.0321],\n",
      "         [ 0.0787,  0.1383,  0.0379,  ...,  0.0433, -0.1095, -0.0489],\n",
      "         [ 0.0532,  0.0170,  0.0429,  ..., -0.0544, -0.0772,  0.0524],\n",
      "         ...,\n",
      "         [-0.0327, -0.0068, -0.0207,  ..., -0.0100, -0.0320,  0.0965],\n",
      "         [-0.0578,  0.0325,  0.0043,  ..., -0.0294, -0.0008,  0.1039],\n",
      "         [-0.0843, -0.0443, -0.0199,  ...,  0.0033, -0.0821,  0.0427]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.1113,  0.1022, -0.0102,  ...,  0.0436,  0.1283,  0.0321],\n",
      "         [ 0.0787,  0.1383,  0.0379,  ...,  0.0433, -0.1095, -0.0489],\n",
      "         [ 0.0532,  0.0170,  0.0429,  ..., -0.0544, -0.0772,  0.0524],\n",
      "         ...,\n",
      "         [-0.0327, -0.0068, -0.0207,  ..., -0.0100, -0.0320,  0.0965],\n",
      "         [-0.0578,  0.0325,  0.0043,  ..., -0.0294, -0.0008,  0.1039],\n",
      "         [-0.0843, -0.0443, -0.0199,  ...,  0.0033, -0.0821,  0.0427]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.1034,  0.1132,  0.0096,  ...,  0.0453,  0.1477,  0.0301],\n",
      "         [ 0.1811,  0.1685,  0.0172,  ...,  0.0336, -0.0941, -0.0261],\n",
      "         [ 0.1037,  0.0104, -0.0040,  ..., -0.1203, -0.0542,  0.0695],\n",
      "         ...,\n",
      "         [-0.0327,  0.0188, -0.0209,  ...,  0.0168, -0.0446,  0.0733],\n",
      "         [-0.0663, -0.0253,  0.0131,  ..., -0.0328,  0.0543,  0.1176],\n",
      "         [-0.0765, -0.0123, -0.0141,  ...,  0.0315, -0.0488,  0.0305]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.1034,  0.1132,  0.0096,  ...,  0.0453,  0.1477,  0.0301],\n",
      "         [ 0.1811,  0.1685,  0.0172,  ...,  0.0336, -0.0941, -0.0261],\n",
      "         [ 0.1037,  0.0104, -0.0040,  ..., -0.1203, -0.0542,  0.0695],\n",
      "         ...,\n",
      "         [-0.0327,  0.0188, -0.0209,  ...,  0.0168, -0.0446,  0.0733],\n",
      "         [-0.0663, -0.0253,  0.0131,  ..., -0.0328,  0.0543,  0.1176],\n",
      "         [-0.0765, -0.0123, -0.0141,  ...,  0.0315, -0.0488,  0.0305]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.1071,  0.1253,  0.0211,  ...,  0.0126,  0.1586,  0.0278],\n",
      "         [ 0.0695,  0.1431, -0.0123,  ...,  0.0075, -0.0650,  0.0385],\n",
      "         [ 0.1057, -0.0255,  0.0277,  ..., -0.1432, -0.0302,  0.0875],\n",
      "         ...,\n",
      "         [ 0.0283,  0.0598,  0.0576,  ...,  0.1238, -0.1340,  0.1120],\n",
      "         [-0.0382,  0.0594,  0.0430,  ..., -0.0396,  0.0174,  0.0888],\n",
      "         [-0.0526, -0.0022,  0.0167,  ...,  0.0625, -0.0696,  0.0573]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.1071,  0.1253,  0.0211,  ...,  0.0126,  0.1586,  0.0278],\n",
      "         [ 0.0695,  0.1431, -0.0123,  ...,  0.0075, -0.0650,  0.0385],\n",
      "         [ 0.1057, -0.0255,  0.0277,  ..., -0.1432, -0.0302,  0.0875],\n",
      "         ...,\n",
      "         [ 0.0283,  0.0598,  0.0576,  ...,  0.1238, -0.1340,  0.1120],\n",
      "         [-0.0382,  0.0594,  0.0430,  ..., -0.0396,  0.0174,  0.0888],\n",
      "         [-0.0526, -0.0022,  0.0167,  ...,  0.0625, -0.0696,  0.0573]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.0945,  0.1848,  0.0036,  ..., -0.0473,  0.1489,  0.0536],\n",
      "         [ 0.0993,  0.1181, -0.0258,  ..., -0.0340, -0.1944,  0.0402],\n",
      "         [ 0.1518,  0.0268, -0.0084,  ..., -0.2067, -0.0995,  0.0875],\n",
      "         ...,\n",
      "         [ 0.0721,  0.0054, -0.0676,  ...,  0.1441, -0.1090,  0.0284],\n",
      "         [-0.0676,  0.0050, -0.0243,  ...,  0.0639,  0.0647,  0.1261],\n",
      "         [-0.0337, -0.0045, -0.1017,  ...,  0.0564, -0.1168,  0.0511]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.0945,  0.1848,  0.0036,  ..., -0.0473,  0.1489,  0.0536],\n",
      "         [ 0.0993,  0.1181, -0.0258,  ..., -0.0340, -0.1944,  0.0402],\n",
      "         [ 0.1518,  0.0268, -0.0084,  ..., -0.2067, -0.0995,  0.0875],\n",
      "         ...,\n",
      "         [ 0.0721,  0.0054, -0.0676,  ...,  0.1441, -0.1090,  0.0284],\n",
      "         [-0.0676,  0.0050, -0.0243,  ...,  0.0639,  0.0647,  0.1261],\n",
      "         [-0.0337, -0.0045, -0.1017,  ...,  0.0564, -0.1168,  0.0511]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.1060,  0.2113,  0.0270,  ..., -0.1141,  0.1610,  0.0528],\n",
      "         [ 0.1420,  0.1763, -0.0384,  ..., -0.0638, -0.2357,  0.0090],\n",
      "         [ 0.1792,  0.1388,  0.0064,  ..., -0.2364, -0.1258,  0.0734],\n",
      "         ...,\n",
      "         [ 0.0491,  0.0188, -0.0456,  ...,  0.1950, -0.0540, -0.0360],\n",
      "         [-0.1439, -0.0785,  0.0176,  ...,  0.0137,  0.0630,  0.0609],\n",
      "         [-0.0655,  0.0274, -0.0490,  ...,  0.0458, -0.0869,  0.0231]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.1060,  0.2113,  0.0270,  ..., -0.1141,  0.1610,  0.0528],\n",
      "         [ 0.1420,  0.1763, -0.0384,  ..., -0.0638, -0.2357,  0.0090],\n",
      "         [ 0.1792,  0.1388,  0.0064,  ..., -0.2364, -0.1258,  0.0734],\n",
      "         ...,\n",
      "         [ 0.0491,  0.0188, -0.0456,  ...,  0.1950, -0.0540, -0.0360],\n",
      "         [-0.1439, -0.0785,  0.0176,  ...,  0.0137,  0.0630,  0.0609],\n",
      "         [-0.0655,  0.0274, -0.0490,  ...,  0.0458, -0.0869,  0.0231]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.0954,  0.2493,  0.0073,  ..., -0.1568,  0.1818,  0.0855],\n",
      "         [ 0.0870,  0.1475, -0.0070,  ..., -0.0514, -0.2036, -0.0373],\n",
      "         [ 0.1638,  0.1142,  0.0155,  ..., -0.2482, -0.1015, -0.0059],\n",
      "         ...,\n",
      "         [ 0.0206,  0.0297, -0.0298,  ...,  0.1222, -0.0399, -0.0115],\n",
      "         [-0.1602, -0.0591,  0.0269,  ...,  0.0304,  0.1377,  0.1059],\n",
      "         [ 0.0571,  0.0221, -0.0337,  ..., -0.0082,  0.0462,  0.0415]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.0954,  0.2493,  0.0073,  ..., -0.1568,  0.1818,  0.0855],\n",
      "         [ 0.0870,  0.1475, -0.0070,  ..., -0.0514, -0.2036, -0.0373],\n",
      "         [ 0.1638,  0.1142,  0.0155,  ..., -0.2482, -0.1015, -0.0059],\n",
      "         ...,\n",
      "         [ 0.0206,  0.0297, -0.0298,  ...,  0.1222, -0.0399, -0.0115],\n",
      "         [-0.1602, -0.0591,  0.0269,  ...,  0.0304,  0.1377,  0.1059],\n",
      "         [ 0.0571,  0.0221, -0.0337,  ..., -0.0082,  0.0462,  0.0415]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.0152,  0.3340,  0.0011,  ..., -0.2750,  0.1933,  0.1034],\n",
      "         [ 0.1284,  0.0833,  0.0197,  ..., -0.1237, -0.1637, -0.0013],\n",
      "         [ 0.1725, -0.0021, -0.0360,  ..., -0.2570, -0.1917,  0.0308],\n",
      "         ...,\n",
      "         [-0.0448,  0.0113, -0.0802,  ...,  0.1203, -0.0731,  0.0545],\n",
      "         [-0.0950, -0.0180, -0.0021,  ...,  0.0215,  0.1829,  0.1170],\n",
      "         [ 0.0463,  0.0329, -0.0718,  ...,  0.0408, -0.0070,  0.1095]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.0152,  0.3340,  0.0011,  ..., -0.2750,  0.1933,  0.1034],\n",
      "         [ 0.1284,  0.0833,  0.0197,  ..., -0.1237, -0.1637, -0.0013],\n",
      "         [ 0.1725, -0.0021, -0.0360,  ..., -0.2570, -0.1917,  0.0308],\n",
      "         ...,\n",
      "         [-0.0448,  0.0113, -0.0802,  ...,  0.1203, -0.0731,  0.0545],\n",
      "         [-0.0950, -0.0180, -0.0021,  ...,  0.0215,  0.1829,  0.1170],\n",
      "         [ 0.0463,  0.0329, -0.0718,  ...,  0.0408, -0.0070,  0.1095]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.0356,  0.3483, -0.0366,  ..., -0.2962,  0.2088,  0.1065],\n",
      "         [ 0.1389,  0.0583,  0.0102,  ..., -0.1887, -0.1201,  0.0024],\n",
      "         [ 0.1203,  0.0056, -0.0512,  ..., -0.3135, -0.1035, -0.0049],\n",
      "         ...,\n",
      "         [ 0.0746, -0.0493, -0.0324,  ...,  0.0494,  0.0228,  0.0658],\n",
      "         [-0.1031, -0.0551, -0.0164,  ..., -0.0881,  0.1943,  0.0085],\n",
      "         [ 0.0275, -0.0107, -0.0466,  ..., -0.0241,  0.0736, -0.0046]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.0356,  0.3483, -0.0366,  ..., -0.2962,  0.2088,  0.1065],\n",
      "         [ 0.1389,  0.0583,  0.0102,  ..., -0.1887, -0.1201,  0.0024],\n",
      "         [ 0.1203,  0.0056, -0.0512,  ..., -0.3135, -0.1035, -0.0049],\n",
      "         ...,\n",
      "         [ 0.0746, -0.0493, -0.0324,  ...,  0.0494,  0.0228,  0.0658],\n",
      "         [-0.1031, -0.0551, -0.0164,  ..., -0.0881,  0.1943,  0.0085],\n",
      "         [ 0.0275, -0.0107, -0.0466,  ..., -0.0241,  0.0736, -0.0046]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.0127,  0.3661, -0.0316,  ..., -0.3678,  0.1973,  0.1019],\n",
      "         [ 0.1783,  0.0402,  0.1093,  ..., -0.1566, -0.0691,  0.0453],\n",
      "         [ 0.1420,  0.0054, -0.0236,  ..., -0.2937, -0.1245, -0.0133],\n",
      "         ...,\n",
      "         [-0.0462, -0.0510,  0.0226,  ...,  0.0233, -0.0291,  0.0420],\n",
      "         [-0.1479, -0.0358,  0.0234,  ..., -0.0668,  0.1851,  0.0503],\n",
      "         [-0.0636, -0.0228, -0.0609,  ...,  0.0973, -0.0053, -0.0089]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.0127,  0.3661, -0.0316,  ..., -0.3678,  0.1973,  0.1019],\n",
      "         [ 0.1783,  0.0402,  0.1093,  ..., -0.1566, -0.0691,  0.0453],\n",
      "         [ 0.1420,  0.0054, -0.0236,  ..., -0.2937, -0.1245, -0.0133],\n",
      "         ...,\n",
      "         [-0.0462, -0.0510,  0.0226,  ...,  0.0233, -0.0291,  0.0420],\n",
      "         [-0.1479, -0.0358,  0.0234,  ..., -0.0668,  0.1851,  0.0503],\n",
      "         [-0.0636, -0.0228, -0.0609,  ...,  0.0973, -0.0053, -0.0089]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.0089,  0.3233, -0.0779,  ..., -0.4491,  0.1845,  0.0951],\n",
      "         [ 0.0876, -0.0378,  0.0650,  ..., -0.1609, -0.1307,  0.0634],\n",
      "         [ 0.1126,  0.0375, -0.0338,  ..., -0.2273, -0.1683,  0.0426],\n",
      "         ...,\n",
      "         [-0.0613, -0.0307,  0.1063,  ..., -0.0620, -0.0811,  0.1334],\n",
      "         [-0.2122,  0.0331,  0.1286,  ..., -0.1015,  0.1138,  0.0687],\n",
      "         [-0.0291,  0.1095,  0.0107,  ...,  0.0314, -0.0285,  0.0861]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.0089,  0.3233, -0.0779,  ..., -0.4491,  0.1845,  0.0951],\n",
      "         [ 0.0876, -0.0378,  0.0650,  ..., -0.1609, -0.1307,  0.0634],\n",
      "         [ 0.1126,  0.0375, -0.0338,  ..., -0.2273, -0.1683,  0.0426],\n",
      "         ...,\n",
      "         [-0.0613, -0.0307,  0.1063,  ..., -0.0620, -0.0811,  0.1334],\n",
      "         [-0.2122,  0.0331,  0.1286,  ..., -0.1015,  0.1138,  0.0687],\n",
      "         [-0.0291,  0.1095,  0.0107,  ...,  0.0314, -0.0285,  0.0861]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.0244,  0.2828, -0.0532,  ..., -0.4592,  0.2300,  0.0142],\n",
      "         [ 0.0944, -0.0936, -0.0147,  ..., -0.1366, -0.1534, -0.0272],\n",
      "         [ 0.0993, -0.0461, -0.0860,  ..., -0.1622, -0.1612, -0.1024],\n",
      "         ...,\n",
      "         [ 0.0066, -0.1566,  0.1033,  ..., -0.0375, -0.0527,  0.0900],\n",
      "         [-0.2458,  0.0250,  0.0975,  ..., -0.0487,  0.1037, -0.0203],\n",
      "         [-0.0246,  0.1068,  0.0149,  ...,  0.0276, -0.1406,  0.1656]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.0244,  0.2828, -0.0532,  ..., -0.4592,  0.2300,  0.0142],\n",
      "         [ 0.0944, -0.0936, -0.0147,  ..., -0.1366, -0.1534, -0.0272],\n",
      "         [ 0.0993, -0.0461, -0.0860,  ..., -0.1622, -0.1612, -0.1024],\n",
      "         ...,\n",
      "         [ 0.0066, -0.1566,  0.1033,  ..., -0.0375, -0.0527,  0.0900],\n",
      "         [-0.2458,  0.0250,  0.0975,  ..., -0.0487,  0.1037, -0.0203],\n",
      "         [-0.0246,  0.1068,  0.0149,  ...,  0.0276, -0.1406,  0.1656]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.0928,  0.1485,  0.0158,  ..., -0.5380,  0.2664, -0.0333],\n",
      "         [ 0.0014, -0.0886,  0.0081,  ..., -0.1876, -0.0614, -0.0293],\n",
      "         [ 0.0892,  0.0302,  0.0543,  ..., -0.1160, -0.2124,  0.0263],\n",
      "         ...,\n",
      "         [ 0.1213, -0.1498,  0.0694,  ..., -0.0363, -0.1020,  0.1136],\n",
      "         [-0.1488,  0.0236,  0.0063,  ..., -0.0687,  0.0389, -0.0799],\n",
      "         [ 0.0577,  0.0606, -0.0756,  ...,  0.0044, -0.3648,  0.1194]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.0928,  0.1485,  0.0158,  ..., -0.5380,  0.2664, -0.0333],\n",
      "         [ 0.0014, -0.0886,  0.0081,  ..., -0.1876, -0.0614, -0.0293],\n",
      "         [ 0.0892,  0.0302,  0.0543,  ..., -0.1160, -0.2124,  0.0263],\n",
      "         ...,\n",
      "         [ 0.1213, -0.1498,  0.0694,  ..., -0.0363, -0.1020,  0.1136],\n",
      "         [-0.1488,  0.0236,  0.0063,  ..., -0.0687,  0.0389, -0.0799],\n",
      "         [ 0.0577,  0.0606, -0.0756,  ...,  0.0044, -0.3648,  0.1194]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.1000,  0.1323,  0.0241,  ..., -0.5470,  0.2844, -0.0457],\n",
      "         [-0.0081, -0.0243, -0.0393,  ..., -0.2120, -0.1843, -0.0956],\n",
      "         [ 0.0679,  0.0335, -0.0007,  ..., -0.0537, -0.3334,  0.0226],\n",
      "         ...,\n",
      "         [ 0.2101, -0.1057,  0.0595,  ..., -0.0153, -0.1451,  0.2348],\n",
      "         [-0.0948,  0.0292, -0.0026,  ..., -0.0319,  0.0056,  0.0548],\n",
      "         [ 0.1353,  0.2383,  0.0332,  ...,  0.1398, -0.4129,  0.1071]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.1000,  0.1323,  0.0241,  ..., -0.5470,  0.2844, -0.0457],\n",
      "         [-0.0081, -0.0243, -0.0393,  ..., -0.2120, -0.1843, -0.0956],\n",
      "         [ 0.0679,  0.0335, -0.0007,  ..., -0.0537, -0.3334,  0.0226],\n",
      "         ...,\n",
      "         [ 0.2101, -0.1057,  0.0595,  ..., -0.0153, -0.1451,  0.2348],\n",
      "         [-0.0948,  0.0292, -0.0026,  ..., -0.0319,  0.0056,  0.0548],\n",
      "         [ 0.1353,  0.2383,  0.0332,  ...,  0.1398, -0.4129,  0.1071]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.1286,  0.0833,  0.0239,  ..., -0.5657,  0.3694, -0.0725],\n",
      "         [-0.1141,  0.1939,  0.0788,  ..., -0.2522, -0.2947, -0.2268],\n",
      "         [-0.0443,  0.0541,  0.1138,  ...,  0.0025, -0.4310, -0.0284],\n",
      "         ...,\n",
      "         [ 0.4202, -0.0748,  0.0699,  ..., -0.0766, -0.1873,  0.1547],\n",
      "         [ 0.0467, -0.0011, -0.0054,  ..., -0.0282, -0.1311, -0.0682],\n",
      "         [ 0.1974,  0.2374,  0.0920,  ...,  0.1569, -0.4887, -0.0304]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.1286,  0.0833,  0.0239,  ..., -0.5657,  0.3694, -0.0725],\n",
      "         [-0.1141,  0.1939,  0.0788,  ..., -0.2522, -0.2947, -0.2268],\n",
      "         [-0.0443,  0.0541,  0.1138,  ...,  0.0025, -0.4310, -0.0284],\n",
      "         ...,\n",
      "         [ 0.4202, -0.0748,  0.0699,  ..., -0.0766, -0.1873,  0.1547],\n",
      "         [ 0.0467, -0.0011, -0.0054,  ..., -0.0282, -0.1311, -0.0682],\n",
      "         [ 0.1974,  0.2374,  0.0920,  ...,  0.1569, -0.4887, -0.0304]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.1417,  0.0703,  0.0343,  ..., -0.5549,  0.3175, -0.0847],\n",
      "         [-0.2019,  0.1484,  0.0513,  ..., -0.2560, -0.2434, -0.2805],\n",
      "         [ 0.0846, -0.1162,  0.1246,  ...,  0.0102, -0.3441, -0.1258],\n",
      "         ...,\n",
      "         [ 0.3804, -0.0783,  0.0107,  ..., -0.1919, -0.1366,  0.1360],\n",
      "         [-0.0358,  0.0007, -0.1027,  ..., -0.2803, -0.1219, -0.1059],\n",
      "         [ 0.1439,  0.1933,  0.1384,  ...,  0.1300, -0.4161, -0.0436]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.1417,  0.0703,  0.0343,  ..., -0.5549,  0.3175, -0.0847],\n",
      "         [-0.2019,  0.1484,  0.0513,  ..., -0.2560, -0.2434, -0.2805],\n",
      "         [ 0.0846, -0.1162,  0.1246,  ...,  0.0102, -0.3441, -0.1258],\n",
      "         ...,\n",
      "         [ 0.3804, -0.0783,  0.0107,  ..., -0.1919, -0.1366,  0.1360],\n",
      "         [-0.0358,  0.0007, -0.1027,  ..., -0.2803, -0.1219, -0.1059],\n",
      "         [ 0.1439,  0.1933,  0.1384,  ...,  0.1300, -0.4161, -0.0436]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.1528,  0.0725,  0.0609,  ..., -0.5624,  0.3103, -0.0293],\n",
      "         [-0.1874,  0.0715,  0.0308,  ..., -0.2624, -0.3543, -0.2802],\n",
      "         [ 0.0907, -0.1459,  0.1963,  ...,  0.0025, -0.5116, -0.1735],\n",
      "         ...,\n",
      "         [ 0.4291, -0.0997, -0.0389,  ..., -0.1898, -0.0345,  0.0469],\n",
      "         [-0.0505, -0.0739, -0.0550,  ..., -0.3591, -0.0345, -0.0920],\n",
      "         [ 0.1452,  0.0677,  0.1760,  ...,  0.1193, -0.3179, -0.0347]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.1528,  0.0725,  0.0609,  ..., -0.5624,  0.3103, -0.0293],\n",
      "         [-0.1874,  0.0715,  0.0308,  ..., -0.2624, -0.3543, -0.2802],\n",
      "         [ 0.0907, -0.1459,  0.1963,  ...,  0.0025, -0.5116, -0.1735],\n",
      "         ...,\n",
      "         [ 0.4291, -0.0997, -0.0389,  ..., -0.1898, -0.0345,  0.0469],\n",
      "         [-0.0505, -0.0739, -0.0550,  ..., -0.3591, -0.0345, -0.0920],\n",
      "         [ 0.1452,  0.0677,  0.1760,  ...,  0.1193, -0.3179, -0.0347]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.1559,  0.0520,  0.0566,  ..., -0.5805,  0.2614, -0.0032],\n",
      "         [-0.1419,  0.0930,  0.1430,  ..., -0.3933, -0.3058, -0.2410],\n",
      "         [ 0.0152, -0.1655,  0.2442,  ..., -0.2168, -0.5410, -0.0948],\n",
      "         ...,\n",
      "         [ 0.4665, -0.0750, -0.0067,  ...,  0.0569,  0.0093,  0.0705],\n",
      "         [ 0.0066, -0.0256, -0.1850,  ..., -0.3162, -0.1078, -0.0852],\n",
      "         [ 0.2017, -0.0495,  0.1331,  ...,  0.1195, -0.3043,  0.0183]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.1559,  0.0520,  0.0566,  ..., -0.5805,  0.2614, -0.0032],\n",
      "         [-0.1419,  0.0930,  0.1430,  ..., -0.3933, -0.3058, -0.2410],\n",
      "         [ 0.0152, -0.1655,  0.2442,  ..., -0.2168, -0.5410, -0.0948],\n",
      "         ...,\n",
      "         [ 0.4665, -0.0750, -0.0067,  ...,  0.0569,  0.0093,  0.0705],\n",
      "         [ 0.0066, -0.0256, -0.1850,  ..., -0.3162, -0.1078, -0.0852],\n",
      "         [ 0.2017, -0.0495,  0.1331,  ...,  0.1195, -0.3043,  0.0183]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.1488,  0.0445,  0.0765,  ..., -0.5456,  0.2702,  0.0098],\n",
      "         [-0.2823,  0.1836,  0.0732,  ..., -0.4145, -0.3683, -0.2513],\n",
      "         [ 0.0481, -0.0739,  0.1309,  ..., -0.3878, -0.7262, -0.0505],\n",
      "         ...,\n",
      "         [ 0.4105, -0.1388, -0.0828,  ...,  0.0895, -0.0810,  0.1088],\n",
      "         [-0.0040,  0.0896, -0.1337,  ..., -0.3631, -0.2115,  0.1037],\n",
      "         [ 0.2480, -0.0936,  0.0348,  ...,  0.1504, -0.4193, -0.0331]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.1488,  0.0445,  0.0765,  ..., -0.5456,  0.2702,  0.0098],\n",
      "         [-0.2823,  0.1836,  0.0732,  ..., -0.4145, -0.3683, -0.2513],\n",
      "         [ 0.0481, -0.0739,  0.1309,  ..., -0.3878, -0.7262, -0.0505],\n",
      "         ...,\n",
      "         [ 0.4105, -0.1388, -0.0828,  ...,  0.0895, -0.0810,  0.1088],\n",
      "         [-0.0040,  0.0896, -0.1337,  ..., -0.3631, -0.2115,  0.1037],\n",
      "         [ 0.2480, -0.0936,  0.0348,  ...,  0.1504, -0.4193, -0.0331]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-1.2677e-01,  3.3335e-02,  9.3478e-02,  ..., -5.3204e-01,\n",
      "           2.7953e-01,  5.9812e-04],\n",
      "         [-1.6815e-01,  3.6147e-01,  5.7386e-02,  ..., -4.7255e-01,\n",
      "          -3.6264e-01, -2.6088e-01],\n",
      "         [ 1.9265e-01,  9.3307e-02,  5.8776e-02,  ..., -4.4134e-01,\n",
      "          -6.4749e-01, -7.4449e-02],\n",
      "         ...,\n",
      "         [ 5.6216e-01, -1.2897e-01, -6.1349e-02,  ...,  1.7556e-01,\n",
      "          -5.2609e-02,  8.9631e-02],\n",
      "         [ 3.1276e-03,  1.5401e-01,  1.3983e-03,  ..., -3.8699e-01,\n",
      "          -1.7932e-01,  3.8258e-02],\n",
      "         [ 3.3160e-01, -7.0471e-02,  7.6541e-02,  ...,  2.5558e-01,\n",
      "          -4.4961e-01, -1.6256e-02]]], device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-1.2677e-01,  3.3335e-02,  9.3478e-02,  ..., -5.3204e-01,\n",
      "           2.7953e-01,  5.9812e-04],\n",
      "         [-1.6815e-01,  3.6147e-01,  5.7386e-02,  ..., -4.7255e-01,\n",
      "          -3.6264e-01, -2.6088e-01],\n",
      "         [ 1.9265e-01,  9.3307e-02,  5.8776e-02,  ..., -4.4134e-01,\n",
      "          -6.4749e-01, -7.4449e-02],\n",
      "         ...,\n",
      "         [ 5.6216e-01, -1.2897e-01, -6.1349e-02,  ...,  1.7556e-01,\n",
      "          -5.2609e-02,  8.9631e-02],\n",
      "         [ 3.1276e-03,  1.5401e-01,  1.3983e-03,  ..., -3.8699e-01,\n",
      "          -1.7932e-01,  3.8258e-02],\n",
      "         [ 3.3160e-01, -7.0471e-02,  7.6541e-02,  ...,  2.5558e-01,\n",
      "          -4.4961e-01, -1.6256e-02]]], device='cuda:0'),) and output (tensor([[[-1.3547e-01,  2.2723e-02,  5.5414e-02,  ..., -5.4726e-01,\n",
      "           2.4703e-01, -2.2850e-02],\n",
      "         [-1.9992e-01,  2.3515e-01,  6.9203e-02,  ..., -4.9433e-01,\n",
      "          -3.8908e-01, -2.7753e-01],\n",
      "         [ 2.3866e-01, -1.6777e-02, -6.7893e-02,  ..., -4.4608e-01,\n",
      "          -4.7904e-01, -1.9001e-01],\n",
      "         ...,\n",
      "         [ 5.4300e-01, -1.0081e-01, -1.5373e-01,  ...,  6.2438e-02,\n",
      "          -1.6229e-04,  5.0338e-02],\n",
      "         [-1.5324e-01,  1.5757e-01, -2.1669e-02,  ..., -3.9214e-01,\n",
      "          -1.6119e-01,  1.3054e-01],\n",
      "         [ 5.2795e-01, -1.4664e-01,  7.9825e-02,  ...,  3.6108e-01,\n",
      "          -4.3662e-01, -6.5819e-02]]], device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-1.3547e-01,  2.2723e-02,  5.5414e-02,  ..., -5.4726e-01,\n",
      "           2.4703e-01, -2.2850e-02],\n",
      "         [-1.9992e-01,  2.3515e-01,  6.9203e-02,  ..., -4.9433e-01,\n",
      "          -3.8908e-01, -2.7753e-01],\n",
      "         [ 2.3866e-01, -1.6777e-02, -6.7893e-02,  ..., -4.4608e-01,\n",
      "          -4.7904e-01, -1.9001e-01],\n",
      "         ...,\n",
      "         [ 5.4300e-01, -1.0081e-01, -1.5373e-01,  ...,  6.2438e-02,\n",
      "          -1.6229e-04,  5.0338e-02],\n",
      "         [-1.5324e-01,  1.5757e-01, -2.1669e-02,  ..., -3.9214e-01,\n",
      "          -1.6119e-01,  1.3054e-01],\n",
      "         [ 5.2795e-01, -1.4664e-01,  7.9825e-02,  ...,  3.6108e-01,\n",
      "          -4.3662e-01, -6.5819e-02]]], device='cuda:0'),) and output (tensor([[[-0.1350,  0.0163,  0.0576,  ..., -0.5555,  0.2435,  0.0051],\n",
      "         [-0.3317,  0.3646,  0.1343,  ..., -0.4289, -0.4529, -0.1883],\n",
      "         [-0.0606, -0.0314, -0.0014,  ..., -0.4678, -0.4770, -0.2079],\n",
      "         ...,\n",
      "         [ 0.4566, -0.2601, -0.1766,  ...,  0.2064,  0.2227,  0.1522],\n",
      "         [-0.0201,  0.3578, -0.1756,  ..., -0.3040, -0.1808,  0.0801],\n",
      "         [ 0.6614, -0.1687,  0.1286,  ...,  0.5154, -0.3944,  0.0537]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.1350,  0.0163,  0.0576,  ..., -0.5555,  0.2435,  0.0051],\n",
      "         [-0.3317,  0.3646,  0.1343,  ..., -0.4289, -0.4529, -0.1883],\n",
      "         [-0.0606, -0.0314, -0.0014,  ..., -0.4678, -0.4770, -0.2079],\n",
      "         ...,\n",
      "         [ 0.4566, -0.2601, -0.1766,  ...,  0.2064,  0.2227,  0.1522],\n",
      "         [-0.0201,  0.3578, -0.1756,  ..., -0.3040, -0.1808,  0.0801],\n",
      "         [ 0.6614, -0.1687,  0.1286,  ...,  0.5154, -0.3944,  0.0537]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.1301, -0.0029,  0.0279,  ..., -0.5537,  0.2277,  0.0164],\n",
      "         [-0.3853,  0.3264,  0.1211,  ..., -0.4373, -0.4477, -0.1545],\n",
      "         [-0.1723, -0.0685,  0.0445,  ..., -0.4755, -0.4485, -0.2909],\n",
      "         ...,\n",
      "         [ 0.4494, -0.1322,  0.0651,  ...,  0.0883,  0.2192,  0.1408],\n",
      "         [-0.1681,  0.3440, -0.2899,  ..., -0.5693, -0.0247, -0.1185],\n",
      "         [ 0.5832, -0.0505,  0.2489,  ...,  0.4105, -0.2386, -0.0319]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.1301, -0.0029,  0.0279,  ..., -0.5537,  0.2277,  0.0164],\n",
      "         [-0.3853,  0.3264,  0.1211,  ..., -0.4373, -0.4477, -0.1545],\n",
      "         [-0.1723, -0.0685,  0.0445,  ..., -0.4755, -0.4485, -0.2909],\n",
      "         ...,\n",
      "         [ 0.4494, -0.1322,  0.0651,  ...,  0.0883,  0.2192,  0.1408],\n",
      "         [-0.1681,  0.3440, -0.2899,  ..., -0.5693, -0.0247, -0.1185],\n",
      "         [ 0.5832, -0.0505,  0.2489,  ...,  0.4105, -0.2386, -0.0319]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.1219,  0.0061, -0.0021,  ..., -0.5922,  0.2788, -0.0020],\n",
      "         [-0.3193,  0.1719,  0.0112,  ..., -0.5574, -0.3999, -0.1725],\n",
      "         [-0.0842,  0.0848,  0.1168,  ..., -0.5796, -0.5976, -0.1784],\n",
      "         ...,\n",
      "         [ 0.6255,  0.0685,  0.1002,  ...,  0.1960,  0.2653,  0.1498],\n",
      "         [-0.1051,  0.3723, -0.2528,  ..., -0.5272,  0.0632, -0.0065],\n",
      "         [ 0.6566,  0.0683,  0.3911,  ...,  0.5796,  0.0202,  0.2135]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.1219,  0.0061, -0.0021,  ..., -0.5922,  0.2788, -0.0020],\n",
      "         [-0.3193,  0.1719,  0.0112,  ..., -0.5574, -0.3999, -0.1725],\n",
      "         [-0.0842,  0.0848,  0.1168,  ..., -0.5796, -0.5976, -0.1784],\n",
      "         ...,\n",
      "         [ 0.6255,  0.0685,  0.1002,  ...,  0.1960,  0.2653,  0.1498],\n",
      "         [-0.1051,  0.3723, -0.2528,  ..., -0.5272,  0.0632, -0.0065],\n",
      "         [ 0.6566,  0.0683,  0.3911,  ...,  0.5796,  0.0202,  0.2135]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.1067,  0.0272,  0.0495,  ..., -0.5550,  0.3449,  0.0266],\n",
      "         [-0.3070,  0.2186,  0.0994,  ..., -0.5842, -0.4816, -0.2161],\n",
      "         [-0.0564, -0.1224,  0.1994,  ..., -0.3929, -0.7367, -0.0585],\n",
      "         ...,\n",
      "         [ 0.6381,  0.2971,  0.1075,  ...,  0.1060,  0.0449,  0.2618],\n",
      "         [-0.1265,  0.5498, -0.1407,  ..., -0.5081, -0.0249,  0.0931],\n",
      "         [ 0.6981,  0.1193,  0.3764,  ...,  0.6669, -0.2706,  0.2724]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.1067,  0.0272,  0.0495,  ..., -0.5550,  0.3449,  0.0266],\n",
      "         [-0.3070,  0.2186,  0.0994,  ..., -0.5842, -0.4816, -0.2161],\n",
      "         [-0.0564, -0.1224,  0.1994,  ..., -0.3929, -0.7367, -0.0585],\n",
      "         ...,\n",
      "         [ 0.6381,  0.2971,  0.1075,  ...,  0.1060,  0.0449,  0.2618],\n",
      "         [-0.1265,  0.5498, -0.1407,  ..., -0.5081, -0.0249,  0.0931],\n",
      "         [ 0.6981,  0.1193,  0.3764,  ...,  0.6669, -0.2706,  0.2724]]],\n",
      "       device='cuda:0'),) and output (tensor([[[-0.0592,  0.0731,  0.0500,  ..., -0.5517,  0.3541,  0.0595],\n",
      "         [-0.2908,  0.1588,  0.1309,  ..., -0.6193, -0.5229, -0.0910],\n",
      "         [-0.1945, -0.1511,  0.3216,  ..., -0.2843, -0.4297, -0.2655],\n",
      "         ...,\n",
      "         [ 0.7137,  0.3080,  0.4051,  ...,  0.0423,  0.0922,  0.1978],\n",
      "         [-0.2856,  0.6070, -0.2139,  ..., -0.7761,  0.1911, -0.3088],\n",
      "         [ 0.5271,  0.0613,  0.5133,  ...,  0.4835, -0.2057, -0.0499]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[-0.0592,  0.0731,  0.0500,  ..., -0.5517,  0.3541,  0.0595],\n",
      "         [-0.2908,  0.1588,  0.1309,  ..., -0.6193, -0.5229, -0.0910],\n",
      "         [-0.1945, -0.1511,  0.3216,  ..., -0.2843, -0.4297, -0.2655],\n",
      "         ...,\n",
      "         [ 0.7137,  0.3080,  0.4051,  ...,  0.0423,  0.0922,  0.1978],\n",
      "         [-0.2856,  0.6070, -0.2139,  ..., -0.7761,  0.1911, -0.3088],\n",
      "         [ 0.5271,  0.0613,  0.5133,  ...,  0.4835, -0.2057, -0.0499]]],\n",
      "       device='cuda:0'),) and output (tensor([[[ 0.1143,  0.2237,  0.3292,  ..., -0.7706,  0.2608,  0.0269],\n",
      "         [-0.2509, -0.0361,  0.3868,  ..., -0.6120, -0.6605, -0.5176],\n",
      "         [-0.3505, -0.2680,  0.3085,  ..., -0.4658, -0.4505, -0.4037],\n",
      "         ...,\n",
      "         [ 0.6894,  0.0695,  0.5243,  ..., -0.0246, -0.0439,  0.7014],\n",
      "         [-0.2191,  0.3786, -0.3911,  ..., -0.7538,  0.0293,  0.0524],\n",
      "         [ 0.6130, -0.1255,  0.6697,  ...,  0.5630, -0.2365,  0.1515]]],\n",
      "       device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[ 0.1143,  0.2237,  0.3292,  ..., -0.7706,  0.2608,  0.0269],\n",
      "         [-0.2509, -0.0361,  0.3868,  ..., -0.6120, -0.6605, -0.5176],\n",
      "         [-0.3505, -0.2680,  0.3085,  ..., -0.4658, -0.4505, -0.4037],\n",
      "         ...,\n",
      "         [ 0.6894,  0.0695,  0.5243,  ..., -0.0246, -0.0439,  0.7014],\n",
      "         [-0.2191,  0.3786, -0.3911,  ..., -0.7538,  0.0293,  0.0524],\n",
      "         [ 0.6130, -0.1255,  0.6697,  ...,  0.5630, -0.2365,  0.1515]]],\n",
      "       device='cuda:0'),) and output (tensor([[[ 3.7106e-01,  5.4886e-01,  6.1002e-01,  ..., -7.4991e-01,\n",
      "           2.0801e-01,  4.1948e-01],\n",
      "         [-2.1303e-01, -5.5765e-04,  2.5329e-01,  ..., -6.4643e-01,\n",
      "          -4.9494e-01, -6.2195e-01],\n",
      "         [ 1.4047e-01, -5.0238e-01, -1.5404e-01,  ..., -4.3533e-01,\n",
      "          -3.9211e-01, -6.9312e-01],\n",
      "         ...,\n",
      "         [ 3.7479e-01, -2.1635e-01,  1.6090e-01,  ..., -1.7682e-01,\n",
      "          -3.3180e-01,  3.1722e-01],\n",
      "         [-5.6511e-01, -1.1186e-01, -4.9845e-01,  ..., -9.3370e-01,\n",
      "          -1.3489e-01, -7.0703e-01],\n",
      "         [ 5.6962e-01, -5.7771e-01,  3.1007e-01,  ...,  5.3580e-01,\n",
      "          -3.4153e-01, -3.7270e-01]]], device='cuda:0'),)\n",
      "[Hook] Layer LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ") received input (tensor([[[ 3.7106e-01,  5.4886e-01,  6.1002e-01,  ..., -7.4991e-01,\n",
      "           2.0801e-01,  4.1948e-01],\n",
      "         [-2.1303e-01, -5.5765e-04,  2.5329e-01,  ..., -6.4643e-01,\n",
      "          -4.9494e-01, -6.2195e-01],\n",
      "         [ 1.4047e-01, -5.0238e-01, -1.5404e-01,  ..., -4.3533e-01,\n",
      "          -3.9211e-01, -6.9312e-01],\n",
      "         ...,\n",
      "         [ 3.7479e-01, -2.1635e-01,  1.6090e-01,  ..., -1.7682e-01,\n",
      "          -3.3180e-01,  3.1722e-01],\n",
      "         [-5.6511e-01, -1.1186e-01, -4.9845e-01,  ..., -9.3370e-01,\n",
      "          -1.3489e-01, -7.0703e-01],\n",
      "         [ 5.6962e-01, -5.7771e-01,  3.1007e-01,  ...,  5.3580e-01,\n",
      "          -3.4153e-01, -3.7270e-01]]], device='cuda:0'),) and output (tensor([[[ 1.1544,  3.2965,  1.6534,  ..., -2.7009,  2.6553,  2.2700],\n",
      "         [-0.3046,  0.2879,  1.0885,  ..., -0.7002, -0.1047,  0.1341],\n",
      "         [ 0.1525, -0.2762,  0.1399,  ..., -1.2273, -0.2308, -0.4191],\n",
      "         ...,\n",
      "         [-0.5149, -1.1639,  0.6283,  ..., -0.0451, -0.4050,  1.1853],\n",
      "         [-0.5201, -0.9664, -0.4907,  ..., -0.0340, -1.0849,  0.2843],\n",
      "         [ 0.3856, -1.4590, -0.3499,  ...,  1.7031,  0.0741,  0.1297]]],\n",
      "       device='cuda:0'),)\n"
     ]
    }
   ],
   "source": [
    "logit_lens.plot_logit_lens_plotly(\n",
    "    model=llama8b_bnb4_float32,\n",
    "    tokenizer=llama8b_tokenizer,\n",
    "    inputs=nq_answers,\n",
    "    start_ix=0, end_ix=15,\n",
    "    topk=5,\n",
    "    plot_topk_lens=False,\n",
    "    #json_log_path=None,\n",
    "    json_log_path='logs/nq_answers/llama.8b-bnb4bit.fp32', # 20 samples\n",
    "    #save_fig_path=None,\n",
    "    #save_fig_path='Outputs/LogitLens/DH3B/logits_3b_fp32_math.jpg',\n",
    "    #entropy=True,\n",
    "    model_precision=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dh8b_bnb8_float32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "json_path = 'logs/gsm8k/llama.8b-1.58.fp32'\n",
    "# Load your JSON file\n",
    "with open(json_path, 'r') as f:\n",
    "    log_data = json.load(f)\n",
    "\n",
    "# Convert the loaded JSON data to a DataFrame\n",
    "df = pd.json_normalize(log_data)\n",
    "\n",
    "# If you want to see the dataframe\n",
    "print(df)\n",
    "\n",
    "# Optionally, display it in a Jupyter notebook in a more readable format\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "# Load the JSON file and normalize into DataFrame\n",
    "with open('logs/gsm8k/dh.3b-ptsq.fp32', 'r') as f:\n",
    "    log_data = json.load(f)\n",
    "df = pd.json_normalize(log_data)\n",
    "\n",
    "# Ensure each row's layer_names and entropy have matching length\n",
    "num_layers = len(df.loc[0, 'layer_names'])\n",
    "sum_entropy = [0.0] * num_layers\n",
    "valid_rows = 0\n",
    "\n",
    "for _, row in df.iterrows():\n",
    "    layer_names = row['layer_names']\n",
    "    entropy = row['entropy']\n",
    "    \n",
    "    # Validate that both lists are the expected length\n",
    "    if isinstance(entropy, list) and len(entropy) == num_layers:\n",
    "        sum_entropy = [s + e for s, e in zip(sum_entropy, entropy)]\n",
    "        valid_rows += 1\n",
    "\n",
    "# Compute average\n",
    "if valid_rows > 0:\n",
    "    avg_entropy = [e / valid_rows for e in sum_entropy]\n",
    "    layer_labels = df.loc[0, 'layer_names']\n",
    "\n",
    "    # Plotting\n",
    "    plt.figure(figsize=(14, 6))\n",
    "    plt.plot(layer_labels, avg_entropy, marker='o', linestyle='-', color='b')\n",
    "    plt.xlabel('Layer Name')\n",
    "    plt.ylabel('Average Entropy')\n",
    "    plt.title(f'Average Entropy Across Layers (n = {valid_rows} samples)')\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No valid rows matched expected layer length.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['entropy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['normalized_entropy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logit_lens.plot_logit_lens(\n",
    "    model=llama8b_fp32,\n",
    "    tokenizer=llama8b_tokenizer,\n",
    "    input_ids=MiscPrompts.Q12.value,\n",
    "    start_ix=0, end_ix=15,\n",
    "    save_fig_path=None,\n",
    "    #save_fig_path='Outputs/LogitLens/LI8B/probs_hf1bit_fp32_qa_blockstep10.jpg',\n",
    "    probs=True,\n",
    "    block_step=5,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logit_lens.plot_logit_lens(\n",
    "    model=hfbit1_fp32,\n",
    "    tokenizer=hfbit1_tokenizer,\n",
    "    input_ids=MiscPrompts.Q12.value,\n",
    "    start_ix=0, end_ix=15,\n",
    "    save_fig_path=None,\n",
    "    #save_fig_path='Outputs/LogitLens/LI8B/probs_hf1bit_fp32_qa_blockstep10.jpg',\n",
    "    #probs=True,\n",
    "    block_step=5,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logit_lens.plot_logit_lens(\n",
    "    model=dh3b_bitnet_fp32_qlmhead,\n",
    "    tokenizer=dh3b_tokenizer,\n",
    "    input_ids=MiscPrompts.Q12.value,\n",
    "    start_ix=0, end_ix=15,\n",
    "    save_fig_path=None,\n",
    "    #save_fig_path='Outputs/LogitLens/LI8B/probs_hf1bit_fp32_qa_blockstep10.jpg',\n",
    "    entropy=True,\n",
    "    block_step=5,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logit_lens.plot_logit_lens(\n",
    "    model=dh3b_bitnet_fp32_flmhead,\n",
    "    tokenizer=dh3b_tokenizer,\n",
    "    input_ids=MiscPrompts.Q12.value,\n",
    "    start_ix=0, end_ix=15,\n",
    "    #save_fig_path=None,\n",
    "    save_fig_path='Outputs/LogitLens/DH3B/logits_flmhead_fp32_qa.jpg',\n",
    "    #kl=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logit_lens.plot_comparing_lens(\n",
    "    models=(dh3b_fp32, dh3b_bitnet_fp32_qlmhead),\n",
    "    tokenizer=dh3b_tokenizer,\n",
    "    input_ids=MiscPrompts.Q12.value,\n",
    "    start_ix=0, end_ix=6,\n",
    "    #save_fig_path='Outputs/LogitLens/LI8B/nwd_llama_hf1bit_qa_blockstep10.jpg',\n",
    "    save_fig_path=None,\n",
    "    wasserstein=True,\n",
    "    #top_down=False,\n",
    "    block_step=5,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logit_lens.plot_comparing_lens(\n",
    "    models=(olmo1b_fp32, olmo1b_bitnet_fp32_qlmhead),\n",
    "    tokenizer=olmo1b_tokenizer,\n",
    "    input_ids=MiscPrompts.Q12.value,\n",
    "    start_ix=0, end_ix=15,\n",
    "    #save_fig_path='Outputs/LogitLens/OLMo1B/nwd_flmhead_qlmhead_math.jpg',\n",
    "    #save_fig_path=None,\n",
    "    wasserstein=True,\n",
    "    #top_down=False,\n",
    "    #block_step=3,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logit_lens.plot_comparing_lens(\n",
    "    models=(llama8b_fp32, hfbit1_fp32),\n",
    "    tokenizer=hfbit1_tokenizer,\n",
    "    input_ids=MiscPrompts.Q12.value,\n",
    "    start_ix=0, end_ix=15,\n",
    "    #save_fig_path='Outputs/LogitLens/LI8B/nwd_instruct_bitnet_fp32_qa.jpg',\n",
    "    save_fig_path=None,\n",
    "    wasserstein=True,\n",
    "    #top_down=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logit_lens.plot_comparing_lens(\n",
    "    models=(llama8b_fp32, llama8b_bnb4_fp16),\n",
    "    tokenizer=llama8b_tokenizer,\n",
    "    input_ids=MiscPrompts.Q12.value,\n",
    "    start_ix=0, end_ix=15,\n",
    "    #save_fig_path='Outputs/LogitLens/DH3B/nwd_3bfp32_ptdq_math.jpg',\n",
    "    save_fig_path=None,\n",
    "    wasserstein=True,\n",
    "    #top_down=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logit_lens.plot_topk_lens(\n",
    "    model=olmo1b_fp32,\n",
    "    tokenizer=olmo1b_tokenizer,\n",
    "    input_ids=MiscPrompts.Q12.value,\n",
    "    start_ix=0, end_ix=15,\n",
    "    topk_n=5,\n",
    "    #save_fig_path='Outputs/LogitLens/LI8B/topk5logits_bnb4bit_qa.jpg'\n",
    "    #save_fig_path=None,\n",
    "    #top_down=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logit_lens.plot_topk_lens(\n",
    "    model=olmo1b_bitnet_fp32_qlmhead,\n",
    "    tokenizer=olmo1b_tokenizer,\n",
    "    input_ids=MiscPrompts.Q12.value,\n",
    "    start_ix=0, end_ix=15,\n",
    "    topk_n=5,\n",
    "    #save_fig_path='Outputs/LogitLens/LI8B/topk5logits_ptsq_fp32_math.jpg'\n",
    "    #save_fig_path=None,\n",
    "    #entropy=True,\n",
    "    #top_down=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activation_lens.plot_activation_lens(\n",
    "    model=olmo1b_fp32,\n",
    "    tokenizer=olmo1b_tokenizer,\n",
    "    input_ids=MiscPrompts.Q12.value,\n",
    "    start_ix=0, end_ix=15,\n",
    "    metric='norm',\n",
    "    save_fig_path='Outputs/LogitLens/OLMo1B/actnorm_fp_qa.jpg', #HUSK FOR BITNET hfb1 Q12 qa!\n",
    "    #save_fig_path=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activation_lens.plot_activation_lens(\n",
    "    model=olmo1b_bitnet_fp32_qlmhead,\n",
    "    tokenizer=olmo1b_tokenizer,\n",
    "    input_ids=MiscPrompts.Q12.value,\n",
    "    start_ix=0, end_ix=15,\n",
    "    metric='norm',\n",
    "    save_fig_path='Outputs/LogitLens/OLMo1B/actnorm_ptsq_qa.jpg',\n",
    "    #save_fig_path=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activation_lens.plot_comparing_act_lens(\n",
    "    models=(olmo1b_fp32, olmo1b_bitnet_fp32_qlmhead),\n",
    "    tokenizer=olmo1b_tokenizer,\n",
    "    input_ids=MiscPrompts.Q12.value,\n",
    "    start_ix=0, end_ix=15,\n",
    "    metric='norm',\n",
    "    metric_name='l2',\n",
    "    save_fig_path='Outputs/LogitLens/OLMo1B/actnorm_fp_compare_ptsq_qa.jpg',\n",
    "    #save_fig_path=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dictionary Learning: SAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch._dynamo\n",
    "torch._dynamo.config.suppress_errors = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dh3b_bitnet_fp32_ptsq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary_learning.plot_sae_heatmap(\n",
    "    model=dh3b_fp32,\n",
    "    tokenizer=dh3b_tokenizer,\n",
    "    inputs=gsm8k_questions_sae,\n",
    "    plot_sae=True,\n",
    "    do_log=True,\n",
    "    top_k=5,\n",
    "    tokens_per_row=30,\n",
    "    target_layers=[2, 9, 16, 20, 26],\n",
    "    log_path='logs/sae_logs/DH3B/fp',\n",
    "    log_name='dh.3b-ptsq.fp32',\n",
    "    fig_path=None,\n",
    "    deterministic_sae=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary_learning.plot_sae_tokens(\n",
    "    model=dh3b_fp32,\n",
    "    tokenizer=dh3b_tokenizer,\n",
    "    inputs=PARAMS.get('prompt'),\n",
    "    multi_tokens=False,\n",
    "    do_log=False,\n",
    "    target_layers=[5],\n",
    "    vis_projection=None,\n",
    "    log_path=None,\n",
    "    log_name=None,\n",
    "    fig_path=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary_learning.plot_sae_tokens(\n",
    "    model=dh3b_fp32,\n",
    "    tokenizer=dh3b_tokenizer,\n",
    "    inputs=PARAMS.get('prompt'),\n",
    "    multi_tokens=True,\n",
    "    do_log=False,\n",
    "    target_layers=[5],\n",
    "    vis_projection=None,\n",
    "    log_path=None,\n",
    "    log_name=None,\n",
    "    fig_path=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary_learning.plot_sae_tokens(\n",
    "    model=dh3b_bitnet_fp32,\n",
    "    tokenizer=dh3b_tokenizer,\n",
    "    inputs=PARAMS.get('prompt'),\n",
    "    multi_tokens=True,\n",
    "    do_log=False,\n",
    "    target_layers=[5],\n",
    "    vis_projection=None,\n",
    "    log_path=None,\n",
    "    log_name=None,\n",
    "    fig_path=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary_learning.plot_sae_heatmap(\n",
    "    model=olmo1b_fp32,\n",
    "    tokenizer=olmo1b_tokenizer,\n",
    "    inputs=Texts.T1.value,\n",
    "    do_log=False,\n",
    "    top_k=5,\n",
    "    tokens_per_row=30,\n",
    "    target_layers=[5, 10, 15],\n",
    "    log_path=None,\n",
    "    log_name=None,\n",
    "    fig_path=None,\n",
    "    deterministic_sae=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary_learning.plot_sae_heatmap(\n",
    "    model=olmo1b_fp32,\n",
    "    tokenizer=olmo1b_tokenizer,\n",
    "    inputs=Texts.T1.value,\n",
    "    do_log=False,\n",
    "    top_k=5,\n",
    "    tokens_per_row=30,\n",
    "    target_layers=[5, 10, 15],\n",
    "    log_path=None,\n",
    "    log_name=None,\n",
    "    fig_path=None,\n",
    "    deterministic_sae=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary_learning.plot_sae_heatmap(\n",
    "    model=olmo2t_fp32,\n",
    "    tokenizer=olmo2t_tokenizer,\n",
    "    inputs=Texts.T1.value,\n",
    "    do_log=False,\n",
    "    top_k=5,\n",
    "    tokens_per_row=30,\n",
    "    target_layers=[5, 10, 15],\n",
    "    log_path=None,\n",
    "    log_name=None,\n",
    "    fig_path=None,\n",
    "    deterministic_sae=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary_learning.plot_comparing_heatmap(\n",
    "    models=(dh3b_fp32, dh3b_bitnet_fp32_qlmhead),\n",
    "    tokenizer=dh3b_tokenizer,\n",
    "    inputs=Texts.T1.value,\n",
    "    top_k=5,\n",
    "    tokens_per_row=30,\n",
    "    target_layers=[5, 15, 25],\n",
    "    fig_path=None,\n",
    "    deterministic_sae=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary_learning.plot_comparing_heatmap(\n",
    "    models=(dh3b_fp32, dh3b_bitnet_fp32_ptsq),\n",
    "    tokenizer=dh3b_tokenizer,\n",
    "    inputs=gsm8k_questions_sae,\n",
    "    top_k=5,\n",
    "    tokens_per_row=30,\n",
    "    target_layers=[2, 9, 16, 20, 26],\n",
    "    fig_path=None,\n",
    "    deterministic_sae=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary_learning.plot_comparing_heatmap(\n",
    "    models=(llama8b_fp32, hfbit1_fp32),\n",
    "    tokenizer=llama8b_tokenizer,\n",
    "    inputs=gsm8k_questions_sae,\n",
    "    top_k=5,\n",
    "    tokens_per_row=30,\n",
    "    target_layers=[2, 9, 16, 23, 30],\n",
    "    fig_path=None\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Hermes Chatbot Analysis (template only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_dict = {\n",
    "    #'dh.3b-llama.fp32': dh3b_fp32,\n",
    "    #'dh.3b-bnb4bit.fp16': dh3b_bnb4_fp16,\n",
    "    #'dh.3b-1.58.ptdq': dh3b_bitnet_fp32, \n",
    "    #'dh.3b-1.58.ptsq': dh3b_bitnet_fp32,\n",
    "    #'dh.8b-llama.fp32': dh8b_fp32,\n",
    "    #'dh.8b-bnb4bit.fp16': dh8b_bnb4_fp16,\n",
    "    #'dh.8b-1.58.ptdq': dh8b_bitnet_fp32,\n",
    "    #'dh.8b-1.58.ptsq': dh8b_bitnet_fp32,\n",
    "    #'llama.8b-instruct.fp32': llama8b_fp32,\n",
    "    #'llama.8b-bnb4bit.fp16': llama8b_bnb4_fp16,\n",
    "    #'llama.8b-1.58.fp32': hfbit1_fp32,\n",
    "    #'llama.8b-1.58.ptdq': llama8b_bitnet_fp32,\n",
    "    #'llama.8b-1.58.ptsq': llama8b_bitnet_fp32,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PARAMS:Dict = {\n",
    "    'context': Contexts.C1.value,\n",
    "    'prompt': MiscPrompts.Q2.value,\n",
    "    'max_new_tokens': 100,\n",
    "    'temperature': 0.8,\n",
    "    'repetition_penalty': 1.1,\n",
    "    'sample': True,\n",
    "    'device': None\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chatbot_analysis.run_gsm8k_analysis(\n",
    "    model=dh3b_bnb4_float32,\n",
    "    tokenizer=dh3b_tokenizer,\n",
    "    model_name='dh.3b-bnb4bit.fp32',\n",
    "    dataset=gsm8k_dataset['train'],\n",
    "    save_path='logs/gsm8k_logs/DH3B',\n",
    "    num_samples=10,\n",
    "    deterministic_backend=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chatbot_analysis.run_chatbot_analysis(\n",
    "    models=chat_dict,\n",
    "    tokenizer=dh3b_tokenizer,\n",
    "    deep_thinking=False,\n",
    "    full_path='logs/chatbot_logs',\n",
    "    deterministic_backend=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chatbot_analysis.plot_chatbot_analysis(\n",
    "    json_logs='logs/gsm8k_logs',\n",
    "    parallel_plot=True,\n",
    "    reference_file='logs/gsm8k_logs/llama.8b-1.58.fp32.json'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chatbot_analysis.plot_chatbot_analysis(\n",
    "    json_logs='logs/gsm8k_logs',\n",
    "    parallel_plot=False,\n",
    "    reference_file='logs/gsm8k_logs/llama.8b-1.58.fp32.json',\n",
    "    title=\"Model Metrics ('What is y if y=2*2-4+(3*2)')\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import matplotlib.cm as cm\n",
    "# Path to your results folder\n",
    "results_dir = \"logs/gsm8k_logs\"\n",
    "\n",
    "# Load all JSONs into a DataFrame\n",
    "all_results = []\n",
    "for filename in os.listdir(results_dir):\n",
    "    if filename.endswith(\".json\"):\n",
    "        with open(os.path.join(results_dir, filename), 'r') as f:\n",
    "            data = json.load(f)\n",
    "            all_results.append(data)\n",
    "\n",
    "df = pd.DataFrame(all_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define metrics\n",
    "metrics = [\n",
    "    'Perplexity',\n",
    "    'CPU Usage (%)',\n",
    "    'RAM Usage (%)',\n",
    "    'GPU Memory (MB)',\n",
    "    'Activation Similarity',\n",
    "    'Latency (s)'\n",
    "]\n",
    "\n",
    "# Generate a color map\n",
    "models = df['Model'].tolist()\n",
    "num_models = len(models)\n",
    "colors = cm.get_cmap('tab20c', num_models)  # You can try 'Set2', 'hsv', 'tab10', etc.\n",
    "\n",
    "model_colors = {model: colors(i) for i, model in enumerate(models)}\n",
    "\n",
    "# Plot\n",
    "fig, axes = plt.subplots(nrows=2, ncols=3, figsize=(18, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, metric in enumerate(metrics):\n",
    "    ax = axes[i]\n",
    "    for j, model in enumerate(models):\n",
    "        ax.bar(model, df.loc[j, metric], color=model_colors[model])\n",
    "    ax.set_title(metric)\n",
    "    ax.set_xticklabels(models, rotation=45, ha='right')\n",
    "    ax.grid(True)\n",
    "\n",
    "# Custom legend\n",
    "handles = [plt.Rectangle((0,0),1,1, color=model_colors[model]) for model in models]\n",
    "fig.legend(handles, models, loc='upper center', ncol=num_models)\n",
    "\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.92])\n",
    "plt.suptitle(\"GSM8K for n=10\", fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = [\n",
    "    'Perplexity',\n",
    "    'CPU Usage (%)',\n",
    "    'RAM Usage (%)',\n",
    "    'GPU Memory (MB)',\n",
    "    'Activation Similarity',\n",
    "    'Latency (s)',\n",
    "    \"Last Layer Mean Activation\",\n",
    "    \"Last Layer Activation Std\",\n",
    "    \"Mean Logits\",\n",
    "    \"Logit Std\",\n",
    "]\n",
    "\n",
    "# Generate a color map\n",
    "models = df['Model'].tolist()\n",
    "num_models = len(models)\n",
    "colors = cm.get_cmap('coolwarm', num_models)\n",
    "model_colors = {model: colors(i) for i, model in enumerate(models)}\n",
    "\n",
    "# Prepare subplot grid dynamically\n",
    "n_metrics = len(metrics)\n",
    "ncols = 5\n",
    "nrows = int(np.ceil(n_metrics / ncols))\n",
    "\n",
    "fig, axes = plt.subplots(nrows=nrows, ncols=ncols, figsize=(20, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, metric in enumerate(metrics):\n",
    "    ax = axes[i]\n",
    "    if metric in df.columns:\n",
    "        for j, model in enumerate(models):\n",
    "            ax.bar(model, df.loc[j, metric], color=model_colors[model])\n",
    "        ax.set_title(metric, fontsize=10)\n",
    "        ax.set_xticks(range(len(models)))\n",
    "        ax.set_xticklabels(models, rotation=45, ha='right', fontsize=8)\n",
    "        ax.grid(True)\n",
    "\n",
    "# Hide unused axes\n",
    "for j in range(i + 1, len(axes)):\n",
    "    fig.delaxes(axes[j])\n",
    "\n",
    "# Legend\n",
    "##handles = [plt.Rectangle((0, 0), 1, 1, color=model_colors[model]) for model in models]\n",
    "#fig.legend(handles, models, loc='upper center', ncol=min(num_models, 5))\n",
    "\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.93])\n",
    "plt.suptitle(\"Deep Hermes LLaMA 3B & LLaMA Instruct 8B GSM8K (n=10)\", fontsize=12)\n",
    "plt.savefig('Outputs/Report/llama8bdh3b_subplots.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = [\n",
    "    'Perplexity',\n",
    "    'CPU Usage (%)',\n",
    "    'RAM Usage (%)',\n",
    "    'GPU Memory (MB)',\n",
    "    'Activation Similarity',\n",
    "    'Latency (s)',\n",
    "    \"Last Layer Mean Activation\",\n",
    "    \"Last Layer Activation Std\",\n",
    "    \"Mean Logits\",\n",
    "    \"Logit Std\",\n",
    "]\n",
    "\n",
    "# Generate a color map\n",
    "models = df['Model'].tolist()\n",
    "num_models = len(models)\n",
    "colors = cm.get_cmap('coolwarm', num_models)\n",
    "model_colors = {model: colors(i) for i, model in enumerate(models)}\n",
    "\n",
    "# Prepare subplot grid dynamically\n",
    "n_metrics = len(metrics)\n",
    "ncols = 5\n",
    "nrows = int(np.ceil(n_metrics / ncols))\n",
    "\n",
    "fig, axes = plt.subplots(nrows=nrows, ncols=ncols, figsize=(20, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, metric in enumerate(metrics):\n",
    "    ax = axes[i]\n",
    "    if metric in df.columns:\n",
    "        values = df[metric].copy()\n",
    "\n",
    "        # Normalize perplexity via log-scale\n",
    "        if metric == \"Perplexity\":\n",
    "            values = np.log1p(values)  # log1p handles 0 gracefully\n",
    "\n",
    "        for j, model in enumerate(models):\n",
    "            ax.bar(model, values[j], color=model_colors[model])\n",
    "        ax.set_title(metric + (\" (log)\" if metric == \"Perplexity\" else \"\"), fontsize=10)\n",
    "        ax.set_xticks(range(len(models)))\n",
    "        ax.set_xticklabels(models, rotation=45, ha='right', fontsize=8)\n",
    "        ax.grid(True)\n",
    "\n",
    "# Hide unused axes\n",
    "for j in range(i + 1, len(axes)):\n",
    "    fig.delaxes(axes[j])\n",
    "\n",
    "# Legend\n",
    "##handles = [plt.Rectangle((0, 0), 1, 1, color=model_colors[model]) for model in models]\n",
    "#fig.legend(handles, models, loc='upper center', ncol=min(num_models, 5))\n",
    "\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.93])\n",
    "plt.suptitle(\"Deep Hermes LLaMA 3B & LLaMA Instruct 8B GSM8K (n=10)\", fontsize=12)\n",
    "plt.savefig('Outputs/Report/llama8bdh3b_subplots_normalized_perplexity.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "# Metrics to include (excluding 'Activation Similarity')\n",
    "metrics_for_corr = [\n",
    "    'Perplexity',\n",
    "    'CPU Usage (%)',\n",
    "    'RAM Usage (%)',\n",
    "    'GPU Memory (MB)',\n",
    "    'Latency (s)',\n",
    "    \"Last Layer Mean Activation\",\n",
    "    \"Last Layer Activation Std\",\n",
    "    \"Mean Logits\",\n",
    "    \"Logit Std\",\n",
    "]\n",
    "\n",
    "# Compute correlation\n",
    "corr_matrix = df[metrics_for_corr].corr()\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(\n",
    "    corr_matrix, \n",
    "    annot=True, \n",
    "    cmap='coolwarm', \n",
    "    fmt=\".2f\", \n",
    "    linewidths=0.5, \n",
    "    square=True,\n",
    "    cbar_kws={\"shrink\": 0.75}\n",
    ")\n",
    "plt.title(\"Deep Hermes LLaMA 3B & LLaMA Instruct 8B GSM8K Correlation (n=10)\", fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.savefig('Outputs/Report/llama8bdh3b_corr_heatmap.png')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MechInterp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
