{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ampir\\anaconda3\\envs\\MechInterp\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from __future__ import annotations\n",
    "from typing import Tuple, List, Dict, Optional, Any\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from pathlib import Path\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from datasets import load_dataset, DownloadMode\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import tqdm\n",
    "\n",
    "from helper_utils.enum_keys import (\n",
    "    FPKey,\n",
    "    ModelKey,\n",
    "    QuantStyle,\n",
    "    MiscPrompts,\n",
    "    Contexts,\n",
    "    Texts\n",
    ")\n",
    "\n",
    "from PTQ.bitlinear_wrapper_class import BitLinear\n",
    "from PTQ.apply_ptq import applyPTQ\n",
    "from PTQ.olmo_act_fns import patch_olmo_mlp\n",
    "import helper_utils.utils as utils\n",
    "from helper_utils.models_loader import load_4bit_auto, load_8bit_auto\n",
    "from mech_interp_utils.utils_main.src.transformer_utils import (\n",
    "    logit_lens,\n",
    "    activation_lens,\n",
    "    dictionary_learning,\n",
    "    chatbot_analysis\n",
    ")\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "original = torch.randn(512) * 0.5  # Original activations\n",
    "\n",
    "def quantize_dequantize(tensor, scale_value):\n",
    "    scale = max(scale_value, 1e-8)\n",
    "    qmin, qmax = -127, 127\n",
    "    tensor_int = (tensor / scale).round().clamp(qmin, qmax).to(torch.int8)\n",
    "    tensor_dequant = tensor_int.float() * scale\n",
    "    return tensor_int, tensor_dequant\n",
    "\n",
    "# Quantize with different scales\n",
    "_, dequant_1e2 = quantize_dequantize(original, 1e-2)\n",
    "_, dequant_1e5 = quantize_dequantize(original, 1e-5)\n",
    "\n",
    "# L2 distance\n",
    "print(\"L2 Distance (scale=1e-2):\", torch.norm(original - dequant_1e2).item())\n",
    "print(\"L2 Distance (scale=1e-5):\", torch.norm(original - dequant_1e5).item())\n",
    "\n",
    "# Plot histograms + KDEs\n",
    "plt.rcParams['font.family'] = 'Times New Roman'\n",
    "plt.figure(figsize=(14, 6))\n",
    "sns.histplot(original.numpy(),bel='Original', kde=True, stat=\"count\", bins=50, color='black', alpha=0.5)\n",
    "sns.histplot(dequant_1e2.numpy(), label='Dequant (scale=1e-2)', kde=True, stat=\"count\", bins=50, color='red', alpha=0.5)\n",
    "sns.histplot(dequant_1e5.numpy(), label='Dequant (scale=1e-5)', kde=True, stat=\"count\", bins=50, color='blue', alpha=0.5)\n",
    "\n",
    "plt.title(\"Histogram (Count) + KDE of Quantized vs Original Activations\")\n",
    "plt.xlabel(\"Activation Value\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.ylim(0, 40)  \n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch._dynamo\n",
    "torch._dynamo.config.suppress_errors = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datasets for calibrating activations and analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = r'D:\\ThesisData\\wikitext'\n",
    "\n",
    "destination_path = str(Path(filepath))\n",
    "dataset = load_dataset(\n",
    "    'wikitext', 'wikitext-103-raw-v1',\n",
    "    split={\n",
    "        'train': 'train[:200]',\n",
    "    },\n",
    "    cache_dir=destination_path,\n",
    "    download_mode=DownloadMode.REUSE_DATASET_IF_EXISTS,\n",
    "    keep_in_memory=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_texts = dataset['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calibration_texts = [t for t in dataset['train'][\"text\"] if isinstance(t, str) and t.strip()]\n",
    "#calibration_texts = [t for t in sub_txts[\"text\"] if isinstance(t, str) and t.strip()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_txts = train_texts.take(200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GSM8K (Math) \"gsm8k\"\n",
    "#### LogiQA (Logic & Reasoning): \"logiq\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = r'D:\\ThesisData\\nq'\n",
    "\n",
    "destination_path = str(Path(filepath))\n",
    "nq_dataset = load_dataset(\n",
    "    'sentence-transformers/natural-questions',\n",
    "    split={\n",
    "        'train': 'train[:20]'\n",
    "    },\n",
    "    cache_dir=destination_path,\n",
    "    download_mode=DownloadMode.REUSE_DATASET_IF_EXISTS,\n",
    "    keep_in_memory=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nq_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nq_queries= nq_dataset['train']['query']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nq_queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nq_answers = nq_dataset['train']['answer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nq_answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = r'D:\\ThesisData\\logiqa'\n",
    "\n",
    "destination_path = str(Path(filepath))\n",
    "logiqa_dataset = load_dataset(\n",
    "    'lucasmccabe/logiqa',\n",
    "    split={\n",
    "        'train': 'train[:20]'\n",
    "    },\n",
    "    cache_dir=destination_path,\n",
    "    download_mode=DownloadMode.REUSE_DATASET_IF_EXISTS,\n",
    "    keep_in_memory=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logiqa_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = r'D:\\ThesisData\\gsm8k'\n",
    "\n",
    "destination_path = str(Path(filepath))\n",
    "gsm8k_dataset = load_dataset(\n",
    "    'gsm8k', 'main',\n",
    "    split={\n",
    "        'train': 'train[:20]'\n",
    "    },\n",
    "    cache_dir=destination_path,\n",
    "    download_mode=DownloadMode.REUSE_DATASET_IF_EXISTS,\n",
    "    keep_in_memory=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gsm8k_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gsm8k_questions = gsm8k_dataset['train']['question']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gsm8k_answers = gsm8k_dataset['train']['answer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gsm8k_answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gsm8k_questions_sae = \"\"\"\n",
    "Natalia sold clips to 48 of her friends in April, and then she sold half as many clips in May. How many clips did Natalia sell altogether in April and May?\n",
    "Weng earns $12 an hour for babysitting. Yesterday, she just did 50 minutes of babysitting. How much did she earn?\n",
    "Betty is saving money for a new wallet which costs $100. Betty has only half of the money she needs. Her parents decided to give her $15 for that purpose, and her grandparents twice as much as her parents. How much more money does Betty need to buy the wallet?\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "datasets = {\n",
    "    \"GSM8K Query\": gsm8k_questions,  # 20 samples\n",
    "    \"GSM8K Answer\": gsm8k_answers,\n",
    "    \"Natural Questions Query\": nq_queries,\n",
    "    \"Natural Questions\": nq_answers,\n",
    "}\n",
    "\n",
    "\n",
    "length_data = []\n",
    "for name, samples in datasets.items():\n",
    "    for sample in samples:\n",
    "        length_data.append({\n",
    "            \"Dataset\": name,\n",
    "            \"Length\": len(sample) \n",
    "        })\n",
    "\n",
    "\n",
    "df = pd.DataFrame(length_data)\n",
    "\n",
    "plt.rcParams['font.family'] = 'Times New Roman'\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.boxplot(data=df, x=\"Dataset\", y=\"Length\", palette=\"coolwarm\")\n",
    "sns.stripplot(data=df, x=\"Dataset\", y=\"Length\", color=\"black\", alpha=0.5, jitter=True)\n",
    "\n",
    "plt.title(\"Sequence Length Distribution by Dataset\")\n",
    "plt.ylabel(\"Length\")\n",
    "plt.grid(True, linestyle=\"--\", alpha=0.4)\n",
    "plt.tight_layout()\n",
    "plt.savefig('Outputs/Report/datasets_sequence_length.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models and Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_test_model(model_path:str, dtype=torch.dtype) -> AutoModelForCausalLM:\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_path,\n",
    "        return_dict=True,\n",
    "        output_hidden_states=True,\n",
    "        torch_dtype=dtype,\n",
    "        low_cpu_mem_usage=True,\n",
    "        local_files_only=True,\n",
    "        use_safetensors=True,\n",
    "        #trust_remote_code=True\n",
    "    )\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "hfbit1_tokenizer = AutoTokenizer.from_pretrained(FPKey.HFBIT1_TOKENIZER.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You have loaded a BitNet model on CPU and have a CUDA device available, make sure to set your model on a GPU device in order to run your model.\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  3.32it/s]\n"
     ]
    }
   ],
   "source": [
    "hfbit1_fp32 = load_test_model(FPKey.HFBIT1_8B.value, dtype=torch.float32) # https://huggingface.co/HF1BitLLM/Llama3-8B-1.58-100B-tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hfbit1_fp32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print model layers to inspect their names\n",
    "for name, module in hfbit1_fp32.named_modules():\n",
    "    print(f\"Layer name: {name}, Module: {module}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llama8b_tokenizer = AutoTokenizer.from_pretrained(FPKey.LINSTRUCT_TOKENIZER.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llama8b_fp32 = load_test_model(FPKey.LINSTRUCT_8B.value, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llama8b_fp32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model with 8-bit quantization using BNB\n",
    "llama8b_bnb8_float32 = AutoModelForCausalLM.from_pretrained(\n",
    "    ModelKey.LLINSTRUCT8B.value,           # Replace with your actual model\n",
    "    torch_dtype=torch.float32,     # Specify the dtype (for 8-bit, use uint8)\n",
    "    #device_map=\"cpu\",           # Automatically map the model to available devices (GPU/CPU)\n",
    "    load_in_8bit=True,\n",
    "    return_dict=True,\n",
    "    output_hidden_states=True,            # Set to True for 8-bit quantization\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llama8b_bnb8_float32.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model with 4-bit quantization using BNB\n",
    "llama8b_bnb4_float32 = AutoModelForCausalLM.from_pretrained(\n",
    "    ModelKey.LLINSTRUCT8B.value,           # Replace with your actual model\n",
    "    torch_dtype=torch.float32,     # This would still use uint8 or another type based on quantization method\n",
    "    #device_map=\"auto\",           # Automatically map the model to available devices (GPU/CPU)\n",
    "    load_in_4bit=True,            # Set to True for 4-bit quantization (if available)\n",
    "    return_dict=True,\n",
    "    output_hidden_states=True, \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llama8b_ptsq_float32 = applyPTQ(\n",
    "    load_test_model(ModelKey.LLINSTRUCT8B.value, dtype=torch.float32),\n",
    "    tokenizer=llama8b_tokenizer,\n",
    "    #calibration_input=None,\n",
    "    #calibration_input=sub_txts['text'],\n",
    "    calibration_input=Texts.T1.value,\n",
    "    mode='1.58bit',\n",
    "    safer_quant=True,\n",
    "    q_lmhead=True,\n",
    "    model_half=False,\n",
    "    quant_half=False,\n",
    "    layers_to_quant_weights=QuantStyle.BITNET.value,\n",
    "    layers_to_quant_activations=QuantStyle.BITNET.value,\n",
    "    fragile_layers=False,\n",
    "    act_quant=True,\n",
    "    act_bits=8,\n",
    "    torch_backends=False,\n",
    "    debugging=True,\n",
    "    plot_debugging=False,\n",
    "    plot_quantization=False,\n",
    "    freeze_modules=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hfbit1_tokenizer.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llama8b_tokenizer.vocab_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### allenai/OLMo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "olmo1b_tokenizer = AutoTokenizer.from_pretrained(FPKey.OLMO1B_TOKENIZER.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "olmo2t_tokenizer = AutoTokenizer.from_pretrained(FPKey.OLMO7B2T_TOKENIZER.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "olmo1b_fp32 = load_test_model(FPKey.OLMO1B_FP.value, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "olmo2t_fp32 = load_test_model(FPKey.OLMO7B2T_FP.value, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "olmo1b_bitnet_fp32_ptsq = applyPTQ(\n",
    "    load_test_model(FPKey.OLMO1B_FP.value, dtype=torch.float32),\n",
    "    tokenizer=olmo1b_tokenizer,\n",
    "    #calibration_input=None,\n",
    "    #calibration_input=sub_txts['text'],\n",
    "    calibration_input=Texts.T1.value,\n",
    "    mode='1.58bit',\n",
    "    safer_quant=True,\n",
    "    q_lmhead=True,\n",
    "    model_half=False,\n",
    "    quant_half=False,\n",
    "    layers_to_quant_weights=QuantStyle.BITNET.value,\n",
    "    layers_to_quant_activations=QuantStyle.BITNET.value,\n",
    "    fragile_layers=False,\n",
    "    act_quant=True,\n",
    "    act_bits=8,\n",
    "    torch_backends=True,\n",
    "    debugging=True,\n",
    "    plot_debugging=False,\n",
    "    plot_quantization=False,\n",
    "    freeze_modules=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "olmo2t_bitnet_fp32_ptsq = applyPTQ(\n",
    "    load_test_model(FPKey.OLMO7B2T_FP.value, dtype=torch.float32),\n",
    "    tokenizer=olmo2t_tokenizer,\n",
    "    #calibration_input=None,\n",
    "    #calibration_input=sub_txts['text'],\n",
    "    calibration_input=Texts.T1.value,\n",
    "    mode='1.58bit',\n",
    "    safer_quant=True,\n",
    "    q_lmhead=True,\n",
    "    model_half=False,\n",
    "    quant_half=False,\n",
    "    layers_to_quant_weights=QuantStyle.BITNET.value,\n",
    "    layers_to_quant_activations=QuantStyle.BITNET.value,\n",
    "    fragile_layers=False,\n",
    "    act_quant=True,\n",
    "    act_bits=8,\n",
    "    torch_backends=False,\n",
    "    debugging=True,\n",
    "    plot_debugging=False,\n",
    "    plot_quantization=False,\n",
    "    freeze_modules=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model with 8-bit quantization using BNB\n",
    "olmo1b_bnb8_float32 = AutoModelForCausalLM.from_pretrained(\n",
    "    FPKey.OLMO1B_FP.value,           # Replace with your actual model\n",
    "    torch_dtype=torch.float32,     # Specify the dtype (for 8-bit, use uint8)\n",
    "    #device_map=\"cpu\",           # Automatically map the model to available devices (GPU/CPU)\n",
    "    load_in_8bit=True,\n",
    "    return_dict=True,\n",
    "    output_hidden_states=True,            # Set to True for 8-bit quantization\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model with 4-bit quantization using BNB\n",
    "olmo1b_bnb4_float32 = AutoModelForCausalLM.from_pretrained(\n",
    "    FPKey.OLMO1B_FP.value,           # Replace with your actual model\n",
    "    torch_dtype=torch.float32,     # This would still use uint8 or another type based on quantization method\n",
    "    #device_map=\"auto\",           # Automatically map the model to available devices (GPU/CPU)\n",
    "    load_in_4bit=True,            # Set to True for 4-bit quantization (if available)\n",
    "    return_dict=True,\n",
    "    output_hidden_states=True, \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NousResearch/DeepHermes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dh3b_tokenizer = AutoTokenizer.from_pretrained(FPKey.TOKENIZER_3B.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dh8b_tokenizer = AutoTokenizer.from_pretrained(FPKey.TOKENIZER_8B.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dh3b_fp32 = load_test_model(FPKey.FP_3B.value, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dh3b_fp32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dh8b_fp32 = load_test_model(FPKey.FP_8B.value, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dh3b_bitnet_fp32_ptsq = applyPTQ(\n",
    "    load_test_model(FPKey.FP_3B.value, dtype=torch.float32),\n",
    "    tokenizer=dh3b_tokenizer,\n",
    "    #calibration_input=None,\n",
    "    #calibration_input=sub_txts['text'],\n",
    "    calibration_input=Texts.T1.value,\n",
    "    mode='1.58bit',\n",
    "    safer_quant=True,\n",
    "    q_lmhead=True,\n",
    "    model_half=False,\n",
    "    quant_half=False,\n",
    "    layers_to_quant_weights=QuantStyle.BITNET.value,\n",
    "    layers_to_quant_activations=QuantStyle.BITNET.value,\n",
    "    fragile_layers=False,\n",
    "    act_quant=True,\n",
    "    act_bits=8,\n",
    "    torch_backends=False,\n",
    "    debugging=True,\n",
    "    plot_debugging=False,\n",
    "    plot_quantization=False,\n",
    "    freeze_modules=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model with 8-bit quantization using BNB\n",
    "dh3b_bnb8_float32 = AutoModelForCausalLM.from_pretrained(\n",
    "    FPKey.FP_3B.value,           # Replace with your actual model\n",
    "    torch_dtype=torch.float32,     # Specify the dtype (for 8-bit, use uint8)\n",
    "    #device_map=\"cpu\",           # Automatically map the model to available devices (GPU/CPU)\n",
    "    load_in_8bit=True,\n",
    "    return_dict=True,\n",
    "    output_hidden_states=True,            # Set to True for 8-bit quantization\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model with 4-bit quantization using BNB\n",
    "dh3b_bnb4_float32 = AutoModelForCausalLM.from_pretrained(\n",
    "    FPKey.FP_3B.value,           # Replace with your actual model\n",
    "    torch_dtype=torch.float32,     # This would still use uint8 or another type based on quantization method\n",
    "    #device_map=\"auto\",           # Automatically map the model to available devices (GPU/CPU)\n",
    "    load_in_4bit=True,            # Set to True for 4-bit quantization (if available)\n",
    "    return_dict=True,\n",
    "    output_hidden_states=True, \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model with 8-bit quantization using BNB\n",
    "dh8b_bnb8_float32 = AutoModelForCausalLM.from_pretrained(\n",
    "    FPKey.FP_8B.value,           # Replace with your actual model\n",
    "    torch_dtype=torch.float32,     # Specify the dtype (for 8-bit, use uint8)\n",
    "    #device_map=\"cpu\",           # Automatically map the model to available devices (GPU/CPU)\n",
    "    load_in_8bit=True,\n",
    "    return_dict=True,\n",
    "    output_hidden_states=True,            # Set to True for 8-bit quantization\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model with 4-bit quantization using BNB\n",
    "dh8b_bnb4_float32 = AutoModelForCausalLM.from_pretrained(\n",
    "    FPKey.FP_8B.value,           # Replace with your actual model\n",
    "    torch_dtype=torch.float32,     # This would still use uint8 or another type based on quantization method\n",
    "    #device_map=\"auto\",           # Automatically map the model to available devices (GPU/CPU)\n",
    "    load_in_4bit=True,            # Set to True for 4-bit quantization (if available)\n",
    "    return_dict=True,\n",
    "    output_hidden_states=True, \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dh8b_bnb4_float32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activation Lens and Logit Lens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_inputs = [\n",
    "    # Language understanding\n",
    "    \"The quick brown fox jumps over the lazy dog.\",\n",
    "    \"Despite the rain, the event continued as planned.\",\n",
    "    \n",
    "    # Logic/reasoning\n",
    "    \"If all humans are mortal and Socrates is a human, then Socrates is mortal.\",\n",
    "    \"Either the lights are off or the power is out. The lights are on, so the power must be out.\",\n",
    "\n",
    "    # Math/numerical\n",
    "    \"The derivative of sin(x) with respect to x is cos(x).\",\n",
    "    \"What is the sum of the first 100 natural numbers?\",\n",
    "\n",
    "    # Programming\n",
    "    \"In Python, list comprehensions provide a concise way to create lists.\",\n",
    "    \"To define a function in JavaScript, use the 'function' keyword.\",\n",
    "\n",
    "    # Commonsense knowledge\n",
    "    \"You should refrigerate milk after opening it to keep it fresh.\",\n",
    "    \"People usually eat breakfast in the morning before starting their day.\",\n",
    "\n",
    "    # Scientific knowledge\n",
    "    \"Water boils at 100 degrees Celsius under standard atmospheric pressure.\",\n",
    "    \"Photosynthesis is the process by which plants convert sunlight into chemical energy.\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gsm8k_questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gsm8k_answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nq_queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [\"The quick brown fox jumps over the lazy dog.\", \"Despite the rain, the event continued as planned.\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch._dynamo\n",
    "torch._dynamo.config.suppress_errors = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "olmo1b_fp32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logit_lens.plot_topk_comparing_lens(\n",
    "    models=(olmo1b_fp32, olmo1b_bitnet_fp32_ptsq),\n",
    "    tokenizers=(olmo1b_tokenizer, olmo1b_tokenizer),\n",
    "    inputs=MiscPrompts.Q11.value,\n",
    "    start_ix=0, end_ix=6,\n",
    "    topk=5,\n",
    "    topk_mean=True,\n",
    "    #js=True,\n",
    "    #js=True,\n",
    "    block_step=1,\n",
    "    token_font_size=16,\n",
    "    #json_log_path=None,\n",
    "    #json_log_path='logs/nq_answers/dh.3b-bnb4bit.fp32', # 20 samples\n",
    "    #save_fig_path=None,\n",
    "    #save_fig_path='Outputs/LogitLens/DH3B/logits_3b_fp32_math.jpg',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logit_lens.plot_topk_comparing_lens(\n",
    "    models=(llama8b_fp32, hfbit1_fp32),\n",
    "    tokenizers=(llama8b_tokenizer, hfbit1_tokenizer),\n",
    "    inputs=MiscPrompts.Q11.value,\n",
    "    start_ix=0, end_ix=15,\n",
    "    topk=5,\n",
    "    topk_mean=True,\n",
    "    #js=True,\n",
    "    #js=True,\n",
    "    block_step=1,\n",
    "    token_font_size=16,\n",
    "    #json_log_path=None,\n",
    "    #json_log_path='logs/nq_answers/dh.3b-bnb4bit.fp32', # 20 samples\n",
    "    #save_fig_path=None,\n",
    "    #save_fig_path='Outputs/LogitLens/DH3B/logits_3b_fp32_math.jpg',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "colorscale": [
          [
           0,
           "rgb(5,48,97)"
          ],
          [
           0.1,
           "rgb(33,102,172)"
          ],
          [
           0.2,
           "rgb(67,147,195)"
          ],
          [
           0.3,
           "rgb(146,197,222)"
          ],
          [
           0.4,
           "rgb(209,229,240)"
          ],
          [
           0.5,
           "rgb(247,247,247)"
          ],
          [
           0.6,
           "rgb(253,219,199)"
          ],
          [
           0.7,
           "rgb(244,165,130)"
          ],
          [
           0.8,
           "rgb(214,96,77)"
          ],
          [
           0.9,
           "rgb(178,24,43)"
          ],
          [
           1,
           "rgb(103,0,31)"
          ]
         ],
         "hoverinfo": "text",
         "hovertext": [
          [
           "<b>Entropy:</b> 0.589<br><b>Pred:</b>  <br><b>Top-5:</b><br>&nbsp;&nbsp; : 0.181<br>&nbsp;&nbsp; the: 0.030<br>&nbsp;&nbsp; (: 0.015<br>&nbsp;&nbsp; The: 0.014<br>&nbsp;&nbsp; and: 0.012<br>",
           "<b>Entropy:</b> 0.242<br><b>Pred:</b> ://<br><b>Top-5:</b><br>&nbsp;&nbsp;://: 0.031<br>&nbsp;&nbsp;3: 0.007<br>&nbsp;&nbsp;ical: 0.007<br>&nbsp;&nbsp;201: 0.007<br>&nbsp;&nbsp;9: 0.007<br>",
           "<b>Entropy:</b> 0.979<br><b>Pred:</b> ,<br><b>Top-5:</b><br>&nbsp;&nbsp;,: 0.124<br>&nbsp;&nbsp;.: 0.074<br>&nbsp;&nbsp; and: 0.072<br>&nbsp;&nbsp;\n: 0.060<br>&nbsp;&nbsp;.\n: 0.060<br>",
           "<b>Entropy:</b> 0.661<br><b>Pred:</b>  be<br><b>Top-5:</b><br>&nbsp;&nbsp; be: 0.527<br>&nbsp;&nbsp; have: 0.036<br>&nbsp;&nbsp; help: 0.029<br>&nbsp;&nbsp; make: 0.011<br>&nbsp;&nbsp; provide: 0.011<br>",
           "<b>Entropy:</b> 0.676<br><b>Pred:</b>  measured<br><b>Top-5:</b><br>&nbsp;&nbsp; measured: 0.082<br>&nbsp;&nbsp; defined: 0.042<br>&nbsp;&nbsp; separated: 0.041<br>&nbsp;&nbsp; understood: 0.031<br>&nbsp;&nbsp; known: 0.027<br>",
           "<b>Entropy:</b> 1.161<br><b>Pred:</b>  in<br><b>Top-5:</b><br>&nbsp;&nbsp; in: 0.261<br>&nbsp;&nbsp; without: 0.243<br>&nbsp;&nbsp; to: 0.066<br>&nbsp;&nbsp; at: 0.060<br>&nbsp;&nbsp;,: 0.036<br>",
           "<b>Entropy:</b> 0.979<br><b>Pred:</b>  the<br><b>Top-5:</b><br>&nbsp;&nbsp; the: 0.183<br>&nbsp;&nbsp; a: 0.165<br>&nbsp;&nbsp; consciousness: 0.040<br>&nbsp;&nbsp; an: 0.037<br>&nbsp;&nbsp; being: 0.036<br>",
           "<b>Entropy:</b> 1.215<br><b>Pred:</b> .<br><b>Top-5:</b><br>&nbsp;&nbsp;.: 0.265<br>&nbsp;&nbsp;,: 0.179<br>&nbsp;&nbsp;.\n: 0.112<br>&nbsp;&nbsp; and: 0.056<br>&nbsp;&nbsp;;: 0.049<br>",
           "<b>Entropy:</b> 0.655<br><b>Pred:</b>  It<br><b>Top-5:</b><br>&nbsp;&nbsp; It: 0.080<br>&nbsp;&nbsp; The: 0.074<br>&nbsp;&nbsp; This: 0.028<br>&nbsp;&nbsp; In: 0.022<br>&nbsp;&nbsp; (: 0.020<br>",
           "<b>Entropy:</b> 0.863<br><b>Pred:</b>  understanding<br><b>Top-5:</b><br>&nbsp;&nbsp; understanding: 0.152<br>&nbsp;&nbsp; one: 0.123<br>&nbsp;&nbsp; matter: 0.066<br>&nbsp;&nbsp; knowledge: 0.018<br>&nbsp;&nbsp; amount: 0.017<br>",
           "<b>Entropy:</b> 1.028<br><b>Pred:</b>  can<br><b>Top-5:</b><br>&nbsp;&nbsp; can: 0.436<br>&nbsp;&nbsp; program: 0.100<br>&nbsp;&nbsp; is: 0.065<br>&nbsp;&nbsp;,: 0.045<br>&nbsp;&nbsp; could: 0.036<br>",
           "<b>Entropy:</b> 1.110<br><b>Pred:</b>  ever<br><b>Top-5:</b><br>&nbsp;&nbsp; ever: 0.394<br>&nbsp;&nbsp; the: 0.094<br>&nbsp;&nbsp; been: 0.093<br>&nbsp;&nbsp; yet: 0.063<br>&nbsp;&nbsp; a: 0.039<br>",
           "<b>Entropy:</b> 0.810<br><b>Pred:</b>  understanding<br><b>Top-5:</b><br>&nbsp;&nbsp; understanding: 0.281<br>&nbsp;&nbsp; consciousness: 0.043<br>&nbsp;&nbsp; idea: 0.039<br>&nbsp;&nbsp; kind: 0.029<br>&nbsp;&nbsp; ability: 0.023<br>",
           "<b>Entropy:</b> 1.138<br><b>Pred:</b>  of<br><b>Top-5:</b><br>&nbsp;&nbsp; of: 0.468<br>&nbsp;&nbsp;,: 0.114<br>&nbsp;&nbsp;.: 0.102<br>&nbsp;&nbsp; or: 0.073<br>&nbsp;&nbsp;.\n: 0.032<br>",
           "<b>Entropy:</b> 1.211<br><b>Pred:</b>  its<br><b>Top-5:</b><br>&nbsp;&nbsp; its: 0.204<br>&nbsp;&nbsp; what: 0.195<br>&nbsp;&nbsp; the: 0.183<br>&nbsp;&nbsp; itself: 0.069<br>&nbsp;&nbsp; consciousness: 0.018<br>"
          ],
          [
           "<b>Entropy:</b> 0.022<br><b>Pred:</b>  be<br><b>Top-5:</b><br>&nbsp;&nbsp; be: 0.001<br>&nbsp;&nbsp; been: 0.001<br>&nbsp;&nbsp;'gc: 0.000<br>&nbsp;&nbsp; have: 0.000<br>&nbsp;&nbsp;contri: 0.000<br>",
           "<b>Entropy:</b> 0.014<br><b>Pred:</b> contri<br><b>Top-5:</b><br>&nbsp;&nbsp;contri: 0.000<br>&nbsp;&nbsp;��: 0.000<br>&nbsp;&nbsp;/*\r\n: 0.000<br>&nbsp;&nbsp;$LANG: 0.000<br>&nbsp;&nbsp;chartInstance: 0.000<br>",
           "<b>Entropy:</b> 0.909<br><b>Pred:</b> ,<br><b>Top-5:</b><br>&nbsp;&nbsp;,: 0.086<br>&nbsp;&nbsp;\n: 0.072<br>&nbsp;&nbsp;.com: 0.072<br>&nbsp;&nbsp; (: 0.066<br>&nbsp;&nbsp;:: 0.046<br>",
           "<b>Entropy:</b> 0.467<br><b>Pred:</b>  be<br><b>Top-5:</b><br>&nbsp;&nbsp; be: 0.193<br>&nbsp;&nbsp; take: 0.008<br>&nbsp;&nbsp; go: 0.008<br>&nbsp;&nbsp; have: 0.008<br>&nbsp;&nbsp; make: 0.007<br>",
           "<b>Entropy:</b> 0.979<br><b>Pred:</b>  quant<br><b>Top-5:</b><br>&nbsp;&nbsp; quant: 0.168<br>&nbsp;&nbsp; defined: 0.095<br>&nbsp;&nbsp; separated: 0.063<br>&nbsp;&nbsp; fully: 0.047<br>&nbsp;&nbsp; reduced: 0.044<br>",
           "<b>Entropy:</b> 0.804<br><b>Pred:</b>  without<br><b>Top-5:</b><br>&nbsp;&nbsp; without: 0.704<br>&nbsp;&nbsp; in: 0.114<br>&nbsp;&nbsp; unless: 0.046<br>&nbsp;&nbsp; at: 0.037<br>&nbsp;&nbsp; apart: 0.010<br>",
           "<b>Entropy:</b> 1.176<br><b>Pred:</b>  matter<br><b>Top-5:</b><br>&nbsp;&nbsp; matter: 0.366<br>&nbsp;&nbsp; a: 0.129<br>&nbsp;&nbsp; the: 0.075<br>&nbsp;&nbsp; Matter: 0.070<br>&nbsp;&nbsp; thought: 0.057<br>",
           "<b>Entropy:</b> 1.283<br><b>Pred:</b> ;<br><b>Top-5:</b><br>&nbsp;&nbsp;;: 0.282<br>&nbsp;&nbsp;.: 0.245<br>&nbsp;&nbsp; being: 0.112<br>&nbsp;&nbsp;,: 0.089<br>&nbsp;&nbsp;\n: 0.037<br>",
           "<b>Entropy:</b> 0.525<br><b>Pred:</b>  It<br><b>Top-5:</b><br>&nbsp;&nbsp; It: 0.075<br>&nbsp;&nbsp; The: 0.033<br>&nbsp;&nbsp; Being: 0.028<br>&nbsp;&nbsp; Faith: 0.014<br>&nbsp;&nbsp; Now: 0.013<br>",
           "<b>Entropy:</b> 1.259<br><b>Pred:</b>  matter<br><b>Top-5:</b><br>&nbsp;&nbsp; matter: 0.306<br>&nbsp;&nbsp; understanding: 0.140<br>&nbsp;&nbsp;am: 0.130<br>&nbsp;&nbsp; one: 0.122<br>&nbsp;&nbsp; amount: 0.028<br>",
           "<b>Entropy:</b> 0.937<br><b>Pred:</b>  program<br><b>Top-5:</b><br>&nbsp;&nbsp; program: 0.500<br>&nbsp;&nbsp; can: 0.355<br>&nbsp;&nbsp; programme: 0.026<br>&nbsp;&nbsp; could: 0.021<br>&nbsp;&nbsp; system: 0.010<br>",
           "<b>Entropy:</b> 0.860<br><b>Pred:</b>  ever<br><b>Top-5:</b><br>&nbsp;&nbsp; ever: 0.644<br>&nbsp;&nbsp; yet: 0.241<br>&nbsp;&nbsp; been: 0.025<br>&nbsp;&nbsp; any: 0.021<br>&nbsp;&nbsp; the: 0.014<br>",
           "<b>Entropy:</b> 0.976<br><b>Pred:</b>  understanding<br><b>Top-5:</b><br>&nbsp;&nbsp; understanding: 0.424<br>&nbsp;&nbsp; kind: 0.075<br>&nbsp;&nbsp; way: 0.051<br>&nbsp;&nbsp; idea: 0.045<br>&nbsp;&nbsp; capacity: 0.039<br>",
           "<b>Entropy:</b> 0.598<br><b>Pred:</b>  of<br><b>Top-5:</b><br>&nbsp;&nbsp; of: 0.813<br>&nbsp;&nbsp; No: 0.079<br>&nbsp;&nbsp; or: 0.054<br>&nbsp;&nbsp; no: 0.009<br>&nbsp;&nbsp;,: 0.005<br>",
           "<b>Entropy:</b> 1.123<br><b>Pred:</b>  its<br><b>Top-5:</b><br>&nbsp;&nbsp; its: 0.496<br>&nbsp;&nbsp; what: 0.290<br>&nbsp;&nbsp; itself: 0.107<br>&nbsp;&nbsp; the: 0.045<br>&nbsp;&nbsp; being: 0.008<br>"
          ],
          [
           "<b>Entropy:</b> 0.031<br><b>Pred:</b> 'gc<br><b>Top-5:</b><br>&nbsp;&nbsp;'gc: 0.003<br>&nbsp;&nbsp;﻿using: 0.001<br>&nbsp;&nbsp;��: 0.001<br>&nbsp;&nbsp;GuidId: 0.000<br>&nbsp;&nbsp;��: 0.000<br>",
           "<b>Entropy:</b> 0.027<br><b>Pred:</b> 'gc<br><b>Top-5:</b><br>&nbsp;&nbsp;'gc: 0.002<br>&nbsp;&nbsp;﻿using: 0.001<br>&nbsp;&nbsp;.scalablytyped: 0.001<br>&nbsp;&nbsp;HeaderCode: 0.000<br>&nbsp;&nbsp;��: 0.000<br>",
           "<b>Entropy:</b> 0.205<br><b>Pred:</b> .com<br><b>Top-5:</b><br>&nbsp;&nbsp;.com: 0.031<br>&nbsp;&nbsp;.ca: 0.007<br>&nbsp;&nbsp; Offer: 0.005<br>&nbsp;&nbsp; Partnership: 0.003<br>&nbsp;&nbsp;-than: 0.003<br>",
           "<b>Entropy:</b> 0.049<br><b>Pred:</b>  be<br><b>Top-5:</b><br>&nbsp;&nbsp; be: 0.005<br>&nbsp;&nbsp;aeda: 0.001<br>&nbsp;&nbsp;字幕: 0.001<br>&nbsp;&nbsp; happen: 0.001<br>&nbsp;&nbsp;(PHP: 0.001<br>",
           "<b>Entropy:</b> 0.810<br><b>Pred:</b>  defined<br><b>Top-5:</b><br>&nbsp;&nbsp; defined: 0.081<br>&nbsp;&nbsp; quant: 0.076<br>&nbsp;&nbsp; reduced: 0.069<br>&nbsp;&nbsp; separated: 0.034<br>&nbsp;&nbsp; measured: 0.032<br>",
           "<b>Entropy:</b> 0.615<br><b>Pred:</b>  without<br><b>Top-5:</b><br>&nbsp;&nbsp; without: 0.783<br>&nbsp;&nbsp; unless: 0.112<br>&nbsp;&nbsp; apart: 0.018<br>&nbsp;&nbsp;without: 0.016<br>&nbsp;&nbsp; WITHOUT: 0.008<br>",
           "<b>Entropy:</b> 0.972<br><b>Pred:</b>  matter<br><b>Top-5:</b><br>&nbsp;&nbsp; matter: 0.481<br>&nbsp;&nbsp; Matter: 0.221<br>&nbsp;&nbsp; something: 0.037<br>&nbsp;&nbsp; thought: 0.028<br>&nbsp;&nbsp; some: 0.016<br>",
           "<b>Entropy:</b> 0.953<br><b>Pred:</b>  being<br><b>Top-5:</b><br>&nbsp;&nbsp; being: 0.431<br>&nbsp;&nbsp; it: 0.083<br>&nbsp;&nbsp;;: 0.079<br>&nbsp;&nbsp; It: 0.025<br>&nbsp;&nbsp; Being: 0.024<br>",
           "<b>Entropy:</b> 0.259<br><b>Pred:</b> faith<br><b>Top-5:</b><br>&nbsp;&nbsp;faith: 0.017<br>&nbsp;&nbsp; Thought: 0.014<br>&nbsp;&nbsp; Being: 0.013<br>&nbsp;&nbsp; Faith: 0.008<br>&nbsp;&nbsp; Knowledge: 0.007<br>",
           "<b>Entropy:</b> 1.020<br><b>Pred:</b>  understanding<br><b>Top-5:</b><br>&nbsp;&nbsp; understanding: 0.392<br>&nbsp;&nbsp; matter: 0.343<br>&nbsp;&nbsp; one: 0.033<br>&nbsp;&nbsp; Understanding: 0.024<br>&nbsp;&nbsp; sooner: 0.022<br>",
           "<b>Entropy:</b> 1.040<br><b>Pred:</b>  program<br><b>Top-5:</b><br>&nbsp;&nbsp; program: 0.481<br>&nbsp;&nbsp; can: 0.196<br>&nbsp;&nbsp; programme: 0.080<br>&nbsp;&nbsp; programs: 0.022<br>&nbsp;&nbsp; could: 0.021<br>",
           "<b>Entropy:</b> 0.162<br><b>Pred:</b>  ever<br><b>Top-5:</b><br>&nbsp;&nbsp; ever: 0.966<br>&nbsp;&nbsp; yet: 0.023<br>&nbsp;&nbsp; EVER: 0.005<br>&nbsp;&nbsp; understanding: 0.002<br>&nbsp;&nbsp; any: 0.001<br>",
           "<b>Entropy:</b> 0.463<br><b>Pred:</b>  understanding<br><b>Top-5:</b><br>&nbsp;&nbsp; understanding: 0.791<br>&nbsp;&nbsp; kind: 0.023<br>&nbsp;&nbsp; capacity: 0.018<br>&nbsp;&nbsp; ability: 0.016<br>&nbsp;&nbsp; chance: 0.011<br>",
           "<b>Entropy:</b> 1.082<br><b>Pred:</b>  whatever<br><b>Top-5:</b><br>&nbsp;&nbsp; whatever: 0.609<br>&nbsp;&nbsp; of: 0.172<br>&nbsp;&nbsp; whatsoever: 0.073<br>&nbsp;&nbsp;whatever: 0.049<br>&nbsp;&nbsp; Whatever: 0.044<br>",
           "<b>Entropy:</b> 0.718<br><b>Pred:</b>  itself<br><b>Top-5:</b><br>&nbsp;&nbsp; itself: 0.675<br>&nbsp;&nbsp; its: 0.297<br>&nbsp;&nbsp; what: 0.021<br>&nbsp;&nbsp; anything: 0.001<br>&nbsp;&nbsp; data: 0.000<br>"
          ],
          [
           "<b>Entropy:</b> 0.039<br><b>Pred:</b> 'gc<br><b>Top-5:</b><br>&nbsp;&nbsp;'gc: 0.003<br>&nbsp;&nbsp;﻿using: 0.001<br>&nbsp;&nbsp;actionDate: 0.001<br>&nbsp;&nbsp;GuidId: 0.001<br>&nbsp;&nbsp;��: 0.001<br>",
           "<b>Entropy:</b> 0.030<br><b>Pred:</b> 'gc<br><b>Top-5:</b><br>&nbsp;&nbsp;'gc: 0.002<br>&nbsp;&nbsp;﻿using: 0.001<br>&nbsp;&nbsp;HeaderCode: 0.001<br>&nbsp;&nbsp;GuidId: 0.001<br>&nbsp;&nbsp;actionDate: 0.001<br>",
           "<b>Entropy:</b> 0.137<br><b>Pred:</b> .com<br><b>Top-5:</b><br>&nbsp;&nbsp;.com: 0.012<br>&nbsp;&nbsp; Haram: 0.008<br>&nbsp;&nbsp;.gov: 0.004<br>&nbsp;&nbsp; LOL: 0.002<br>&nbsp;&nbsp; Offer: 0.002<br>",
           "<b>Entropy:</b> 0.038<br><b>Pred:</b> 字幕<br><b>Top-5:</b><br>&nbsp;&nbsp;字幕: 0.002<br>&nbsp;&nbsp;aeda: 0.001<br>&nbsp;&nbsp;페이지: 0.001<br>&nbsp;&nbsp;(PHP: 0.001<br>&nbsp;&nbsp;نوع: 0.001<br>",
           "<b>Entropy:</b> 0.498<br><b>Pred:</b>  quant<br><b>Top-5:</b><br>&nbsp;&nbsp; quant: 0.044<br>&nbsp;&nbsp; measured: 0.032<br>&nbsp;&nbsp; defined: 0.028<br>&nbsp;&nbsp; seper: 0.019<br>&nbsp;&nbsp; reduced: 0.018<br>",
           "<b>Entropy:</b> 0.960<br><b>Pred:</b>  without<br><b>Top-5:</b><br>&nbsp;&nbsp; without: 0.572<br>&nbsp;&nbsp; unless: 0.273<br>&nbsp;&nbsp;without: 0.060<br>&nbsp;&nbsp; apart: 0.014<br>&nbsp;&nbsp; WITHOUT: 0.013<br>",
           "<b>Entropy:</b> 0.876<br><b>Pred:</b>  matter<br><b>Top-5:</b><br>&nbsp;&nbsp; matter: 0.406<br>&nbsp;&nbsp; Matter: 0.348<br>&nbsp;&nbsp;matter: 0.013<br>&nbsp;&nbsp; something: 0.012<br>&nbsp;&nbsp; being: 0.007<br>",
           "<b>Entropy:</b> 0.834<br><b>Pred:</b>  being<br><b>Top-5:</b><br>&nbsp;&nbsp; being: 0.389<br>&nbsp;&nbsp; Being: 0.064<br>&nbsp;&nbsp;being: 0.045<br>&nbsp;&nbsp;;: 0.019<br>&nbsp;&nbsp; it: 0.019<br>",
           "<b>Entropy:</b> 0.176<br><b>Pred:</b>  compreh<br><b>Top-5:</b><br>&nbsp;&nbsp; compreh: 0.011<br>&nbsp;&nbsp;faith: 0.009<br>&nbsp;&nbsp; Knowledge: 0.006<br>&nbsp;&nbsp; Being: 0.005<br>&nbsp;&nbsp;rada: 0.005<br>",
           "<b>Entropy:</b> 0.989<br><b>Pred:</b>  understanding<br><b>Top-5:</b><br>&nbsp;&nbsp; understanding: 0.560<br>&nbsp;&nbsp; matter: 0.142<br>&nbsp;&nbsp; Understanding: 0.084<br>&nbsp;&nbsp;things: 0.027<br>&nbsp;&nbsp;emi: 0.021<br>",
           "<b>Entropy:</b> 1.059<br><b>Pred:</b>  program<br><b>Top-5:</b><br>&nbsp;&nbsp; program: 0.322<br>&nbsp;&nbsp; can: 0.104<br>&nbsp;&nbsp; programme: 0.065<br>&nbsp;&nbsp; ever: 0.056<br>&nbsp;&nbsp; understands: 0.036<br>",
           "<b>Entropy:</b> 0.055<br><b>Pred:</b>  ever<br><b>Top-5:</b><br>&nbsp;&nbsp; ever: 0.991<br>&nbsp;&nbsp; EVER: 0.007<br>&nbsp;&nbsp; yet: 0.001<br>&nbsp;&nbsp; understanding: 0.000<br>&nbsp;&nbsp; Ever: 0.000<br>",
           "<b>Entropy:</b> 0.480<br><b>Pred:</b>  understanding<br><b>Top-5:</b><br>&nbsp;&nbsp; understanding: 0.817<br>&nbsp;&nbsp; capacity: 0.026<br>&nbsp;&nbsp; Understanding: 0.020<br>&nbsp;&nbsp; kind: 0.019<br>&nbsp;&nbsp; ability: 0.016<br>",
           "<b>Entropy:</b> 0.982<br><b>Pred:</b>  of<br><b>Top-5:</b><br>&nbsp;&nbsp; of: 0.362<br>&nbsp;&nbsp; whatsoever: 0.122<br>&nbsp;&nbsp;of: 0.050<br>&nbsp;&nbsp; nor: 0.039<br>&nbsp;&nbsp; unless: 0.022<br>",
           "<b>Entropy:</b> 0.697<br><b>Pred:</b>  itself<br><b>Top-5:</b><br>&nbsp;&nbsp; itself: 0.712<br>&nbsp;&nbsp; its: 0.260<br>&nbsp;&nbsp; what: 0.024<br>&nbsp;&nbsp; anything: 0.002<br>&nbsp;&nbsp; Its: 0.000<br>"
          ],
          [
           "<b>Entropy:</b> 0.041<br><b>Pred:</b> 'gc<br><b>Top-5:</b><br>&nbsp;&nbsp;'gc: 0.003<br>&nbsp;&nbsp;﻿using: 0.001<br>&nbsp;&nbsp;actionDate: 0.001<br>&nbsp;&nbsp;GuidId: 0.001<br>&nbsp;&nbsp;��: 0.001<br>",
           "<b>Entropy:</b> 0.031<br><b>Pred:</b> 'gc<br><b>Top-5:</b><br>&nbsp;&nbsp;'gc: 0.001<br>&nbsp;&nbsp;﻿using: 0.001<br>&nbsp;&nbsp;GuidId: 0.001<br>&nbsp;&nbsp;actionDate: 0.001<br>&nbsp;&nbsp;HeaderCode: 0.001<br>",
           "<b>Entropy:</b> 0.082<br><b>Pred:</b>  META<br><b>Top-5:</b><br>&nbsp;&nbsp; META: 0.003<br>&nbsp;&nbsp;.com: 0.003<br>&nbsp;&nbsp;.wordpress: 0.003<br>&nbsp;&nbsp; Schultz: 0.002<br>&nbsp;&nbsp; Haram: 0.002<br>",
           "<b>Entropy:</b> 0.040<br><b>Pred:</b> 字幕<br><b>Top-5:</b><br>&nbsp;&nbsp;字幕: 0.002<br>&nbsp;&nbsp;페이지: 0.001<br>&nbsp;&nbsp;aeda: 0.001<br>&nbsp;&nbsp;نوع: 0.001<br>&nbsp;&nbsp;(PHP: 0.001<br>",
           "<b>Entropy:</b> 0.346<br><b>Pred:</b>  compartment<br><b>Top-5:</b><br>&nbsp;&nbsp; compartment: 0.026<br>&nbsp;&nbsp; measured: 0.024<br>&nbsp;&nbsp; quant: 0.014<br>&nbsp;&nbsp; defined: 0.013<br>&nbsp;&nbsp; gras: 0.010<br>",
           "<b>Entropy:</b> 0.541<br><b>Pred:</b>  without<br><b>Top-5:</b><br>&nbsp;&nbsp; without: 0.858<br>&nbsp;&nbsp;without: 0.060<br>&nbsp;&nbsp; unless: 0.058<br>&nbsp;&nbsp; WITHOUT: 0.010<br>&nbsp;&nbsp; Without: 0.005<br>",
           "<b>Entropy:</b> 0.836<br><b>Pred:</b>  matter<br><b>Top-5:</b><br>&nbsp;&nbsp; matter: 0.500<br>&nbsp;&nbsp; Matter: 0.314<br>&nbsp;&nbsp;matter: 0.012<br>&nbsp;&nbsp; being: 0.011<br>&nbsp;&nbsp; awareness: 0.004<br>",
           "<b>Entropy:</b> 0.838<br><b>Pred:</b>  being<br><b>Top-5:</b><br>&nbsp;&nbsp; being: 0.338<br>&nbsp;&nbsp; Being: 0.090<br>&nbsp;&nbsp;being: 0.036<br>&nbsp;&nbsp;Being: 0.018<br>&nbsp;&nbsp; it: 0.014<br>",
           "<b>Entropy:</b> 0.171<br><b>Pred:</b>  compreh<br><b>Top-5:</b><br>&nbsp;&nbsp; compreh: 0.014<br>&nbsp;&nbsp; Understanding: 0.009<br>&nbsp;&nbsp; Knowledge: 0.005<br>&nbsp;&nbsp;rada: 0.004<br>&nbsp;&nbsp;faith: 0.004<br>",
           "<b>Entropy:</b> 0.720<br><b>Pred:</b>  understanding<br><b>Top-5:</b><br>&nbsp;&nbsp; understanding: 0.672<br>&nbsp;&nbsp; Understanding: 0.250<br>&nbsp;&nbsp; matter: 0.010<br>&nbsp;&nbsp; sooner: 0.007<br>&nbsp;&nbsp;emi: 0.004<br>",
           "<b>Entropy:</b> 0.961<br><b>Pred:</b>  program<br><b>Top-5:</b><br>&nbsp;&nbsp; program: 0.167<br>&nbsp;&nbsp; programs: 0.092<br>&nbsp;&nbsp; can: 0.076<br>&nbsp;&nbsp;programs: 0.043<br>&nbsp;&nbsp; ever: 0.033<br>",
           "<b>Entropy:</b> 0.054<br><b>Pred:</b>  ever<br><b>Top-5:</b><br>&nbsp;&nbsp; ever: 0.991<br>&nbsp;&nbsp; EVER: 0.007<br>&nbsp;&nbsp; understanding: 0.001<br>&nbsp;&nbsp; yet: 0.000<br>&nbsp;&nbsp; Understanding: 0.000<br>",
           "<b>Entropy:</b> 0.392<br><b>Pred:</b>  understanding<br><b>Top-5:</b><br>&nbsp;&nbsp; understanding: 0.872<br>&nbsp;&nbsp; Understanding: 0.041<br>&nbsp;&nbsp; kind: 0.012<br>&nbsp;&nbsp; intelligence: 0.011<br>&nbsp;&nbsp; awareness: 0.008<br>",
           "<b>Entropy:</b> 0.724<br><b>Pred:</b>  of<br><b>Top-5:</b><br>&nbsp;&nbsp; of: 0.358<br>&nbsp;&nbsp; whatsoever: 0.030<br>&nbsp;&nbsp; nor: 0.025<br>&nbsp;&nbsp; unless: 0.023<br>&nbsp;&nbsp; Of: 0.018<br>",
           "<b>Entropy:</b> 0.415<br><b>Pred:</b>  itself<br><b>Top-5:</b><br>&nbsp;&nbsp; itself: 0.875<br>&nbsp;&nbsp; its: 0.115<br>&nbsp;&nbsp; what: 0.008<br>&nbsp;&nbsp; anything: 0.002<br>&nbsp;&nbsp; Its: 0.000<br>"
          ],
          [
           "<b>Entropy:</b> 0.041<br><b>Pred:</b> 'gc<br><b>Top-5:</b><br>&nbsp;&nbsp;'gc: 0.002<br>&nbsp;&nbsp;﻿using: 0.001<br>&nbsp;&nbsp;actionDate: 0.001<br>&nbsp;&nbsp;GuidId: 0.001<br>&nbsp;&nbsp;��: 0.001<br>",
           "<b>Entropy:</b> 0.030<br><b>Pred:</b> 'gc<br><b>Top-5:</b><br>&nbsp;&nbsp;'gc: 0.001<br>&nbsp;&nbsp;﻿using: 0.001<br>&nbsp;&nbsp;actionDate: 0.001<br>&nbsp;&nbsp;GuidId: 0.001<br>&nbsp;&nbsp;HeaderCode: 0.001<br>",
           "<b>Entropy:</b> 0.063<br><b>Pred:</b>  META<br><b>Top-5:</b><br>&nbsp;&nbsp; META: 0.003<br>&nbsp;&nbsp; Schultz: 0.002<br>&nbsp;&nbsp; Haram: 0.002<br>&nbsp;&nbsp;.wordpress: 0.002<br>&nbsp;&nbsp; Tanks: 0.002<br>",
           "<b>Entropy:</b> 0.042<br><b>Pred:</b> 字幕<br><b>Top-5:</b><br>&nbsp;&nbsp;字幕: 0.002<br>&nbsp;&nbsp;페이지: 0.002<br>&nbsp;&nbsp;نوع: 0.001<br>&nbsp;&nbsp;aeda: 0.001<br>&nbsp;&nbsp;سال: 0.001<br>",
           "<b>Entropy:</b> 0.334<br><b>Pred:</b>  compartment<br><b>Top-5:</b><br>&nbsp;&nbsp; compartment: 0.025<br>&nbsp;&nbsp; quant: 0.022<br>&nbsp;&nbsp; directly: 0.014<br>&nbsp;&nbsp; measured: 0.012<br>&nbsp;&nbsp; defined: 0.010<br>",
           "<b>Entropy:</b> 0.541<br><b>Pred:</b>  without<br><b>Top-5:</b><br>&nbsp;&nbsp; without: 0.852<br>&nbsp;&nbsp; unless: 0.063<br>&nbsp;&nbsp;without: 0.060<br>&nbsp;&nbsp; Without: 0.006<br>&nbsp;&nbsp; WITHOUT: 0.006<br>",
           "<b>Entropy:</b> 1.048<br><b>Pred:</b>  matter<br><b>Top-5:</b><br>&nbsp;&nbsp; matter: 0.408<br>&nbsp;&nbsp; Matter: 0.281<br>&nbsp;&nbsp; being: 0.067<br>&nbsp;&nbsp; Being: 0.019<br>&nbsp;&nbsp;matter: 0.017<br>",
           "<b>Entropy:</b> 0.899<br><b>Pred:</b>  being<br><b>Top-5:</b><br>&nbsp;&nbsp; being: 0.426<br>&nbsp;&nbsp; Being: 0.117<br>&nbsp;&nbsp;being: 0.053<br>&nbsp;&nbsp;Being: 0.028<br>&nbsp;&nbsp;Also: 0.005<br>",
           "<b>Entropy:</b> 0.257<br><b>Pred:</b>  compreh<br><b>Top-5:</b><br>&nbsp;&nbsp; compreh: 0.023<br>&nbsp;&nbsp; Understanding: 0.020<br>&nbsp;&nbsp;rada: 0.006<br>&nbsp;&nbsp; understanding: 0.006<br>&nbsp;&nbsp; Knowledge: 0.006<br>",
           "<b>Entropy:</b> 0.940<br><b>Pred:</b>  understanding<br><b>Top-5:</b><br>&nbsp;&nbsp; understanding: 0.553<br>&nbsp;&nbsp; Understanding: 0.254<br>&nbsp;&nbsp;emi: 0.030<br>&nbsp;&nbsp; matter: 0.027<br>&nbsp;&nbsp;thing: 0.015<br>",
           "<b>Entropy:</b> 0.989<br><b>Pred:</b>  program<br><b>Top-5:</b><br>&nbsp;&nbsp; program: 0.195<br>&nbsp;&nbsp; programs: 0.075<br>&nbsp;&nbsp; ever: 0.067<br>&nbsp;&nbsp; can: 0.066<br>&nbsp;&nbsp; programme: 0.034<br>",
           "<b>Entropy:</b> 0.047<br><b>Pred:</b>  ever<br><b>Top-5:</b><br>&nbsp;&nbsp; ever: 0.992<br>&nbsp;&nbsp; EVER: 0.006<br>&nbsp;&nbsp; yet: 0.001<br>&nbsp;&nbsp; Ever: 0.000<br>&nbsp;&nbsp;ever: 0.000<br>",
           "<b>Entropy:</b> 0.442<br><b>Pred:</b>  understanding<br><b>Top-5:</b><br>&nbsp;&nbsp; understanding: 0.854<br>&nbsp;&nbsp; Understanding: 0.051<br>&nbsp;&nbsp; intelligence: 0.017<br>&nbsp;&nbsp;kind: 0.010<br>&nbsp;&nbsp; awareness: 0.008<br>",
           "<b>Entropy:</b> 0.931<br><b>Pred:</b>  nor<br><b>Top-5:</b><br>&nbsp;&nbsp; nor: 0.184<br>&nbsp;&nbsp; of: 0.178<br>&nbsp;&nbsp; whatsoever: 0.067<br>&nbsp;&nbsp; unless: 0.020<br>&nbsp;&nbsp;nor: 0.012<br>",
           "<b>Entropy:</b> 0.203<br><b>Pred:</b>  itself<br><b>Top-5:</b><br>&nbsp;&nbsp; itself: 0.955<br>&nbsp;&nbsp; its: 0.037<br>&nbsp;&nbsp; what: 0.007<br>&nbsp;&nbsp; anything: 0.000<br>&nbsp;&nbsp; being: 0.000<br>"
          ],
          [
           "<b>Entropy:</b> 0.040<br><b>Pred:</b> 'gc<br><b>Top-5:</b><br>&nbsp;&nbsp;'gc: 0.002<br>&nbsp;&nbsp;﻿using: 0.001<br>&nbsp;&nbsp;actionDate: 0.001<br>&nbsp;&nbsp;GuidId: 0.001<br>&nbsp;&nbsp;��: 0.001<br>",
           "<b>Entropy:</b> 0.029<br><b>Pred:</b> 'gc<br><b>Top-5:</b><br>&nbsp;&nbsp;'gc: 0.001<br>&nbsp;&nbsp;﻿using: 0.001<br>&nbsp;&nbsp;actionDate: 0.001<br>&nbsp;&nbsp;GuidId: 0.001<br>&nbsp;&nbsp;HeaderCode: 0.001<br>",
           "<b>Entropy:</b> 0.070<br><b>Pred:</b> .Focused<br><b>Top-5:</b><br>&nbsp;&nbsp;.Focused: 0.003<br>&nbsp;&nbsp; META: 0.002<br>&nbsp;&nbsp; Tanks: 0.002<br>&nbsp;&nbsp; Elli: 0.002<br>&nbsp;&nbsp;OSP: 0.002<br>",
           "<b>Entropy:</b> 0.042<br><b>Pred:</b> 字幕<br><b>Top-5:</b><br>&nbsp;&nbsp;字幕: 0.002<br>&nbsp;&nbsp;페이지: 0.002<br>&nbsp;&nbsp;aeda: 0.001<br>&nbsp;&nbsp;نوع: 0.001<br>&nbsp;&nbsp;سال: 0.001<br>",
           "<b>Entropy:</b> 0.361<br><b>Pred:</b>  directly<br><b>Top-5:</b><br>&nbsp;&nbsp; directly: 0.033<br>&nbsp;&nbsp; quant: 0.021<br>&nbsp;&nbsp; compartment: 0.019<br>&nbsp;&nbsp; measured: 0.012<br>&nbsp;&nbsp; gras: 0.007<br>",
           "<b>Entropy:</b> 0.949<br><b>Pred:</b>  without<br><b>Top-5:</b><br>&nbsp;&nbsp; without: 0.562<br>&nbsp;&nbsp; unless: 0.324<br>&nbsp;&nbsp;without: 0.055<br>&nbsp;&nbsp;unless: 0.017<br>&nbsp;&nbsp; Without: 0.006<br>",
           "<b>Entropy:</b> 0.861<br><b>Pred:</b>  being<br><b>Top-5:</b><br>&nbsp;&nbsp; being: 0.406<br>&nbsp;&nbsp; Being: 0.104<br>&nbsp;&nbsp; knowing: 0.029<br>&nbsp;&nbsp; awareness: 0.022<br>&nbsp;&nbsp; Mind: 0.018<br>",
           "<b>Entropy:</b> 0.881<br><b>Pred:</b>  being<br><b>Top-5:</b><br>&nbsp;&nbsp; being: 0.553<br>&nbsp;&nbsp; Being: 0.113<br>&nbsp;&nbsp;being: 0.056<br>&nbsp;&nbsp;Being: 0.031<br>&nbsp;&nbsp; it: 0.007<br>",
           "<b>Entropy:</b> 0.224<br><b>Pred:</b>  compreh<br><b>Top-5:</b><br>&nbsp;&nbsp; compreh: 0.020<br>&nbsp;&nbsp; Understanding: 0.012<br>&nbsp;&nbsp;yor: 0.008<br>&nbsp;&nbsp;rada: 0.006<br>&nbsp;&nbsp; comprehension: 0.005<br>",
           "<b>Entropy:</b> 1.157<br><b>Pred:</b>  understanding<br><b>Top-5:</b><br>&nbsp;&nbsp; understanding: 0.434<br>&nbsp;&nbsp; Understanding: 0.279<br>&nbsp;&nbsp; matter: 0.141<br>&nbsp;&nbsp;emi: 0.039<br>&nbsp;&nbsp;thing: 0.007<br>",
           "<b>Entropy:</b> 0.977<br><b>Pred:</b>  can<br><b>Top-5:</b><br>&nbsp;&nbsp; can: 0.110<br>&nbsp;&nbsp; program: 0.098<br>&nbsp;&nbsp; ever: 0.085<br>&nbsp;&nbsp; programs: 0.056<br>&nbsp;&nbsp; could: 0.044<br>",
           "<b>Entropy:</b> 0.045<br><b>Pred:</b>  ever<br><b>Top-5:</b><br>&nbsp;&nbsp; ever: 0.993<br>&nbsp;&nbsp; EVER: 0.006<br>&nbsp;&nbsp;ever: 0.000<br>&nbsp;&nbsp; yet: 0.000<br>&nbsp;&nbsp; Ever: 0.000<br>",
           "<b>Entropy:</b> 0.496<br><b>Pred:</b>  understanding<br><b>Top-5:</b><br>&nbsp;&nbsp; understanding: 0.847<br>&nbsp;&nbsp; Understanding: 0.070<br>&nbsp;&nbsp; intelligence: 0.024<br>&nbsp;&nbsp; awareness: 0.011<br>&nbsp;&nbsp; ability: 0.006<br>",
           "<b>Entropy:</b> 0.783<br><b>Pred:</b>  nor<br><b>Top-5:</b><br>&nbsp;&nbsp; nor: 0.483<br>&nbsp;&nbsp; whatsoever: 0.087<br>&nbsp;&nbsp; of: 0.030<br>&nbsp;&nbsp;nor: 0.014<br>&nbsp;&nbsp; unless: 0.012<br>",
           "<b>Entropy:</b> 0.277<br><b>Pred:</b>  itself<br><b>Top-5:</b><br>&nbsp;&nbsp; itself: 0.935<br>&nbsp;&nbsp; its: 0.048<br>&nbsp;&nbsp; what: 0.014<br>&nbsp;&nbsp; anything: 0.001<br>&nbsp;&nbsp; being: 0.000<br>"
          ],
          [
           "<b>Entropy:</b> 0.041<br><b>Pred:</b> 'gc<br><b>Top-5:</b><br>&nbsp;&nbsp;'gc: 0.002<br>&nbsp;&nbsp;﻿using: 0.001<br>&nbsp;&nbsp;actionDate: 0.001<br>&nbsp;&nbsp;GuidId: 0.001<br>&nbsp;&nbsp;��: 0.001<br>",
           "<b>Entropy:</b> 0.028<br><b>Pred:</b> 'gc<br><b>Top-5:</b><br>&nbsp;&nbsp;'gc: 0.001<br>&nbsp;&nbsp;﻿using: 0.001<br>&nbsp;&nbsp;actionDate: 0.001<br>&nbsp;&nbsp;GuidId: 0.001<br>&nbsp;&nbsp;HeaderCode: 0.001<br>",
           "<b>Entropy:</b> 0.074<br><b>Pred:</b>  META<br><b>Top-5:</b><br>&nbsp;&nbsp; META: 0.003<br>&nbsp;&nbsp; DAR: 0.003<br>&nbsp;&nbsp;.Focused: 0.003<br>&nbsp;&nbsp;eum: 0.002<br>&nbsp;&nbsp;OSP: 0.002<br>",
           "<b>Entropy:</b> 0.042<br><b>Pred:</b> 字幕<br><b>Top-5:</b><br>&nbsp;&nbsp;字幕: 0.002<br>&nbsp;&nbsp;페이지: 0.002<br>&nbsp;&nbsp;aeda: 0.001<br>&nbsp;&nbsp;نوع: 0.001<br>&nbsp;&nbsp;سال: 0.001<br>",
           "<b>Entropy:</b> 0.393<br><b>Pred:</b>  directly<br><b>Top-5:</b><br>&nbsp;&nbsp; directly: 0.053<br>&nbsp;&nbsp; compartment: 0.026<br>&nbsp;&nbsp; measured: 0.014<br>&nbsp;&nbsp; quant: 0.009<br>&nbsp;&nbsp; measurement: 0.009<br>",
           "<b>Entropy:</b> 0.974<br><b>Pred:</b>  without<br><b>Top-5:</b><br>&nbsp;&nbsp; without: 0.533<br>&nbsp;&nbsp; unless: 0.359<br>&nbsp;&nbsp;without: 0.053<br>&nbsp;&nbsp;unless: 0.024<br>&nbsp;&nbsp; Without: 0.005<br>",
           "<b>Entropy:</b> 0.836<br><b>Pred:</b>  being<br><b>Top-5:</b><br>&nbsp;&nbsp; being: 0.224<br>&nbsp;&nbsp; Being: 0.067<br>&nbsp;&nbsp; knowing: 0.054<br>&nbsp;&nbsp; awareness: 0.026<br>&nbsp;&nbsp; Mind: 0.016<br>",
           "<b>Entropy:</b> 0.772<br><b>Pred:</b>  being<br><b>Top-5:</b><br>&nbsp;&nbsp; being: 0.406<br>&nbsp;&nbsp; Being: 0.064<br>&nbsp;&nbsp;being: 0.036<br>&nbsp;&nbsp;Being: 0.017<br>&nbsp;&nbsp; it: 0.009<br>",
           "<b>Entropy:</b> 0.190<br><b>Pred:</b>  Understanding<br><b>Top-5:</b><br>&nbsp;&nbsp; Understanding: 0.013<br>&nbsp;&nbsp; knowing: 0.008<br>&nbsp;&nbsp; gras: 0.007<br>&nbsp;&nbsp; compreh: 0.006<br>&nbsp;&nbsp; knowledge: 0.006<br>",
           "<b>Entropy:</b> 0.938<br><b>Pred:</b>  understanding<br><b>Top-5:</b><br>&nbsp;&nbsp; understanding: 0.509<br>&nbsp;&nbsp; Understanding: 0.338<br>&nbsp;&nbsp; matter: 0.031<br>&nbsp;&nbsp;emi: 0.017<br>&nbsp;&nbsp;thing: 0.011<br>",
           "<b>Entropy:</b> 0.803<br><b>Pred:</b>  ever<br><b>Top-5:</b><br>&nbsp;&nbsp; ever: 0.082<br>&nbsp;&nbsp; can: 0.061<br>&nbsp;&nbsp; programs: 0.055<br>&nbsp;&nbsp; program: 0.046<br>&nbsp;&nbsp; could: 0.039<br>",
           "<b>Entropy:</b> 0.996<br><b>Pred:</b>  ever<br><b>Top-5:</b><br>&nbsp;&nbsp; ever: 0.557<br>&nbsp;&nbsp; yet: 0.301<br>&nbsp;&nbsp;yet: 0.050<br>&nbsp;&nbsp; intelligence: 0.023<br>&nbsp;&nbsp; EVER: 0.018<br>",
           "<b>Entropy:</b> 0.552<br><b>Pred:</b>  understanding<br><b>Top-5:</b><br>&nbsp;&nbsp; understanding: 0.822<br>&nbsp;&nbsp; Understanding: 0.099<br>&nbsp;&nbsp; intelligence: 0.021<br>&nbsp;&nbsp; awareness: 0.012<br>&nbsp;&nbsp; idea: 0.005<br>",
           "<b>Entropy:</b> 0.660<br><b>Pred:</b>  nor<br><b>Top-5:</b><br>&nbsp;&nbsp; nor: 0.633<br>&nbsp;&nbsp; whatsoever: 0.062<br>&nbsp;&nbsp;nor: 0.018<br>&nbsp;&nbsp; unless: 0.016<br>&nbsp;&nbsp; Nor: 0.014<br>",
           "<b>Entropy:</b> 0.080<br><b>Pred:</b>  itself<br><b>Top-5:</b><br>&nbsp;&nbsp; itself: 0.986<br>&nbsp;&nbsp; its: 0.012<br>&nbsp;&nbsp; what: 0.002<br>&nbsp;&nbsp; anything: 0.000<br>&nbsp;&nbsp; Its: 0.000<br>"
          ],
          [
           "<b>Entropy:</b> 0.041<br><b>Pred:</b> 'gc<br><b>Top-5:</b><br>&nbsp;&nbsp;'gc: 0.002<br>&nbsp;&nbsp;actionDate: 0.001<br>&nbsp;&nbsp;﻿using: 0.001<br>&nbsp;&nbsp;GuidId: 0.001<br>&nbsp;&nbsp;��: 0.001<br>",
           "<b>Entropy:</b> 0.028<br><b>Pred:</b> 'gc<br><b>Top-5:</b><br>&nbsp;&nbsp;'gc: 0.001<br>&nbsp;&nbsp;﻿using: 0.001<br>&nbsp;&nbsp;actionDate: 0.001<br>&nbsp;&nbsp;GuidId: 0.001<br>&nbsp;&nbsp;HeaderCode: 0.001<br>",
           "<b>Entropy:</b> 0.078<br><b>Pred:</b> .Focused<br><b>Top-5:</b><br>&nbsp;&nbsp;.Focused: 0.003<br>&nbsp;&nbsp;eum: 0.003<br>&nbsp;&nbsp;ラック: 0.003<br>&nbsp;&nbsp;eview: 0.002<br>&nbsp;&nbsp;.Peek: 0.002<br>",
           "<b>Entropy:</b> 0.043<br><b>Pred:</b> 字幕<br><b>Top-5:</b><br>&nbsp;&nbsp;字幕: 0.002<br>&nbsp;&nbsp;페이지: 0.002<br>&nbsp;&nbsp;aeda: 0.001<br>&nbsp;&nbsp;نوع: 0.001<br>&nbsp;&nbsp;سال: 0.001<br>",
           "<b>Entropy:</b> 0.225<br><b>Pred:</b>  compartment<br><b>Top-5:</b><br>&nbsp;&nbsp; compartment: 0.028<br>&nbsp;&nbsp; knowing: 0.008<br>&nbsp;&nbsp; defined: 0.006<br>&nbsp;&nbsp; mens: 0.005<br>&nbsp;&nbsp; without: 0.005<br>",
           "<b>Entropy:</b> 0.951<br><b>Pred:</b>  without<br><b>Top-5:</b><br>&nbsp;&nbsp; without: 0.615<br>&nbsp;&nbsp; unless: 0.263<br>&nbsp;&nbsp;without: 0.065<br>&nbsp;&nbsp;unless: 0.018<br>&nbsp;&nbsp; Without: 0.012<br>",
           "<b>Entropy:</b> 0.685<br><b>Pred:</b>  being<br><b>Top-5:</b><br>&nbsp;&nbsp; being: 0.183<br>&nbsp;&nbsp; Being: 0.054<br>&nbsp;&nbsp; knowing: 0.025<br>&nbsp;&nbsp; Mind: 0.020<br>&nbsp;&nbsp; thoughts: 0.010<br>",
           "<b>Entropy:</b> 0.693<br><b>Pred:</b>  being<br><b>Top-5:</b><br>&nbsp;&nbsp; being: 0.495<br>&nbsp;&nbsp; Being: 0.040<br>&nbsp;&nbsp;being: 0.036<br>&nbsp;&nbsp;Being: 0.016<br>&nbsp;&nbsp; nor: 0.007<br>",
           "<b>Entropy:</b> 0.217<br><b>Pred:</b>  Understanding<br><b>Top-5:</b><br>&nbsp;&nbsp; Understanding: 0.017<br>&nbsp;&nbsp; understanding: 0.012<br>&nbsp;&nbsp; knowing: 0.009<br>&nbsp;&nbsp; knowledge: 0.006<br>&nbsp;&nbsp; Knowledge: 0.004<br>",
           "<b>Entropy:</b> 1.065<br><b>Pred:</b>  matter<br><b>Top-5:</b><br>&nbsp;&nbsp; matter: 0.238<br>&nbsp;&nbsp;emi: 0.141<br>&nbsp;&nbsp; understanding: 0.058<br>&nbsp;&nbsp; Understanding: 0.046<br>&nbsp;&nbsp;isy: 0.045<br>",
           "<b>Entropy:</b> 0.790<br><b>Pred:</b>  can<br><b>Top-5:</b><br>&nbsp;&nbsp; can: 0.128<br>&nbsp;&nbsp; ever: 0.112<br>&nbsp;&nbsp; programs: 0.036<br>&nbsp;&nbsp; could: 0.027<br>&nbsp;&nbsp; EVER: 0.016<br>",
           "<b>Entropy:</b> 0.463<br><b>Pred:</b>  ever<br><b>Top-5:</b><br>&nbsp;&nbsp; ever: 0.872<br>&nbsp;&nbsp; yet: 0.092<br>&nbsp;&nbsp; EVER: 0.016<br>&nbsp;&nbsp;yet: 0.010<br>&nbsp;&nbsp; intelligence: 0.002<br>",
           "<b>Entropy:</b> 1.031<br><b>Pred:</b>  understanding<br><b>Top-5:</b><br>&nbsp;&nbsp; understanding: 0.420<br>&nbsp;&nbsp; intelligence: 0.101<br>&nbsp;&nbsp; chance: 0.063<br>&nbsp;&nbsp; Understanding: 0.044<br>&nbsp;&nbsp; awareness: 0.038<br>",
           "<b>Entropy:</b> 0.815<br><b>Pred:</b>  nor<br><b>Top-5:</b><br>&nbsp;&nbsp; nor: 0.543<br>&nbsp;&nbsp; unless: 0.064<br>&nbsp;&nbsp;nor: 0.051<br>&nbsp;&nbsp; except: 0.022<br>&nbsp;&nbsp; whatsoever: 0.018<br>",
           "<b>Entropy:</b> 0.036<br><b>Pred:</b>  itself<br><b>Top-5:</b><br>&nbsp;&nbsp; itself: 0.994<br>&nbsp;&nbsp; its: 0.005<br>&nbsp;&nbsp; what: 0.000<br>&nbsp;&nbsp; Its: 0.000<br>&nbsp;&nbsp; himself: 0.000<br>"
          ],
          [
           "<b>Entropy:</b> 0.041<br><b>Pred:</b> 'gc<br><b>Top-5:</b><br>&nbsp;&nbsp;'gc: 0.002<br>&nbsp;&nbsp;actionDate: 0.001<br>&nbsp;&nbsp;﻿using: 0.001<br>&nbsp;&nbsp;GuidId: 0.001<br>&nbsp;&nbsp;��: 0.001<br>",
           "<b>Entropy:</b> 0.028<br><b>Pred:</b> 'gc<br><b>Top-5:</b><br>&nbsp;&nbsp;'gc: 0.001<br>&nbsp;&nbsp;﻿using: 0.001<br>&nbsp;&nbsp;actionDate: 0.001<br>&nbsp;&nbsp;GuidId: 0.001<br>&nbsp;&nbsp;페이지: 0.001<br>",
           "<b>Entropy:</b> 0.088<br><b>Pred:</b> eview<br><b>Top-5:</b><br>&nbsp;&nbsp;eview: 0.004<br>&nbsp;&nbsp;eum: 0.004<br>&nbsp;&nbsp;gard: 0.003<br>&nbsp;&nbsp; je: 0.003<br>&nbsp;&nbsp;BTN: 0.002<br>",
           "<b>Entropy:</b> 0.043<br><b>Pred:</b> 페이지<br><b>Top-5:</b><br>&nbsp;&nbsp;페이지: 0.002<br>&nbsp;&nbsp;字幕: 0.002<br>&nbsp;&nbsp;aeda: 0.001<br>&nbsp;&nbsp;نوع: 0.001<br>&nbsp;&nbsp;سال: 0.001<br>",
           "<b>Entropy:</b> 0.224<br><b>Pred:</b>  compartment<br><b>Top-5:</b><br>&nbsp;&nbsp; compartment: 0.021<br>&nbsp;&nbsp; without: 0.015<br>&nbsp;&nbsp;without: 0.007<br>&nbsp;&nbsp; mens: 0.005<br>&nbsp;&nbsp; Without: 0.004<br>",
           "<b>Entropy:</b> 0.741<br><b>Pred:</b>  without<br><b>Top-5:</b><br>&nbsp;&nbsp; without: 0.762<br>&nbsp;&nbsp;without: 0.113<br>&nbsp;&nbsp; unless: 0.052<br>&nbsp;&nbsp; Without: 0.024<br>&nbsp;&nbsp;Without: 0.009<br>",
           "<b>Entropy:</b> 0.514<br><b>Pred:</b>  being<br><b>Top-5:</b><br>&nbsp;&nbsp; being: 0.135<br>&nbsp;&nbsp; Being: 0.031<br>&nbsp;&nbsp; knowing: 0.012<br>&nbsp;&nbsp; Mind: 0.009<br>&nbsp;&nbsp;-know: 0.008<br>",
           "<b>Entropy:</b> 0.203<br><b>Pred:</b>  being<br><b>Top-5:</b><br>&nbsp;&nbsp; being: 0.012<br>&nbsp;&nbsp;ood: 0.011<br>&nbsp;&nbsp; it: 0.007<br>&nbsp;&nbsp; nor: 0.006<br>&nbsp;&nbsp;;:: 0.006<br>",
           "<b>Entropy:</b> 0.126<br><b>Pred:</b>  Understanding<br><b>Top-5:</b><br>&nbsp;&nbsp; Understanding: 0.007<br>&nbsp;&nbsp; understanding: 0.006<br>&nbsp;&nbsp; knowing: 0.004<br>&nbsp;&nbsp;knowledge: 0.003<br>&nbsp;&nbsp;eview: 0.003<br>",
           "<b>Entropy:</b> 1.114<br><b>Pred:</b>  understanding<br><b>Top-5:</b><br>&nbsp;&nbsp; understanding: 0.165<br>&nbsp;&nbsp; matter: 0.155<br>&nbsp;&nbsp;emi: 0.078<br>&nbsp;&nbsp; Understanding: 0.074<br>&nbsp;&nbsp;thing: 0.043<br>",
           "<b>Entropy:</b> 0.513<br><b>Pred:</b>  ever<br><b>Top-5:</b><br>&nbsp;&nbsp; ever: 0.074<br>&nbsp;&nbsp; can: 0.054<br>&nbsp;&nbsp;UGH: 0.014<br>&nbsp;&nbsp;ever: 0.013<br>&nbsp;&nbsp; programs: 0.010<br>",
           "<b>Entropy:</b> 0.201<br><b>Pred:</b>  ever<br><b>Top-5:</b><br>&nbsp;&nbsp; ever: 0.945<br>&nbsp;&nbsp; EVER: 0.014<br>&nbsp;&nbsp; yet: 0.010<br>&nbsp;&nbsp; been: 0.006<br>&nbsp;&nbsp; ability: 0.002<br>",
           "<b>Entropy:</b> 0.790<br><b>Pred:</b>  understanding<br><b>Top-5:</b><br>&nbsp;&nbsp; understanding: 0.624<br>&nbsp;&nbsp; Understanding: 0.079<br>&nbsp;&nbsp; idea: 0.029<br>&nbsp;&nbsp; chance: 0.028<br>&nbsp;&nbsp; awareness: 0.024<br>",
           "<b>Entropy:</b> 0.617<br><b>Pred:</b>  nor<br><b>Top-5:</b><br>&nbsp;&nbsp; nor: 0.128<br>&nbsp;&nbsp; of: 0.040<br>&nbsp;&nbsp; unless: 0.029<br>&nbsp;&nbsp;nor: 0.022<br>&nbsp;&nbsp; except: 0.008<br>",
           "<b>Entropy:</b> 0.119<br><b>Pred:</b>  itself<br><b>Top-5:</b><br>&nbsp;&nbsp; itself: 0.967<br>&nbsp;&nbsp; what: 0.011<br>&nbsp;&nbsp; its: 0.002<br>&nbsp;&nbsp; themselves: 0.002<br>&nbsp;&nbsp; anything: 0.001<br>"
          ],
          [
           "<b>Entropy:</b> 0.041<br><b>Pred:</b> 'gc<br><b>Top-5:</b><br>&nbsp;&nbsp;'gc: 0.002<br>&nbsp;&nbsp;actionDate: 0.001<br>&nbsp;&nbsp;﻿using: 0.001<br>&nbsp;&nbsp;GuidId: 0.001<br>&nbsp;&nbsp;��: 0.001<br>",
           "<b>Entropy:</b> 0.027<br><b>Pred:</b> 'gc<br><b>Top-5:</b><br>&nbsp;&nbsp;'gc: 0.001<br>&nbsp;&nbsp;﻿using: 0.001<br>&nbsp;&nbsp;actionDate: 0.001<br>&nbsp;&nbsp;GuidId: 0.001<br>&nbsp;&nbsp;페이지: 0.001<br>",
           "<b>Entropy:</b> 0.070<br><b>Pred:</b> сок<br><b>Top-5:</b><br>&nbsp;&nbsp;сок: 0.003<br>&nbsp;&nbsp;contri: 0.002<br>&nbsp;&nbsp;BTN: 0.002<br>&nbsp;&nbsp;_lane: 0.002<br>&nbsp;&nbsp;eum: 0.002<br>",
           "<b>Entropy:</b> 0.044<br><b>Pred:</b> 페이지<br><b>Top-5:</b><br>&nbsp;&nbsp;페이지: 0.002<br>&nbsp;&nbsp;字幕: 0.002<br>&nbsp;&nbsp;نوع: 0.001<br>&nbsp;&nbsp;aeda: 0.001<br>&nbsp;&nbsp;سال: 0.001<br>",
           "<b>Entropy:</b> 0.158<br><b>Pred:</b>  without<br><b>Top-5:</b><br>&nbsp;&nbsp; without: 0.008<br>&nbsp;&nbsp; objective: 0.007<br>&nbsp;&nbsp;ulet: 0.006<br>&nbsp;&nbsp;without: 0.006<br>&nbsp;&nbsp; seperate: 0.005<br>",
           "<b>Entropy:</b> 0.819<br><b>Pred:</b>  without<br><b>Top-5:</b><br>&nbsp;&nbsp; without: 0.701<br>&nbsp;&nbsp;without: 0.105<br>&nbsp;&nbsp; unless: 0.054<br>&nbsp;&nbsp; Without: 0.033<br>&nbsp;&nbsp;Without: 0.015<br>",
           "<b>Entropy:</b> 0.537<br><b>Pred:</b>  being<br><b>Top-5:</b><br>&nbsp;&nbsp; being: 0.191<br>&nbsp;&nbsp; Being: 0.034<br>&nbsp;&nbsp; Conscious: 0.008<br>&nbsp;&nbsp; somewhere: 0.007<br>&nbsp;&nbsp;being: 0.007<br>",
           "<b>Entropy:</b> 0.184<br><b>Pred:</b> nor<br><b>Top-5:</b><br>&nbsp;&nbsp;nor: 0.011<br>&nbsp;&nbsp;ton: 0.011<br>&nbsp;&nbsp; nor: 0.007<br>&nbsp;&nbsp;GORITH: 0.005<br>&nbsp;&nbsp;bine: 0.005<br>",
           "<b>Entropy:</b> 0.077<br><b>Pred:</b> overe<br><b>Top-5:</b><br>&nbsp;&nbsp;overe: 0.003<br>&nbsp;&nbsp;yor: 0.003<br>&nbsp;&nbsp;MISS: 0.002<br>&nbsp;&nbsp;gard: 0.002<br>&nbsp;&nbsp; knowing: 0.002<br>",
           "<b>Entropy:</b> 0.842<br><b>Pred:</b>  matter<br><b>Top-5:</b><br>&nbsp;&nbsp; matter: 0.446<br>&nbsp;&nbsp;thing: 0.061<br>&nbsp;&nbsp; Matter: 0.043<br>&nbsp;&nbsp; amount: 0.025<br>&nbsp;&nbsp;matter: 0.022<br>",
           "<b>Entropy:</b> 0.415<br><b>Pred:</b>  ever<br><b>Top-5:</b><br>&nbsp;&nbsp; ever: 0.088<br>&nbsp;&nbsp;UGH: 0.017<br>&nbsp;&nbsp; EVER: 0.011<br>&nbsp;&nbsp;ever: 0.009<br>&nbsp;&nbsp; machine: 0.008<br>",
           "<b>Entropy:</b> 0.122<br><b>Pred:</b>  ever<br><b>Top-5:</b><br>&nbsp;&nbsp; ever: 0.970<br>&nbsp;&nbsp; EVER: 0.019<br>&nbsp;&nbsp;ever: 0.001<br>&nbsp;&nbsp; yet: 0.001<br>&nbsp;&nbsp;-ever: 0.001<br>",
           "<b>Entropy:</b> 0.936<br><b>Pred:</b>  chance<br><b>Top-5:</b><br>&nbsp;&nbsp; chance: 0.133<br>&nbsp;&nbsp; understanding: 0.104<br>&nbsp;&nbsp; awareness: 0.060<br>&nbsp;&nbsp;sembl: 0.044<br>&nbsp;&nbsp; unless: 0.039<br>",
           "<b>Entropy:</b> 0.578<br><b>Pred:</b>  nor<br><b>Top-5:</b><br>&nbsp;&nbsp; nor: 0.120<br>&nbsp;&nbsp; unless: 0.032<br>&nbsp;&nbsp;nor: 0.031<br>&nbsp;&nbsp;aby: 0.012<br>&nbsp;&nbsp; consciousness: 0.012<br>",
           "<b>Entropy:</b> 1.083<br><b>Pred:</b>  itself<br><b>Top-5:</b><br>&nbsp;&nbsp; itself: 0.218<br>&nbsp;&nbsp; what: 0.152<br>&nbsp;&nbsp; anything: 0.109<br>&nbsp;&nbsp; being: 0.038<br>&nbsp;&nbsp; its: 0.028<br>"
          ],
          [
           "<b>Entropy:</b> 0.041<br><b>Pred:</b> 'gc<br><b>Top-5:</b><br>&nbsp;&nbsp;'gc: 0.002<br>&nbsp;&nbsp;﻿using: 0.001<br>&nbsp;&nbsp;actionDate: 0.001<br>&nbsp;&nbsp;GuidId: 0.001<br>&nbsp;&nbsp;��: 0.001<br>",
           "<b>Entropy:</b> 0.027<br><b>Pred:</b> 'gc<br><b>Top-5:</b><br>&nbsp;&nbsp;'gc: 0.001<br>&nbsp;&nbsp;﻿using: 0.001<br>&nbsp;&nbsp;actionDate: 0.001<br>&nbsp;&nbsp;페이지: 0.001<br>&nbsp;&nbsp;字幕: 0.001<br>",
           "<b>Entropy:</b> 0.069<br><b>Pred:</b> сок<br><b>Top-5:</b><br>&nbsp;&nbsp;сок: 0.003<br>&nbsp;&nbsp;BTN: 0.002<br>&nbsp;&nbsp; Gür: 0.002<br>&nbsp;&nbsp;eum: 0.002<br>&nbsp;&nbsp;Neg: 0.002<br>",
           "<b>Entropy:</b> 0.045<br><b>Pred:</b> 페이지<br><b>Top-5:</b><br>&nbsp;&nbsp;페이지: 0.002<br>&nbsp;&nbsp;字幕: 0.002<br>&nbsp;&nbsp;نوع: 0.001<br>&nbsp;&nbsp;aeda: 0.001<br>&nbsp;&nbsp;سال: 0.001<br>",
           "<b>Entropy:</b> 0.161<br><b>Pred:</b> wing<br><b>Top-5:</b><br>&nbsp;&nbsp;wing: 0.008<br>&nbsp;&nbsp; compartment: 0.008<br>&nbsp;&nbsp;PAL: 0.006<br>&nbsp;&nbsp; without: 0.006<br>&nbsp;&nbsp; Ont: 0.004<br>",
           "<b>Entropy:</b> 0.904<br><b>Pred:</b>  without<br><b>Top-5:</b><br>&nbsp;&nbsp; without: 0.513<br>&nbsp;&nbsp;without: 0.132<br>&nbsp;&nbsp; Without: 0.036<br>&nbsp;&nbsp;Without: 0.024<br>&nbsp;&nbsp; unless: 0.023<br>",
           "<b>Entropy:</b> 0.418<br><b>Pred:</b>  being<br><b>Top-5:</b><br>&nbsp;&nbsp; being: 0.053<br>&nbsp;&nbsp; corresponding: 0.032<br>&nbsp;&nbsp;rete: 0.022<br>&nbsp;&nbsp;.scalablytyped: 0.008<br>&nbsp;&nbsp; also: 0.005<br>",
           "<b>Entropy:</b> 0.311<br><b>Pred:</b> nor<br><b>Top-5:</b><br>&nbsp;&nbsp;nor: 0.029<br>&nbsp;&nbsp; nor: 0.025<br>&nbsp;&nbsp;ton: 0.010<br>&nbsp;&nbsp; Fallon: 0.008<br>&nbsp;&nbsp;ledge: 0.006<br>",
           "<b>Entropy:</b> 0.099<br><b>Pred:</b> alink<br><b>Top-5:</b><br>&nbsp;&nbsp;alink: 0.005<br>&nbsp;&nbsp;gard: 0.003<br>&nbsp;&nbsp;MISS: 0.003<br>&nbsp;&nbsp;isel: 0.003<br>&nbsp;&nbsp; nors: 0.003<br>",
           "<b>Entropy:</b> 0.763<br><b>Pred:</b>  matter<br><b>Top-5:</b><br>&nbsp;&nbsp; matter: 0.195<br>&nbsp;&nbsp; amount: 0.047<br>&nbsp;&nbsp;thing: 0.034<br>&nbsp;&nbsp; doubt: 0.028<br>&nbsp;&nbsp;isy: 0.023<br>",
           "<b>Entropy:</b> 0.638<br><b>Pred:</b>  ever<br><b>Top-5:</b><br>&nbsp;&nbsp; ever: 0.146<br>&nbsp;&nbsp;ize: 0.037<br>&nbsp;&nbsp;ever: 0.027<br>&nbsp;&nbsp; EVER: 0.020<br>&nbsp;&nbsp;ization: 0.014<br>",
           "<b>Entropy:</b> 0.475<br><b>Pred:</b>  ever<br><b>Top-5:</b><br>&nbsp;&nbsp; ever: 0.811<br>&nbsp;&nbsp; EVER: 0.075<br>&nbsp;&nbsp; yet: 0.008<br>&nbsp;&nbsp; been: 0.008<br>&nbsp;&nbsp;ever: 0.007<br>",
           "<b>Entropy:</b> 0.729<br><b>Pred:</b> sembl<br><b>Top-5:</b><br>&nbsp;&nbsp;sembl: 0.090<br>&nbsp;&nbsp; kind: 0.053<br>&nbsp;&nbsp; meaningful: 0.051<br>&nbsp;&nbsp;kind: 0.032<br>&nbsp;&nbsp; unless: 0.026<br>",
           "<b>Entropy:</b> 0.533<br><b>Pred:</b>  nor<br><b>Top-5:</b><br>&nbsp;&nbsp; nor: 0.118<br>&nbsp;&nbsp;nor: 0.048<br>&nbsp;&nbsp; unless: 0.012<br>&nbsp;&nbsp;atz: 0.009<br>&nbsp;&nbsp; Nor: 0.008<br>",
           "<b>Entropy:</b> 0.694<br><b>Pred:</b>  being<br><b>Top-5:</b><br>&nbsp;&nbsp; being: 0.122<br>&nbsp;&nbsp; itself: 0.100<br>&nbsp;&nbsp; its: 0.024<br>&nbsp;&nbsp; existence: 0.014<br>&nbsp;&nbsp; nor: 0.013<br>"
          ],
          [
           "<b>Entropy:</b> 0.041<br><b>Pred:</b> 'gc<br><b>Top-5:</b><br>&nbsp;&nbsp;'gc: 0.002<br>&nbsp;&nbsp;﻿using: 0.001<br>&nbsp;&nbsp;actionDate: 0.001<br>&nbsp;&nbsp;GuidId: 0.001<br>&nbsp;&nbsp;��: 0.001<br>",
           "<b>Entropy:</b> 0.028<br><b>Pred:</b> 'gc<br><b>Top-5:</b><br>&nbsp;&nbsp;'gc: 0.001<br>&nbsp;&nbsp;﻿using: 0.001<br>&nbsp;&nbsp;페이지: 0.001<br>&nbsp;&nbsp;字幕: 0.001<br>&nbsp;&nbsp;actionDate: 0.001<br>",
           "<b>Entropy:</b> 0.114<br><b>Pred:</b> eum<br><b>Top-5:</b><br>&nbsp;&nbsp;eum: 0.010<br>&nbsp;&nbsp;сок: 0.004<br>&nbsp;&nbsp;eview: 0.003<br>&nbsp;&nbsp;_misc: 0.003<br>&nbsp;&nbsp;ustil: 0.002<br>",
           "<b>Entropy:</b> 0.044<br><b>Pred:</b> 페이지<br><b>Top-5:</b><br>&nbsp;&nbsp;페이지: 0.002<br>&nbsp;&nbsp;字幕: 0.002<br>&nbsp;&nbsp;نوع: 0.001<br>&nbsp;&nbsp;aeda: 0.001<br>&nbsp;&nbsp;سال: 0.001<br>",
           "<b>Entropy:</b> 0.201<br><b>Pred:</b> Ace<br><b>Top-5:</b><br>&nbsp;&nbsp;Ace: 0.016<br>&nbsp;&nbsp;wing: 0.010<br>&nbsp;&nbsp; directly: 0.006<br>&nbsp;&nbsp; Ace: 0.006<br>&nbsp;&nbsp; without: 0.005<br>",
           "<b>Entropy:</b> 0.552<br><b>Pred:</b>  without<br><b>Top-5:</b><br>&nbsp;&nbsp; without: 0.076<br>&nbsp;&nbsp;-day: 0.049<br>&nbsp;&nbsp;without: 0.025<br>&nbsp;&nbsp;Without: 0.014<br>&nbsp;&nbsp;iment: 0.013<br>",
           "<b>Entropy:</b> 0.286<br><b>Pred:</b>  being<br><b>Top-5:</b><br>&nbsp;&nbsp; being: 0.038<br>&nbsp;&nbsp; corresponding: 0.011<br>&nbsp;&nbsp; adequate: 0.009<br>&nbsp;&nbsp; having: 0.007<br>&nbsp;&nbsp; Ade: 0.006<br>",
           "<b>Entropy:</b> 0.190<br><b>Pred:</b> ton<br><b>Top-5:</b><br>&nbsp;&nbsp;ton: 0.010<br>&nbsp;&nbsp; Mig: 0.009<br>&nbsp;&nbsp;ior: 0.008<br>&nbsp;&nbsp;nor: 0.007<br>&nbsp;&nbsp;bine: 0.006<br>",
           "<b>Entropy:</b> 0.129<br><b>Pred:</b> alink<br><b>Top-5:</b><br>&nbsp;&nbsp;alink: 0.007<br>&nbsp;&nbsp;overe: 0.005<br>&nbsp;&nbsp;isel: 0.005<br>&nbsp;&nbsp;MISS: 0.005<br>&nbsp;&nbsp;gard: 0.004<br>",
           "<b>Entropy:</b> 0.652<br><b>Pred:</b>  matter<br><b>Top-5:</b><br>&nbsp;&nbsp; matter: 0.124<br>&nbsp;&nbsp; amount: 0.050<br>&nbsp;&nbsp;isy: 0.036<br>&nbsp;&nbsp; longer: 0.016<br>&nbsp;&nbsp; doubt: 0.014<br>",
           "<b>Entropy:</b> 0.574<br><b>Pred:</b> ize<br><b>Top-5:</b><br>&nbsp;&nbsp;ize: 0.083<br>&nbsp;&nbsp; ever: 0.033<br>&nbsp;&nbsp;ization: 0.032<br>&nbsp;&nbsp;ized: 0.019<br>&nbsp;&nbsp;ever: 0.016<br>",
           "<b>Entropy:</b> 0.556<br><b>Pred:</b>  ever<br><b>Top-5:</b><br>&nbsp;&nbsp; ever: 0.721<br>&nbsp;&nbsp; EVER: 0.060<br>&nbsp;&nbsp; been: 0.026<br>&nbsp;&nbsp;htag: 0.007<br>&nbsp;&nbsp;ever: 0.004<br>",
           "<b>Entropy:</b> 0.582<br><b>Pred:</b> THING<br><b>Top-5:</b><br>&nbsp;&nbsp;THING: 0.096<br>&nbsp;&nbsp;sembl: 0.039<br>&nbsp;&nbsp;place: 0.028<br>&nbsp;&nbsp; meaningful: 0.020<br>&nbsp;&nbsp; apprec: 0.013<br>",
           "<b>Entropy:</b> 0.280<br><b>Pred:</b>  nor<br><b>Top-5:</b><br>&nbsp;&nbsp; nor: 0.029<br>&nbsp;&nbsp;nor: 0.023<br>&nbsp;&nbsp;TEX: 0.006<br>&nbsp;&nbsp;atz: 0.006<br>&nbsp;&nbsp;entin: 0.005<br>",
           "<b>Entropy:</b> 0.379<br><b>Pred:</b>  itself<br><b>Top-5:</b><br>&nbsp;&nbsp; itself: 0.039<br>&nbsp;&nbsp; existence: 0.024<br>&nbsp;&nbsp; being: 0.017<br>&nbsp;&nbsp; its: 0.014<br>&nbsp;&nbsp; how: 0.006<br>"
          ],
          [
           "<b>Entropy:</b> 0.042<br><b>Pred:</b> 'gc<br><b>Top-5:</b><br>&nbsp;&nbsp;'gc: 0.002<br>&nbsp;&nbsp;﻿using: 0.001<br>&nbsp;&nbsp;actionDate: 0.001<br>&nbsp;&nbsp;GuidId: 0.001<br>&nbsp;&nbsp;��: 0.001<br>",
           "<b>Entropy:</b> 0.029<br><b>Pred:</b> 'gc<br><b>Top-5:</b><br>&nbsp;&nbsp;'gc: 0.001<br>&nbsp;&nbsp;﻿using: 0.001<br>&nbsp;&nbsp;字幕: 0.001<br>&nbsp;&nbsp;페이지: 0.001<br>&nbsp;&nbsp;actionDate: 0.001<br>",
           "<b>Entropy:</b> 0.118<br><b>Pred:</b> eum<br><b>Top-5:</b><br>&nbsp;&nbsp;eum: 0.006<br>&nbsp;&nbsp;eview: 0.006<br>&nbsp;&nbsp;'gc: 0.006<br>&nbsp;&nbsp;inclu: 0.002<br>&nbsp;&nbsp;acente: 0.002<br>",
           "<b>Entropy:</b> 0.045<br><b>Pred:</b> 페이지<br><b>Top-5:</b><br>&nbsp;&nbsp;페이지: 0.002<br>&nbsp;&nbsp;字幕: 0.002<br>&nbsp;&nbsp;نوع: 0.001<br>&nbsp;&nbsp;aeda: 0.001<br>&nbsp;&nbsp;سال: 0.001<br>",
           "<b>Entropy:</b> 0.219<br><b>Pred:</b> Ace<br><b>Top-5:</b><br>&nbsp;&nbsp;Ace: 0.026<br>&nbsp;&nbsp;eing: 0.010<br>&nbsp;&nbsp; without: 0.007<br>&nbsp;&nbsp;deny: 0.004<br>&nbsp;&nbsp; directly: 0.004<br>",
           "<b>Entropy:</b> 0.463<br><b>Pred:</b> -day<br><b>Top-5:</b><br>&nbsp;&nbsp;-day: 0.066<br>&nbsp;&nbsp; simultaneously: 0.021<br>&nbsp;&nbsp; simult: 0.018<br>&nbsp;&nbsp; without: 0.016<br>&nbsp;&nbsp; itself: 0.015<br>",
           "<b>Entropy:</b> 0.139<br><b>Pred:</b>  adequate<br><b>Top-5:</b><br>&nbsp;&nbsp; adequate: 0.007<br>&nbsp;&nbsp; being: 0.006<br>&nbsp;&nbsp; corresponding: 0.005<br>&nbsp;&nbsp; correspondent: 0.005<br>&nbsp;&nbsp; Trace: 0.004<br>",
           "<b>Entropy:</b> 0.163<br><b>Pred:</b> ton<br><b>Top-5:</b><br>&nbsp;&nbsp;ton: 0.010<br>&nbsp;&nbsp; Norris: 0.009<br>&nbsp;&nbsp; Mig: 0.006<br>&nbsp;&nbsp; Fallon: 0.005<br>&nbsp;&nbsp;Of: 0.003<br>",
           "<b>Entropy:</b> 0.130<br><b>Pred:</b> alink<br><b>Top-5:</b><br>&nbsp;&nbsp;alink: 0.009<br>&nbsp;&nbsp;MISS: 0.007<br>&nbsp;&nbsp;eview: 0.003<br>&nbsp;&nbsp;yor: 0.003<br>&nbsp;&nbsp;gard: 0.003<br>",
           "<b>Entropy:</b> 0.632<br><b>Pred:</b>  nor<br><b>Top-5:</b><br>&nbsp;&nbsp; nor: 0.058<br>&nbsp;&nbsp; matter: 0.055<br>&nbsp;&nbsp;nor: 0.032<br>&nbsp;&nbsp;isy: 0.030<br>&nbsp;&nbsp; amount: 0.024<br>",
           "<b>Entropy:</b> 0.658<br><b>Pred:</b> ize<br><b>Top-5:</b><br>&nbsp;&nbsp;ize: 0.075<br>&nbsp;&nbsp; ever: 0.071<br>&nbsp;&nbsp;ization: 0.029<br>&nbsp;&nbsp; anywhere: 0.028<br>&nbsp;&nbsp;ever: 0.019<br>",
           "<b>Entropy:</b> 0.404<br><b>Pred:</b>  ever<br><b>Top-5:</b><br>&nbsp;&nbsp; ever: 0.821<br>&nbsp;&nbsp; EVER: 0.064<br>&nbsp;&nbsp;ever: 0.006<br>&nbsp;&nbsp; Ever: 0.003<br>&nbsp;&nbsp; been: 0.003<br>",
           "<b>Entropy:</b> 0.347<br><b>Pred:</b>  meaningful<br><b>Top-5:</b><br>&nbsp;&nbsp; meaningful: 0.024<br>&nbsp;&nbsp;place: 0.018<br>&nbsp;&nbsp; whatsoever: 0.016<br>&nbsp;&nbsp;sembl: 0.014<br>&nbsp;&nbsp;THING: 0.013<br>",
           "<b>Entropy:</b> 0.113<br><b>Pred:</b> aby<br><b>Top-5:</b><br>&nbsp;&nbsp;aby: 0.005<br>&nbsp;&nbsp;atz: 0.004<br>&nbsp;&nbsp;TEX: 0.004<br>&nbsp;&nbsp; Til: 0.004<br>&nbsp;&nbsp;shade: 0.003<br>",
           "<b>Entropy:</b> 0.403<br><b>Pred:</b>  itself<br><b>Top-5:</b><br>&nbsp;&nbsp; itself: 0.082<br>&nbsp;&nbsp; existence: 0.024<br>&nbsp;&nbsp; its: 0.009<br>&nbsp;&nbsp; ever: 0.008<br>&nbsp;&nbsp; being: 0.005<br>"
          ],
          [
           "<b>Entropy:</b> 0.041<br><b>Pred:</b> 'gc<br><b>Top-5:</b><br>&nbsp;&nbsp;'gc: 0.002<br>&nbsp;&nbsp;﻿using: 0.001<br>&nbsp;&nbsp;actionDate: 0.001<br>&nbsp;&nbsp;��: 0.001<br>&nbsp;&nbsp;GuidId: 0.001<br>",
           "<b>Entropy:</b> 0.029<br><b>Pred:</b> ﻿using<br><b>Top-5:</b><br>&nbsp;&nbsp;﻿using: 0.001<br>&nbsp;&nbsp;페이지: 0.001<br>&nbsp;&nbsp;字幕: 0.001<br>&nbsp;&nbsp;'gc: 0.001<br>&nbsp;&nbsp;actionDate: 0.001<br>",
           "<b>Entropy:</b> 0.121<br><b>Pred:</b> 'gc<br><b>Top-5:</b><br>&nbsp;&nbsp;'gc: 0.007<br>&nbsp;&nbsp;eview: 0.007<br>&nbsp;&nbsp;/Instruction: 0.004<br>&nbsp;&nbsp;Gratis: 0.003<br>&nbsp;&nbsp;inclu: 0.002<br>",
           "<b>Entropy:</b> 0.047<br><b>Pred:</b> 페이지<br><b>Top-5:</b><br>&nbsp;&nbsp;페이지: 0.002<br>&nbsp;&nbsp;字幕: 0.002<br>&nbsp;&nbsp;نوع: 0.001<br>&nbsp;&nbsp;aeda: 0.001<br>&nbsp;&nbsp;سال: 0.001<br>",
           "<b>Entropy:</b> 0.185<br><b>Pred:</b>  Cant<br><b>Top-5:</b><br>&nbsp;&nbsp; Cant: 0.014<br>&nbsp;&nbsp;Ace: 0.007<br>&nbsp;&nbsp;eing: 0.006<br>&nbsp;&nbsp; without: 0.006<br>&nbsp;&nbsp;IRST: 0.006<br>",
           "<b>Entropy:</b> 0.451<br><b>Pred:</b>  without<br><b>Top-5:</b><br>&nbsp;&nbsp; without: 0.068<br>&nbsp;&nbsp; itself: 0.023<br>&nbsp;&nbsp;entially: 0.018<br>&nbsp;&nbsp;Without: 0.013<br>&nbsp;&nbsp;ATIONAL: 0.011<br>",
           "<b>Entropy:</b> 0.104<br><b>Pred:</b> ongodb<br><b>Top-5:</b><br>&nbsp;&nbsp;ongodb: 0.005<br>&nbsp;&nbsp; consent: 0.005<br>&nbsp;&nbsp; necessarily: 0.003<br>&nbsp;&nbsp;urai: 0.003<br>&nbsp;&nbsp; adm: 0.002<br>",
           "<b>Entropy:</b> 0.152<br><b>Pred:</b> ton<br><b>Top-5:</b><br>&nbsp;&nbsp;ton: 0.015<br>&nbsp;&nbsp;bine: 0.005<br>&nbsp;&nbsp;ledge: 0.004<br>&nbsp;&nbsp;;:: 0.004<br>&nbsp;&nbsp; Norris: 0.004<br>",
           "<b>Entropy:</b> 0.085<br><b>Pred:</b> MISS<br><b>Top-5:</b><br>&nbsp;&nbsp;MISS: 0.005<br>&nbsp;&nbsp;alink: 0.003<br>&nbsp;&nbsp;anou: 0.003<br>&nbsp;&nbsp;eview: 0.002<br>&nbsp;&nbsp;페이지: 0.002<br>",
           "<b>Entropy:</b> 0.586<br><b>Pred:</b>  nor<br><b>Top-5:</b><br>&nbsp;&nbsp; nor: 0.095<br>&nbsp;&nbsp;nor: 0.044<br>&nbsp;&nbsp; vice: 0.030<br>&nbsp;&nbsp; matter: 0.015<br>&nbsp;&nbsp; doubt: 0.013<br>",
           "<b>Entropy:</b> 0.326<br><b>Pred:</b>  anywhere<br><b>Top-5:</b><br>&nbsp;&nbsp; anywhere: 0.023<br>&nbsp;&nbsp;ize: 0.018<br>&nbsp;&nbsp;unge: 0.015<br>&nbsp;&nbsp;without: 0.014<br>&nbsp;&nbsp;ization: 0.010<br>",
           "<b>Entropy:</b> 0.637<br><b>Pred:</b>  ever<br><b>Top-5:</b><br>&nbsp;&nbsp; ever: 0.240<br>&nbsp;&nbsp; EVER: 0.023<br>&nbsp;&nbsp; nor: 0.023<br>&nbsp;&nbsp; been: 0.015<br>&nbsp;&nbsp;been: 0.014<br>",
           "<b>Entropy:</b> 0.397<br><b>Pred:</b> THING<br><b>Top-5:</b><br>&nbsp;&nbsp;THING: 0.032<br>&nbsp;&nbsp; meaningful: 0.028<br>&nbsp;&nbsp; apprec: 0.016<br>&nbsp;&nbsp; Crush: 0.015<br>&nbsp;&nbsp; whatsoever: 0.014<br>",
           "<b>Entropy:</b> 0.169<br><b>Pred:</b>  nor<br><b>Top-5:</b><br>&nbsp;&nbsp; nor: 0.011<br>&nbsp;&nbsp; neither: 0.008<br>&nbsp;&nbsp;aby: 0.006<br>&nbsp;&nbsp;ITHER: 0.005<br>&nbsp;&nbsp;atz: 0.005<br>",
           "<b>Entropy:</b> 0.380<br><b>Pred:</b>  itself<br><b>Top-5:</b><br>&nbsp;&nbsp; itself: 0.087<br>&nbsp;&nbsp; its: 0.012<br>&nbsp;&nbsp; ever: 0.011<br>&nbsp;&nbsp; anything: 0.007<br>&nbsp;&nbsp;aha: 0.006<br>"
          ],
          [
           "<b>Entropy:</b> 0.041<br><b>Pred:</b> 'gc<br><b>Top-5:</b><br>&nbsp;&nbsp;'gc: 0.002<br>&nbsp;&nbsp;﻿using: 0.001<br>&nbsp;&nbsp;actionDate: 0.001<br>&nbsp;&nbsp;��: 0.001<br>&nbsp;&nbsp;GuidId: 0.001<br>",
           "<b>Entropy:</b> 0.030<br><b>Pred:</b> 字幕<br><b>Top-5:</b><br>&nbsp;&nbsp;字幕: 0.001<br>&nbsp;&nbsp;페이지: 0.001<br>&nbsp;&nbsp;﻿using: 0.001<br>&nbsp;&nbsp;'gc: 0.001<br>&nbsp;&nbsp;actionDate: 0.001<br>",
           "<b>Entropy:</b> 0.109<br><b>Pred:</b> 'gc<br><b>Top-5:</b><br>&nbsp;&nbsp;'gc: 0.009<br>&nbsp;&nbsp;contri: 0.004<br>&nbsp;&nbsp;Gratis: 0.003<br>&nbsp;&nbsp;/Instruction: 0.003<br>&nbsp;&nbsp; Gür: 0.003<br>",
           "<b>Entropy:</b> 0.049<br><b>Pred:</b> 페이지<br><b>Top-5:</b><br>&nbsp;&nbsp;페이지: 0.002<br>&nbsp;&nbsp;字幕: 0.002<br>&nbsp;&nbsp;نوع: 0.001<br>&nbsp;&nbsp;aeda: 0.001<br>&nbsp;&nbsp;سال: 0.001<br>",
           "<b>Entropy:</b> 0.138<br><b>Pred:</b>  itself<br><b>Top-5:</b><br>&nbsp;&nbsp; itself: 0.006<br>&nbsp;&nbsp; any: 0.005<br>&nbsp;&nbsp; without: 0.005<br>&nbsp;&nbsp;IRST: 0.005<br>&nbsp;&nbsp; sund: 0.004<br>",
           "<b>Entropy:</b> 0.217<br><b>Pred:</b> -day<br><b>Top-5:</b><br>&nbsp;&nbsp;-day: 0.015<br>&nbsp;&nbsp; itself: 0.011<br>&nbsp;&nbsp; without: 0.008<br>&nbsp;&nbsp;atı: 0.006<br>&nbsp;&nbsp;entially: 0.006<br>",
           "<b>Entropy:</b> 0.121<br><b>Pred:</b> lán<br><b>Top-5:</b><br>&nbsp;&nbsp;lán: 0.010<br>&nbsp;&nbsp;urai: 0.004<br>&nbsp;&nbsp;ouz: 0.003<br>&nbsp;&nbsp;yne: 0.003<br>&nbsp;&nbsp; consent: 0.003<br>",
           "<b>Entropy:</b> 0.169<br><b>Pred:</b> bine<br><b>Top-5:</b><br>&nbsp;&nbsp;bine: 0.017<br>&nbsp;&nbsp;ton: 0.007<br>&nbsp;&nbsp; dispens: 0.005<br>&nbsp;&nbsp;ably: 0.004<br>&nbsp;&nbsp; Fallon: 0.003<br>",
           "<b>Entropy:</b> 0.077<br><b>Pred:</b> MISS<br><b>Top-5:</b><br>&nbsp;&nbsp;MISS: 0.006<br>&nbsp;&nbsp;laz: 0.002<br>&nbsp;&nbsp;anou: 0.002<br>&nbsp;&nbsp;ommen: 0.002<br>&nbsp;&nbsp;…\": 0.002<br>",
           "<b>Entropy:</b> 0.433<br><b>Pred:</b>  vice<br><b>Top-5:</b><br>&nbsp;&nbsp; vice: 0.073<br>&nbsp;&nbsp; nor: 0.016<br>&nbsp;&nbsp;isy: 0.016<br>&nbsp;&nbsp; doubt: 0.013<br>&nbsp;&nbsp;orsk: 0.012<br>",
           "<b>Entropy:</b> 0.268<br><b>Pred:</b> ize<br><b>Top-5:</b><br>&nbsp;&nbsp;ize: 0.040<br>&nbsp;&nbsp;ization: 0.008<br>&nbsp;&nbsp;unge: 0.008<br>&nbsp;&nbsp;enko: 0.006<br>&nbsp;&nbsp;ium: 0.006<br>",
           "<b>Entropy:</b> 0.335<br><b>Pred:</b>  ever<br><b>Top-5:</b><br>&nbsp;&nbsp; ever: 0.067<br>&nbsp;&nbsp; been: 0.010<br>&nbsp;&nbsp;been: 0.008<br>&nbsp;&nbsp; nor: 0.007<br>&nbsp;&nbsp; Ecc: 0.006<br>",
           "<b>Entropy:</b> 0.341<br><b>Pred:</b> THING<br><b>Top-5:</b><br>&nbsp;&nbsp;THING: 0.035<br>&nbsp;&nbsp;sembl: 0.021<br>&nbsp;&nbsp; meaningful: 0.012<br>&nbsp;&nbsp; particular: 0.012<br>&nbsp;&nbsp;located: 0.008<br>",
           "<b>Entropy:</b> 0.142<br><b>Pred:</b> lun<br><b>Top-5:</b><br>&nbsp;&nbsp;lun: 0.010<br>&nbsp;&nbsp;ink: 0.005<br>&nbsp;&nbsp;TEX: 0.004<br>&nbsp;&nbsp;ht: 0.004<br>&nbsp;&nbsp;inks: 0.004<br>",
           "<b>Entropy:</b> 0.275<br><b>Pred:</b>  itself<br><b>Top-5:</b><br>&nbsp;&nbsp; itself: 0.034<br>&nbsp;&nbsp; its: 0.017<br>&nbsp;&nbsp; themselves: 0.007<br>&nbsp;&nbsp; Ether: 0.006<br>&nbsp;&nbsp;screen: 0.005<br>"
          ],
          [
           "<b>Entropy:</b> 0.038<br><b>Pred:</b> 'gc<br><b>Top-5:</b><br>&nbsp;&nbsp;'gc: 0.002<br>&nbsp;&nbsp;﻿using: 0.001<br>&nbsp;&nbsp;��: 0.001<br>&nbsp;&nbsp;actionDate: 0.001<br>&nbsp;&nbsp;GuidId: 0.001<br>",
           "<b>Entropy:</b> 0.030<br><b>Pred:</b> 字幕<br><b>Top-5:</b><br>&nbsp;&nbsp;字幕: 0.001<br>&nbsp;&nbsp;페이지: 0.001<br>&nbsp;&nbsp;﻿using: 0.001<br>&nbsp;&nbsp;'gc: 0.001<br>&nbsp;&nbsp;actionDate: 0.001<br>",
           "<b>Entropy:</b> 0.078<br><b>Pred:</b> 'gc<br><b>Top-5:</b><br>&nbsp;&nbsp;'gc: 0.004<br>&nbsp;&nbsp;contri: 0.003<br>&nbsp;&nbsp;eview: 0.003<br>&nbsp;&nbsp; Gür: 0.002<br>&nbsp;&nbsp; je: 0.002<br>",
           "<b>Entropy:</b> 0.051<br><b>Pred:</b> 페이지<br><b>Top-5:</b><br>&nbsp;&nbsp;페이지: 0.002<br>&nbsp;&nbsp;字幕: 0.002<br>&nbsp;&nbsp;نوع: 0.001<br>&nbsp;&nbsp;aeda: 0.001<br>&nbsp;&nbsp;سال: 0.001<br>",
           "<b>Entropy:</b> 0.134<br><b>Pred:</b>  any<br><b>Top-5:</b><br>&nbsp;&nbsp; any: 0.006<br>&nbsp;&nbsp;wing: 0.006<br>&nbsp;&nbsp;izer: 0.006<br>&nbsp;&nbsp; pyramid: 0.004<br>&nbsp;&nbsp;Ace: 0.004<br>",
           "<b>Entropy:</b> 0.217<br><b>Pred:</b> -day<br><b>Top-5:</b><br>&nbsp;&nbsp;-day: 0.019<br>&nbsp;&nbsp; itself: 0.009<br>&nbsp;&nbsp;oned: 0.008<br>&nbsp;&nbsp;üml: 0.006<br>&nbsp;&nbsp;ational: 0.006<br>",
           "<b>Entropy:</b> 0.123<br><b>Pred:</b> lán<br><b>Top-5:</b><br>&nbsp;&nbsp;lán: 0.005<br>&nbsp;&nbsp; correspondent: 0.005<br>&nbsp;&nbsp;ceiver: 0.005<br>&nbsp;&nbsp; Ade: 0.004<br>&nbsp;&nbsp;eve: 0.004<br>",
           "<b>Entropy:</b> 0.149<br><b>Pred:</b> bine<br><b>Top-5:</b><br>&nbsp;&nbsp;bine: 0.014<br>&nbsp;&nbsp; : 0.005<br>&nbsp;&nbsp; beat: 0.004<br>&nbsp;&nbsp;fone: 0.004<br>&nbsp;&nbsp;560: 0.003<br>",
           "<b>Entropy:</b> 0.076<br><b>Pred:</b> ommen<br><b>Top-5:</b><br>&nbsp;&nbsp;ommen: 0.004<br>&nbsp;&nbsp;anou: 0.003<br>&nbsp;&nbsp;MISS: 0.003<br>&nbsp;&nbsp;…\": 0.002<br>&nbsp;&nbsp;laz: 0.002<br>",
           "<b>Entropy:</b> 0.212<br><b>Pred:</b>  nor<br><b>Top-5:</b><br>&nbsp;&nbsp; nor: 0.011<br>&nbsp;&nbsp;-body: 0.010<br>&nbsp;&nbsp; doubt: 0.009<br>&nbsp;&nbsp;isy: 0.009<br>&nbsp;&nbsp; Grey: 0.006<br>",
           "<b>Entropy:</b> 0.209<br><b>Pred:</b> ization<br><b>Top-5:</b><br>&nbsp;&nbsp;ization: 0.018<br>&nbsp;&nbsp;ize: 0.012<br>&nbsp;&nbsp;itas: 0.007<br>&nbsp;&nbsp;ized: 0.005<br>&nbsp;&nbsp;jack: 0.005<br>",
           "<b>Entropy:</b> 0.326<br><b>Pred:</b>  nor<br><b>Top-5:</b><br>&nbsp;&nbsp; nor: 0.022<br>&nbsp;&nbsp; unless: 0.016<br>&nbsp;&nbsp;unless: 0.015<br>&nbsp;&nbsp;alue: 0.015<br>&nbsp;&nbsp; ever: 0.011<br>",
           "<b>Entropy:</b> 0.312<br><b>Pred:</b> sembl<br><b>Top-5:</b><br>&nbsp;&nbsp;sembl: 0.031<br>&nbsp;&nbsp;THING: 0.015<br>&nbsp;&nbsp;ht: 0.014<br>&nbsp;&nbsp;place: 0.010<br>&nbsp;&nbsp; meaningful: 0.008<br>",
           "<b>Entropy:</b> 0.135<br><b>Pred:</b> ht<br><b>Top-5:</b><br>&nbsp;&nbsp;ht: 0.008<br>&nbsp;&nbsp;inks: 0.005<br>&nbsp;&nbsp;ethyst: 0.005<br>&nbsp;&nbsp;HT: 0.004<br>&nbsp;&nbsp; Himself: 0.003<br>",
           "<b>Entropy:</b> 0.230<br><b>Pred:</b>  itself<br><b>Top-5:</b><br>&nbsp;&nbsp; itself: 0.024<br>&nbsp;&nbsp;vetica: 0.015<br>&nbsp;&nbsp; its: 0.007<br>&nbsp;&nbsp; Bison: 0.005<br>&nbsp;&nbsp;uja: 0.003<br>"
          ],
          [
           "<b>Entropy:</b> 0.037<br><b>Pred:</b> 'gc<br><b>Top-5:</b><br>&nbsp;&nbsp;'gc: 0.002<br>&nbsp;&nbsp;﻿using: 0.001<br>&nbsp;&nbsp;��: 0.001<br>&nbsp;&nbsp;actionDate: 0.001<br>&nbsp;&nbsp;GuidId: 0.001<br>",
           "<b>Entropy:</b> 0.031<br><b>Pred:</b> 字幕<br><b>Top-5:</b><br>&nbsp;&nbsp;字幕: 0.001<br>&nbsp;&nbsp;페이지: 0.001<br>&nbsp;&nbsp;﻿using: 0.001<br>&nbsp;&nbsp;'gc: 0.001<br>&nbsp;&nbsp;نوع: 0.001<br>",
           "<b>Entropy:</b> 0.073<br><b>Pred:</b> 'gc<br><b>Top-5:</b><br>&nbsp;&nbsp;'gc: 0.005<br>&nbsp;&nbsp;contri: 0.002<br>&nbsp;&nbsp;/Instruction: 0.002<br>&nbsp;&nbsp;��: 0.002<br>&nbsp;&nbsp;krv: 0.002<br>",
           "<b>Entropy:</b> 0.053<br><b>Pred:</b> 페이지<br><b>Top-5:</b><br>&nbsp;&nbsp;페이지: 0.003<br>&nbsp;&nbsp;字幕: 0.002<br>&nbsp;&nbsp;نوع: 0.001<br>&nbsp;&nbsp;سال: 0.001<br>&nbsp;&nbsp;aeda: 0.001<br>",
           "<b>Entropy:</b> 0.177<br><b>Pred:</b> wing<br><b>Top-5:</b><br>&nbsp;&nbsp;wing: 0.017<br>&nbsp;&nbsp;mate: 0.010<br>&nbsp;&nbsp;ogue: 0.005<br>&nbsp;&nbsp;ix: 0.004<br>&nbsp;&nbsp;onical: 0.003<br>",
           "<b>Entropy:</b> 0.137<br><b>Pred:</b>  itself<br><b>Top-5:</b><br>&nbsp;&nbsp; itself: 0.008<br>&nbsp;&nbsp;implify: 0.006<br>&nbsp;&nbsp; without: 0.004<br>&nbsp;&nbsp;OMP: 0.004<br>&nbsp;&nbsp;üml: 0.004<br>",
           "<b>Entropy:</b> 0.219<br><b>Pred:</b>  Pf<br><b>Top-5:</b><br>&nbsp;&nbsp; Pf: 0.017<br>&nbsp;&nbsp; Block: 0.011<br>&nbsp;&nbsp;ceiver: 0.010<br>&nbsp;&nbsp; A: 0.006<br>&nbsp;&nbsp; necessarily: 0.004<br>",
           "<b>Entropy:</b> 0.152<br><b>Pred:</b> bine<br><b>Top-5:</b><br>&nbsp;&nbsp;bine: 0.009<br>&nbsp;&nbsp; tire: 0.007<br>&nbsp;&nbsp; : 0.006<br>&nbsp;&nbsp; foul: 0.005<br>&nbsp;&nbsp;fone: 0.003<br>",
           "<b>Entropy:</b> 0.074<br><b>Pred:</b> ommen<br><b>Top-5:</b><br>&nbsp;&nbsp;ommen: 0.004<br>&nbsp;&nbsp;MISS: 0.002<br>&nbsp;&nbsp;anou: 0.002<br>&nbsp;&nbsp;سال: 0.002<br>&nbsp;&nbsp;…\": 0.002<br>",
           "<b>Entropy:</b> 0.206<br><b>Pred:</b> akh<br><b>Top-5:</b><br>&nbsp;&nbsp;akh: 0.011<br>&nbsp;&nbsp;/no: 0.011<br>&nbsp;&nbsp; longer: 0.007<br>&nbsp;&nbsp; doubt: 0.007<br>&nbsp;&nbsp; nor: 0.007<br>",
           "<b>Entropy:</b> 0.155<br><b>Pred:</b> idge<br><b>Top-5:</b><br>&nbsp;&nbsp;idge: 0.013<br>&nbsp;&nbsp;/ion: 0.005<br>&nbsp;&nbsp;ostel: 0.005<br>&nbsp;&nbsp;/com: 0.004<br>&nbsp;&nbsp;inyin: 0.004<br>",
           "<b>Entropy:</b> 0.460<br><b>Pred:</b>  nor<br><b>Top-5:</b><br>&nbsp;&nbsp; nor: 0.058<br>&nbsp;&nbsp;unless: 0.027<br>&nbsp;&nbsp; unless: 0.019<br>&nbsp;&nbsp;lf: 0.015<br>&nbsp;&nbsp; ever: 0.014<br>",
           "<b>Entropy:</b> 0.265<br><b>Pred:</b>  Cann<br><b>Top-5:</b><br>&nbsp;&nbsp; Cann: 0.034<br>&nbsp;&nbsp;orsk: 0.014<br>&nbsp;&nbsp;umber: 0.009<br>&nbsp;&nbsp; comparable: 0.004<br>&nbsp;&nbsp; meaningful: 0.004<br>",
           "<b>Entropy:</b> 0.123<br><b>Pred:</b>  foul<br><b>Top-5:</b><br>&nbsp;&nbsp; foul: 0.007<br>&nbsp;&nbsp;prs: 0.005<br>&nbsp;&nbsp; Second: 0.004<br>&nbsp;&nbsp; Baxter: 0.003<br>&nbsp;&nbsp; bladder: 0.003<br>",
           "<b>Entropy:</b> 0.156<br><b>Pred:</b>  itself<br><b>Top-5:</b><br>&nbsp;&nbsp; itself: 0.019<br>&nbsp;&nbsp;lobber: 0.004<br>&nbsp;&nbsp;TAIL: 0.004<br>&nbsp;&nbsp;sumer: 0.003<br>&nbsp;&nbsp; Baxter: 0.003<br>"
          ],
          [
           "<b>Entropy:</b> 0.034<br><b>Pred:</b> 'gc<br><b>Top-5:</b><br>&nbsp;&nbsp;'gc: 0.002<br>&nbsp;&nbsp;﻿using: 0.001<br>&nbsp;&nbsp;��: 0.001<br>&nbsp;&nbsp;字幕: 0.001<br>&nbsp;&nbsp;actionDate: 0.001<br>",
           "<b>Entropy:</b> 0.034<br><b>Pred:</b> 페이지<br><b>Top-5:</b><br>&nbsp;&nbsp;페이지: 0.002<br>&nbsp;&nbsp;字幕: 0.002<br>&nbsp;&nbsp;﻿using: 0.001<br>&nbsp;&nbsp;نوع: 0.001<br>&nbsp;&nbsp;Produto: 0.001<br>",
           "<b>Entropy:</b> 0.077<br><b>Pred:</b> 'gc<br><b>Top-5:</b><br>&nbsp;&nbsp;'gc: 0.005<br>&nbsp;&nbsp;contri: 0.003<br>&nbsp;&nbsp;krv: 0.002<br>&nbsp;&nbsp;/Instruction: 0.002<br>&nbsp;&nbsp;��: 0.002<br>",
           "<b>Entropy:</b> 0.057<br><b>Pred:</b> 페이지<br><b>Top-5:</b><br>&nbsp;&nbsp;페이지: 0.003<br>&nbsp;&nbsp;字幕: 0.003<br>&nbsp;&nbsp;نوع: 0.002<br>&nbsp;&nbsp;سال: 0.001<br>&nbsp;&nbsp;aeda: 0.001<br>",
           "<b>Entropy:</b> 0.163<br><b>Pred:</b> wing<br><b>Top-5:</b><br>&nbsp;&nbsp;wing: 0.018<br>&nbsp;&nbsp;ogue: 0.006<br>&nbsp;&nbsp; Wing: 0.004<br>&nbsp;&nbsp;onical: 0.004<br>&nbsp;&nbsp; Moff: 0.003<br>",
           "<b>Entropy:</b> 0.130<br><b>Pred:</b>  unless<br><b>Top-5:</b><br>&nbsp;&nbsp; unless: 0.008<br>&nbsp;&nbsp; tense: 0.005<br>&nbsp;&nbsp;eted: 0.005<br>&nbsp;&nbsp;ROID: 0.004<br>&nbsp;&nbsp;atı: 0.004<br>",
           "<b>Entropy:</b> 0.194<br><b>Pred:</b> folio<br><b>Top-5:</b><br>&nbsp;&nbsp;folio: 0.012<br>&nbsp;&nbsp;itten: 0.012<br>&nbsp;&nbsp;ecessarily: 0.008<br>&nbsp;&nbsp;ogne: 0.005<br>&nbsp;&nbsp;ceiver: 0.004<br>",
           "<b>Entropy:</b> 0.296<br><b>Pred:</b> bine<br><b>Top-5:</b><br>&nbsp;&nbsp;bine: 0.033<br>&nbsp;&nbsp; foul: 0.016<br>&nbsp;&nbsp;nd: 0.012<br>&nbsp;&nbsp; Mig: 0.007<br>&nbsp;&nbsp;475: 0.006<br>",
           "<b>Entropy:</b> 0.049<br><b>Pred:</b> ommen<br><b>Top-5:</b><br>&nbsp;&nbsp;ommen: 0.002<br>&nbsp;&nbsp;anou: 0.002<br>&nbsp;&nbsp;krv: 0.001<br>&nbsp;&nbsp;lotte: 0.001<br>&nbsp;&nbsp;overe: 0.001<br>",
           "<b>Entropy:</b> 0.447<br><b>Pred:</b>  matter<br><b>Top-5:</b><br>&nbsp;&nbsp; matter: 0.112<br>&nbsp;&nbsp; doubt: 0.012<br>&nbsp;&nbsp;linger: 0.012<br>&nbsp;&nbsp;isce: 0.011<br>&nbsp;&nbsp;matter: 0.010<br>",
           "<b>Entropy:</b> 0.155<br><b>Pred:</b> idge<br><b>Top-5:</b><br>&nbsp;&nbsp;idge: 0.011<br>&nbsp;&nbsp; overtime: 0.007<br>&nbsp;&nbsp;ize: 0.005<br>&nbsp;&nbsp;aar: 0.004<br>&nbsp;&nbsp; nay: 0.004<br>",
           "<b>Entropy:</b> 0.578<br><b>Pred:</b>  nor<br><b>Top-5:</b><br>&nbsp;&nbsp; nor: 0.251<br>&nbsp;&nbsp; unless: 0.026<br>&nbsp;&nbsp; yet: 0.019<br>&nbsp;&nbsp;unless: 0.006<br>&nbsp;&nbsp;endar: 0.005<br>",
           "<b>Entropy:</b> 0.195<br><b>Pred:</b>  Cann<br><b>Top-5:</b><br>&nbsp;&nbsp; Cann: 0.012<br>&nbsp;&nbsp; unless: 0.009<br>&nbsp;&nbsp;lectric: 0.007<br>&nbsp;&nbsp; nor: 0.007<br>&nbsp;&nbsp; except: 0.006<br>",
           "<b>Entropy:</b> 0.156<br><b>Pred:</b>  foul<br><b>Top-5:</b><br>&nbsp;&nbsp; foul: 0.013<br>&nbsp;&nbsp;PELL: 0.005<br>&nbsp;&nbsp; pret: 0.005<br>&nbsp;&nbsp;idge: 0.005<br>&nbsp;&nbsp;ibal: 0.004<br>",
           "<b>Entropy:</b> 0.111<br><b>Pred:</b> lobber<br><b>Top-5:</b><br>&nbsp;&nbsp;lobber: 0.005<br>&nbsp;&nbsp;afone: 0.004<br>&nbsp;&nbsp; itself: 0.004<br>&nbsp;&nbsp;TAIL: 0.004<br>&nbsp;&nbsp;ivé: 0.003<br>"
          ],
          [
           "<b>Entropy:</b> 0.035<br><b>Pred:</b> 'gc<br><b>Top-5:</b><br>&nbsp;&nbsp;'gc: 0.002<br>&nbsp;&nbsp;字幕: 0.001<br>&nbsp;&nbsp;��: 0.001<br>&nbsp;&nbsp;﻿using: 0.001<br>&nbsp;&nbsp;페이지: 0.001<br>",
           "<b>Entropy:</b> 0.037<br><b>Pred:</b> 페이지<br><b>Top-5:</b><br>&nbsp;&nbsp;페이지: 0.002<br>&nbsp;&nbsp;字幕: 0.002<br>&nbsp;&nbsp;نوع: 0.001<br>&nbsp;&nbsp;Produto: 0.001<br>&nbsp;&nbsp;﻿using: 0.001<br>",
           "<b>Entropy:</b> 0.090<br><b>Pred:</b> 'gc<br><b>Top-5:</b><br>&nbsp;&nbsp;'gc: 0.008<br>&nbsp;&nbsp;contri: 0.003<br>&nbsp;&nbsp;krv: 0.002<br>&nbsp;&nbsp;��: 0.002<br>&nbsp;&nbsp;porno: 0.002<br>",
           "<b>Entropy:</b> 0.059<br><b>Pred:</b> 페이지<br><b>Top-5:</b><br>&nbsp;&nbsp;페이지: 0.003<br>&nbsp;&nbsp;字幕: 0.003<br>&nbsp;&nbsp;نوع: 0.002<br>&nbsp;&nbsp;سال: 0.001<br>&nbsp;&nbsp;…\": 0.001<br>",
           "<b>Entropy:</b> 0.316<br><b>Pred:</b> wing<br><b>Top-5:</b><br>&nbsp;&nbsp;wing: 0.058<br>&nbsp;&nbsp; Wing: 0.011<br>&nbsp;&nbsp;hoe: 0.008<br>&nbsp;&nbsp; wing: 0.007<br>&nbsp;&nbsp; Wesley: 0.006<br>",
           "<b>Entropy:</b> 0.155<br><b>Pred:</b> ROID<br><b>Top-5:</b><br>&nbsp;&nbsp;ROID: 0.013<br>&nbsp;&nbsp; unless: 0.008<br>&nbsp;&nbsp;adin: 0.003<br>&nbsp;&nbsp;まま: 0.003<br>&nbsp;&nbsp;enance: 0.003<br>",
           "<b>Entropy:</b> 0.184<br><b>Pred:</b>  Folk<br><b>Top-5:</b><br>&nbsp;&nbsp; Folk: 0.009<br>&nbsp;&nbsp;folio: 0.009<br>&nbsp;&nbsp;opensource: 0.008<br>&nbsp;&nbsp;ogne: 0.006<br>&nbsp;&nbsp; Contr: 0.005<br>",
           "<b>Entropy:</b> 0.271<br><b>Pred:</b>  foul<br><b>Top-5:</b><br>&nbsp;&nbsp; foul: 0.020<br>&nbsp;&nbsp;nd: 0.019<br>&nbsp;&nbsp;bine: 0.016<br>&nbsp;&nbsp; Wind: 0.005<br>&nbsp;&nbsp; Due: 0.004<br>",
           "<b>Entropy:</b> 0.049<br><b>Pred:</b> 'gc<br><b>Top-5:</b><br>&nbsp;&nbsp;'gc: 0.003<br>&nbsp;&nbsp;krv: 0.001<br>&nbsp;&nbsp;ritel: 0.001<br>&nbsp;&nbsp;zdy: 0.001<br>&nbsp;&nbsp;mpar: 0.001<br>",
           "<b>Entropy:</b> 0.360<br><b>Pred:</b>  matter<br><b>Top-5:</b><br>&nbsp;&nbsp; matter: 0.039<br>&nbsp;&nbsp;/no: 0.029<br>&nbsp;&nbsp;ifies: 0.009<br>&nbsp;&nbsp;isce: 0.009<br>&nbsp;&nbsp;els: 0.009<br>",
           "<b>Entropy:</b> 0.183<br><b>Pred:</b> ostel<br><b>Top-5:</b><br>&nbsp;&nbsp;ostel: 0.012<br>&nbsp;&nbsp;alles: 0.010<br>&nbsp;&nbsp;ize: 0.008<br>&nbsp;&nbsp; nor: 0.005<br>&nbsp;&nbsp;aar: 0.004<br>",
           "<b>Entropy:</b> 0.394<br><b>Pred:</b>  nor<br><b>Top-5:</b><br>&nbsp;&nbsp; nor: 0.096<br>&nbsp;&nbsp; unless: 0.024<br>&nbsp;&nbsp;mind: 0.006<br>&nbsp;&nbsp;undry: 0.005<br>&nbsp;&nbsp; Babe: 0.004<br>",
           "<b>Entropy:</b> 0.134<br><b>Pred:</b> lectric<br><b>Top-5:</b><br>&nbsp;&nbsp;lectric: 0.007<br>&nbsp;&nbsp; unless: 0.005<br>&nbsp;&nbsp;闲: 0.005<br>&nbsp;&nbsp;THING: 0.005<br>&nbsp;&nbsp;Idle: 0.004<br>",
           "<b>Entropy:</b> 0.177<br><b>Pred:</b> PELL<br><b>Top-5:</b><br>&nbsp;&nbsp;PELL: 0.012<br>&nbsp;&nbsp;fat: 0.008<br>&nbsp;&nbsp;idge: 0.006<br>&nbsp;&nbsp;aden: 0.006<br>&nbsp;&nbsp; guts: 0.004<br>",
           "<b>Entropy:</b> 0.158<br><b>Pred:</b> afone<br><b>Top-5:</b><br>&nbsp;&nbsp;afone: 0.021<br>&nbsp;&nbsp; Breed: 0.004<br>&nbsp;&nbsp; Mits: 0.003<br>&nbsp;&nbsp; Balls: 0.003<br>&nbsp;&nbsp;ement: 0.003<br>"
          ],
          [
           "<b>Entropy:</b> 0.034<br><b>Pred:</b> 'gc<br><b>Top-5:</b><br>&nbsp;&nbsp;'gc: 0.001<br>&nbsp;&nbsp;字幕: 0.001<br>&nbsp;&nbsp;페이지: 0.001<br>&nbsp;&nbsp;��: 0.001<br>&nbsp;&nbsp;﻿using: 0.001<br>",
           "<b>Entropy:</b> 0.040<br><b>Pred:</b> 페이지<br><b>Top-5:</b><br>&nbsp;&nbsp;페이지: 0.002<br>&nbsp;&nbsp;字幕: 0.002<br>&nbsp;&nbsp;نوع: 0.001<br>&nbsp;&nbsp;Produto: 0.001<br>&nbsp;&nbsp;سال: 0.001<br>",
           "<b>Entropy:</b> 0.071<br><b>Pred:</b> 'gc<br><b>Top-5:</b><br>&nbsp;&nbsp;'gc: 0.006<br>&nbsp;&nbsp;contri: 0.002<br>&nbsp;&nbsp;��: 0.002<br>&nbsp;&nbsp;porno: 0.001<br>&nbsp;&nbsp;krv: 0.001<br>",
           "<b>Entropy:</b> 0.060<br><b>Pred:</b> 페이지<br><b>Top-5:</b><br>&nbsp;&nbsp;페이지: 0.003<br>&nbsp;&nbsp;字幕: 0.003<br>&nbsp;&nbsp;نوع: 0.002<br>&nbsp;&nbsp;سال: 0.001<br>&nbsp;&nbsp;…\": 0.001<br>",
           "<b>Entropy:</b> 0.184<br><b>Pred:</b> wing<br><b>Top-5:</b><br>&nbsp;&nbsp;wing: 0.015<br>&nbsp;&nbsp; heated: 0.007<br>&nbsp;&nbsp;hoe: 0.006<br>&nbsp;&nbsp;ington: 0.006<br>&nbsp;&nbsp; Wing: 0.005<br>",
           "<b>Entropy:</b> 0.190<br><b>Pred:</b> �回<br><b>Top-5:</b><br>&nbsp;&nbsp;�回: 0.009<br>&nbsp;&nbsp;eted: 0.008<br>&nbsp;&nbsp;Anywhere: 0.008<br>&nbsp;&nbsp;velle: 0.007<br>&nbsp;&nbsp; anywhere: 0.006<br>",
           "<b>Entropy:</b> 0.170<br><b>Pred:</b> tesy<br><b>Top-5:</b><br>&nbsp;&nbsp;tesy: 0.011<br>&nbsp;&nbsp; Criterion: 0.008<br>&nbsp;&nbsp; Westbrook: 0.006<br>&nbsp;&nbsp;ogne: 0.005<br>&nbsp;&nbsp;ardless: 0.005<br>",
           "<b>Entropy:</b> 0.139<br><b>Pred:</b> bine<br><b>Top-5:</b><br>&nbsp;&nbsp;bine: 0.009<br>&nbsp;&nbsp; foul: 0.007<br>&nbsp;&nbsp; vain: 0.005<br>&nbsp;&nbsp; Gund: 0.003<br>&nbsp;&nbsp; Eck: 0.003<br>",
           "<b>Entropy:</b> 0.050<br><b>Pred:</b> 'gc<br><b>Top-5:</b><br>&nbsp;&nbsp;'gc: 0.003<br>&nbsp;&nbsp;aeda: 0.001<br>&nbsp;&nbsp;overe: 0.001<br>&nbsp;&nbsp;페이지: 0.001<br>&nbsp;&nbsp;mpar: 0.001<br>",
           "<b>Entropy:</b> 0.445<br><b>Pred:</b>  matter<br><b>Top-5:</b><br>&nbsp;&nbsp; matter: 0.147<br>&nbsp;&nbsp;/no: 0.013<br>&nbsp;&nbsp; doubt: 0.009<br>&nbsp;&nbsp; haha: 0.008<br>&nbsp;&nbsp;matter: 0.005<br>",
           "<b>Entropy:</b> 0.114<br><b>Pred:</b>  nor<br><b>Top-5:</b><br>&nbsp;&nbsp; nor: 0.006<br>&nbsp;&nbsp;ization: 0.004<br>&nbsp;&nbsp;undry: 0.004<br>&nbsp;&nbsp;oce: 0.004<br>&nbsp;&nbsp;/com: 0.003<br>",
           "<b>Entropy:</b> 0.171<br><b>Pred:</b>  unless<br><b>Top-5:</b><br>&nbsp;&nbsp; unless: 0.009<br>&nbsp;&nbsp;engo: 0.009<br>&nbsp;&nbsp;undry: 0.006<br>&nbsp;&nbsp; nor: 0.006<br>&nbsp;&nbsp;rvine: 0.005<br>",
           "<b>Entropy:</b> 0.179<br><b>Pred:</b> VICE<br><b>Top-5:</b><br>&nbsp;&nbsp;VICE: 0.012<br>&nbsp;&nbsp;frei: 0.008<br>&nbsp;&nbsp;owe: 0.007<br>&nbsp;&nbsp; Pratt: 0.006<br>&nbsp;&nbsp;泳: 0.004<br>",
           "<b>Entropy:</b> 0.254<br><b>Pred:</b> aden<br><b>Top-5:</b><br>&nbsp;&nbsp;aden: 0.029<br>&nbsp;&nbsp;PELL: 0.028<br>&nbsp;&nbsp; Bund: 0.003<br>&nbsp;&nbsp; Breed: 0.003<br>&nbsp;&nbsp;slack: 0.003<br>",
           "<b>Entropy:</b> 0.226<br><b>Pred:</b>  Licence<br><b>Top-5:</b><br>&nbsp;&nbsp; Licence: 0.025<br>&nbsp;&nbsp; Breed: 0.020<br>&nbsp;&nbsp;rowave: 0.004<br>&nbsp;&nbsp; Bubble: 0.003<br>&nbsp;&nbsp; licence: 0.003<br>"
          ],
          [
           "<b>Entropy:</b> 0.034<br><b>Pred:</b> 'gc<br><b>Top-5:</b><br>&nbsp;&nbsp;'gc: 0.001<br>&nbsp;&nbsp;페이지: 0.001<br>&nbsp;&nbsp;字幕: 0.001<br>&nbsp;&nbsp;��: 0.001<br>&nbsp;&nbsp;﻿using: 0.001<br>",
           "<b>Entropy:</b> 0.045<br><b>Pred:</b> 페이지<br><b>Top-5:</b><br>&nbsp;&nbsp;페이지: 0.003<br>&nbsp;&nbsp;字幕: 0.002<br>&nbsp;&nbsp;نوع: 0.001<br>&nbsp;&nbsp;Produto: 0.001<br>&nbsp;&nbsp;سال: 0.001<br>",
           "<b>Entropy:</b> 0.058<br><b>Pred:</b> 'gc<br><b>Top-5:</b><br>&nbsp;&nbsp;'gc: 0.004<br>&nbsp;&nbsp;��: 0.002<br>&nbsp;&nbsp;contri: 0.001<br>&nbsp;&nbsp;krv: 0.001<br>&nbsp;&nbsp;��: 0.001<br>",
           "<b>Entropy:</b> 0.062<br><b>Pred:</b> 페이지<br><b>Top-5:</b><br>&nbsp;&nbsp;페이지: 0.003<br>&nbsp;&nbsp;字幕: 0.003<br>&nbsp;&nbsp;نوع: 0.002<br>&nbsp;&nbsp;سال: 0.001<br>&nbsp;&nbsp;…\": 0.001<br>",
           "<b>Entropy:</b> 0.225<br><b>Pred:</b> wing<br><b>Top-5:</b><br>&nbsp;&nbsp;wing: 0.026<br>&nbsp;&nbsp;ichert: 0.007<br>&nbsp;&nbsp; Black: 0.006<br>&nbsp;&nbsp;ccd: 0.006<br>&nbsp;&nbsp;exual: 0.006<br>",
           "<b>Entropy:</b> 0.171<br><b>Pred:</b> �回<br><b>Top-5:</b><br>&nbsp;&nbsp;�回: 0.015<br>&nbsp;&nbsp;RTC: 0.006<br>&nbsp;&nbsp;anky: 0.005<br>&nbsp;&nbsp;velle: 0.005<br>&nbsp;&nbsp;orthy: 0.004<br>",
           "<b>Entropy:</b> 0.164<br><b>Pred:</b> ardless<br><b>Top-5:</b><br>&nbsp;&nbsp;ardless: 0.008<br>&nbsp;&nbsp; either: 0.007<br>&nbsp;&nbsp; Either: 0.006<br>&nbsp;&nbsp; judging: 0.006<br>&nbsp;&nbsp; Contr: 0.005<br>",
           "<b>Entropy:</b> 0.112<br><b>Pred:</b>  foul<br><b>Top-5:</b><br>&nbsp;&nbsp; foul: 0.005<br>&nbsp;&nbsp;andum: 0.004<br>&nbsp;&nbsp;tee: 0.004<br>&nbsp;&nbsp;ession: 0.003<br>&nbsp;&nbsp;bine: 0.003<br>",
           "<b>Entropy:</b> 0.045<br><b>Pred:</b> 'gc<br><b>Top-5:</b><br>&nbsp;&nbsp;'gc: 0.002<br>&nbsp;&nbsp;aeda: 0.002<br>&nbsp;&nbsp;페이지: 0.001<br>&nbsp;&nbsp;overe: 0.001<br>&nbsp;&nbsp;krv: 0.001<br>",
           "<b>Entropy:</b> 0.366<br><b>Pred:</b>  matter<br><b>Top-5:</b><br>&nbsp;&nbsp; matter: 0.090<br>&nbsp;&nbsp;/no: 0.014<br>&nbsp;&nbsp;matter: 0.008<br>&nbsp;&nbsp; haha: 0.005<br>&nbsp;&nbsp;isce: 0.004<br>",
           "<b>Entropy:</b> 0.156<br><b>Pred:</b> oux<br><b>Top-5:</b><br>&nbsp;&nbsp;oux: 0.008<br>&nbsp;&nbsp;dehyde: 0.008<br>&nbsp;&nbsp;undry: 0.006<br>&nbsp;&nbsp;alles: 0.005<br>&nbsp;&nbsp;CAF: 0.005<br>",
           "<b>Entropy:</b> 0.139<br><b>Pred:</b> iki<br><b>Top-5:</b><br>&nbsp;&nbsp;iki: 0.011<br>&nbsp;&nbsp;unless: 0.004<br>&nbsp;&nbsp;nte: 0.004<br>&nbsp;&nbsp; nor: 0.004<br>&nbsp;&nbsp;rvine: 0.004<br>",
           "<b>Entropy:</b> 0.181<br><b>Pred:</b> frei<br><b>Top-5:</b><br>&nbsp;&nbsp;frei: 0.011<br>&nbsp;&nbsp;泳: 0.010<br>&nbsp;&nbsp;YLON: 0.009<br>&nbsp;&nbsp;�回: 0.004<br>&nbsp;&nbsp;eum: 0.004<br>",
           "<b>Entropy:</b> 0.233<br><b>Pred:</b> aden<br><b>Top-5:</b><br>&nbsp;&nbsp;aden: 0.044<br>&nbsp;&nbsp;iben: 0.006<br>&nbsp;&nbsp; Je: 0.004<br>&nbsp;&nbsp; foul: 0.004<br>&nbsp;&nbsp; Babe: 0.003<br>",
           "<b>Entropy:</b> 0.202<br><b>Pred:</b>  Licence<br><b>Top-5:</b><br>&nbsp;&nbsp; Licence: 0.013<br>&nbsp;&nbsp;slack: 0.011<br>&nbsp;&nbsp;iners: 0.010<br>&nbsp;&nbsp;akk: 0.006<br>&nbsp;&nbsp;afen: 0.003<br>"
          ],
          [
           "<b>Entropy:</b> 0.036<br><b>Pred:</b> 페이지<br><b>Top-5:</b><br>&nbsp;&nbsp;페이지: 0.001<br>&nbsp;&nbsp;字幕: 0.001<br>&nbsp;&nbsp;'gc: 0.001<br>&nbsp;&nbsp;��: 0.001<br>&nbsp;&nbsp;﻿using: 0.001<br>",
           "<b>Entropy:</b> 0.050<br><b>Pred:</b> 페이지<br><b>Top-5:</b><br>&nbsp;&nbsp;페이지: 0.003<br>&nbsp;&nbsp;字幕: 0.002<br>&nbsp;&nbsp;Produto: 0.001<br>&nbsp;&nbsp;نوع: 0.001<br>&nbsp;&nbsp;سال: 0.001<br>",
           "<b>Entropy:</b> 0.055<br><b>Pred:</b> 'gc<br><b>Top-5:</b><br>&nbsp;&nbsp;'gc: 0.004<br>&nbsp;&nbsp;��: 0.001<br>&nbsp;&nbsp;raci: 0.001<br>&nbsp;&nbsp;��: 0.001<br>&nbsp;&nbsp;contri: 0.001<br>",
           "<b>Entropy:</b> 0.064<br><b>Pred:</b> 페이지<br><b>Top-5:</b><br>&nbsp;&nbsp;페이지: 0.003<br>&nbsp;&nbsp;字幕: 0.003<br>&nbsp;&nbsp;نوع: 0.002<br>&nbsp;&nbsp;سال: 0.001<br>&nbsp;&nbsp;…\": 0.001<br>",
           "<b>Entropy:</b> 0.251<br><b>Pred:</b> ogue<br><b>Top-5:</b><br>&nbsp;&nbsp;ogue: 0.020<br>&nbsp;&nbsp; sim: 0.020<br>&nbsp;&nbsp;idan: 0.008<br>&nbsp;&nbsp;ichert: 0.006<br>&nbsp;&nbsp; Black: 0.005<br>",
           "<b>Entropy:</b> 0.218<br><b>Pred:</b> RTC<br><b>Top-5:</b><br>&nbsp;&nbsp;RTC: 0.015<br>&nbsp;&nbsp; dial: 0.010<br>&nbsp;&nbsp;ROID: 0.008<br>&nbsp;&nbsp;orthy: 0.007<br>&nbsp;&nbsp; Contr: 0.007<br>",
           "<b>Entropy:</b> 0.196<br><b>Pred:</b>  either<br><b>Top-5:</b><br>&nbsp;&nbsp; either: 0.011<br>&nbsp;&nbsp;ptune: 0.008<br>&nbsp;&nbsp;oug: 0.008<br>&nbsp;&nbsp; Either: 0.007<br>&nbsp;&nbsp;orthy: 0.006<br>",
           "<b>Entropy:</b> 0.126<br><b>Pred:</b>  invite<br><b>Top-5:</b><br>&nbsp;&nbsp; invite: 0.010<br>&nbsp;&nbsp;andum: 0.004<br>&nbsp;&nbsp;icone: 0.004<br>&nbsp;&nbsp; degree: 0.003<br>&nbsp;&nbsp; invitation: 0.003<br>",
           "<b>Entropy:</b> 0.044<br><b>Pred:</b> 'gc<br><b>Top-5:</b><br>&nbsp;&nbsp;'gc: 0.002<br>&nbsp;&nbsp;aeda: 0.002<br>&nbsp;&nbsp;페이지: 0.001<br>&nbsp;&nbsp;overe: 0.001<br>&nbsp;&nbsp;anou: 0.001<br>",
           "<b>Entropy:</b> 0.263<br><b>Pred:</b>  matter<br><b>Top-5:</b><br>&nbsp;&nbsp; matter: 0.036<br>&nbsp;&nbsp;licht: 0.014<br>&nbsp;&nbsp; haha: 0.009<br>&nbsp;&nbsp;erve: 0.004<br>&nbsp;&nbsp;matter: 0.003<br>",
           "<b>Entropy:</b> 0.165<br><b>Pred:</b> ull<br><b>Top-5:</b><br>&nbsp;&nbsp;ull: 0.011<br>&nbsp;&nbsp;CAF: 0.008<br>&nbsp;&nbsp;ization: 0.006<br>&nbsp;&nbsp;reeze: 0.005<br>&nbsp;&nbsp;ful: 0.004<br>",
           "<b>Entropy:</b> 0.159<br><b>Pred:</b> nte<br><b>Top-5:</b><br>&nbsp;&nbsp;nte: 0.012<br>&nbsp;&nbsp; Priest: 0.006<br>&nbsp;&nbsp;iki: 0.005<br>&nbsp;&nbsp;mill: 0.005<br>&nbsp;&nbsp; Mind: 0.004<br>",
           "<b>Entropy:</b> 0.170<br><b>Pred:</b>  enh<br><b>Top-5:</b><br>&nbsp;&nbsp; enh: 0.015<br>&nbsp;&nbsp;ucken: 0.008<br>&nbsp;&nbsp;lang: 0.004<br>&nbsp;&nbsp;frei: 0.004<br>&nbsp;&nbsp; Gesch: 0.004<br>",
           "<b>Entropy:</b> 0.214<br><b>Pred:</b> aden<br><b>Top-5:</b><br>&nbsp;&nbsp;aden: 0.034<br>&nbsp;&nbsp;ington: 0.005<br>&nbsp;&nbsp;appable: 0.005<br>&nbsp;&nbsp; SAM: 0.004<br>&nbsp;&nbsp;lang: 0.004<br>",
           "<b>Entropy:</b> 0.117<br><b>Pred:</b>  ►<br><b>Top-5:</b><br>&nbsp;&nbsp; ►: 0.005<br>&nbsp;&nbsp;afen: 0.005<br>&nbsp;&nbsp; Licence: 0.004<br>&nbsp;&nbsp;ELY: 0.004<br>&nbsp;&nbsp;temps: 0.003<br>"
          ],
          [
           "<b>Entropy:</b> 0.037<br><b>Pred:</b> 페이지<br><b>Top-5:</b><br>&nbsp;&nbsp;페이지: 0.002<br>&nbsp;&nbsp;字幕: 0.001<br>&nbsp;&nbsp;…\": 0.001<br>&nbsp;&nbsp;'gc: 0.001<br>&nbsp;&nbsp;��: 0.001<br>",
           "<b>Entropy:</b> 0.052<br><b>Pred:</b> 페이지<br><b>Top-5:</b><br>&nbsp;&nbsp;페이지: 0.003<br>&nbsp;&nbsp;字幕: 0.002<br>&nbsp;&nbsp;Produto: 0.001<br>&nbsp;&nbsp;نوع: 0.001<br>&nbsp;&nbsp;سال: 0.001<br>",
           "<b>Entropy:</b> 0.047<br><b>Pred:</b> 'gc<br><b>Top-5:</b><br>&nbsp;&nbsp;'gc: 0.003<br>&nbsp;&nbsp;��: 0.001<br>&nbsp;&nbsp;(EXPR: 0.001<br>&nbsp;&nbsp;podob: 0.001<br>&nbsp;&nbsp;��: 0.001<br>",
           "<b>Entropy:</b> 0.064<br><b>Pred:</b> 페이지<br><b>Top-5:</b><br>&nbsp;&nbsp;페이지: 0.003<br>&nbsp;&nbsp;字幕: 0.003<br>&nbsp;&nbsp;نوع: 0.002<br>&nbsp;&nbsp;سال: 0.001<br>&nbsp;&nbsp;…\": 0.001<br>",
           "<b>Entropy:</b> 0.145<br><b>Pred:</b> idan<br><b>Top-5:</b><br>&nbsp;&nbsp;idan: 0.009<br>&nbsp;&nbsp;compat: 0.007<br>&nbsp;&nbsp; HAL: 0.004<br>&nbsp;&nbsp;ogue: 0.004<br>&nbsp;&nbsp;activity: 0.004<br>",
           "<b>Entropy:</b> 0.167<br><b>Pred:</b> orthy<br><b>Top-5:</b><br>&nbsp;&nbsp;orthy: 0.015<br>&nbsp;&nbsp;atı: 0.008<br>&nbsp;&nbsp;dependent: 0.005<br>&nbsp;&nbsp; dial: 0.004<br>&nbsp;&nbsp;adel: 0.004<br>",
           "<b>Entropy:</b> 0.159<br><b>Pred:</b> erap<br><b>Top-5:</b><br>&nbsp;&nbsp;erap: 0.009<br>&nbsp;&nbsp;yer: 0.006<br>&nbsp;&nbsp; either: 0.006<br>&nbsp;&nbsp;práv: 0.006<br>&nbsp;&nbsp; Restr: 0.005<br>",
           "<b>Entropy:</b> 0.078<br><b>Pred:</b>  Econ<br><b>Top-5:</b><br>&nbsp;&nbsp; Econ: 0.003<br>&nbsp;&nbsp; invite: 0.003<br>&nbsp;&nbsp;olu: 0.002<br>&nbsp;&nbsp; leaf: 0.002<br>&nbsp;&nbsp;HT: 0.002<br>",
           "<b>Entropy:</b> 0.043<br><b>Pred:</b> aeda<br><b>Top-5:</b><br>&nbsp;&nbsp;aeda: 0.002<br>&nbsp;&nbsp;'gc: 0.002<br>&nbsp;&nbsp;anou: 0.001<br>&nbsp;&nbsp;페이지: 0.001<br>&nbsp;&nbsp;overe: 0.001<br>",
           "<b>Entropy:</b> 0.142<br><b>Pred:</b>  matter<br><b>Top-5:</b><br>&nbsp;&nbsp; matter: 0.009<br>&nbsp;&nbsp;erve: 0.005<br>&nbsp;&nbsp;jen: 0.005<br>&nbsp;&nbsp;ilar: 0.005<br>&nbsp;&nbsp;/place: 0.004<br>",
           "<b>Entropy:</b> 0.190<br><b>Pred:</b> kle<br><b>Top-5:</b><br>&nbsp;&nbsp;kle: 0.011<br>&nbsp;&nbsp;ization: 0.009<br>&nbsp;&nbsp; Çağ: 0.007<br>&nbsp;&nbsp;CAF: 0.006<br>&nbsp;&nbsp;reeze: 0.006<br>",
           "<b>Entropy:</b> 0.181<br><b>Pred:</b> nte<br><b>Top-5:</b><br>&nbsp;&nbsp;nte: 0.018<br>&nbsp;&nbsp; Bare: 0.007<br>&nbsp;&nbsp;rou: 0.005<br>&nbsp;&nbsp;raz: 0.005<br>&nbsp;&nbsp;isol: 0.004<br>",
           "<b>Entropy:</b> 0.150<br><b>Pred:</b> YLON<br><b>Top-5:</b><br>&nbsp;&nbsp;YLON: 0.007<br>&nbsp;&nbsp; Marty: 0.007<br>&nbsp;&nbsp;obe: 0.006<br>&nbsp;&nbsp;room: 0.005<br>&nbsp;&nbsp;LAR: 0.004<br>",
           "<b>Entropy:</b> 0.260<br><b>Pred:</b> aden<br><b>Top-5:</b><br>&nbsp;&nbsp;aden: 0.035<br>&nbsp;&nbsp; SAM: 0.012<br>&nbsp;&nbsp;491: 0.006<br>&nbsp;&nbsp;obook: 0.006<br>&nbsp;&nbsp;iance: 0.006<br>",
           "<b>Entropy:</b> 0.114<br><b>Pred:</b> aris<br><b>Top-5:</b><br>&nbsp;&nbsp;aris: 0.006<br>&nbsp;&nbsp;temps: 0.004<br>&nbsp;&nbsp; Bounds: 0.004<br>&nbsp;&nbsp;obook: 0.004<br>&nbsp;&nbsp; Wage: 0.003<br>"
          ],
          [
           "<b>Entropy:</b> 0.041<br><b>Pred:</b> 페이지<br><b>Top-5:</b><br>&nbsp;&nbsp;페이지: 0.002<br>&nbsp;&nbsp;字幕: 0.001<br>&nbsp;&nbsp;…\": 0.001<br>&nbsp;&nbsp;'gc: 0.001<br>&nbsp;&nbsp;��: 0.001<br>",
           "<b>Entropy:</b> 0.056<br><b>Pred:</b> 페이지<br><b>Top-5:</b><br>&nbsp;&nbsp;페이지: 0.003<br>&nbsp;&nbsp;字幕: 0.002<br>&nbsp;&nbsp;Produto: 0.001<br>&nbsp;&nbsp;نوع: 0.001<br>&nbsp;&nbsp;…\": 0.001<br>",
           "<b>Entropy:</b> 0.050<br><b>Pred:</b> 'gc<br><b>Top-5:</b><br>&nbsp;&nbsp;'gc: 0.003<br>&nbsp;&nbsp;��: 0.002<br>&nbsp;&nbsp;podob: 0.001<br>&nbsp;&nbsp;(EXPR: 0.001<br>&nbsp;&nbsp;raci: 0.001<br>",
           "<b>Entropy:</b> 0.065<br><b>Pred:</b> 페이지<br><b>Top-5:</b><br>&nbsp;&nbsp;페이지: 0.003<br>&nbsp;&nbsp;字幕: 0.003<br>&nbsp;&nbsp;نوع: 0.002<br>&nbsp;&nbsp;…\": 0.001<br>&nbsp;&nbsp;سال: 0.001<br>",
           "<b>Entropy:</b> 0.121<br><b>Pred:</b> YRO<br><b>Top-5:</b><br>&nbsp;&nbsp;YRO: 0.006<br>&nbsp;&nbsp; HAL: 0.005<br>&nbsp;&nbsp; yup: 0.004<br>&nbsp;&nbsp;ccd: 0.004<br>&nbsp;&nbsp;弹: 0.003<br>",
           "<b>Entropy:</b> 0.246<br><b>Pred:</b>  dispro<br><b>Top-5:</b><br>&nbsp;&nbsp; dispro: 0.016<br>&nbsp;&nbsp;jee: 0.011<br>&nbsp;&nbsp;yscale: 0.011<br>&nbsp;&nbsp;imde: 0.009<br>&nbsp;&nbsp;orthy: 0.009<br>",
           "<b>Entropy:</b> 0.167<br><b>Pred:</b> enin<br><b>Top-5:</b><br>&nbsp;&nbsp;enin: 0.012<br>&nbsp;&nbsp;oldt: 0.006<br>&nbsp;&nbsp; Bott: 0.006<br>&nbsp;&nbsp;onso: 0.005<br>&nbsp;&nbsp;akis: 0.005<br>",
           "<b>Entropy:</b> 0.107<br><b>Pred:</b>  landsc<br><b>Top-5:</b><br>&nbsp;&nbsp; landsc: 0.005<br>&nbsp;&nbsp; paternal: 0.005<br>&nbsp;&nbsp;icone: 0.004<br>&nbsp;&nbsp;haf: 0.003<br>&nbsp;&nbsp; Tradition: 0.003<br>",
           "<b>Entropy:</b> 0.044<br><b>Pred:</b> 'gc<br><b>Top-5:</b><br>&nbsp;&nbsp;'gc: 0.002<br>&nbsp;&nbsp;aeda: 0.002<br>&nbsp;&nbsp;페이지: 0.001<br>&nbsp;&nbsp;unma: 0.001<br>&nbsp;&nbsp;字幕: 0.001<br>",
           "<b>Entropy:</b> 0.132<br><b>Pred:</b> /place<br><b>Top-5:</b><br>&nbsp;&nbsp;/place: 0.013<br>&nbsp;&nbsp;izza: 0.004<br>&nbsp;&nbsp;ocks: 0.003<br>&nbsp;&nbsp;ilar: 0.003<br>&nbsp;&nbsp;perial: 0.003<br>",
           "<b>Entropy:</b> 0.260<br><b>Pred:</b> CAF<br><b>Top-5:</b><br>&nbsp;&nbsp;CAF: 0.020<br>&nbsp;&nbsp;archical: 0.013<br>&nbsp;&nbsp;chine: 0.011<br>&nbsp;&nbsp; Çağ: 0.009<br>&nbsp;&nbsp;kle: 0.007<br>",
           "<b>Entropy:</b> 0.100<br><b>Pred:</b> iki<br><b>Top-5:</b><br>&nbsp;&nbsp;iki: 0.005<br>&nbsp;&nbsp;�: 0.004<br>&nbsp;&nbsp;ocks: 0.004<br>&nbsp;&nbsp; Rear: 0.003<br>&nbsp;&nbsp; authorized: 0.003<br>",
           "<b>Entropy:</b> 0.130<br><b>Pred:</b> asm<br><b>Top-5:</b><br>&nbsp;&nbsp;asm: 0.005<br>&nbsp;&nbsp;.scalablytyped: 0.005<br>&nbsp;&nbsp; Assurance: 0.005<br>&nbsp;&nbsp; fle: 0.005<br>&nbsp;&nbsp;λία: 0.005<br>",
           "<b>Entropy:</b> 0.179<br><b>Pred:</b> .scalablytyped<br><b>Top-5:</b><br>&nbsp;&nbsp;.scalablytyped: 0.011<br>&nbsp;&nbsp;FUL: 0.008<br>&nbsp;&nbsp;ayd: 0.006<br>&nbsp;&nbsp;aden: 0.006<br>&nbsp;&nbsp; itemprop: 0.005<br>",
           "<b>Entropy:</b> 0.165<br><b>Pred:</b> ELY<br><b>Top-5:</b><br>&nbsp;&nbsp;ELY: 0.012<br>&nbsp;&nbsp;bate: 0.011<br>&nbsp;&nbsp;ippy: 0.004<br>&nbsp;&nbsp;ful: 0.004<br>&nbsp;&nbsp;lore: 0.004<br>"
          ],
          [
           "<b>Entropy:</b> 0.046<br><b>Pred:</b> 페이지<br><b>Top-5:</b><br>&nbsp;&nbsp;페이지: 0.002<br>&nbsp;&nbsp;…\": 0.002<br>&nbsp;&nbsp;字幕: 0.001<br>&nbsp;&nbsp; […: 0.001<br>&nbsp;&nbsp;Produto: 0.001<br>",
           "<b>Entropy:</b> 0.060<br><b>Pred:</b> 페이지<br><b>Top-5:</b><br>&nbsp;&nbsp;페이지: 0.004<br>&nbsp;&nbsp;字幕: 0.002<br>&nbsp;&nbsp;Produto: 0.001<br>&nbsp;&nbsp;نوع: 0.001<br>&nbsp;&nbsp;…\": 0.001<br>",
           "<b>Entropy:</b> 0.043<br><b>Pred:</b> 'gc<br><b>Top-5:</b><br>&nbsp;&nbsp;'gc: 0.002<br>&nbsp;&nbsp;aeda: 0.001<br>&nbsp;&nbsp;��: 0.001<br>&nbsp;&nbsp;페이지: 0.001<br>&nbsp;&nbsp;podob: 0.001<br>",
           "<b>Entropy:</b> 0.066<br><b>Pred:</b> 페이지<br><b>Top-5:</b><br>&nbsp;&nbsp;페이지: 0.004<br>&nbsp;&nbsp;字幕: 0.003<br>&nbsp;&nbsp;نوع: 0.002<br>&nbsp;&nbsp;…\": 0.002<br>&nbsp;&nbsp;سال: 0.001<br>",
           "<b>Entropy:</b> 0.144<br><b>Pred:</b> ccd<br><b>Top-5:</b><br>&nbsp;&nbsp;ccd: 0.007<br>&nbsp;&nbsp;YRO: 0.006<br>&nbsp;&nbsp;opies: 0.005<br>&nbsp;&nbsp;opian: 0.005<br>&nbsp;&nbsp;ridor: 0.005<br>",
           "<b>Entropy:</b> 0.154<br><b>Pred:</b> ubb<br><b>Top-5:</b><br>&nbsp;&nbsp;ubb: 0.009<br>&nbsp;&nbsp;yer: 0.008<br>&nbsp;&nbsp;bidden: 0.006<br>&nbsp;&nbsp; Barton: 0.004<br>&nbsp;&nbsp;rada: 0.004<br>",
           "<b>Entropy:</b> 0.132<br><b>Pred:</b> yer<br><b>Top-5:</b><br>&nbsp;&nbsp;yer: 0.006<br>&nbsp;&nbsp;agger: 0.006<br>&nbsp;&nbsp; overst: 0.005<br>&nbsp;&nbsp;orthy: 0.004<br>&nbsp;&nbsp; Nab: 0.003<br>",
           "<b>Entropy:</b> 0.157<br><b>Pred:</b> olic<br><b>Top-5:</b><br>&nbsp;&nbsp;olic: 0.010<br>&nbsp;&nbsp;uali: 0.007<br>&nbsp;&nbsp; degree: 0.005<br>&nbsp;&nbsp; Tradition: 0.005<br>&nbsp;&nbsp; Lyons: 0.003<br>",
           "<b>Entropy:</b> 0.045<br><b>Pred:</b> aeda<br><b>Top-5:</b><br>&nbsp;&nbsp;aeda: 0.002<br>&nbsp;&nbsp;페이지: 0.001<br>&nbsp;&nbsp;'gc: 0.001<br>&nbsp;&nbsp;字幕: 0.001<br>&nbsp;&nbsp;overe: 0.001<br>",
           "<b>Entropy:</b> 0.083<br><b>Pred:</b> /place<br><b>Top-5:</b><br>&nbsp;&nbsp;/place: 0.004<br>&nbsp;&nbsp;yonel: 0.003<br>&nbsp;&nbsp;olib: 0.003<br>&nbsp;&nbsp; haha: 0.002<br>&nbsp;&nbsp;ünchen: 0.002<br>",
           "<b>Entropy:</b> 0.135<br><b>Pred:</b>  Reserved<br><b>Top-5:</b><br>&nbsp;&nbsp; Reserved: 0.007<br>&nbsp;&nbsp;ull: 0.006<br>&nbsp;&nbsp;chine: 0.006<br>&nbsp;&nbsp;archical: 0.004<br>&nbsp;&nbsp;CAF: 0.004<br>",
           "<b>Entropy:</b> 0.173<br><b>Pred:</b>  Fle<br><b>Top-5:</b><br>&nbsp;&nbsp; Fle: 0.017<br>&nbsp;&nbsp; nay: 0.006<br>&nbsp;&nbsp;raz: 0.005<br>&nbsp;&nbsp;tin: 0.004<br>&nbsp;&nbsp; backward: 0.004<br>",
           "<b>Entropy:</b> 0.173<br><b>Pred:</b> YLON<br><b>Top-5:</b><br>&nbsp;&nbsp;YLON: 0.010<br>&nbsp;&nbsp; Fle: 0.007<br>&nbsp;&nbsp;asm: 0.007<br>&nbsp;&nbsp;ovel: 0.006<br>&nbsp;&nbsp;obe: 0.005<br>",
           "<b>Entropy:</b> 0.162<br><b>Pred:</b> olen<br><b>Top-5:</b><br>&nbsp;&nbsp;olen: 0.009<br>&nbsp;&nbsp;uali: 0.009<br>&nbsp;&nbsp;FUL: 0.008<br>&nbsp;&nbsp;afs: 0.004<br>&nbsp;&nbsp;ably: 0.003<br>",
           "<b>Entropy:</b> 0.259<br><b>Pred:</b> bate<br><b>Top-5:</b><br>&nbsp;&nbsp;bate: 0.019<br>&nbsp;&nbsp; indeb: 0.015<br>&nbsp;&nbsp;yer: 0.014<br>&nbsp;&nbsp;fone: 0.006<br>&nbsp;&nbsp; bew: 0.006<br>"
          ],
          [
           "<b>Entropy:</b> 0.056<br><b>Pred:</b> 페이지<br><b>Top-5:</b><br>&nbsp;&nbsp;페이지: 0.003<br>&nbsp;&nbsp;…\": 0.003<br>&nbsp;&nbsp;字幕: 0.002<br>&nbsp;&nbsp;,…: 0.001<br>&nbsp;&nbsp; […: 0.001<br>",
           "<b>Entropy:</b> 0.064<br><b>Pred:</b> 페이지<br><b>Top-5:</b><br>&nbsp;&nbsp;페이지: 0.004<br>&nbsp;&nbsp;字幕: 0.002<br>&nbsp;&nbsp;Produto: 0.001<br>&nbsp;&nbsp;…\": 0.001<br>&nbsp;&nbsp;نوع: 0.001<br>",
           "<b>Entropy:</b> 0.050<br><b>Pred:</b> aeda<br><b>Top-5:</b><br>&nbsp;&nbsp;aeda: 0.002<br>&nbsp;&nbsp;페이지: 0.002<br>&nbsp;&nbsp;'gc: 0.001<br>&nbsp;&nbsp;ritel: 0.001<br>&nbsp;&nbsp;��: 0.001<br>",
           "<b>Entropy:</b> 0.067<br><b>Pred:</b> 페이지<br><b>Top-5:</b><br>&nbsp;&nbsp;페이지: 0.004<br>&nbsp;&nbsp;字幕: 0.003<br>&nbsp;&nbsp;نوع: 0.002<br>&nbsp;&nbsp;…\": 0.002<br>&nbsp;&nbsp;سال: 0.001<br>",
           "<b>Entropy:</b> 0.204<br><b>Pred:</b> YRO<br><b>Top-5:</b><br>&nbsp;&nbsp;YRO: 0.028<br>&nbsp;&nbsp;alist: 0.006<br>&nbsp;&nbsp;ridor: 0.005<br>&nbsp;&nbsp; jobs: 0.004<br>&nbsp;&nbsp; job: 0.004<br>",
           "<b>Entropy:</b> 0.134<br><b>Pred:</b> alion<br><b>Top-5:</b><br>&nbsp;&nbsp;alion: 0.007<br>&nbsp;&nbsp; ranges: 0.005<br>&nbsp;&nbsp;atı: 0.005<br>&nbsp;&nbsp; Wr: 0.004<br>&nbsp;&nbsp; Bail: 0.004<br>",
           "<b>Entropy:</b> 0.274<br><b>Pred:</b>  Nab<br><b>Top-5:</b><br>&nbsp;&nbsp; Nab: 0.024<br>&nbsp;&nbsp; summ: 0.022<br>&nbsp;&nbsp;achi: 0.007<br>&nbsp;&nbsp;orthy: 0.007<br>&nbsp;&nbsp; seal: 0.006<br>",
           "<b>Entropy:</b> 0.120<br><b>Pred:</b> ably<br><b>Top-5:</b><br>&nbsp;&nbsp;ably: 0.007<br>&nbsp;&nbsp;HT: 0.005<br>&nbsp;&nbsp; te: 0.004<br>&nbsp;&nbsp;DED: 0.003<br>&nbsp;&nbsp;olis: 0.003<br>",
           "<b>Entropy:</b> 0.053<br><b>Pred:</b> aeda<br><b>Top-5:</b><br>&nbsp;&nbsp;aeda: 0.003<br>&nbsp;&nbsp;페이지: 0.002<br>&nbsp;&nbsp;字幕: 0.001<br>&nbsp;&nbsp;…\": 0.001<br>&nbsp;&nbsp;overe: 0.001<br>",
           "<b>Entropy:</b> 0.079<br><b>Pred:</b> aeda<br><b>Top-5:</b><br>&nbsp;&nbsp;aeda: 0.005<br>&nbsp;&nbsp;unma: 0.003<br>&nbsp;&nbsp;linky: 0.002<br>&nbsp;&nbsp;pedo: 0.002<br>&nbsp;&nbsp;overe: 0.002<br>",
           "<b>Entropy:</b> 0.103<br><b>Pred:</b> ��<br><b>Top-5:</b><br>&nbsp;&nbsp;��: 0.005<br>&nbsp;&nbsp;CAF: 0.004<br>&nbsp;&nbsp;ick: 0.004<br>&nbsp;&nbsp; Classes: 0.003<br>&nbsp;&nbsp; dil: 0.003<br>",
           "<b>Entropy:</b> 0.164<br><b>Pred:</b> enna<br><b>Top-5:</b><br>&nbsp;&nbsp;enna: 0.009<br>&nbsp;&nbsp; Frames: 0.009<br>&nbsp;&nbsp; Fle: 0.007<br>&nbsp;&nbsp;ynn: 0.005<br>&nbsp;&nbsp;bare: 0.004<br>",
           "<b>Entropy:</b> 0.186<br><b>Pred:</b> ella<br><b>Top-5:</b><br>&nbsp;&nbsp;ella: 0.015<br>&nbsp;&nbsp;obe: 0.009<br>&nbsp;&nbsp;ones: 0.006<br>&nbsp;&nbsp; graded: 0.005<br>&nbsp;&nbsp;izi: 0.005<br>",
           "<b>Entropy:</b> 0.124<br><b>Pred:</b> ISP<br><b>Top-5:</b><br>&nbsp;&nbsp;ISP: 0.005<br>&nbsp;&nbsp;oric: 0.005<br>&nbsp;&nbsp;apist: 0.005<br>&nbsp;&nbsp; degree: 0.005<br>&nbsp;&nbsp;olen: 0.004<br>",
           "<b>Entropy:</b> 0.094<br><b>Pred:</b> beit<br><b>Top-5:</b><br>&nbsp;&nbsp;beit: 0.004<br>&nbsp;&nbsp;族自治: 0.003<br>&nbsp;&nbsp;jective: 0.003<br>&nbsp;&nbsp; Zuk: 0.003<br>&nbsp;&nbsp;idden: 0.003<br>"
          ],
          [
           "<b>Entropy:</b> 0.065<br><b>Pred:</b> 페이지<br><b>Top-5:</b><br>&nbsp;&nbsp;페이지: 0.004<br>&nbsp;&nbsp;…\": 0.003<br>&nbsp;&nbsp;字幕: 0.002<br>&nbsp;&nbsp;,…: 0.001<br>&nbsp;&nbsp;…and: 0.001<br>",
           "<b>Entropy:</b> 0.065<br><b>Pred:</b> 페이지<br><b>Top-5:</b><br>&nbsp;&nbsp;페이지: 0.004<br>&nbsp;&nbsp;字幕: 0.002<br>&nbsp;&nbsp;نوع: 0.001<br>&nbsp;&nbsp;Produto: 0.001<br>&nbsp;&nbsp;…\": 0.001<br>",
           "<b>Entropy:</b> 0.052<br><b>Pred:</b> aeda<br><b>Top-5:</b><br>&nbsp;&nbsp;aeda: 0.004<br>&nbsp;&nbsp;페이지: 0.003<br>&nbsp;&nbsp;字幕: 0.001<br>&nbsp;&nbsp;…\": 0.001<br>&nbsp;&nbsp;dete: 0.001<br>",
           "<b>Entropy:</b> 0.067<br><b>Pred:</b> 페이지<br><b>Top-5:</b><br>&nbsp;&nbsp;페이지: 0.004<br>&nbsp;&nbsp;字幕: 0.003<br>&nbsp;&nbsp;نوع: 0.002<br>&nbsp;&nbsp;…\": 0.002<br>&nbsp;&nbsp;سال: 0.001<br>",
           "<b>Entropy:</b> 0.119<br><b>Pred:</b> alu<br><b>Top-5:</b><br>&nbsp;&nbsp;alu: 0.006<br>&nbsp;&nbsp;orientation: 0.005<br>&nbsp;&nbsp;alist: 0.004<br>&nbsp;&nbsp;oxide: 0.004<br>&nbsp;&nbsp;alic: 0.003<br>",
           "<b>Entropy:</b> 0.148<br><b>Pred:</b> rippling<br><b>Top-5:</b><br>&nbsp;&nbsp;rippling: 0.008<br>&nbsp;&nbsp; Wr: 0.005<br>&nbsp;&nbsp;ichert: 0.005<br>&nbsp;&nbsp;-abs: 0.005<br>&nbsp;&nbsp;orie: 0.005<br>",
           "<b>Entropy:</b> 0.195<br><b>Pred:</b> LL<br><b>Top-5:</b><br>&nbsp;&nbsp;LL: 0.019<br>&nbsp;&nbsp; Nab: 0.006<br>&nbsp;&nbsp;obili: 0.006<br>&nbsp;&nbsp;irst: 0.006<br>&nbsp;&nbsp;IRST: 0.005<br>",
           "<b>Entropy:</b> 0.138<br><b>Pred:</b> ably<br><b>Top-5:</b><br>&nbsp;&nbsp;ably: 0.006<br>&nbsp;&nbsp;ein: 0.006<br>&nbsp;&nbsp;ADDE: 0.005<br>&nbsp;&nbsp;δια: 0.005<br>&nbsp;&nbsp;auga: 0.005<br>",
           "<b>Entropy:</b> 0.066<br><b>Pred:</b> 페이지<br><b>Top-5:</b><br>&nbsp;&nbsp;페이지: 0.004<br>&nbsp;&nbsp;aeda: 0.004<br>&nbsp;&nbsp;字幕: 0.002<br>&nbsp;&nbsp;…\": 0.001<br>&nbsp;&nbsp;关于: 0.001<br>",
           "<b>Entropy:</b> 0.096<br><b>Pred:</b> aeda<br><b>Top-5:</b><br>&nbsp;&nbsp;aeda: 0.009<br>&nbsp;&nbsp;unma: 0.004<br>&nbsp;&nbsp;linky: 0.002<br>&nbsp;&nbsp;/mit: 0.001<br>&nbsp;&nbsp;apg: 0.001<br>",
           "<b>Entropy:</b> 0.158<br><b>Pred:</b> mere<br><b>Top-5:</b><br>&nbsp;&nbsp;mere: 0.009<br>&nbsp;&nbsp;aines: 0.007<br>&nbsp;&nbsp;tě: 0.005<br>&nbsp;&nbsp;ful: 0.005<br>&nbsp;&nbsp;dl: 0.005<br>",
           "<b>Entropy:</b> 0.177<br><b>Pred:</b> enna<br><b>Top-5:</b><br>&nbsp;&nbsp;enna: 0.022<br>&nbsp;&nbsp;fea: 0.006<br>&nbsp;&nbsp; borders: 0.005<br>&nbsp;&nbsp;en: 0.003<br>&nbsp;&nbsp; Frames: 0.003<br>",
           "<b>Entropy:</b> 0.151<br><b>Pred:</b>  Connected<br><b>Top-5:</b><br>&nbsp;&nbsp; Connected: 0.011<br>&nbsp;&nbsp;aec: 0.006<br>&nbsp;&nbsp;ella: 0.004<br>&nbsp;&nbsp; spare: 0.004<br>&nbsp;&nbsp;Connected: 0.004<br>",
           "<b>Entropy:</b> 0.167<br><b>Pred:</b> aben<br><b>Top-5:</b><br>&nbsp;&nbsp;aben: 0.017<br>&nbsp;&nbsp;mut: 0.005<br>&nbsp;&nbsp;aint: 0.005<br>&nbsp;&nbsp;ndern: 0.004<br>&nbsp;&nbsp; faint: 0.003<br>",
           "<b>Entropy:</b> 0.185<br><b>Pred:</b> aben<br><b>Top-5:</b><br>&nbsp;&nbsp;aben: 0.015<br>&nbsp;&nbsp;lie: 0.009<br>&nbsp;&nbsp;maid: 0.006<br>&nbsp;&nbsp;aleigh: 0.005<br>&nbsp;&nbsp;ipa: 0.004<br>"
          ],
          [
           "<b>Entropy:</b> 0.075<br><b>Pred:</b> 페이지<br><b>Top-5:</b><br>&nbsp;&nbsp;페이지: 0.004<br>&nbsp;&nbsp;…\": 0.003<br>&nbsp;&nbsp;字幕: 0.002<br>&nbsp;&nbsp;…and: 0.002<br>&nbsp;&nbsp;…”: 0.002<br>",
           "<b>Entropy:</b> 0.067<br><b>Pred:</b> 페이지<br><b>Top-5:</b><br>&nbsp;&nbsp;페이지: 0.004<br>&nbsp;&nbsp;字幕: 0.002<br>&nbsp;&nbsp;نوع: 0.002<br>&nbsp;&nbsp;Produto: 0.002<br>&nbsp;&nbsp;…\": 0.001<br>",
           "<b>Entropy:</b> 0.057<br><b>Pred:</b> 페이지<br><b>Top-5:</b><br>&nbsp;&nbsp;페이지: 0.003<br>&nbsp;&nbsp;aeda: 0.003<br>&nbsp;&nbsp;字幕: 0.001<br>&nbsp;&nbsp;نم: 0.001<br>&nbsp;&nbsp;…\": 0.001<br>",
           "<b>Entropy:</b> 0.068<br><b>Pred:</b> 페이지<br><b>Top-5:</b><br>&nbsp;&nbsp;페이지: 0.004<br>&nbsp;&nbsp;字幕: 0.003<br>&nbsp;&nbsp;نوع: 0.002<br>&nbsp;&nbsp;…\": 0.002<br>&nbsp;&nbsp;سال: 0.001<br>",
           "<b>Entropy:</b> 0.100<br><b>Pred:</b> zeichnet<br><b>Top-5:</b><br>&nbsp;&nbsp;zeichnet: 0.005<br>&nbsp;&nbsp;они: 0.004<br>&nbsp;&nbsp;羽: 0.003<br>&nbsp;&nbsp;jet: 0.003<br>&nbsp;&nbsp;contri: 0.003<br>",
           "<b>Entropy:</b> 0.236<br><b>Pred:</b> isce<br><b>Top-5:</b><br>&nbsp;&nbsp;isce: 0.017<br>&nbsp;&nbsp;dl: 0.016<br>&nbsp;&nbsp;onor: 0.011<br>&nbsp;&nbsp;elves: 0.006<br>&nbsp;&nbsp;inds: 0.004<br>",
           "<b>Entropy:</b> 0.208<br><b>Pred:</b> thinkable<br><b>Top-5:</b><br>&nbsp;&nbsp;thinkable: 0.016<br>&nbsp;&nbsp;obili: 0.014<br>&nbsp;&nbsp; Split: 0.006<br>&nbsp;&nbsp;isons: 0.005<br>&nbsp;&nbsp;azer: 0.005<br>",
           "<b>Entropy:</b> 0.145<br><b>Pred:</b> uali<br><b>Top-5:</b><br>&nbsp;&nbsp;uali: 0.013<br>&nbsp;&nbsp;dw: 0.006<br>&nbsp;&nbsp;ridor: 0.004<br>&nbsp;&nbsp;olis: 0.004<br>&nbsp;&nbsp;apore: 0.003<br>",
           "<b>Entropy:</b> 0.067<br><b>Pred:</b> 페이지<br><b>Top-5:</b><br>&nbsp;&nbsp;페이지: 0.004<br>&nbsp;&nbsp;aeda: 0.003<br>&nbsp;&nbsp;字幕: 0.002<br>&nbsp;&nbsp;…\": 0.002<br>&nbsp;&nbsp;关于: 0.001<br>",
           "<b>Entropy:</b> 0.066<br><b>Pred:</b> aeda<br><b>Top-5:</b><br>&nbsp;&nbsp;aeda: 0.005<br>&nbsp;&nbsp;linky: 0.002<br>&nbsp;&nbsp;unma: 0.001<br>&nbsp;&nbsp;ylland: 0.001<br>&nbsp;&nbsp;페이지: 0.001<br>",
           "<b>Entropy:</b> 0.093<br><b>Pred:</b> mere<br><b>Top-5:</b><br>&nbsp;&nbsp;mere: 0.005<br>&nbsp;&nbsp; Maver: 0.003<br>&nbsp;&nbsp; parole: 0.003<br>&nbsp;&nbsp;ática: 0.003<br>&nbsp;&nbsp;재: 0.003<br>",
           "<b>Entropy:</b> 0.089<br><b>Pred:</b>  dated<br><b>Top-5:</b><br>&nbsp;&nbsp; dated: 0.004<br>&nbsp;&nbsp; spare: 0.003<br>&nbsp;&nbsp;osa: 0.003<br>&nbsp;&nbsp;ope: 0.003<br>&nbsp;&nbsp; ageing: 0.003<br>",
           "<b>Entropy:</b> 0.213<br><b>Pred:</b> ope<br><b>Top-5:</b><br>&nbsp;&nbsp;ope: 0.022<br>&nbsp;&nbsp;place: 0.008<br>&nbsp;&nbsp;ones: 0.007<br>&nbsp;&nbsp;ither: 0.006<br>&nbsp;&nbsp;URA: 0.004<br>",
           "<b>Entropy:</b> 0.241<br><b>Pred:</b> icone<br><b>Top-5:</b><br>&nbsp;&nbsp;icone: 0.027<br>&nbsp;&nbsp;ndern: 0.019<br>&nbsp;&nbsp;люча: 0.005<br>&nbsp;&nbsp;かって: 0.004<br>&nbsp;&nbsp;yg: 0.004<br>",
           "<b>Entropy:</b> 0.188<br><b>Pred:</b> icone<br><b>Top-5:</b><br>&nbsp;&nbsp;icone: 0.016<br>&nbsp;&nbsp;ewire: 0.009<br>&nbsp;&nbsp;jective: 0.007<br>&nbsp;&nbsp; faint: 0.005<br>&nbsp;&nbsp;люча: 0.004<br>"
          ],
          [
           "<b>Entropy:</b> 0.071<br><b>Pred:</b> 페이지<br><b>Top-5:</b><br>&nbsp;&nbsp;페이지: 0.004<br>&nbsp;&nbsp;…\": 0.002<br>&nbsp;&nbsp;字幕: 0.002<br>&nbsp;&nbsp;…and: 0.002<br>&nbsp;&nbsp;…”: 0.002<br>",
           "<b>Entropy:</b> 0.067<br><b>Pred:</b> 페이지<br><b>Top-5:</b><br>&nbsp;&nbsp;페이지: 0.005<br>&nbsp;&nbsp;字幕: 0.002<br>&nbsp;&nbsp;نوع: 0.002<br>&nbsp;&nbsp;Produto: 0.002<br>&nbsp;&nbsp;سال: 0.001<br>",
           "<b>Entropy:</b> 0.067<br><b>Pred:</b> 페이지<br><b>Top-5:</b><br>&nbsp;&nbsp;페이지: 0.004<br>&nbsp;&nbsp;aeda: 0.003<br>&nbsp;&nbsp;字幕: 0.002<br>&nbsp;&nbsp;نم: 0.002<br>&nbsp;&nbsp;…”: 0.001<br>",
           "<b>Entropy:</b> 0.068<br><b>Pred:</b> 페이지<br><b>Top-5:</b><br>&nbsp;&nbsp;페이지: 0.004<br>&nbsp;&nbsp;字幕: 0.002<br>&nbsp;&nbsp;نوع: 0.002<br>&nbsp;&nbsp;…\": 0.002<br>&nbsp;&nbsp;سال: 0.002<br>",
           "<b>Entropy:</b> 0.115<br><b>Pred:</b> onu<br><b>Top-5:</b><br>&nbsp;&nbsp;onu: 0.009<br>&nbsp;&nbsp;jet: 0.004<br>&nbsp;&nbsp;zeichnet: 0.004<br>&nbsp;&nbsp;alu: 0.003<br>&nbsp;&nbsp; jet: 0.003<br>",
           "<b>Entropy:</b> 0.120<br><b>Pred:</b> yonel<br><b>Top-5:</b><br>&nbsp;&nbsp;yonel: 0.009<br>&nbsp;&nbsp;ámara: 0.004<br>&nbsp;&nbsp;onor: 0.004<br>&nbsp;&nbsp;.libs: 0.003<br>&nbsp;&nbsp;ogue: 0.003<br>",
           "<b>Entropy:</b> 0.117<br><b>Pred:</b> isons<br><b>Top-5:</b><br>&nbsp;&nbsp;isons: 0.008<br>&nbsp;&nbsp;obili: 0.005<br>&nbsp;&nbsp;achi: 0.004<br>&nbsp;&nbsp;olders: 0.003<br>&nbsp;&nbsp;оды: 0.002<br>",
           "<b>Entropy:</b> 0.185<br><b>Pred:</b> icone<br><b>Top-5:</b><br>&nbsp;&nbsp;icone: 0.013<br>&nbsp;&nbsp;uali: 0.013<br>&nbsp;&nbsp;adget: 0.005<br>&nbsp;&nbsp;isons: 0.004<br>&nbsp;&nbsp;odate: 0.004<br>",
           "<b>Entropy:</b> 0.071<br><b>Pred:</b> 페이지<br><b>Top-5:</b><br>&nbsp;&nbsp;페이지: 0.004<br>&nbsp;&nbsp;字幕: 0.002<br>&nbsp;&nbsp;aeda: 0.002<br>&nbsp;&nbsp;…\": 0.002<br>&nbsp;&nbsp;نوع: 0.001<br>",
           "<b>Entropy:</b> 0.059<br><b>Pred:</b> aeda<br><b>Top-5:</b><br>&nbsp;&nbsp;aeda: 0.003<br>&nbsp;&nbsp;페이지: 0.002<br>&nbsp;&nbsp;![: 0.001<br>&nbsp;&nbsp;…\": 0.001<br>&nbsp;&nbsp;linky: 0.001<br>",
           "<b>Entropy:</b> 0.123<br><b>Pred:</b> mx<br><b>Top-5:</b><br>&nbsp;&nbsp;mx: 0.008<br>&nbsp;&nbsp;immel: 0.004<br>&nbsp;&nbsp;mere: 0.004<br>&nbsp;&nbsp;CAF: 0.003<br>&nbsp;&nbsp;-client: 0.003<br>",
           "<b>Entropy:</b> 0.135<br><b>Pred:</b> errat<br><b>Top-5:</b><br>&nbsp;&nbsp;errat: 0.008<br>&nbsp;&nbsp;animate: 0.006<br>&nbsp;&nbsp;htag: 0.005<br>&nbsp;&nbsp; Vig: 0.004<br>&nbsp;&nbsp; vigor: 0.003<br>",
           "<b>Entropy:</b> 0.143<br><b>Pred:</b> ANEL<br><b>Top-5:</b><br>&nbsp;&nbsp;ANEL: 0.009<br>&nbsp;&nbsp;ANNEL: 0.005<br>&nbsp;&nbsp;ones: 0.005<br>&nbsp;&nbsp;Connected: 0.004<br>&nbsp;&nbsp;onec: 0.004<br>",
           "<b>Entropy:</b> 0.140<br><b>Pred:</b> icone<br><b>Top-5:</b><br>&nbsp;&nbsp;icone: 0.012<br>&nbsp;&nbsp;akens: 0.006<br>&nbsp;&nbsp;nesday: 0.004<br>&nbsp;&nbsp;siz: 0.003<br>&nbsp;&nbsp;dete: 0.003<br>",
           "<b>Entropy:</b> 0.155<br><b>Pred:</b> ルク<br><b>Top-5:</b><br>&nbsp;&nbsp;ルク: 0.017<br>&nbsp;&nbsp;icone: 0.006<br>&nbsp;&nbsp;aeda: 0.004<br>&nbsp;&nbsp;efon: 0.003<br>&nbsp;&nbsp;$MESS: 0.003<br>"
          ],
          [
           "<b>Entropy:</b> 0.078<br><b>Pred:</b> 페이지<br><b>Top-5:</b><br>&nbsp;&nbsp;페이지: 0.004<br>&nbsp;&nbsp;…and: 0.003<br>&nbsp;&nbsp;…\": 0.002<br>&nbsp;&nbsp;…I: 0.002<br>&nbsp;&nbsp;…”: 0.002<br>",
           "<b>Entropy:</b> 0.068<br><b>Pred:</b> 페이지<br><b>Top-5:</b><br>&nbsp;&nbsp;페이지: 0.004<br>&nbsp;&nbsp;字幕: 0.002<br>&nbsp;&nbsp;نوع: 0.002<br>&nbsp;&nbsp;Produto: 0.002<br>&nbsp;&nbsp;سال: 0.001<br>",
           "<b>Entropy:</b> 0.060<br><b>Pred:</b> aeda<br><b>Top-5:</b><br>&nbsp;&nbsp;aeda: 0.003<br>&nbsp;&nbsp;…”: 0.002<br>&nbsp;&nbsp;نم: 0.002<br>&nbsp;&nbsp;字幕: 0.002<br>&nbsp;&nbsp;페이지: 0.001<br>",
           "<b>Entropy:</b> 0.068<br><b>Pred:</b> 페이지<br><b>Top-5:</b><br>&nbsp;&nbsp;페이지: 0.004<br>&nbsp;&nbsp;字幕: 0.003<br>&nbsp;&nbsp;نوع: 0.002<br>&nbsp;&nbsp;…\": 0.002<br>&nbsp;&nbsp;سال: 0.002<br>",
           "<b>Entropy:</b> 0.098<br><b>Pred:</b> yonel<br><b>Top-5:</b><br>&nbsp;&nbsp;yonel: 0.005<br>&nbsp;&nbsp;RIEND: 0.003<br>&nbsp;&nbsp;.openg: 0.003<br>&nbsp;&nbsp;ldr: 0.003<br>&nbsp;&nbsp; linker: 0.003<br>",
           "<b>Entropy:</b> 0.133<br><b>Pred:</b> yonel<br><b>Top-5:</b><br>&nbsp;&nbsp;yonel: 0.006<br>&nbsp;&nbsp;jet: 0.006<br>&nbsp;&nbsp; Concurrent: 0.006<br>&nbsp;&nbsp;contri: 0.004<br>&nbsp;&nbsp;onus: 0.004<br>",
           "<b>Entropy:</b> 0.099<br><b>Pred:</b> ona<br><b>Top-5:</b><br>&nbsp;&nbsp;ona: 0.006<br>&nbsp;&nbsp; Wikispecies: 0.003<br>&nbsp;&nbsp;stants: 0.003<br>&nbsp;&nbsp; defaultCenter: 0.003<br>&nbsp;&nbsp;aeda: 0.003<br>",
           "<b>Entropy:</b> 0.276<br><b>Pred:</b> flen<br><b>Top-5:</b><br>&nbsp;&nbsp;flen: 0.034<br>&nbsp;&nbsp;icone: 0.019<br>&nbsp;&nbsp;ONUS: 0.006<br>&nbsp;&nbsp;efon: 0.005<br>&nbsp;&nbsp;isons: 0.005<br>",
           "<b>Entropy:</b> 0.075<br><b>Pred:</b> 페이지<br><b>Top-5:</b><br>&nbsp;&nbsp;페이지: 0.005<br>&nbsp;&nbsp;字幕: 0.003<br>&nbsp;&nbsp;…\": 0.002<br>&nbsp;&nbsp;aeda: 0.002<br>&nbsp;&nbsp;关于: 0.001<br>",
           "<b>Entropy:</b> 0.062<br><b>Pred:</b> aeda<br><b>Top-5:</b><br>&nbsp;&nbsp;aeda: 0.003<br>&nbsp;&nbsp;…\": 0.002<br>&nbsp;&nbsp;…”: 0.002<br>&nbsp;&nbsp;…I: 0.002<br>&nbsp;&nbsp;…): 0.001<br>",
           "<b>Entropy:</b> 0.127<br><b>Pred:</b> illi<br><b>Top-5:</b><br>&nbsp;&nbsp;illi: 0.008<br>&nbsp;&nbsp;icone: 0.004<br>&nbsp;&nbsp;azer: 0.004<br>&nbsp;&nbsp;immel: 0.004<br>&nbsp;&nbsp;oví: 0.004<br>",
           "<b>Entropy:</b> 0.108<br><b>Pred:</b>  Vig<br><b>Top-5:</b><br>&nbsp;&nbsp; Vig: 0.005<br>&nbsp;&nbsp; Pap: 0.004<br>&nbsp;&nbsp;apist: 0.004<br>&nbsp;&nbsp; Kag: 0.004<br>&nbsp;&nbsp;PO: 0.003<br>",
           "<b>Entropy:</b> 0.063<br><b>Pred:</b>  MISS<br><b>Top-5:</b><br>&nbsp;&nbsp; MISS: 0.003<br>&nbsp;&nbsp;ả: 0.002<br>&nbsp;&nbsp;ones: 0.002<br>&nbsp;&nbsp;ISCO: 0.002<br>&nbsp;&nbsp;tone: 0.002<br>",
           "<b>Entropy:</b> 0.150<br><b>Pred:</b> odiac<br><b>Top-5:</b><br>&nbsp;&nbsp;odiac: 0.007<br>&nbsp;&nbsp;icone: 0.006<br>&nbsp;&nbsp;dete: 0.006<br>&nbsp;&nbsp;ndern: 0.005<br>&nbsp;&nbsp;alore: 0.005<br>",
           "<b>Entropy:</b> 0.163<br><b>Pred:</b> efon<br><b>Top-5:</b><br>&nbsp;&nbsp;efon: 0.015<br>&nbsp;&nbsp;'gc: 0.005<br>&nbsp;&nbsp;$MESS: 0.005<br>&nbsp;&nbsp;icone: 0.005<br>&nbsp;&nbsp;idlo: 0.004<br>"
          ],
          [
           "<b>Entropy:</b> 0.143<br><b>Pred:</b> rone<br><b>Top-5:</b><br>&nbsp;&nbsp;rone: 0.007<br>&nbsp;&nbsp;utron: 0.007<br>&nbsp;&nbsp;�: 0.007<br>&nbsp;&nbsp;ardown: 0.004<br>&nbsp;&nbsp;hev: 0.003<br>",
           "<b>Entropy:</b> 0.174<br><b>Pred:</b>  dangling<br><b>Top-5:</b><br>&nbsp;&nbsp; dangling: 0.010<br>&nbsp;&nbsp;-en: 0.009<br>&nbsp;&nbsp; Dul: 0.006<br>&nbsp;&nbsp;w: 0.006<br>&nbsp;&nbsp; trouble: 0.005<br>",
           "<b>Entropy:</b> 0.311<br><b>Pred:</b> pek<br><b>Top-5:</b><br>&nbsp;&nbsp;pek: 0.028<br>&nbsp;&nbsp;:: 0.015<br>&nbsp;&nbsp; Liber: 0.014<br>&nbsp;&nbsp;,: 0.012<br>&nbsp;&nbsp;-: 0.007<br>",
           "<b>Entropy:</b> 0.437<br><b>Pred:</b>  be<br><b>Top-5:</b><br>&nbsp;&nbsp; be: 0.112<br>&nbsp;&nbsp;enschaft: 0.016<br>&nbsp;&nbsp;ersed: 0.015<br>&nbsp;&nbsp;mpar: 0.007<br>&nbsp;&nbsp;idlo: 0.005<br>",
           "<b>Entropy:</b> 0.071<br><b>Pred:</b>  Thor<br><b>Top-5:</b><br>&nbsp;&nbsp; Thor: 0.003<br>&nbsp;&nbsp; offset: 0.003<br>&nbsp;&nbsp; Jewel: 0.002<br>&nbsp;&nbsp; Offset: 0.002<br>&nbsp;&nbsp;nos: 0.002<br>",
           "<b>Entropy:</b> 0.071<br><b>Pred:</b> VEL<br><b>Top-5:</b><br>&nbsp;&nbsp;VEL: 0.003<br>&nbsp;&nbsp; fre: 0.002<br>&nbsp;&nbsp; SY: 0.002<br>&nbsp;&nbsp; Tavern: 0.002<br>&nbsp;&nbsp;st: 0.002<br>",
           "<b>Entropy:</b> 0.097<br><b>Pred:</b>  Diameter<br><b>Top-5:</b><br>&nbsp;&nbsp; Diameter: 0.004<br>&nbsp;&nbsp; Revision: 0.004<br>&nbsp;&nbsp; robust: 0.003<br>&nbsp;&nbsp; Lage: 0.003<br>&nbsp;&nbsp; Liber: 0.003<br>",
           "<b>Entropy:</b> 0.141<br><b>Pred:</b> incinn<br><b>Top-5:</b><br>&nbsp;&nbsp;incinn: 0.009<br>&nbsp;&nbsp;ement: 0.008<br>&nbsp;&nbsp;icone: 0.004<br>&nbsp;&nbsp;ixed: 0.003<br>&nbsp;&nbsp; supp: 0.003<br>",
           "<b>Entropy:</b> 0.114<br><b>Pred:</b> kart<br><b>Top-5:</b><br>&nbsp;&nbsp;kart: 0.006<br>&nbsp;&nbsp;alone: 0.005<br>&nbsp;&nbsp; oto: 0.003<br>&nbsp;&nbsp;erialized: 0.003<br>&nbsp;&nbsp;udi: 0.003<br>",
           "<b>Entropy:</b> 0.202<br><b>Pred:</b>  longer<br><b>Top-5:</b><br>&nbsp;&nbsp; longer: 0.023<br>&nbsp;&nbsp;xious: 0.013<br>&nbsp;&nbsp; Bowen: 0.004<br>&nbsp;&nbsp; ag: 0.003<br>&nbsp;&nbsp; Longer: 0.003<br>",
           "<b>Entropy:</b> 0.109<br><b>Pred:</b>  Screw<br><b>Top-5:</b><br>&nbsp;&nbsp; Screw: 0.005<br>&nbsp;&nbsp; Twelve: 0.004<br>&nbsp;&nbsp; McGregor: 0.004<br>&nbsp;&nbsp;icone: 0.004<br>&nbsp;&nbsp; keys: 0.003<br>",
           "<b>Entropy:</b> 0.090<br><b>Pred:</b> INTER<br><b>Top-5:</b><br>&nbsp;&nbsp;INTER: 0.004<br>&nbsp;&nbsp; Garrison: 0.003<br>&nbsp;&nbsp;eward: 0.003<br>&nbsp;&nbsp;htag: 0.003<br>&nbsp;&nbsp; Gold: 0.003<br>",
           "<b>Entropy:</b> 0.158<br><b>Pred:</b> ones<br><b>Top-5:</b><br>&nbsp;&nbsp;ones: 0.008<br>&nbsp;&nbsp;.: 0.007<br>&nbsp;&nbsp;on: 0.006<br>&nbsp;&nbsp;akan: 0.005<br>&nbsp;&nbsp;hq: 0.004<br>",
           "<b>Entropy:</b> 0.197<br><b>Pred:</b> izio<br><b>Top-5:</b><br>&nbsp;&nbsp;izio: 0.014<br>&nbsp;&nbsp;oft: 0.009<br>&nbsp;&nbsp;itag: 0.008<br>&nbsp;&nbsp; Liber: 0.006<br>&nbsp;&nbsp;icone: 0.005<br>",
           "<b>Entropy:</b> 0.172<br><b>Pred:</b>  Lage<br><b>Top-5:</b><br>&nbsp;&nbsp; Lage: 0.014<br>&nbsp;&nbsp; Lehr: 0.007<br>&nbsp;&nbsp;icone: 0.006<br>&nbsp;&nbsp;ense: 0.005<br>&nbsp;&nbsp;ic: 0.004<br>"
          ],
          [
           "<b>Entropy:</b> 0.131<br><b>Pred:</b> greg<br><b>Top-5:</b><br>&nbsp;&nbsp;greg: 0.011<br>&nbsp;&nbsp;ister: 0.004<br>&nbsp;&nbsp;utron: 0.004<br>&nbsp;&nbsp;otron: 0.003<br>&nbsp;&nbsp;Forward: 0.003<br>",
           "<b>Entropy:</b> 0.106<br><b>Pred:</b> alama<br><b>Top-5:</b><br>&nbsp;&nbsp;alama: 0.004<br>&nbsp;&nbsp;907: 0.004<br>&nbsp;&nbsp;aken: 0.004<br>&nbsp;&nbsp;909: 0.004<br>&nbsp;&nbsp;ovel: 0.003<br>",
           "<b>Entropy:</b> 0.263<br><b>Pred:</b> stra<br><b>Top-5:</b><br>&nbsp;&nbsp;stra: 0.027<br>&nbsp;&nbsp;abr: 0.014<br>&nbsp;&nbsp;estion: 0.008<br>&nbsp;&nbsp;est: 0.007<br>&nbsp;&nbsp;anz: 0.007<br>",
           "<b>Entropy:</b> 0.165<br><b>Pred:</b> abler<br><b>Top-5:</b><br>&nbsp;&nbsp;abler: 0.013<br>&nbsp;&nbsp;ersed: 0.006<br>&nbsp;&nbsp;abs: 0.005<br>&nbsp;&nbsp;arih: 0.005<br>&nbsp;&nbsp;_Abstract: 0.004<br>",
           "<b>Entropy:</b> 0.088<br><b>Pred:</b> sip<br><b>Top-5:</b><br>&nbsp;&nbsp;sip: 0.007<br>&nbsp;&nbsp;inta: 0.003<br>&nbsp;&nbsp;Liv: 0.002<br>&nbsp;&nbsp;riger: 0.002<br>&nbsp;&nbsp;jing: 0.002<br>",
           "<b>Entropy:</b> 0.168<br><b>Pred:</b> aspect<br><b>Top-5:</b><br>&nbsp;&nbsp;aspect: 0.025<br>&nbsp;&nbsp; Ri: 0.006<br>&nbsp;&nbsp;ety: 0.003<br>&nbsp;&nbsp;啪: 0.003<br>&nbsp;&nbsp;居: 0.002<br>",
           "<b>Entropy:</b> 0.158<br><b>Pred:</b> agger<br><b>Top-5:</b><br>&nbsp;&nbsp;agger: 0.010<br>&nbsp;&nbsp;stry: 0.007<br>&nbsp;&nbsp;icz: 0.007<br>&nbsp;&nbsp;ropa: 0.006<br>&nbsp;&nbsp;ollapse: 0.003<br>",
           "<b>Entropy:</b> 0.253<br><b>Pred:</b>  of<br><b>Top-5:</b><br>&nbsp;&nbsp; of: 0.029<br>&nbsp;&nbsp;esser: 0.018<br>&nbsp;&nbsp;户: 0.006<br>&nbsp;&nbsp; Aw: 0.005<br>&nbsp;&nbsp; indexed: 0.003<br>",
           "<b>Entropy:</b> 0.111<br><b>Pred:</b> ysi<br><b>Top-5:</b><br>&nbsp;&nbsp;ysi: 0.012<br>&nbsp;&nbsp;iaux: 0.004<br>&nbsp;&nbsp;lon: 0.002<br>&nbsp;&nbsp; Gest: 0.002<br>&nbsp;&nbsp;avour: 0.002<br>",
           "<b>Entropy:</b> 0.127<br><b>Pred:</b> xious<br><b>Top-5:</b><br>&nbsp;&nbsp;xious: 0.009<br>&nbsp;&nbsp; Cycl: 0.007<br>&nbsp;&nbsp;warts: 0.003<br>&nbsp;&nbsp; Bowen: 0.003<br>&nbsp;&nbsp; longer: 0.002<br>",
           "<b>Entropy:</b> 0.170<br><b>Pred:</b>  Reply<br><b>Top-5:</b><br>&nbsp;&nbsp; Reply: 0.013<br>&nbsp;&nbsp;igos: 0.012<br>&nbsp;&nbsp; Ya: 0.004<br>&nbsp;&nbsp;mare: 0.004<br>&nbsp;&nbsp;igel: 0.003<br>",
           "<b>Entropy:</b> 0.119<br><b>Pred:</b> azen<br><b>Top-5:</b><br>&nbsp;&nbsp;azen: 0.007<br>&nbsp;&nbsp;แรง: 0.005<br>&nbsp;&nbsp;牙: 0.004<br>&nbsp;&nbsp; been: 0.004<br>&nbsp;&nbsp; Quadr: 0.003<br>",
           "<b>Entropy:</b> 0.180<br><b>Pred:</b> ION<br><b>Top-5:</b><br>&nbsp;&nbsp;ION: 0.017<br>&nbsp;&nbsp;ione: 0.007<br>&nbsp;&nbsp;sing: 0.005<br>&nbsp;&nbsp;akan: 0.004<br>&nbsp;&nbsp; DSP: 0.004<br>",
           "<b>Entropy:</b> 0.221<br><b>Pred:</b> fulness<br><b>Top-5:</b><br>&nbsp;&nbsp;fulness: 0.016<br>&nbsp;&nbsp;oft: 0.014<br>&nbsp;&nbsp;ayar: 0.011<br>&nbsp;&nbsp;ozo: 0.005<br>&nbsp;&nbsp;ness: 0.003<br>",
           "<b>Entropy:</b> 0.099<br><b>Pred:</b>  course<br><b>Top-5:</b><br>&nbsp;&nbsp; course: 0.006<br>&nbsp;&nbsp;course: 0.005<br>&nbsp;&nbsp; dossier: 0.002<br>&nbsp;&nbsp;pell: 0.002<br>&nbsp;&nbsp;icina: 0.002<br>"
          ]
         ],
         "text": [
          [
           " ",
           "://",
           ",",
           " be",
           " measured",
           " in",
           " the",
           ".",
           " It",
           " understanding",
           " can",
           " ever",
           " understanding",
           " of",
           " its"
          ],
          [
           " be",
           "contri",
           ",",
           " be",
           " quant",
           " without",
           " matter",
           ";",
           " It",
           " matter",
           " program",
           " ever",
           " understanding",
           " of",
           " its"
          ],
          [
           "'gc",
           "'gc",
           ".com",
           " be",
           " defined",
           " without",
           " matter",
           " being",
           "faith",
           " understanding",
           " program",
           " ever",
           " understanding",
           " whatever",
           " itself"
          ],
          [
           "'gc",
           "'gc",
           ".com",
           "字幕",
           " quant",
           " without",
           " matter",
           " being",
           " compreh",
           " understanding",
           " program",
           " ever",
           " understanding",
           " of",
           " itself"
          ],
          [
           "'gc",
           "'gc",
           " META",
           "字幕",
           " compartment",
           " without",
           " matter",
           " being",
           " compreh",
           " understanding",
           " program",
           " ever",
           " understanding",
           " of",
           " itself"
          ],
          [
           "'gc",
           "'gc",
           " META",
           "字幕",
           " compartment",
           " without",
           " matter",
           " being",
           " compreh",
           " understanding",
           " program",
           " ever",
           " understanding",
           " nor",
           " itself"
          ],
          [
           "'gc",
           "'gc",
           ".Focused",
           "字幕",
           " directly",
           " without",
           " being",
           " being",
           " compreh",
           " understanding",
           " can",
           " ever",
           " understanding",
           " nor",
           " itself"
          ],
          [
           "'gc",
           "'gc",
           " META",
           "字幕",
           " directly",
           " without",
           " being",
           " being",
           " Understanding",
           " understanding",
           " ever",
           " ever",
           " understanding",
           " nor",
           " itself"
          ],
          [
           "'gc",
           "'gc",
           ".Focused",
           "字幕",
           " compartment",
           " without",
           " being",
           " being",
           " Understanding",
           " matter",
           " can",
           " ever",
           " understanding",
           " nor",
           " itself"
          ],
          [
           "'gc",
           "'gc",
           "eview",
           "페이지",
           " compartment",
           " without",
           " being",
           " being",
           " Understanding",
           " understanding",
           " ever",
           " ever",
           " understanding",
           " nor",
           " itself"
          ],
          [
           "'gc",
           "'gc",
           "сок",
           "페이지",
           " without",
           " without",
           " being",
           "nor",
           "overe",
           " matter",
           " ever",
           " ever",
           " chance",
           " nor",
           " itself"
          ],
          [
           "'gc",
           "'gc",
           "сок",
           "페이지",
           "wing",
           " without",
           " being",
           "nor",
           "alink",
           " matter",
           " ever",
           " ever",
           "sembl",
           " nor",
           " being"
          ],
          [
           "'gc",
           "'gc",
           "eum",
           "페이지",
           "Ace",
           " without",
           " being",
           "ton",
           "alink",
           " matter",
           "ize",
           " ever",
           "THING",
           " nor",
           " itself"
          ],
          [
           "'gc",
           "'gc",
           "eum",
           "페이지",
           "Ace",
           "-day",
           " adequate",
           "ton",
           "alink",
           " nor",
           "ize",
           " ever",
           " meaningful",
           "aby",
           " itself"
          ],
          [
           "'gc",
           "﻿using",
           "'gc",
           "페이지",
           " Cant",
           " without",
           "ongodb",
           "ton",
           "MISS",
           " nor",
           " anywhere",
           " ever",
           "THING",
           " nor",
           " itself"
          ],
          [
           "'gc",
           "字幕",
           "'gc",
           "페이지",
           " itself",
           "-day",
           "lán",
           "bine",
           "MISS",
           " vice",
           "ize",
           " ever",
           "THING",
           "lun",
           " itself"
          ],
          [
           "'gc",
           "字幕",
           "'gc",
           "페이지",
           " any",
           "-day",
           "lán",
           "bine",
           "ommen",
           " nor",
           "ization",
           " nor",
           "sembl",
           "ht",
           " itself"
          ],
          [
           "'gc",
           "字幕",
           "'gc",
           "페이지",
           "wing",
           " itself",
           " Pf",
           "bine",
           "ommen",
           "akh",
           "idge",
           " nor",
           " Cann",
           " foul",
           " itself"
          ],
          [
           "'gc",
           "페이지",
           "'gc",
           "페이지",
           "wing",
           " unless",
           "folio",
           "bine",
           "ommen",
           " matter",
           "idge",
           " nor",
           " Cann",
           " foul",
           "lobber"
          ],
          [
           "'gc",
           "페이지",
           "'gc",
           "페이지",
           "wing",
           "ROID",
           " Folk",
           " foul",
           "'gc",
           " matter",
           "ostel",
           " nor",
           "lectric",
           "PELL",
           "afone"
          ],
          [
           "'gc",
           "페이지",
           "'gc",
           "페이지",
           "wing",
           "�回",
           "tesy",
           "bine",
           "'gc",
           " matter",
           " nor",
           " unless",
           "VICE",
           "aden",
           " Licence"
          ],
          [
           "'gc",
           "페이지",
           "'gc",
           "페이지",
           "wing",
           "�回",
           "ardless",
           " foul",
           "'gc",
           " matter",
           "oux",
           "iki",
           "frei",
           "aden",
           " Licence"
          ],
          [
           "페이지",
           "페이지",
           "'gc",
           "페이지",
           "ogue",
           "RTC",
           " either",
           " invite",
           "'gc",
           " matter",
           "ull",
           "nte",
           " enh",
           "aden",
           " ►"
          ],
          [
           "페이지",
           "페이지",
           "'gc",
           "페이지",
           "idan",
           "orthy",
           "erap",
           " Econ",
           "aeda",
           " matter",
           "kle",
           "nte",
           "YLON",
           "aden",
           "aris"
          ],
          [
           "페이지",
           "페이지",
           "'gc",
           "페이지",
           "YRO",
           " dispro",
           "enin",
           " landsc",
           "'gc",
           "/place",
           "CAF",
           "iki",
           "asm",
           ".scalablytyped",
           "ELY"
          ],
          [
           "페이지",
           "페이지",
           "'gc",
           "페이지",
           "ccd",
           "ubb",
           "yer",
           "olic",
           "aeda",
           "/place",
           " Reserved",
           " Fle",
           "YLON",
           "olen",
           "bate"
          ],
          [
           "페이지",
           "페이지",
           "aeda",
           "페이지",
           "YRO",
           "alion",
           " Nab",
           "ably",
           "aeda",
           "aeda",
           "��",
           "enna",
           "ella",
           "ISP",
           "beit"
          ],
          [
           "페이지",
           "페이지",
           "aeda",
           "페이지",
           "alu",
           "rippling",
           "LL",
           "ably",
           "페이지",
           "aeda",
           "mere",
           "enna",
           " Connected",
           "aben",
           "aben"
          ],
          [
           "페이지",
           "페이지",
           "페이지",
           "페이지",
           "zeichnet",
           "isce",
           "thinkable",
           "uali",
           "페이지",
           "aeda",
           "mere",
           " dated",
           "ope",
           "icone",
           "icone"
          ],
          [
           "페이지",
           "페이지",
           "페이지",
           "페이지",
           "onu",
           "yonel",
           "isons",
           "icone",
           "페이지",
           "aeda",
           "mx",
           "errat",
           "ANEL",
           "icone",
           "ルク"
          ],
          [
           "페이지",
           "페이지",
           "aeda",
           "페이지",
           "yonel",
           "yonel",
           "ona",
           "flen",
           "페이지",
           "aeda",
           "illi",
           " Vig",
           " MISS",
           "odiac",
           "efon"
          ],
          [
           "rone",
           " dangling",
           "pek",
           " be",
           " Thor",
           "VEL",
           " Diameter",
           "incinn",
           "kart",
           " longer",
           " Screw",
           "INTER",
           "ones",
           "izio",
           " Lage"
          ],
          [
           "greg",
           "alama",
           "stra",
           "abler",
           "sip",
           "aspect",
           "agger",
           " of",
           "ysi",
           "xious",
           " Reply",
           "azen",
           "ION",
           "fulness",
           " course"
          ]
         ],
         "textfont": {
          "size": 16
         },
         "texttemplate": "%{text}",
         "type": "heatmap",
         "x": [
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14
         ],
         "y": [
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32
         ],
         "z": {
          "bdata": "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",
          "dtype": "f4",
          "shape": "33, 15"
         },
         "zmax": 1,
         "zmin": 0
        },
        {
         "hoverinfo": "skip",
         "marker": {
          "opacity": 0
         },
         "mode": "markers",
         "showlegend": false,
         "type": "scatter",
         "x": [
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14
         ],
         "xaxis": "x2",
         "y": [
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null
         ]
        }
       ],
       "layout": {
        "font": {
         "family": "DejaVu Sans",
         "size": 14
        },
        "height": 1155,
        "margin": {
         "b": 10,
         "l": 20,
         "r": 10,
         "t": 40
        },
        "shapes": [
         {
          "layer": "above",
          "line": {
           "color": "black",
           "width": 2
          },
          "type": "rect",
          "x0": 2.5,
          "x1": 3.5,
          "y0": -0.5,
          "y1": 0.5
         },
         {
          "layer": "above",
          "line": {
           "color": "black",
           "width": 2
          },
          "type": "rect",
          "x0": 6.5,
          "x1": 7.5,
          "y0": -0.5,
          "y1": 0.5
         },
         {
          "layer": "above",
          "line": {
           "color": "black",
           "width": 2
          },
          "type": "rect",
          "x0": 12.5,
          "x1": 13.5,
          "y0": -0.5,
          "y1": 0.5
         },
         {
          "layer": "above",
          "line": {
           "color": "black",
           "width": 2
          },
          "type": "rect",
          "x0": 2.5,
          "x1": 3.5,
          "y0": 0.5,
          "y1": 1.5
         },
         {
          "layer": "above",
          "line": {
           "color": "black",
           "width": 2
          },
          "type": "rect",
          "x0": 4.5,
          "x1": 5.5,
          "y0": 0.5,
          "y1": 1.5
         },
         {
          "layer": "above",
          "line": {
           "color": "black",
           "width": 2
          },
          "type": "rect",
          "x0": 12.5,
          "x1": 13.5,
          "y0": 0.5,
          "y1": 1.5
         },
         {
          "layer": "above",
          "line": {
           "color": "black",
           "width": 2
          },
          "type": "rect",
          "x0": 2.5,
          "x1": 3.5,
          "y0": 1.5,
          "y1": 2.5
         },
         {
          "layer": "above",
          "line": {
           "color": "black",
           "width": 2
          },
          "type": "rect",
          "x0": 4.5,
          "x1": 5.5,
          "y0": 1.5,
          "y1": 2.5
         },
         {
          "layer": "above",
          "line": {
           "color": "black",
           "width": 2
          },
          "type": "rect",
          "x0": 4.5,
          "x1": 5.5,
          "y0": 2.5,
          "y1": 3.5
         },
         {
          "layer": "above",
          "line": {
           "color": "black",
           "width": 2
          },
          "type": "rect",
          "x0": 12.5,
          "x1": 13.5,
          "y0": 2.5,
          "y1": 3.5
         },
         {
          "layer": "above",
          "line": {
           "color": "black",
           "width": 2
          },
          "type": "rect",
          "x0": 4.5,
          "x1": 5.5,
          "y0": 3.5,
          "y1": 4.5
         },
         {
          "layer": "above",
          "line": {
           "color": "black",
           "width": 2
          },
          "type": "rect",
          "x0": 12.5,
          "x1": 13.5,
          "y0": 3.5,
          "y1": 4.5
         },
         {
          "layer": "above",
          "line": {
           "color": "black",
           "width": 2
          },
          "type": "rect",
          "x0": 4.5,
          "x1": 5.5,
          "y0": 4.5,
          "y1": 5.5
         },
         {
          "layer": "above",
          "line": {
           "color": "black",
           "width": 2
          },
          "type": "rect",
          "x0": 4.5,
          "x1": 5.5,
          "y0": 5.5,
          "y1": 6.5
         },
         {
          "layer": "above",
          "line": {
           "color": "black",
           "width": 2
          },
          "type": "rect",
          "x0": 4.5,
          "x1": 5.5,
          "y0": 6.5,
          "y1": 7.5
         },
         {
          "layer": "above",
          "line": {
           "color": "black",
           "width": 2
          },
          "type": "rect",
          "x0": 4.5,
          "x1": 5.5,
          "y0": 7.5,
          "y1": 8.5
         },
         {
          "layer": "above",
          "line": {
           "color": "black",
           "width": 2
          },
          "type": "rect",
          "x0": 4.5,
          "x1": 5.5,
          "y0": 8.5,
          "y1": 9.5
         },
         {
          "layer": "above",
          "line": {
           "color": "black",
           "width": 2
          },
          "type": "rect",
          "x0": 4.5,
          "x1": 5.5,
          "y0": 9.5,
          "y1": 10.5
         },
         {
          "layer": "above",
          "line": {
           "color": "black",
           "width": 2
          },
          "type": "rect",
          "x0": 4.5,
          "x1": 5.5,
          "y0": 10.5,
          "y1": 11.5
         },
         {
          "layer": "above",
          "line": {
           "color": "black",
           "width": 2
          },
          "type": "rect",
          "x0": 4.5,
          "x1": 5.5,
          "y0": 11.5,
          "y1": 12.5
         },
         {
          "layer": "above",
          "line": {
           "color": "black",
           "width": 2
          },
          "type": "rect",
          "x0": 4.5,
          "x1": 5.5,
          "y0": 13.5,
          "y1": 14.5
         },
         {
          "layer": "above",
          "line": {
           "color": "black",
           "width": 2
          },
          "type": "rect",
          "x0": 2.5,
          "x1": 3.5,
          "y0": 30.5,
          "y1": 31.5
         }
        ],
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermap": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermap"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "width": 1500,
        "xaxis": {
         "anchor": "y",
         "domain": [
          0,
          1
         ],
         "side": "bottom",
         "tickmode": "array",
         "ticktext": [
          "<|begin_of_text|>",
          "Int",
          "elligence",
          " cannot",
          " be",
          " present",
          " without",
          " understanding",
          ".",
          " No",
          " computer",
          " has",
          " any",
          " awareness",
          " of"
         ],
         "tickvals": [
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14
         ],
         "title": {
          "text": "Input Token"
         }
        },
        "xaxis2": {
         "anchor": "free",
         "overlaying": "x",
         "position": 1,
         "showline": true,
         "side": "top",
         "tickmode": "array",
         "ticks": "outside",
         "ticktext": [
          "Int",
          "elligence",
          " cannot",
          " be",
          " present",
          " without",
          " understanding",
          ".",
          " No",
          " computer",
          " has",
          " any",
          " awareness",
          " of",
          " what"
         ],
         "tickvals": [
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14
         ]
        },
        "yaxis": {
         "autorange": "reversed",
         "tickmode": "array",
         "ticktext": [
          "layers.31",
          "layers.30",
          "layers.29",
          "layers.28",
          "layers.27",
          "layers.26",
          "layers.25",
          "layers.24",
          "layers.23",
          "layers.22",
          "layers.21",
          "layers.20",
          "layers.19",
          "layers.18",
          "layers.17",
          "layers.16",
          "layers.15",
          "layers.14",
          "layers.13",
          "layers.12",
          "layers.11",
          "layers.10",
          "layers.9",
          "layers.8",
          "layers.7",
          "layers.6",
          "layers.5",
          "layers.4",
          "layers.3",
          "layers.2",
          "layers.1",
          "layers.0",
          "embed_tokens"
         ],
         "tickvals": [
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32
         ],
         "title": {
          "text": "Layer"
         }
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "logit_lens.plot_topk_logit_lens(\n",
    "    model=hfbit1_fp32,\n",
    "    tokenizer=hfbit1_tokenizer,\n",
    "    inputs=MiscPrompts.Q11.value,\n",
    "    start_ix=0, end_ix=15,\n",
    "    topk=5,\n",
    "    topk_mean=True,\n",
    "    plot_topk_lens=True,\n",
    "    entropy=True,\n",
    "    block_step=1,\n",
    "    token_font_size=16,\n",
    "    #json_log_path=None,\n",
    "    #json_log_path='logs/nq_answers/dh.3b-bnb4bit.fp32', # 20 samples\n",
    "    #save_fig_path=None,\n",
    "    #save_fig_path='Outputs/LogitLens/DH3B/logits_3b_fp32_math.jpg',\n",
    "    model_precision=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logit_lens.plot_topk_logit_lens(\n",
    "    model=hfbit1_fp32,\n",
    "    tokenizer=hfbit1_tokenizer,\n",
    "    inputs=MiscPrompts.Q11.value,\n",
    "    start_ix=0, end_ix=15,\n",
    "    topk=5,\n",
    "    topk_mean=True,\n",
    "    plot_topk_lens=True,\n",
    "    entropy=True,\n",
    "    block_step=1,\n",
    "    token_font_size=18,\n",
    "    #json_log_path=None,\n",
    "    #json_log_path='logs/nq_answers/dh.3b-bnb4bit.fp32', # 20 samples\n",
    "    #save_fig_path=None,\n",
    "    #save_fig_path='Outputs/LogitLens/DH3B/logits_3b_fp32_math.jpg',\n",
    "    model_precision=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logit_lens.plot_topk_comparing_lens(\n",
    "    models=(hfbit1_fp32, llama8b_fp32),\n",
    "    tokenizers=(hfbit1_tokenizer, llama8b_tokenizer),\n",
    "    inputs=MiscPrompts.Q11.value,\n",
    "    start_ix=0, end_ix=15,\n",
    "    topk=5,\n",
    "    topk_mean=False,\n",
    "    #js=True,\n",
    "    block_step=1,\n",
    "    token_font_size=18,\n",
    "    #json_log_path=None,\n",
    "    #json_log_path='logs/nq_answers/dh.3b-bnb4bit.fp32', # 20 samples\n",
    "    #save_fig_path=None,\n",
    "    #save_fig_path='Outputs/LogitLens/DH3B/logits_3b_fp32_math.jpg',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dh8b_bnb8_float32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "json_path = 'logs/gsm8k/llama.8b-1.58.fp32'\n",
    "# Load your JSON file\n",
    "with open(json_path, 'r') as f:\n",
    "    log_data = json.load(f)\n",
    "\n",
    "# Convert the loaded JSON data to a DataFrame\n",
    "df = pd.json_normalize(log_data)\n",
    "\n",
    "# If you want to see the dataframe\n",
    "print(df)\n",
    "\n",
    "# Optionally, display it in a Jupyter notebook in a more readable format\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "# Load the JSON file and normalize into DataFrame\n",
    "with open('logs/gsm8k/dh.3b-ptsq.fp32', 'r') as f:\n",
    "    log_data = json.load(f)\n",
    "df = pd.json_normalize(log_data)\n",
    "\n",
    "# Ensure each row's layer_names and entropy have matching length\n",
    "num_layers = len(df.loc[0, 'layer_names'])\n",
    "sum_entropy = [0.0] * num_layers\n",
    "valid_rows = 0\n",
    "\n",
    "for _, row in df.iterrows():\n",
    "    layer_names = row['layer_names']\n",
    "    entropy = row['entropy']\n",
    "    \n",
    "    # Validate that both lists are the expected length\n",
    "    if isinstance(entropy, list) and len(entropy) == num_layers:\n",
    "        sum_entropy = [s + e for s, e in zip(sum_entropy, entropy)]\n",
    "        valid_rows += 1\n",
    "\n",
    "# Compute average\n",
    "if valid_rows > 0:\n",
    "    avg_entropy = [e / valid_rows for e in sum_entropy]\n",
    "    layer_labels = df.loc[0, 'layer_names']\n",
    "\n",
    "    # Plotting\n",
    "    plt.figure(figsize=(14, 6))\n",
    "    plt.plot(layer_labels, avg_entropy, marker='o', linestyle='-', color='b')\n",
    "    plt.xlabel('Layer Name')\n",
    "    plt.ylabel('Average Entropy')\n",
    "    plt.title(f'Average Entropy Across Layers (n = {valid_rows} samples)')\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No valid rows matched expected layer length.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['entropy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['normalized_entropy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logit_lens.plot_logit_lens(\n",
    "    model=hfbit1_fp32,\n",
    "    tokenizer=hfbit1_tokenizer,\n",
    "    input_ids=MiscPrompts.Q12.value,\n",
    "    start_ix=0, end_ix=15,\n",
    "    save_fig_path=None,\n",
    "    #save_fig_path='Outputs/LogitLens/LI8B/probs_hf1bit_fp32_qa_blockstep10.jpg',\n",
    "    #probs=True,\n",
    "    #block_step=5,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logit_lens.plot_logit_lens(\n",
    "    model=dh3b_bitnet_fp32_ptsq,\n",
    "    tokenizer=dh3b_tokenizer,\n",
    "    input_ids=nq_answers[0],\n",
    "    start_ix=2, end_ix=7,\n",
    "    save_fig_path='Outputs/Report/LogitLens/DH3B/ranks_bs5_ptsq_nq_a0.png',\n",
    "    #save_fig_path=None,\n",
    "    ranks=True,\n",
    "    #top_down=False,\n",
    "    block_step=5,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logit_lens.plot_comparing_lens(\n",
    "    models=(llama8b_fp32, hfbit1_fp32),\n",
    "    tokenizer=llama8b_tokenizer,\n",
    "    input_ids=nq_queries[5],\n",
    "    start_ix=1, end_ix=8,\n",
    "    #save_fig_path='Outputs/Report/LogitLens/LI8B/nwd_hf1bitllm_nq_a0.png',\n",
    "    #save_fig_path=None,\n",
    "    js=True,\n",
    "    #top_down=False,\n",
    "    block_step=3,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logit_lens.plot_topk_lens(\n",
    "    model=dh3b_bitnet_fp32_ptsq,\n",
    "    tokenizer=dh3b_tokenizer,\n",
    "    input_ids=nq_answers[0],\n",
    "    start_ix=0, end_ix=15,\n",
    "    topk_n=5,\n",
    "    save_fig_path='Outputs/Report/LogitLens/DH3B/topk5logits_ptsq_nq_a0.png',\n",
    "    #save_fig_path=None,\n",
    "    #top_down=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activation_lens.plot_activation_lens(\n",
    "    model=dh3b_bitnet_fp32_ptsq,\n",
    "    tokenizer=dh3b_tokenizer,\n",
    "    input_ids=nq_answers[0],\n",
    "    start_ix=0, end_ix=15,\n",
    "    metric='var',\n",
    "    save_fig_path='Outputs/Report/LogitLens/DH3B/actvar_ptsq_nq_a0.png',\n",
    "    #save_fig_path=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activation_lens.plot_activation_lens(\n",
    "    model=dh3b_fp32,\n",
    "    tokenizer=dh3b_tokenizer,\n",
    "    input_ids=nq_answers[0],\n",
    "    start_ix=0, end_ix=15,\n",
    "    metric='var',\n",
    "    save_fig_path='Outputs/Report/LogitLens/DH3B/actvar_dh3bsingle_nq_a0.png',\n",
    "    #save_fig_path=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activation_lens.plot_comparing_act_lens(\n",
    "    models=(dh3b_fp32, dh3b_bitnet_fp32_ptsq),\n",
    "    tokenizer=dh3b_tokenizer,\n",
    "    input_ids=nq_answers[0],\n",
    "    start_ix=0, end_ix=15,\n",
    "    metric='norm',\n",
    "    metric_name='l2',\n",
    "    save_fig_path='Outputs/Report/LogitLens/DH3B/actnorml2_dh3b_nq_a0.png',\n",
    "    #save_fig_path=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dictionary Learning: SAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch._dynamo\n",
    "torch._dynamo.config.suppress_errors = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dh3b_bitnet_fp32_ptsq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary_learning.plot_sae_heatmap(\n",
    "    model=dh3b_fp32,\n",
    "    tokenizer=dh3b_tokenizer,\n",
    "    inputs=gsm8k_questions_sae,\n",
    "    plot_sae=True,\n",
    "    do_log=True,\n",
    "    top_k=5,\n",
    "    tokens_per_row=30,\n",
    "    target_layers=[2, 9, 16, 20, 26],\n",
    "    log_path='logs/sae_logs/DH3B/fp',\n",
    "    log_name='dh.3b-ptsq.fp32',\n",
    "    fig_path=None,\n",
    "    deterministic_sae=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary_learning.plot_sae_heatmap(\n",
    "    model=olmo1b_fp32,\n",
    "    tokenizer=olmo1b_tokenizer,\n",
    "    inputs=Texts.T1.value,\n",
    "    do_log=False,\n",
    "    top_k=5,\n",
    "    tokens_per_row=30,\n",
    "    target_layers=[5, 10, 15],\n",
    "    log_path=None,\n",
    "    log_name=None,\n",
    "    fig_path=None,\n",
    "    deterministic_sae=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary_learning.plot_sae_heatmap(\n",
    "    model=olmo1b_fp32,\n",
    "    tokenizer=olmo1b_tokenizer,\n",
    "    inputs=Texts.T1.value,\n",
    "    do_log=False,\n",
    "    top_k=5,\n",
    "    tokens_per_row=30,\n",
    "    target_layers=[5, 10, 15],\n",
    "    log_path=None,\n",
    "    log_name=None,\n",
    "    fig_path=None,\n",
    "    deterministic_sae=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary_learning.plot_comparing_heatmap(\n",
    "    models=(dh3b_fp32, dh3b_bitnet_fp32_ptsq),\n",
    "    tokenizer=dh3b_tokenizer,\n",
    "    inputs=gsm8k_questions_sae,\n",
    "    top_k=5,\n",
    "    tokens_per_row=30,\n",
    "    target_layers=[2, 9, 16, 20, 26],\n",
    "    fig_path=None,\n",
    "    deterministic_sae=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary_learning.plot_comparing_heatmap(\n",
    "    models=(llama8b_fp32, hfbit1_fp32),\n",
    "    tokenizer=llama8b_tokenizer,\n",
    "    inputs=gsm8k_questions_sae,\n",
    "    top_k=5,\n",
    "    tokens_per_row=30,\n",
    "    target_layers=[2, 9, 16, 23, 30],\n",
    "    fig_path=None\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Hermes Chatbot Analysis (template only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_dict = {\n",
    "    #'dh.3b-llama.fp32': dh3b_fp32,\n",
    "    #'dh.3b-bnb4bit.fp16': dh3b_bnb4_fp16,\n",
    "    #'dh.3b-1.58.ptdq': dh3b_bitnet_fp32, \n",
    "    #'dh.3b-1.58.ptsq': dh3b_bitnet_fp32,\n",
    "    #'dh.8b-llama.fp32': dh8b_fp32,\n",
    "    #'dh.8b-bnb4bit.fp16': dh8b_bnb4_fp16,\n",
    "    #'dh.8b-1.58.ptdq': dh8b_bitnet_fp32,\n",
    "    #'dh.8b-1.58.ptsq': dh8b_bitnet_fp32,\n",
    "    #'llama.8b-instruct.fp32': llama8b_fp32,\n",
    "    #'llama.8b-bnb4bit.fp16': llama8b_bnb4_fp16,\n",
    "    #'llama.8b-1.58.fp32': hfbit1_fp32,\n",
    "    #'llama.8b-1.58.ptdq': llama8b_bitnet_fp32,\n",
    "    #'llama.8b-1.58.ptsq': llama8b_bitnet_fp32,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PARAMS:Dict = {\n",
    "    'context': Contexts.C1.value,\n",
    "    'prompt': MiscPrompts.Q2.value,\n",
    "    'max_new_tokens': 100,\n",
    "    'temperature': 0.8,\n",
    "    'repetition_penalty': 1.1,\n",
    "    'sample': True,\n",
    "    'device': None\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chatbot_analysis.run_nq_analysis(\n",
    "    model=dh3b_bnb4_float32,\n",
    "    tokenizer=dh3b_tokenizer,\n",
    "    model_name='dh.3b-bnb4bit.fp32',\n",
    "    dataset=nq_dataset['train'],\n",
    "    save_path='logs/nq_logs/DH3B',\n",
    "    num_samples=10,\n",
    "    deterministic_backend=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chatbot_analysis.run_gsm8k_analysis(\n",
    "    model=dh3b_bnb4_float32,\n",
    "    tokenizer=dh3b_tokenizer,\n",
    "    model_name='dh.3b-bnb4bit.fp32',\n",
    "    dataset=gsm8k_dataset['train'],\n",
    "    save_path='logs/gsm8k_logs/DH3B',\n",
    "    num_samples=10,\n",
    "    deterministic_backend=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chatbot_analysis.run_chatbot_analysis(\n",
    "    models=chat_dict,\n",
    "    tokenizer=dh3b_tokenizer,\n",
    "    deep_thinking=False,\n",
    "    full_path='logs/chatbot_logs',\n",
    "    deterministic_backend=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chatbot_analysis.plot_chatbot_analysis(\n",
    "    json_logs='logs/gsm8k_logs',\n",
    "    parallel_plot=True,\n",
    "    reference_file='logs/gsm8k_logs/llama.8b-1.58.fp32.json'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chatbot_analysis.plot_chatbot_analysis(\n",
    "    json_logs='logs/gsm8k_logs',\n",
    "    parallel_plot=False,\n",
    "    reference_file='logs/gsm8k_logs/llama.8b-1.58.fp32.json',\n",
    "    title=\"Model Metrics ('What is y if y=2*2-4+(3*2)')\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import matplotlib.cm as cm\n",
    "# Path to your results folder\n",
    "results_dir = \"logs/gsm8k_logs/LI8B\"\n",
    "\n",
    "# Load all JSONs into a DataFrame\n",
    "all_results = []\n",
    "for filename in os.listdir(results_dir):\n",
    "    if filename.endswith(\".json\"):\n",
    "        with open(os.path.join(results_dir, filename), 'r') as f:\n",
    "            data = json.load(f)\n",
    "            all_results.append(data)\n",
    "\n",
    "df = pd.DataFrame(all_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = [\n",
    "    'Perplexity',\n",
    "    'CPU Usage (%)',\n",
    "    'RAM Usage (%)',\n",
    "    'GPU Memory (MB)',\n",
    "    'Activation Similarity',\n",
    "    'Latency (s)',\n",
    "    \"Last Layer Mean Activation\",\n",
    "    \"Last Layer Activation Std\",\n",
    "    \"Mean Logits\",\n",
    "    \"Logit Std\",\n",
    "]\n",
    "\n",
    "# Generate a color map\n",
    "models = df['Model'].tolist()\n",
    "num_models = len(models)\n",
    "colors = cm.get_cmap('coolwarm', num_models)\n",
    "model_colors = {model: colors(i) for i, model in enumerate(models)}\n",
    "\n",
    "# Prepare subplot grid dynamically\n",
    "n_metrics = len(metrics)\n",
    "ncols = 5\n",
    "nrows = int(np.ceil(n_metrics / ncols))\n",
    "\n",
    "plt.rcParams['font.family'] = 'Times New Roman'\n",
    "fig, axes = plt.subplots(nrows=nrows, ncols=ncols, figsize=(20, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, metric in enumerate(metrics):\n",
    "    ax = axes[i]\n",
    "    if metric in df.columns:\n",
    "        for j, model in enumerate(models):\n",
    "            ax.bar(model, df.loc[j, metric], color=model_colors[model])\n",
    "        ax.set_title(metric, fontsize=12)\n",
    "        ax.set_xticks(range(len(models)))\n",
    "        ax.set_xticklabels(models, rotation=45, ha='right', fontsize=10)\n",
    "        ax.grid(True)\n",
    "\n",
    "# Hide unused axes\n",
    "for j in range(i + 1, len(axes)):\n",
    "    fig.delaxes(axes[j])\n",
    "\n",
    "# Legend\n",
    "##handles = [plt.Rectangle((0, 0), 1, 1, color=model_colors[model]) for model in models]\n",
    "#fig.legend(handles, models, loc='upper center', ncol=min(num_models, 5))\n",
    "\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.93])\n",
    "#plt.suptitle(\"Deep Hermes LLaMA 3B & LLaMA Instruct 8B GSM8K (n=10)\", fontsize=12)\n",
    "plt.suptitle(\"LLaMA 8B Instruct GSM8K (n=10)\", fontsize=14)\n",
    "plt.savefig('Outputs/Report/gsm8k_qa/llama8b_subplots.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = [\n",
    "    'Perplexity',\n",
    "    'CPU Usage (%)',\n",
    "    'RAM Usage (%)',\n",
    "    'GPU Memory (MB)',\n",
    "    'Activation Similarity',\n",
    "    'Latency (s)',\n",
    "    \"Last Layer Mean Activation\",\n",
    "    \"Last Layer Activation Std\",\n",
    "    \"Mean Logits\",\n",
    "    \"Logit Std\",\n",
    "]\n",
    "\n",
    "# Generate a color map\n",
    "models = df['Model'].tolist()\n",
    "num_models = len(models)\n",
    "colors = cm.get_cmap('coolwarm', num_models)\n",
    "model_colors = {model: colors(i) for i, model in enumerate(models)}\n",
    "\n",
    "# Prepare subplot grid dynamically\n",
    "n_metrics = len(metrics)\n",
    "ncols = 5\n",
    "nrows = int(np.ceil(n_metrics / ncols))\n",
    "\n",
    "plt.rcParams['font.family'] = 'Times New Roman'\n",
    "fig, axes = plt.subplots(nrows=nrows, ncols=ncols, figsize=(20, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, metric in enumerate(metrics):\n",
    "    ax = axes[i]\n",
    "    if metric in df.columns:\n",
    "        values = df[metric].copy()\n",
    "\n",
    "        # Normalize perplexity via log-scale\n",
    "        if metric == \"Perplexity\":\n",
    "            values = np.log1p(values)  # log1p handles 0 gracefully\n",
    "\n",
    "        for j, model in enumerate(models):\n",
    "            ax.bar(model, values[j], color=model_colors[model])\n",
    "        ax.set_title(metric + (\" (log)\" if metric == \"Perplexity\" else \"\"), fontsize=12)\n",
    "        ax.set_xticks(range(len(models)))\n",
    "        ax.set_xticklabels(models, rotation=45, ha='right', fontsize=10)\n",
    "        ax.grid(True)\n",
    "\n",
    "# Hide unused axes\n",
    "for j in range(i + 1, len(axes)):\n",
    "    fig.delaxes(axes[j])\n",
    "\n",
    "# Legend\n",
    "##handles = [plt.Rectangle((0, 0), 1, 1, color=model_colors[model]) for model in models]\n",
    "#fig.legend(handles, models, loc='upper center', ncol=min(num_models, 5))\n",
    "\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.93])\n",
    "plt.suptitle(\"Deep Hermes 3B LLaMA & LLaMA 8B Instruct GSM8K (n=10)\", fontsize=14)\n",
    "plt.savefig('Outputs/Report/gsm8k_qa/llamadh3b_subplots_normalized_perplexity.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "# Metrics to include (excluding 'Activation Similarity')\n",
    "metrics_for_corr = [\n",
    "    'Perplexity',\n",
    "    'CPU Usage (%)',\n",
    "    'RAM Usage (%)',\n",
    "    'GPU Memory (MB)',\n",
    "    'Latency (s)',\n",
    "    \"Last Layer Mean Activation\",\n",
    "    \"Last Layer Activation Std\",\n",
    "    \"Mean Logits\",\n",
    "    \"Logit Std\",\n",
    "]\n",
    "\n",
    "# Compute correlation\n",
    "corr_matrix = df[metrics_for_corr].corr()\n",
    "\n",
    "# Plot\n",
    "plt.rcParams['font.family'] = 'Times New Roman'\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(\n",
    "    corr_matrix, \n",
    "    annot=True, \n",
    "    cmap='coolwarm', \n",
    "    fmt=\".2f\", \n",
    "    linewidths=0.5, \n",
    "    square=True,\n",
    "    cbar_kws={\"shrink\": 0.75}\n",
    ")\n",
    "plt.title(\"LLaMA 8B Instruct GSM8K (n=10)\", fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.savefig('Outputs/Report/gsm8k_qa/llama8b_corr_heatmap.png')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MechInterp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
