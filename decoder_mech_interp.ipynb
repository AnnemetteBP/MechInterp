{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "from typing import Tuple, List, Dict, Optional, Any\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from pathlib import Path\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from datasets import load_dataset, DownloadMode\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import tqdm\n",
    "\n",
    "from helper_utils.enum_keys import (\n",
    "    FPKey,\n",
    "    ModelKey,\n",
    "    QuantStyle,\n",
    "    MiscPrompts,\n",
    "    Contexts,\n",
    "    Texts\n",
    ")\n",
    "\n",
    "from PTQ.bitlinear_wrapper_class import BitLinear\n",
    "from PTQ.apply_ptq import applyPTQ\n",
    "from PTQ.olmo_act_fns import patch_olmo_mlp\n",
    "import helper_utils.utils as utils\n",
    "from helper_utils.models_loader import load_4bit_auto, load_8bit_auto\n",
    "from mech_interp_utils.utils_main.src.transformer_utils import (\n",
    "    logit_lens,\n",
    "    activation_lens,\n",
    "    dictionary_learning,\n",
    "    chatbot_analysis\n",
    ")\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "original = torch.randn(512) * 0.5  # Original activations\n",
    "\n",
    "def quantize_dequantize(tensor, scale_value):\n",
    "    scale = max(scale_value, 1e-8)\n",
    "    qmin, qmax = -127, 127\n",
    "    tensor_int = (tensor / scale).round().clamp(qmin, qmax).to(torch.int8)\n",
    "    tensor_dequant = tensor_int.float() * scale\n",
    "    return tensor_int, tensor_dequant\n",
    "\n",
    "# Quantize with different scales\n",
    "_, dequant_1e2 = quantize_dequantize(original, 1e-2)\n",
    "_, dequant_1e5 = quantize_dequantize(original, 1e-5)\n",
    "\n",
    "# L2 distance\n",
    "print(\"L2 Distance (scale=1e-2):\", torch.norm(original - dequant_1e2).item())\n",
    "print(\"L2 Distance (scale=1e-5):\", torch.norm(original - dequant_1e5).item())\n",
    "\n",
    "# Plot histograms + KDEs\n",
    "plt.figure(figsize=(14, 6))\n",
    "sns.histplot(original.numpy(),bel='Original', kde=True, stat=\"count\", bins=50, color='black', alpha=0.5)\n",
    "sns.histplot(dequant_1e2.numpy(), label='Dequant (scale=1e-2)', kde=True, stat=\"count\", bins=50, color='red', alpha=0.5)\n",
    "sns.histplot(dequant_1e5.numpy(), label='Dequant (scale=1e-5)', kde=True, stat=\"count\", bins=50, color='blue', alpha=0.5)\n",
    "\n",
    "plt.title(\"Histogram (Count) + KDE of Quantized vs Original Activations\")\n",
    "plt.xlabel(\"Activation Value\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.ylim(0, 40)  \n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.savefig('Outputs/fake_tensor_quant_scale.png')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch._dynamo\n",
    "torch._dynamo.config.suppress_errors = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset for calibrating activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = r'D:\\ThesisData\\wikitext'\n",
    "\n",
    "destination_path = str(Path(filepath))\n",
    "dataset = load_dataset(\n",
    "    'wikitext', 'wikitext-103-raw-v1',\n",
    "    split={\n",
    "        'train': 'train[:30%]',\n",
    "        'validation': 'validation[:10%]',\n",
    "        'test': 'test[:10%]',\n",
    "    },\n",
    "    cache_dir=destination_path,\n",
    "    download_mode=DownloadMode.REUSE_DATASET_IF_EXISTS,\n",
    "    keep_in_memory=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_texts = dataset['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calibration_texts = [t for t in dataset['train'][\"text\"] if isinstance(t, str) and t.strip()]\n",
    "#calibration_texts = [t for t in sub_txts[\"text\"] if isinstance(t, str) and t.strip()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_txts = train_texts.take(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PARAMS:Dict = {\n",
    "    'context': Contexts.C1.value,\n",
    "    'prompt': MiscPrompts.Q11.value,\n",
    "    'max_new_tokens': 250,\n",
    "    'temperature': 0.8,\n",
    "    'repetition_penalty': 1.1,\n",
    "    'sample': True,\n",
    "    'device': None\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models and Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_test_model(model_path:str, dtype=torch.dtype) -> AutoModelForCausalLM:\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_path,\n",
    "        return_dict=True,\n",
    "        output_hidden_states=True,\n",
    "        torch_dtype=dtype,\n",
    "        low_cpu_mem_usage=True,\n",
    "        local_files_only=True,\n",
    "        use_safetensors=True,\n",
    "        #trust_remote_code=True\n",
    "    )\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hfbit1_tokenizer = AutoTokenizer.from_pretrained(FPKey.HFBIT1_TOKENIZER.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hfbit1_fp32 = load_test_model(FPKey.HFBIT1_8B.value, dtype=torch.float32) # https://huggingface.co/HF1BitLLM/Llama3-8B-1.58-100B-tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hfbit1_fp32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llama8b_tokenizer = AutoTokenizer.from_pretrained(FPKey.LINSTRUCT_TOKENIZER.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llama8b_fp32 = load_test_model(FPKey.LINSTRUCT_8B.value, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llama8b_bnb4_fp16 = load_4bit_auto(\n",
    "    KEY=ModelKey.LLINSTRUCT8B.value,\n",
    "    hs=True,\n",
    "    r_dict=True,\n",
    "    precision=torch.float16,\n",
    "    dmap='auto',\n",
    "    sf=True,\n",
    "    trust_remote=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llama8b_bnb4_fp16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hfbit1_tokenizer.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llama8b_tokenizer.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt = Texts.T1.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llama8b_bitnet_fp32 = applyPTQ(\n",
    "    load_test_model(FPKey.LINSTRUCT_8B.value, dtype=torch.float32),\n",
    "    tokenizer=llama8b_tokenizer,\n",
    "    #calibration_input=None,\n",
    "    #calibration_input=sub_txts['text'],\n",
    "    calibration_input=Texts.T1.value,\n",
    "    mode='1.58bit',\n",
    "    safer_quant=True,\n",
    "    model_half=False,\n",
    "    quant_half=False,\n",
    "    layers_to_quant_weights=QuantStyle.BITNET.value,\n",
    "    layers_to_quant_activations=QuantStyle.BITNET.value,\n",
    "    fragile_layers=False,\n",
    "    act_quant=True,\n",
    "    act_bits=8,\n",
    "    debugging=True,\n",
    "    plot_debugging=False,\n",
    "    plot_quantization=False,\n",
    "    freeze_modules=True,\n",
    "    #save_model_path='Decoders/NousResearch/LlamaInstruct8B/Meta-Llama-3.1-8B-Instruct-1.58-ptsq'\n",
    "    save_model_path=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llama8b_bitnet_fp32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.print_model_weights(llama8b_bitnet_fp32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### allenai/OLMo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "olmo1b_tokenizer = AutoTokenizer.from_pretrained(FPKey.OLMO1B_TOKENIZER.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "olmo2t_tokenizer = AutoTokenizer.from_pretrained(FPKey.OLMO7B2T_TOKENIZER.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "olmo7b_tokenizer = AutoTokenizer.from_pretrained(FPKey.OLMO7B_TOKENIZER.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "olmo1b_fp32 = load_test_model(FPKey.OLMO1B_FP.value, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "olmo2t_fp32 = load_test_model(FPKey.OLMO7B2T_FP.value, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "olmo7b_fp32 = load_test_model(FPKey.OLMO7B_FP.value, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "olmo1b_fp32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "olmo1b_fp32.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "QuantStyle.BITNET.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Texts.T1.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "olmo1b_bitnet_fp32_qlmhead = applyPTQ(\n",
    "    load_test_model(FPKey.OLMO1B_FP.value, dtype=torch.float32),\n",
    "    tokenizer=olmo1b_tokenizer,\n",
    "    #calibration_input=None,\n",
    "    #calibration_input=sub_txts['text'],\n",
    "    calibration_input=Texts.T1.value,\n",
    "    mode='1.58bit',\n",
    "    safer_quant=True,\n",
    "    q_lmhead=True,\n",
    "    model_half=False,\n",
    "    quant_half=False,\n",
    "    layers_to_quant_weights=QuantStyle.BITNET.value,\n",
    "    layers_to_quant_activations=QuantStyle.BITNET.value,\n",
    "    fragile_layers=False,\n",
    "    act_quant=True,\n",
    "    act_bits=8,\n",
    "    debugging=True,\n",
    "    plot_debugging=False,\n",
    "    plot_quantization=False,\n",
    "    freeze_modules=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "olmo1b_bitnet_fp32_flmhead = applyPTQ(\n",
    "    load_test_model(FPKey.OLMO1B_FP.value, dtype=torch.float32),\n",
    "    tokenizer=olmo1b_tokenizer,\n",
    "    #calibration_input=None,\n",
    "    #calibration_input=sub_txts['text'],\n",
    "    calibration_input=Texts.T1.value,\n",
    "    mode='1.58bit',\n",
    "    safer_quant=True,\n",
    "    q_lmhead=False,\n",
    "    model_half=False,\n",
    "    quant_half=False,\n",
    "    layers_to_quant_weights=QuantStyle.BITNET.value,\n",
    "    layers_to_quant_activations=QuantStyle.BITNET.value,\n",
    "    fragile_layers=False,\n",
    "    act_quant=True,\n",
    "    act_bits=8,\n",
    "    debugging=True,\n",
    "    plot_debugging=False,\n",
    "    plot_quantization=False,\n",
    "    freeze_modules=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "olmo2t_bitnet_fp32_qlmhead = applyPTQ(\n",
    "    load_test_model(FPKey.OLMO7B2T_FP.value, dtype=torch.float32),\n",
    "    tokenizer=olmo2t_tokenizer,\n",
    "    #calibration_input=None,\n",
    "    #calibration_input=sub_txts['text'],\n",
    "    calibration_input=Texts.T1.value,\n",
    "    mode='1.58bit',\n",
    "    safer_quant=True,\n",
    "    q_lmhead=True,\n",
    "    model_half=False,\n",
    "    quant_half=False,\n",
    "    layers_to_quant_weights=QuantStyle.BITNET.value,\n",
    "    layers_to_quant_activations=QuantStyle.BITNET.value,\n",
    "    fragile_layers=False,\n",
    "    act_quant=True,\n",
    "    act_bits=8,\n",
    "    debugging=True,\n",
    "    plot_debugging=False,\n",
    "    plot_quantization=False,\n",
    "    freeze_modules=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "olmo2t_bitnet_fp32_flmhead = applyPTQ(\n",
    "    load_test_model(FPKey.OLMO7B2T_FP.value, dtype=torch.float32),\n",
    "    tokenizer=olmo2t_tokenizer,\n",
    "    #calibration_input=None,\n",
    "    #calibration_input=sub_txts['text'],\n",
    "    calibration_input=Texts.T1.value,\n",
    "    mode='1.58bit',\n",
    "    safer_quant=True,\n",
    "    q_lmhead=False,\n",
    "    model_half=False,\n",
    "    quant_half=False,\n",
    "    layers_to_quant_weights=QuantStyle.BITNET.value,\n",
    "    layers_to_quant_activations=QuantStyle.BITNET.value,\n",
    "    fragile_layers=False,\n",
    "    act_quant=True,\n",
    "    act_bits=8,\n",
    "    debugging=True,\n",
    "    plot_debugging=False,\n",
    "    plot_quantization=False,\n",
    "    freeze_modules=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NousResearch/OLMo-Bitnet-1B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"NousResearch/OLMo-Bitnet-1B\", trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.save_tokenizer(tokenizer, 'Decoders/NousResearch/OLMoBitnet/OLMo-Bitnet-1B-tokenizer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\"NousResearch/OLMo-Bitnet-1B\", trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.save_model(model, 'Decoders/NousResearch/OLMoBitnet/OLMo-Bitnet-1B')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NousResearch/DeepHermes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dh3b_tokenizer = AutoTokenizer.from_pretrained(FPKey.TOKENIZER_3B.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dh8b_tokenizer = AutoTokenizer.from_pretrained(FPKey.TOKENIZER_8B.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dh3b_fp32 = load_test_model(FPKey.FP_3B.value, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dh3b_bnb4_fp16 = load_4bit_auto(\n",
    "    KEY=ModelKey.DEEP3B.value,\n",
    "    hs=True,\n",
    "    r_dict=True,\n",
    "    precision=torch.float16,\n",
    "    dmap='auto',\n",
    "    sf=True,\n",
    "    trust_remote=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dh3b_bnb4_fp16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dh8b_fp32 = load_test_model(FPKey.FP_8B.value, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dh8b_bnb4_fp16 = load_4bit_auto(\n",
    "    KEY=ModelKey.DEEP8B.value,\n",
    "    hs=True,\n",
    "    r_dict=True,\n",
    "    precision=torch.float16,\n",
    "    dmap='auto',\n",
    "    sf=True,\n",
    "    trust_remote=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dh3b_bitnet_fp32_flmhead = applyPTQ(\n",
    "    load_test_model(FPKey.FP_3B.value, dtype=torch.float32),\n",
    "    tokenizer=dh3b_tokenizer,\n",
    "    calibration_input=None,\n",
    "    #calibration_input=sub_txts['text'],\n",
    "    #calibration_input=Texts.T1.value,\n",
    "    mode='1.58bit',\n",
    "    safer_quant=True,\n",
    "    q_lmhead=False,\n",
    "    model_half=False,\n",
    "    quant_half=False,\n",
    "    layers_to_quant_weights=QuantStyle.BITNET.value,\n",
    "    layers_to_quant_activations=QuantStyle.BITNET.value,\n",
    "    fragile_layers=False,\n",
    "    act_quant=False,\n",
    "    act_bits=8,\n",
    "    debugging=True,\n",
    "    plot_debugging=False,\n",
    "    plot_quantization=False,\n",
    "    freeze_modules=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dh3b_bitnet_fp32_qlmhead = applyPTQ(\n",
    "    load_test_model(FPKey.FP_3B.value, dtype=torch.float32),\n",
    "    tokenizer=dh3b_tokenizer,\n",
    "    #calibration_input=None,\n",
    "    calibration_input=sub_txts['text'],\n",
    "    #calibration_input=Texts.T1.value,\n",
    "    mode='1.58bit',\n",
    "    safer_quant=True,\n",
    "    q_lmhead=True,\n",
    "    model_half=False,\n",
    "    quant_half=False,\n",
    "    layers_to_quant_weights=QuantStyle.BITNET.value,\n",
    "    layers_to_quant_activations=QuantStyle.BITNET.value,\n",
    "    fragile_layers=False,\n",
    "    act_quant=True,\n",
    "    act_bits=8,\n",
    "    debugging=True,\n",
    "    plot_debugging=False,\n",
    "    plot_quantization=False,\n",
    "    freeze_modules=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dh3b_bitnet_fp32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dh8b_bitnet_fp32 = applyPTQ(\n",
    "    load_test_model(FPKey.FP_8B.value, dtype=torch.float32),\n",
    "    tokenizer=dh8b_tokenizer,\n",
    "    #calibration_input=None,\n",
    "    #calibration_input=sub_txts['text'],\n",
    "    calibration_input=txt,\n",
    "    mode='1.58bit',\n",
    "    safer_quant=True,\n",
    "    model_half=False,\n",
    "    quant_half=False,\n",
    "    layers_to_quant_weights=QuantStyle.BITNET.value,\n",
    "    layers_to_quant_activations=QuantStyle.BITNET.value,\n",
    "    fragile_layers=False,\n",
    "    act_quant=True,\n",
    "    act_bits=8,\n",
    "    debugging=True,\n",
    "    plot_debugging=False,\n",
    "    plot_quantization=False,\n",
    "    freeze_modules=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dh8b_bitnet_fp32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activation Lens and Logit Lens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MiscPrompts.Q2.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MiscPrompts.Q11.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MiscPrompts.Q12.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llama8b_fp32.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logit_lens.plot_logit_lens_plotly(\n",
    "    model=olmo1b_fp32,\n",
    "    tokenizer=olmo1b_tokenizer,\n",
    "    input_ids=MiscPrompts.Q2.value,\n",
    "    start_ix=0, end_ix=15,\n",
    "    topk=5,\n",
    "    #save_fig_path=None,\n",
    "    #save_fig_path='Outputs/LogitLens/DH3B/logits_3b_fp32_math.jpg',\n",
    "    entropy=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logit_lens.plot_logit_lens(\n",
    "    model=dh3b_fp32,\n",
    "    tokenizer=dh3b_tokenizer,\n",
    "    input_ids=MiscPrompts.Q12.value,\n",
    "    start_ix=0, end_ix=15,\n",
    "    #save_fig_path=None,\n",
    "    #save_fig_path='Outputs/LogitLens/LI8B/probs_hf1bit_fp32_qa_blockstep10.jpg',\n",
    "    entropy=True,\n",
    "    block_step=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logit_lens.plot_logit_lens(\n",
    "    model=dh3b_bitnet_fp32_qlmhead,\n",
    "    tokenizer=dh3b_tokenizer,\n",
    "    input_ids=MiscPrompts.Q12.value,\n",
    "    start_ix=0, end_ix=15,\n",
    "    #save_fig_path=None,\n",
    "    #save_fig_path='Outputs/LogitLens/LI8B/probs_hf1bit_fp32_qa_blockstep10.jpg',\n",
    "    entropy=True,\n",
    "    block_step=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logit_lens.plot_logit_lens(\n",
    "    model=dh3b_bitnet_fp32_flmhead,\n",
    "    tokenizer=dh3b_tokenizer,\n",
    "    input_ids=MiscPrompts.Q12.value,\n",
    "    start_ix=0, end_ix=15,\n",
    "    #save_fig_path=None,\n",
    "    save_fig_path='Outputs/LogitLens/DH3B/logits_flmhead_fp32_qa.jpg',\n",
    "    #kl=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logit_lens.plot_comparing_lens(\n",
    "    models=(dh3b_fp32, dh3b_bitnet_fp32_qlmhead),\n",
    "    tokenizer=dh3b_tokenizer,\n",
    "    input_ids=MiscPrompts.Q12.value,\n",
    "    start_ix=0, end_ix=15,\n",
    "    #save_fig_path='Outputs/LogitLens/LI8B/nwd_llama_hf1bit_qa_blockstep10.jpg',\n",
    "    save_fig_path=None,\n",
    "    wasserstein=True,\n",
    "    #top_down=False,\n",
    "    block_step=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logit_lens.plot_comparing_lens(\n",
    "    models=(olmo1b_bitnet_fp32_flmhead, olmo1b_bitnet_fp32_qlmhead),\n",
    "    tokenizer=olmo1b_tokenizer,\n",
    "    input_ids=MiscPrompts.Q2.value,\n",
    "    start_ix=0, end_ix=15,\n",
    "    save_fig_path='Outputs/LogitLens/OLMo1B/nwd_flmhead_qlmhead_math.jpg',\n",
    "    #save_fig_path=None,\n",
    "    wasserstein=True,\n",
    "    #top_down=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logit_lens.plot_comparing_lens(\n",
    "    models=(llama8b_fp32, hfbit1_fp32),\n",
    "    tokenizer=hfbit1_tokenizer,\n",
    "    input_ids=MiscPrompts.Q12.value,\n",
    "    start_ix=0, end_ix=15,\n",
    "    #save_fig_path='Outputs/LogitLens/LI8B/nwd_instruct_bitnet_fp32_qa.jpg',\n",
    "    save_fig_path=None,\n",
    "    wasserstein=True,\n",
    "    #top_down=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logit_lens.plot_comparing_lens(\n",
    "    models=(llama8b_fp32, llama8b_bnb4_fp16),\n",
    "    tokenizer=llama8b_tokenizer,\n",
    "    input_ids=MiscPrompts.Q12.value,\n",
    "    start_ix=0, end_ix=15,\n",
    "    #save_fig_path='Outputs/LogitLens/DH3B/nwd_3bfp32_ptdq_math.jpg',\n",
    "    save_fig_path=None,\n",
    "    wasserstein=True,\n",
    "    #top_down=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logit_lens.plot_topk_lens(\n",
    "    model=llama8b_bnb4_fp16,\n",
    "    tokenizer=llama8b_tokenizer,\n",
    "    input_ids=MiscPrompts.Q12.value,\n",
    "    start_ix=0, end_ix=15,\n",
    "    topk_n=5,\n",
    "    save_fig_path='Outputs/LogitLens/LI8B/topk5logits_bnb4bit_qa.jpg'\n",
    "    #save_fig_path=None,\n",
    "    #top_down=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logit_lens.plot_topk_lens(\n",
    "    model=llama8b_bitnet_fp32,\n",
    "    tokenizer=llama8b_tokenizer,\n",
    "    input_ids=MiscPrompts.Q12.value,\n",
    "    start_ix=0, end_ix=15,\n",
    "    topk_n=5,\n",
    "    save_fig_path='Outputs/LogitLens/LI8B/topk5logits_ptsq_fp32_math.jpg'\n",
    "    #save_fig_path=None,\n",
    "    #entropy=True,\n",
    "    #top_down=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activation_lens.plot_activation_lens(\n",
    "    model=dh3b_bitnet_fp32_qlmhead,\n",
    "    tokenizer=dh3b_tokenizer,\n",
    "    input_ids=MiscPrompts.Q2.value,\n",
    "    start_ix=0, end_ix=15,\n",
    "    metric='norm',\n",
    "    save_fig_path='Outputs/LogitLens/DH3B/actnorm_qlmhead_ptdq_math.jpg', #HUSK FOR BITNET hfb1 Q12 qa!\n",
    "    #save_fig_path=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activation_lens.plot_activation_lens(\n",
    "    model=llama8b_bnb4_fp16,\n",
    "    tokenizer=llama8b_tokenizer,\n",
    "    input_ids=MiscPrompts.Q12.value,\n",
    "    start_ix=0, end_ix=15,\n",
    "    metric='norm',\n",
    "    save_fig_path='Outputs/LogitLens/LI8B/actnorm_bnb4bit_fp16_qa.jpg',\n",
    "    #save_fig_path=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activation_lens.plot_comparing_act_lens(\n",
    "    models=(llama8b_fp32, hfbit1_fp32),\n",
    "    tokenizer=llama8b_tokenizer,\n",
    "    input_ids=MiscPrompts.Q12.value,\n",
    "    start_ix=0, end_ix=15,\n",
    "    metric='norm',\n",
    "    metric_name='l2',\n",
    "    save_fig_path='Outputs/LogitLens/LI8B/actnorm_comparing_fp32_qa.jpg',\n",
    "    #save_fig_path=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dictionary Learning: SAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary_learning.plot_sae_tokens(\n",
    "    model=dh3b_fp32,\n",
    "    tokenizer=dh3b_tokenizer,\n",
    "    inputs=PARAMS.get('prompt'),\n",
    "    multi_tokens=False,\n",
    "    do_log=False,\n",
    "    target_layers=[5],\n",
    "    vis_projection=None,\n",
    "    log_path=None,\n",
    "    log_name=None,\n",
    "    fig_path=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary_learning.plot_sae_tokens(\n",
    "    model=dh3b_fp32,\n",
    "    tokenizer=dh3b_tokenizer,\n",
    "    inputs=PARAMS.get('prompt'),\n",
    "    multi_tokens=True,\n",
    "    do_log=False,\n",
    "    target_layers=[5],\n",
    "    vis_projection=None,\n",
    "    log_path=None,\n",
    "    log_name=None,\n",
    "    fig_path=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary_learning.plot_sae_tokens(\n",
    "    model=dh3b_bitnet_fp32,\n",
    "    tokenizer=dh3b_tokenizer,\n",
    "    inputs=PARAMS.get('prompt'),\n",
    "    multi_tokens=True,\n",
    "    do_log=False,\n",
    "    target_layers=[5],\n",
    "    vis_projection=None,\n",
    "    log_path=None,\n",
    "    log_name=None,\n",
    "    fig_path=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary_learning.plot_sae_heatmap(\n",
    "    model=hfbit1_fp32,\n",
    "    tokenizer=hfbit1_tokenizer,\n",
    "    inputs=Texts.T1.value,\n",
    "    do_log=False,\n",
    "    top_k=5,\n",
    "    tokens_per_row=30,\n",
    "    target_layers=[5,25,30],\n",
    "    log_path=None,\n",
    "    log_name=None,\n",
    "    fig_path=None,\n",
    "    deterministic_sae=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary_learning.plot_sae_heatmap(\n",
    "    model=llama8b_bnb4_fp16,\n",
    "    tokenizer=llama8b_tokenizer,\n",
    "    inputs=Texts.T1.value,\n",
    "    do_log=False,\n",
    "    top_k=5,\n",
    "    tokens_per_row=30,\n",
    "    target_layers=[5],\n",
    "    log_path=None,\n",
    "    log_name=None,\n",
    "    fig_path=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary_learning.plot_comparing_heatmap(\n",
    "    models=(llama8b_fp32, hfbit1_fp32),\n",
    "    tokenizer=llama8b_tokenizer,\n",
    "    inputs=Texts.T1.value,\n",
    "    top_k=5,\n",
    "    tokens_per_row=30,\n",
    "    target_layers=[25],\n",
    "    fig_path=None,\n",
    "    deterministic_sae=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary_learning.plot_comparing_heatmap(\n",
    "    models=(olmo1b_fp32, olmo1b_bitnet_fp32),\n",
    "    tokenizer=olmo1b_tokenizer,\n",
    "    inputs=Texts.T1.value,\n",
    "    top_k=5,\n",
    "    tokens_per_row=30,\n",
    "    target_layers=[5],\n",
    "    fig_path=None\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Hermes Chatbot Analysis (template only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_dict = {\n",
    "    #'dh.3b-llama.fp32': dh3b_fp32,\n",
    "    'dh.3b-bnb4bit.fp16': dh3b_bnb4_fp16,\n",
    "    #'dh.3b-1.58.ptdq': dh3b_bitnet_fp32, \n",
    "    #'dh.3b-1.58.ptsq': dh3b_bitnet_fp32,\n",
    "    #'dh.8b-llama.fp32': dh8b_fp32,\n",
    "    #'dh.8b-bnb4bit.fp16': dh8b_bnb4_fp16,\n",
    "    #'dh.8b-1.58.ptdq': dh8b_bitnet_fp32,\n",
    "    #'dh.8b-1.58.ptsq': dh8b_bitnet_fp32,\n",
    "    #'llama.8b-instruct.fp32': llama8b_fp32,\n",
    "    #'llama.8b-bnb4bit.fp16': llama8b_bnb4_fp16,\n",
    "    #'llama.8b-1.58.fp32': hfbit1_fp32,\n",
    "    #'llama.8b-1.58.ptdq': llama8b_bitnet_fp32,\n",
    "    #'llama.8b-1.58.ptsq': llama8b_bitnet_fp32,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PARAMS:Dict = {\n",
    "    'context': Contexts.C1.value,\n",
    "    'prompt': MiscPrompts.Q2.value,\n",
    "    'max_new_tokens': 100,\n",
    "    'temperature': 0.8,\n",
    "    'repetition_penalty': 1.1,\n",
    "    'sample': True,\n",
    "    'device': None\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chatbot_analysis.run_chatbot_analysis(\n",
    "    models=chat_dict,\n",
    "    tokenizer=dh3b_tokenizer,\n",
    "    deep_thinking=False,\n",
    "    full_path='logs/chatbot_logs',\n",
    "    deterministic_backend=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chatbot_analysis.plot_chatbot_analysis(\n",
    "    json_logs='logs/math/chatbot_logs',\n",
    "    parallel_plot=True,\n",
    "    reference_file='logs/chatbot_logs/math/dh.8b-llama.fp32.json'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chatbot_analysis.plot_chatbot_analysis(\n",
    "    json_logs='logs/chatbot_logs/QA',\n",
    "    parallel_plot=False,\n",
    "    reference_file='logs/chatbot_logs/QA/llama.8b-instruct.fp32.json',\n",
    "    title=\"Model Metrics ('What is y if y=2*2-4+(3*2)')\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MechInterp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
