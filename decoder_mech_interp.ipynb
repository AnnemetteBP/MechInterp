{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "from typing import Tuple, List, Dict, Optional, Any\n",
    "\n",
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from datasets import load_dataset, DownloadMode\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import tqdm\n",
    "\n",
    "from helper_utils.enum_keys import (\n",
    "    FPKey,\n",
    "    ModelKey,\n",
    "    QuantStyle,\n",
    "    MiscPrompts,\n",
    "    Contexts,\n",
    "    Texts\n",
    ")\n",
    "\n",
    "from PTQ.apply_ptq import applyPTQ\n",
    "from PTQ.olmo_act_fns import patch_olmo_mlp\n",
    "import helper_utils.utils as utils\n",
    "\n",
    "from mech_interp_utils.utils_main.src.transformer_utils import (\n",
    "    logit_lens,\n",
    "    activation_lens,\n",
    "    dictionary_learning,\n",
    "    chatbot_analysis\n",
    ")\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "def load_metrics_from_directory(directory_path):\n",
    "    all_metrics = []\n",
    "    for filename in os.listdir(directory_path):\n",
    "        if not filename.endswith(\".json\"):\n",
    "            continue\n",
    "        filepath = os.path.join(directory_path, filename)\n",
    "        with open(filepath, 'r') as f:\n",
    "            try:\n",
    "                data = json.load(f)\n",
    "                if isinstance(data, list) and all(isinstance(item, dict) for item in data):\n",
    "                    model_id = Path(filename).stem  # Use filename as model ID\n",
    "                    for item in data:\n",
    "                        item['file_name'] = filename\n",
    "                        item['model_id'] = model_id  # <-- set individual model_id here\n",
    "                    all_metrics.extend(data)\n",
    "                else:\n",
    "                    print(f\"[!] Unexpected structure in {filename}\")\n",
    "            except Exception as e:\n",
    "                print(f\"[!] Failed to load {filename}: {e}\")\n",
    "    print(f\"Loaded {len(all_metrics)} total entries from {directory_path}\")\n",
    "    return all_metrics\n",
    "\n",
    "\n",
    "\"\"\"def load_all_model_metrics(source_map): # NO DIR\n",
    "    records = []\n",
    "\n",
    "    for model_name, path in source_map.items():\n",
    "        path_obj = Path(path)\n",
    "        if path_obj.is_file():\n",
    "            try:\n",
    "                with open(path, \"r\") as f:\n",
    "                    data = json.load(f)\n",
    "                for entry in data:\n",
    "                    entry['model_id'] = model_name\n",
    "                    entry['file_name'] = path_obj.name\n",
    "                    records.append(entry)\n",
    "            except Exception as e:\n",
    "                print(f\"[!] Failed to load {path}: {e}\")\n",
    "        else:\n",
    "            print(f\"[!] Expected file, got directory or invalid path: {path}\")\n",
    "\n",
    "    print(f\"Total entries combined: {len(records)}\")\n",
    "    return pd.DataFrame(records)\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "def load_all_model_metrics(source_map):\n",
    "    \"\"\"\n",
    "    Load metrics from individual JSON files and tag with model_id and file_name.\n",
    "    \n",
    "    Parameters:\n",
    "        source_map (dict): Mapping from model name to file path\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: All metrics combined and annotated\n",
    "    \"\"\"\n",
    "    records = []\n",
    "\n",
    "    for model_name, path in source_map.items():\n",
    "        path_obj = Path(path)\n",
    "\n",
    "        if not path_obj.is_file():\n",
    "            print(f\"[!] Skipping invalid path: {path}\")\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            with open(path_obj, \"r\") as f:\n",
    "                data = json.load(f)\n",
    "\n",
    "            if not isinstance(data, list) or not all(isinstance(d, dict) for d in data):\n",
    "                print(f\"[!] Unexpected structure in file: {path}\")\n",
    "                continue\n",
    "\n",
    "            for entry in data:\n",
    "                entry['model_id'] = model_name  # <- use model name from dict\n",
    "                entry['file_name'] = path_obj.name  # <- actual filename\n",
    "                records.append(entry)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"[!] Failed to load {path}: {e}\")\n",
    "\n",
    "    print(f\"Loaded {len(records)} total entries from {len(source_map)} files.\")\n",
    "    return pd.DataFrame(records)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_sources_1 = {\n",
    "    'OLMo-1B-hf': 'logs/gsm8k_answers/olmo.1b.fp32.json',\n",
    "    'OLMo-1B-8bit': 'logs/gsm8k_answers/olmo.1b-bnb8bit.fp32.json',\n",
    "    'OLMo-1B-4bit': 'logs/gsm8k_answers/olmo.1b-bnb4bit.fp32.json',\n",
    "    'OLMo-1B-1.58bit': 'logs/gsm8k_answers/olmo.1b-ptsq.fp32.json',\n",
    "    'DeepHermes-LLaMA-3B': 'logs/gsm8k_answers/dh.3b-llama.fp32.json',\n",
    "    'DeepHermes-3B-8bit': 'logs/gsm8k_answers/dh.3b-bnb8bit.fp32.json',\n",
    "    'DeepHermes-3B-4bit': 'logs/gsm8k_answers/dh.3b-bnb4bit.fp32.json',\n",
    "    'DeepHermes-3B-1.58bit': 'logs/gsm8k_answers/dh.3b-ptsq.fp32.json',\n",
    "    'LLaMA-Instruct-8B': 'logs/gsm8k_answers/llama.8b-instruct.fp32.json',\n",
    "    'LLaMA-Instruct-8B-8bit': 'logs/gsm8k_answers/llama.8b-bnb8bit.fp32.json',\n",
    "    'LLaMA-Instruct-8B-4bit': 'logs/gsm8k_answers/llama.8b-bnb4bit.fp32.json',\n",
    "    'HF1BitLLM': 'logs/gsm8k_answers/llama.8b-1.58.fp32.json'\n",
    "}\n",
    "\n",
    "df_all_1 = load_all_model_metrics(model_sources_1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_sources_2 = {\n",
    "    'OLMo-1B-hf': 'logs/gsm8k_questions/olmo.1b.fp32.json',\n",
    "    'OLMo-1B-8bit': 'logs/gsm8k_questions/olmo.1b-bnb8bit.fp32.json',\n",
    "    'OLMo-1B-4bit': 'logs/gsm8k_questions/olmo.1b-bnb4bit.fp32.json',\n",
    "    'OLMo-1B-1.58bit': 'logs/gsm8k_questions/olmo.1b-ptsq.fp32.json',\n",
    "    'DeepHermes-LLaMA-3B': 'logs/gsm8k_questions/dh.3b-llama.fp32.json',\n",
    "    'DeepHermes-3B-8bit': 'logs/gsm8k_questions/dh.3b-bnb8bit.fp32.json',\n",
    "    'DeepHermes-3B-4bit': 'logs/gsm8k_questions/dh.3b-bnb4bit.fp32.json',\n",
    "    'DeepHermes-3B-1.58bit': 'logs/gsm8k_questions/dh.3b-ptsq.fp32.json',\n",
    "    'LLaMA-Instruct-8B': 'logs/gsm8k_questions/llama.8b-instruct.fp32.json',\n",
    "    'LLaMA-Instruct-8B-8bit': 'logs/gsm8k_questions/llama.8b-bnb8bit.fp32.json',\n",
    "    'LLaMA-Instruct-8B-4bit': 'logs/gsm8k_questions/llama.8b-bnb4bit.fp32.json',\n",
    "    'HF1BitLLM': 'logs/gsm8k_questions/llama.8b-1.58.fp32.json'\n",
    "}\n",
    "\n",
    "df_all_2 = load_all_model_metrics(model_sources_2)\n",
    "\n",
    "model_sources_3 = {\n",
    "    'OLMo-1B-hf': 'logs/nq_queries/olmo.1b.fp32.json',\n",
    "    'OLMo-1B-8bit': 'logs/nq_queries/olmo.1b-bnb8bit.fp32.json',\n",
    "    'OLMo-1B-4bit': 'logs/nq_queries/olmo.1b-bnb4bit.fp32.json',\n",
    "    'OLMo-1B-1.58bit': 'logs/nq_queries/olmo.1b-ptsq.fp32.json',\n",
    "    'DeepHermes-LLaMA-3B': 'logs/nq_queries/dh.3b-llama.fp32.json',\n",
    "    'DeepHermes-3B-8bit': 'logs/nq_queries/dh.3b-bnb8bit.fp32.json',\n",
    "    'DeepHermes-3B-4bit': 'logs/nq_queries/dh.3b-bnb4bit.fp32.json',\n",
    "    'DeepHermes-3B-1.58bit': 'logs/nq_queries/dh.3b-ptsq.fp32.json',\n",
    "    'LLaMA-Instruct-8B': 'logs/nq_queries/llama.8b-instruct.fp32.json',\n",
    "    'LLaMA-Instruct-8B-8bit': 'logs/nq_queries/llama.8b-bnb8bit.fp32.json',\n",
    "    'LLaMA-Instruct-8B-4bit': 'logs/nq_queries/llama.8b-bnb4bit.fp32.json',\n",
    "    'HF1BitLLM': 'logs/nq_queries/llama.8b-1.58.fp32.json'\n",
    "}\n",
    "\n",
    "df_all_3 = load_all_model_metrics(model_sources_3)\n",
    "\n",
    "model_sources_4 = {\n",
    "    'OLMo-1B-hf': 'logs/nq_answers/olmo.1b.fp32.json',\n",
    "    'OLMo-1B-8bit': 'logs/nq_answers/olmo.1b-bnb8bit.fp32.json',\n",
    "    'OLMo-1B-4bit': 'logs/nq_answers/olmo.1b-bnb4bit.fp32.json',\n",
    "    'OLMo-1B-1.58bit': 'logs/nq_answers/olmo.1b-ptsq.fp32.json',\n",
    "    'DeepHermes-LLaMA-3B': 'logs/nq_answers/dh.3b-llama.fp32.json',\n",
    "    'DeepHermes-3B-8bit': 'logs/nq_answers/dh.3b-bnb8bit.fp32.json',\n",
    "    'DeepHermes-3B-4bit': 'logs/nq_answers/dh.3b-bnb4bit.fp32.json',\n",
    "    'DeepHermes-3B-1.58bit': 'logs/nq_answers/dh.3b-ptsq.fp32.json',\n",
    "    'LLaMA-Instruct-8B': 'logs/nq_answers/llama.8b-instruct.fp32.json',\n",
    "    'LLaMA-Instruct-8B-8bit': 'logs/nq_answers/llama.8b-bnb8bit.fp32.json',\n",
    "    'LLaMA-Instruct-8B-4bit': 'logs/nq_answers/llama.8b-bnb4bit.fp32.json',\n",
    "    'HF1BitLLM': 'logs/nq_answers/llama.8b-1.58.fp32.json'\n",
    "}\n",
    "\n",
    "df_all_4 = load_all_model_metrics(model_sources_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_1.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames = [df_all_1, df_all_2, df_all_3, df_all_4]\n",
    "df_all = pd.concat(frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_all_1['model_id'].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_1['prob_mean'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_long_df(df) -> pd.DataFrame:\n",
    "    # Expand prob_mean into long format\n",
    "    records = []\n",
    "    for _, row in df_all.iterrows():\n",
    "        model_id = row['model_id']\n",
    "        prob_means = row['prob_mean']\n",
    "        entropy_means = row['entropy']\n",
    "        logit_means = row['logit_mean']\n",
    "        layers = row.get('layer_names', list(range(len(prob_means))))  # fallback to index if no names\n",
    "        \n",
    "        for i, (layer, mean_val) in enumerate(zip(layers, prob_means)):\n",
    "            records.append({\n",
    "                'model_id': model_id,\n",
    "                'layer': layer,\n",
    "                'layer_index': i,\n",
    "                'prob_mean': mean_val,\n",
    "                'entropy_mean': mean_val,\n",
    "                'logits_mean': mean_val\n",
    "            })\n",
    "\n",
    "    df_long = pd.DataFrame(records)\n",
    "    return df_long"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def make_long_df(df_all) -> pd.DataFrame:\n",
    "    # Expand prob_mean into long format\n",
    "    records = []\n",
    "    for _, row in df_all.iterrows():\n",
    "        model_id = row['model_id']\n",
    "        prob_means = row['prob_mean']\n",
    "        layers = row.get('layer_names', list(range(len(prob_means))))  # fallback to index if no names\n",
    "        \n",
    "        # Calculate variance of prob_mean\n",
    "        prob_variance = np.var(prob_means)  # Variance of prob_mean across layers\n",
    "        prob_std = np.std(prob_means)\n",
    "        \n",
    "        for i, (layer, mean_val) in enumerate(zip(layers, prob_means)):\n",
    "            records.append({\n",
    "                'model_id': model_id,\n",
    "                'layer': layer,\n",
    "                'layer_index': i,\n",
    "                'prob_mean': mean_val,\n",
    "                'prob_variance': prob_variance,\n",
    "                'prob_std': prob_std,  \n",
    "            })\n",
    "\n",
    "    df_long = pd.DataFrame(records)\n",
    "    return df_long\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_long_1 = make_long_df(df_all_1)\n",
    "df_long_2 = make_long_df(df_all_2)\n",
    "df_long_3 = make_long_df(df_all_3)\n",
    "df_long_4 = make_long_df(df_all_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_long = make_long_df(df_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_long.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all['model_id'].unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Expand prob_mean into long format\n",
    "records = []\n",
    "for _, row in df_all.iterrows():\n",
    "    model_id = row['model_id']\n",
    "    prob_means = row['prob_mean']\n",
    "    entropy_means = row['entropy']\n",
    "    logit_means = row['logit_mean']\n",
    "    layers = row.get('layer_names', list(range(len(prob_means))))  # fallback to index if no names\n",
    "    \n",
    "    for i, (layer, mean_val) in enumerate(zip(layers, prob_means)):\n",
    "        records.append({\n",
    "            'model_id': model_id,\n",
    "            'layer': layer,\n",
    "            'layer_index': i,\n",
    "            'prob_mean': mean_val,\n",
    "            'entropy_mean': mean_val,\n",
    "            'logits_mean': mean_val\n",
    "        })\n",
    "\n",
    "df_long = pd.DataFrame(records)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by model_id and calculate the mean for the metrics (prob_mean, entropy_mean, logits_mean)\n",
    "mean_values = df_long.groupby('model_id')[['prob_mean', 'entropy_mean', 'logits_mean']].mean().reset_index()\n",
    "\n",
    "# Display the result\n",
    "print(mean_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_colmeans_round(df, decimal_places=3):\n",
    "    # Group by model_id and calculate the mean for the metrics (prob_mean, entropy_mean, logits_mean)\n",
    "    mean_values = df.groupby('model_id')[['prob_mean', 'entropy_mean', 'logits_mean']].mean().reset_index()\n",
    "\n",
    "    # Round the values to the specified number of decimal places\n",
    "    mean_values = mean_values.round({'prob_mean': decimal_places, \n",
    "                                     'entropy_mean': decimal_places, \n",
    "                                     'logits_mean': decimal_places})\n",
    "\n",
    "    # Display the result\n",
    "    print(mean_values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_colmeans_round_2(df, decimal_places=6):\n",
    "    # Group by model_id and calculate the mean for the metrics (prob_mean, entropy_mean, logits_mean)\n",
    "    mean_values = df.groupby('model_id')[['prob_mean', 'prob_variance', 'prob_std']].mean().reset_index()\n",
    "\n",
    "    # Round the values to the specified number of decimal places\n",
    "    mean_values = mean_values.round({'prob_mean': decimal_places, \n",
    "                                     'prob_variance': decimal_places,\n",
    "                                     'prob_std': decimal_places})\n",
    "\n",
    "    # Display the result\n",
    "    print(mean_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_colmeans(df):\n",
    "    # Group by model_id and calculate the mean for the metrics (prob_mean, entropy_mean, logits_mean)\n",
    "    mean_values = df.groupby('model_id')[['prob_mean', 'entropy_mean', 'logits_mean']].mean().reset_index()\n",
    "\n",
    "    # Display the result\n",
    "    print(mean_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_all_colmeans_round_2(df_long, decimal_places=6) # all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import norm\n",
    "\n",
    "# Your data\n",
    "data = df_long\n",
    "\n",
    "\n",
    "# Your real data\n",
    "data = pd.DataFrame({\n",
    "    'model_id': [\n",
    "        'DeepHermes-3B-1.58bit', 'DeepHermes-3B-4bit', 'DeepHermes-3B-8bit', 'DeepHermes-LLaMA-3B',\n",
    "        'HF1BitLLM', 'LLaMA-Instruct-8B', 'LLaMA-Instruct-8B-4bit', 'LLaMA-Instruct-8B-8bit',\n",
    "        'OLMo-1B-1.58bit', 'OLMo-1B-4bit', 'OLMo-1B-8bit', 'OLMo-1B-hf'\n",
    "    ],\n",
    "    'prob_mean': [\n",
    "        0.964732, 0.006449, 0.006832, 0.007037,\n",
    "        0.000562, 0.002368, 0.001911, 0.002282,\n",
    "        0.941626, 0.009738, 0.010268, 0.010333\n",
    "    ],\n",
    "    'prob_std': [\n",
    "        0.182370, 0.024772, 0.025868, 0.026763,\n",
    "        0.001193, 0.008599, 0.007310, 0.008270,\n",
    "        0.233004, 0.006250, 0.006153, 0.006164\n",
    "    ]\n",
    "})\n",
    "\n",
    "# Explicit family map (no guessing)\n",
    "model_families = {\n",
    "    'DeepHermes-3B-1.58bit': 'DeepHermes',\n",
    "    'DeepHermes-3B-4bit': 'DeepHermes',\n",
    "    'DeepHermes-3B-8bit': 'DeepHermes',\n",
    "    'DeepHermes-LLaMA-3B': 'DeepHermes',\n",
    "    'HF1BitLLM': 'LLaMA-Instruct',\n",
    "    'LLaMA-Instruct-8B': 'LLaMA-Instruct',\n",
    "    'LLaMA-Instruct-8B-4bit': 'LLaMA-Instruct',\n",
    "    'LLaMA-Instruct-8B-8bit': 'LLaMA-Instruct',\n",
    "    'OLMo-1B-1.58bit': 'OLMo',\n",
    "    'OLMo-1B-4bit': 'OLMo',\n",
    "    'OLMo-1B-8bit': 'OLMo',\n",
    "    'OLMo-1B-hf': 'OLMo'\n",
    "}\n",
    "\n",
    "data[\"model_type\"] = data[\"model_id\"].map(model_families)\n",
    "\n",
    "# Final safety check\n",
    "if data[\"model_type\"].isnull().any():\n",
    "    raise ValueError(\"Some models weren't classified. Fix model_families dictionary.\")\n",
    "\n",
    "# Plot setup\n",
    "plt.rcParams['font.family'] = 'Noto Sans'\n",
    "plt.rcParams['font.size'] = 14\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5), sharey=True)\n",
    "x = np.linspace(-0.1, 1.2, 300)\n",
    "model_types = [\"DeepHermes\", \"LLaMA-Instruct\", \"OLMo\"]\n",
    "\n",
    "for ax, model_type in zip(axes, model_types):\n",
    "    subset = data[data[\"model_type\"] == model_type]\n",
    "    \n",
    "    for _, row in subset.iterrows():\n",
    "        mu, std = row[\"prob_mean\"], row[\"prob_std\"]\n",
    "        if std <= 0:\n",
    "            continue\n",
    "        y = norm.pdf(x, mu, std)\n",
    "        ax.plot(x, y, label=row[\"model_id\"])\n",
    "\n",
    "    ax.set_title(model_type)\n",
    "    ax.set_xlabel(\"Probability\")\n",
    "    ax.set_yscale(\"log\")  # ← LOG SCALE FOR VISIBILITY\n",
    "    ax.legend(fontsize=14)\n",
    "    ax.grid(True, which=\"both\", linestyle=\"--\", linewidth=0.5)\n",
    "\n",
    "axes[0].set_ylabel(\"Log Density\")\n",
    "#plt.suptitle(\"Normal Distributions (Log Scale for Visibility)\", fontsize=16)\n",
    "plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "plt.savefig(\"Outputs/ProbsDistribs/log_probs_2.jpg\", dpi=300, bbox_inches='tight')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create plot\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5), sharey=False)\n",
    "\n",
    "model_types = [\"DeepHermes\", \"LLaMA-Instruct\", \"OLMo\"]\n",
    "\n",
    "plt.rcParams['font.family'] = 'Times New Roman'\n",
    "plt.rcParams['font.size'] = 14\n",
    "\n",
    "for ax, model_type in zip(axes, model_types):\n",
    "    subset = data[data[\"model_type\"] == model_type]\n",
    "\n",
    "    for _, row in subset.iterrows():\n",
    "        mean = row[\"prob_mean\"]\n",
    "        std = row[\"prob_std\"]\n",
    "\n",
    "        if std <= 0:\n",
    "            continue\n",
    "\n",
    "        # Centered x-range for each curve\n",
    "        x = np.linspace(mean - 4*std, mean + 4*std, 300)\n",
    "        y = norm.pdf(x, mean, std)\n",
    "\n",
    "        ax.plot(x, y, label=row[\"model_id\"])\n",
    "\n",
    "    ax.set_title(model_type)\n",
    "    ax.set_xlabel(\"Probability\")\n",
    "    ax.grid(True)\n",
    "    ax.legend(fontsize=14)\n",
    "\n",
    "axes[0].set_ylabel(\"Density\")\n",
    "#plt.suptitle(\"Correct Full Normal Distributions for Model Versions\", fontsize=16)\n",
    "plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "plt.savefig(\"Outputs/ProbsDistribs/normal_distributions_ptsq.jpg\", dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outliers_to_remove = [\"DeepHermes-3B-1.58bit\", \"OLMo-1B-1.58bit\"]\n",
    "data_filtered = data[~data[\"model_id\"].isin(outliers_to_remove)]\n",
    "\n",
    "# Plot setup\n",
    "plt.rcParams['font.family'] = 'Times New Roman'\n",
    "plt.rcParams['font.size'] = 14\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5), sharey=False)\n",
    "model_types = [\"DeepHermes\", \"LLaMA-Instruct\", \"OLMo\"]\n",
    "\n",
    "for ax, model_type in zip(axes, model_types):\n",
    "    subset = data_filtered[data_filtered[\"model_type\"] == model_type]\n",
    "\n",
    "    for _, row in subset.iterrows():\n",
    "        mean = row[\"prob_mean\"]\n",
    "        std = row[\"prob_std\"]\n",
    "\n",
    "        if std <= 0:\n",
    "            continue\n",
    "\n",
    "        x = np.linspace(mean - 4*std, mean + 4*std, 300)\n",
    "        y = norm.pdf(x, mean, std)\n",
    "        ax.plot(x, y, label=row[\"model_id\"])\n",
    "\n",
    "    ax.set_title(model_type)\n",
    "    ax.set_xlabel(\"Probability\")\n",
    "    ax.grid(True)\n",
    "    ax.legend(fontsize=14)\n",
    "\n",
    "axes[0].set_ylabel(\"Density\")\n",
    "#plt.suptitle(\"Normal Distributions Without 1.58-bit Outliers\", fontsize=16)\n",
    "plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "plt.savefig(\"Outputs/ProbsDistribs/normal_distributions_clean.jpg\", dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['prob_std']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(df_long)\n",
    "\n",
    "# Add model group\n",
    "df['group'] = df['model_id'].apply(\n",
    "    lambda x: 'DeepHermes' if 'DeepHermes' in x else \n",
    "              'LLaMA-Instruct' if 'LLaMA-Instruct' in x or 'HF1BitLLM' in x else \n",
    "              'OLMo'\n",
    ")\n",
    "\n",
    "# Plot\n",
    "fig, axs = plt.subplots(1, 3, figsize=(15, 5), sharey=True)\n",
    "groups = df['group'].unique()\n",
    "markers = {\n",
    "    '4bit': 'o', '8bit': 's', 'hf': 'D', 'FP': '^', '1.58bit': '*'\n",
    "}\n",
    "\n",
    "for i, group in enumerate(groups):\n",
    "    sub_df = df[df['group'] == group]\n",
    "    ax = axs[i]\n",
    "    \n",
    "    for _, row in sub_df.iterrows():\n",
    "        mean = row['prob_mean']\n",
    "        std = np.sqrt(row['prob_variance'])\n",
    "        x = np.linspace(mean - 3*std, mean + 3*std, 100)\n",
    "        y = 1 / (std * np.sqrt(2 * np.pi)) * np.exp(- (x - mean)**2 / (2 * std**2))\n",
    "        \n",
    "        label = row['model_id']\n",
    "        if '1.58bit' in label:\n",
    "            marker = markers['1.58bit']\n",
    "        elif '4bit' in label:\n",
    "            marker = markers['4bit']\n",
    "        elif '8bit' in label:\n",
    "            marker = markers['8bit']\n",
    "        elif 'hf' in label:\n",
    "            marker = markers['hf']\n",
    "        else:\n",
    "            marker = markers['FP']\n",
    "        \n",
    "        ax.plot(x, y, marker=marker, markevery=[50], label=label)\n",
    "\n",
    "    ax.set_title(group, fontsize=12)\n",
    "    ax.set_xlabel(\"Probability\", fontsize=11)\n",
    "    if i == 0:\n",
    "        ax.set_ylabel(\"Density\", fontsize=11)\n",
    "    ax.legend(fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "df = df_long\n",
    "\n",
    "# Split data by model group\n",
    "groups = {\n",
    "    \"DeepHermes\": df[df[\"model_id\"].str.contains(\"DeepHermes\")],\n",
    "    \"LLaMA-Instruct + HF1BitLLM\": df[df[\"model_id\"].str.contains(\"LLaMA-Instruct|HF1BitLLM\")],\n",
    "    \"OLMo\": df[df[\"model_id\"].str.contains(\"OLMo\")]\n",
    "}\n",
    "\n",
    "# Define markers\n",
    "markers = {\n",
    "    \"1.58bit\": \"o\",\n",
    "    \"4bit\": \"s\",\n",
    "    \"8bit\": \"^\",\n",
    "    \"hf\": \"P\",\n",
    "    \"default\": \"D\"\n",
    "}\n",
    "\n",
    "# Plot the Gaussian curves\n",
    "x_vals = np.linspace(-0.05, 0.05, 1000)\n",
    "\n",
    "fig, axs = plt.subplots(1, 3, figsize=(18, 6), sharey=True)\n",
    "for ax, (group_name, group_df) in zip(axs, groups.items()):\n",
    "    for _, row in group_df.iterrows():\n",
    "        mean = row[\"prob_mean\"]\n",
    "        var = row[\"prob_variance\"]\n",
    "        std = np.sqrt(var)\n",
    "        if std == 0:\n",
    "            continue\n",
    "        x = np.linspace(mean - 4*std, mean + 4*std, 1000)\n",
    "        y = (1 / (std * np.sqrt(2 * np.pi))) * np.exp(-0.5 * ((x - mean)/std)**2)\n",
    "        label = row[\"model_id\"]\n",
    "        # Determine marker\n",
    "        bit_key = next((key for key in markers if key in label), \"default\")\n",
    "        ax.plot(x, y, marker=markers[bit_key], markevery=200, label=label, linewidth=1.5)\n",
    "\n",
    "    ax.set_title(group_name, fontsize=14, fontfamily=\"serif\")\n",
    "    ax.set_xlabel(\"Probability\", fontsize=12, fontfamily=\"serif\")\n",
    "    ax.grid(True, linestyle=\"--\", alpha=0.6)\n",
    "    ax.legend(fontsize=9)\n",
    "\n",
    "axs[0].set_ylabel(\"Density\", fontsize=12, fontfamily=\"serif\")\n",
    "fig.suptitle(\"Gaussian-Approximated Probability Distributions Across Models\", fontsize=16, fontfamily=\"serif\")\n",
    "plt.tight_layout()\n",
    "plt.subplots_adjust(top=0.88)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "\n",
    "# Set font globally\n",
    "mpl.rcParams['font.family'] = 'serif'\n",
    "mpl.rcParams['font.serif'] = ['Times New Roman']\n",
    "\n",
    "# Data\n",
    "data = df_long\n",
    "# Model type mapping\n",
    "def extract_model_family(name):\n",
    "    if \"OLMo\" in name:\n",
    "        return \"OLMo\"\n",
    "    elif \"DeepHermes\" in name:\n",
    "        return \"DeepHermes\"\n",
    "    elif \"LLaMA-Instruct\" in name or \"HF1BitLLM\" in name:\n",
    "        return \"LLaMA\"\n",
    "    return \"Other\"\n",
    "\n",
    "data['family'] = data['model_id'].apply(extract_model_family)\n",
    "\n",
    "# Marker map\n",
    "marker_map = {\n",
    "    '4bit': 'o',\n",
    "    '8bit': 's',\n",
    "    '1.58bit': '^',\n",
    "    'hf': 'X',\n",
    "    'LLaMA-Instruct-8B': 'P',\n",
    "    'DeepHermes-LLaMA-3B': 'D',\n",
    "}\n",
    "\n",
    "def get_marker(model_id):\n",
    "    for key in marker_map:\n",
    "        if key in model_id:\n",
    "            return marker_map[key]\n",
    "    return 'v'\n",
    "\n",
    "data['marker'] = data['model_id'].apply(get_marker)\n",
    "\n",
    "# X-range for plotting\n",
    "x = np.linspace(0, 1, 500)\n",
    "\n",
    "def plot_distribution(ax, group_data, normalize=False, log_y=False):\n",
    "    for _, row in group_data.iterrows():\n",
    "        mean = row['prob_mean']\n",
    "        var = row['prob_variance']\n",
    "        std = np.sqrt(var)\n",
    "        label = row['model_id']\n",
    "        marker = row['marker']\n",
    "        y = np.exp(-0.5 * ((x - mean) / std)**2)\n",
    "        if normalize:\n",
    "            y /= y.max()\n",
    "        ax.plot(x, y, label=label, marker=marker, markevery=50)\n",
    "\n",
    "    ax.set_title(group_data['family'].iloc[0])\n",
    "    ax.set_xlabel('Probability')\n",
    "    ax.set_ylabel('Density')\n",
    "    if log_y:\n",
    "        ax.set_yscale('log')\n",
    "    ax.legend(fontsize=8)\n",
    "\n",
    "# Plot A: WITH ALL MODELS INCLUDING 1.58-bit (y-axis normalized)\n",
    "fig, axs = plt.subplots(1, 3, figsize=(18, 5), sharey=False)\n",
    "for i, (fam, group) in enumerate(data.groupby('family')):\n",
    "    plot_distribution(axs[i], group, normalize=True, log_y=False)\n",
    "fig.suptitle(\"Probability Distribution Curves (All Models Included)\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Plot B: EXCLUDING OLMo and DeepHermes 1.58-bit models\n",
    "exclude_ids = ['DeepHermes-3B-1.58bit', 'OLMo-1B-1.58bit']\n",
    "data_filtered = data[~data['model_id'].isin(exclude_ids)]\n",
    "\n",
    "fig, axs = plt.subplots(1, 3, figsize=(18, 5), sharey=False)\n",
    "for i, (fam, group) in enumerate(data_filtered.groupby('family')):\n",
    "    plot_distribution(axs[i], group, normalize=False, log_y=False)\n",
    "fig.suptitle(\"Probability Distribution Curves (Excluding Some 1.58-bit Models)\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 = gsm8k answer and 2 = questions, 3 = NQ queries and 4 = answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "df = df_long\n",
    "\n",
    "# Create subplots\n",
    "fig, axs = plt.subplots(3, 1, figsize=(12, 12), sharex=True)\n",
    "\n",
    "# Define groups\n",
    "group1 = df[:4]\n",
    "group2 = df[4:8]\n",
    "group3 = df[8:]\n",
    "\n",
    "# Helper function to plot a line for each model\n",
    "def plot_model_distribution(ax, group, color, label, marker):\n",
    "    x = np.linspace(0, 1, 100)  # X-axis range for the distribution\n",
    "    for _, row in group.iterrows():\n",
    "        mean = row['prob_mean']\n",
    "        var = row['prob_variance']\n",
    "        # Create a normal distribution with the given mean and variance\n",
    "        dist = np.random.normal(mean, np.sqrt(var), 1000)\n",
    "        \n",
    "        # Plot the line\n",
    "        sns.kdeplot(dist, ax=ax, color=color, label=label, linestyle='-', marker=marker, lw=2)\n",
    "        \n",
    "# Plot each group\n",
    "# Group 1\n",
    "plot_model_distribution(axs[0], group1, 'blue', 'Group 1', 'o')\n",
    "\n",
    "# Group 2\n",
    "plot_model_distribution(axs[1], group2, 'green', 'Group 2', 's')\n",
    "\n",
    "# Group 3\n",
    "plot_model_distribution(axs[2], group3, 'red', 'Group 3', 'D')\n",
    "\n",
    "# Set the title and labels\n",
    "axs[0].set_title('Model Distributions for Group 1')\n",
    "axs[1].set_title('Model Distributions for Group 2')\n",
    "axs[2].set_title('Model Distributions for Group 3')\n",
    "\n",
    "# Set x and y axis labels\n",
    "for ax in axs:\n",
    "    ax.set_xlabel('Probability')\n",
    "    ax.set_ylabel('Density')\n",
    "    ax.legend()\n",
    "\n",
    "# Display the plot\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_all_colmeans_round(df_long_1) # 1 = gsm8k answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_all_colmeans_round(df_long_2) # 2 = gsm8k question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_all_colmeans_round(df_long_3) # 3 = NQ query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_all_colmeans_round(df_long_4) # 4 = NQ answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming you have a list of dataframes for different datasets\n",
    "datasets = [df_long_1, df_long_2, df_long_3, df_long_4, df_long]  # Replace with your actual list of dataframes\n",
    "results = []\n",
    "\n",
    "for dataset in datasets:\n",
    "    mean_values = dataset.groupby('model_id')[['prob_mean', 'entropy_mean', 'logits_mean']].mean().reset_index()\n",
    "    results.append(mean_values)\n",
    "\n",
    "# If you want to merge them into one dataframe for easy comparison\n",
    "final_results = pd.concat(results, ignore_index=True)\n",
    "\n",
    "# Display the final result\n",
    "print(final_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_1['model_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_1['correct_1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_1['layer_names']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in df_all_1['correct_1']:\n",
    "    print(sum(i)/len(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "records = []\n",
    "for _, row in df_all.iterrows():\n",
    "    model_id = row['model_id']\n",
    "    prob_means = row['prob_mean']\n",
    "    layers = row.get('layer_names', list(range(len(prob_means))))  # fallback to index if needed\n",
    "\n",
    "    for i, (layer, prob) in enumerate(zip(layers, prob_means)):\n",
    "        records.append({\n",
    "            'model_id': model_id,\n",
    "            'layer_index': i,\n",
    "            'prob_mean': prob\n",
    "        })\n",
    "\n",
    "df_long = pd.DataFrame(records)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Sample data frame setup (based on the description)\n",
    "# Assuming df is the dataframe with the columns: model_id, correct_1, layer_names\n",
    "# Each \"correct_1\" and \"layer_names\" column is a list for each model\n",
    "\n",
    "def plot_correct(df, save_name, metric='stability_top1') -> None:\n",
    "    # Step 1: Expand each model's correct_1 and layer_names into individual rows\n",
    "    expanded_rows = []\n",
    "\n",
    "    for i, row in df.iterrows():\n",
    "        model_id = row['model_id']\n",
    "        correct_1 = row[metric]\n",
    "        layer_names = row['layer_names']\n",
    "        \n",
    "        # Align correct_1 with layer_names (flatten the lists)\n",
    "        for layer, score in zip(layer_names, correct_1):\n",
    "            expanded_rows.append({'model_id': model_id, 'layer_name': layer, metric: score})\n",
    "\n",
    "    # Convert expanded data into a new DataFrame\n",
    "    expanded_df = pd.DataFrame(expanded_rows)\n",
    "\n",
    "    # Step 2: Create a bar plot for each layer comparing the correct_1 scores across models\n",
    "    plt.rcParams['font.family'] = 'Times New Roman'\n",
    "    plt.figure(figsize=(14, 8))\n",
    "\n",
    "    # Group the data by layer_name and plot the correct_1 scores for each model\n",
    "    sns.lineplot(data=expanded_df, x='layer_name', y=metric, hue='model_id')\n",
    "\n",
    "    # Rotate x-axis labels for better readability\n",
    "    plt.xticks(rotation=90, ha='right')\n",
    "\n",
    "    #plt.title('Correct_1 Scores by Layer for Each Model', fontsize=16)\n",
    "    plt.xlabel('Layer Name', fontsize=12)\n",
    "    plt.ylabel('Correct_1 Score', fontsize=12)\n",
    "    plt.legend(title='Model ID', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Save or display the plot\n",
    "    plt.savefig(save_name)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_correct(df_all, 'Outputs/all_datasets/correct_topk_std.jpg', metric='correct_topk_std')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.stats import wasserstein_distance\n",
    "\n",
    "# Normalize all prob_mean vectors per model\n",
    "df_long['prob_mean_norm'] = df_long.groupby('model_id')['prob_mean'].transform(\n",
    "    lambda x: x / x.sum() if x.sum() > 0 else x\n",
    ")\n",
    "\n",
    "# Define base and quantized models\n",
    "all_models = df_long['model_id'].unique()\n",
    "quantized_models = [m for m in all_models if 'bit' in m or 'Bit' in m]\n",
    "\n",
    "# Manual fallback mapping for non-patterned models like HF1BitLLM\n",
    "custom_base_mapping = {\n",
    "    'HF1BitLLM': 'LLaMA-Instruct-8B',\n",
    "}\n",
    "\n",
    "def find_base_model(quant_model):\n",
    "    # Check manual overrides first\n",
    "    if quant_model in custom_base_mapping:\n",
    "        return custom_base_mapping[quant_model]\n",
    "\n",
    "    # Remove \"-8bit\", \"-4bit\", \"-1.58bit\", etc.\n",
    "    stripped = quant_model\n",
    "    for suffix in ['-8bit', '-4bit', '-1.58bit', '-ptsq', '-bnb8bit', '-bnb4bit']:\n",
    "        if quant_model.endswith(suffix):\n",
    "            stripped = quant_model.replace(suffix, '')\n",
    "            break\n",
    "\n",
    "    # Look for exact match or close match\n",
    "    for base in all_models:\n",
    "        if base == stripped or base.lower().startswith(stripped.lower()):\n",
    "            return base\n",
    "    return None\n",
    "\n",
    "plt.rcParams['font.family'] = 'Times New Roman'\n",
    "# 3x3 grid\n",
    "n_cols = 3\n",
    "n_rows = 3\n",
    "fig, axes = plt.subplots(n_rows, n_cols, figsize=(n_cols * 5, n_rows * 5))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, model in enumerate(quantized_models):\n",
    "    ax = axes[i]\n",
    "    base_model = find_base_model(model)\n",
    "\n",
    "    if base_model is None or base_model not in df_long['model_id'].values:\n",
    "        ax.set_title(f\"No match for {model}\")\n",
    "        ax.axis('off')\n",
    "        continue\n",
    "\n",
    "    base_data = df_long[df_long['model_id'] == base_model]['prob_mean_norm'].sort_values().reset_index(drop=True)\n",
    "    comp_data = df_long[df_long['model_id'] == model]['prob_mean_norm'].sort_values().reset_index(drop=True)\n",
    "\n",
    "    min_len = min(len(base_data), len(comp_data))\n",
    "    x = base_data[:min_len]\n",
    "    y = comp_data[:min_len]\n",
    "\n",
    "    reference = np.random.permutation(np.concatenate([x, y]))\n",
    "    w_dist = wasserstein_distance(x, y)\n",
    "    norm = max(\n",
    "        wasserstein_distance(x, reference[:min_len]),\n",
    "        wasserstein_distance(y, reference[:min_len])\n",
    "    )\n",
    "    nwd = w_dist / norm if norm != 0 else 0\n",
    "\n",
    "    ax.scatter(x, y, s=20, alpha=0.7)\n",
    "    ax.plot([0, max(x.max(), y.max())], [0, max(x.max(), y.max())], 'r--')\n",
    "    ax.set_title(f\"{model} vs.\\n{base_model}\\nNWD = {nwd:.3f}\", fontsize=12)\n",
    "    ax.set_xlabel(base_model)\n",
    "    ax.set_ylabel(model)\n",
    "    ax.grid(True)\n",
    "\n",
    "# Clean up unused plots\n",
    "for j in range(i + 1, len(axes)):\n",
    "    axes[j].axis('off')\n",
    "\n",
    "plt.suptitle(\"Q-Q Plots: Quantized vs. Base Models (NWD)\", fontsize=16, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.savefig('Outputs/all_datasets/qq_plot_nwd_quant_vs_fp_fixedgrid_all_datasets.jpg')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reference_model = 'OLMo-1B-hf'  # baseline\n",
    "model_order = (\n",
    "    df_distances[df_distances['model_1'] == reference_model]\n",
    "    .sort_values('wasserstein')['model_2']\n",
    "    .tolist()\n",
    ")\n",
    "model_order = [reference_model] + [m for m in model_order if m != reference_model]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import squareform\n",
    "from scipy.cluster.hierarchy import linkage, leaves_list\n",
    "\n",
    "\n",
    "model_ids = sorted(model_vectors.keys())\n",
    "distance_matrix = np.zeros((len(model_ids), len(model_ids)))\n",
    "\n",
    "for i, m1 in enumerate(model_ids):\n",
    "    for j, m2 in enumerate(model_ids):\n",
    "        if i < j:\n",
    "            d = wasserstein_distance(pad_vector(model_vectors[m1]), pad_vector(model_vectors[m2]))\n",
    "            distance_matrix[i, j] = d\n",
    "            distance_matrix[j, i] = d\n",
    "\n",
    "\n",
    "Z = linkage(squareform(distance_matrix), method='ward')\n",
    "ordered_indices = leaves_list(Z)\n",
    "model_order = [model_ids[i] for i in ordered_indices]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "model_palette = sns.color_palette(\"tab20\", n_colors=df_long['model_id'].nunique())\n",
    "\n",
    "plt.figure(figsize=(18, 6))\n",
    "plt.rcParams['font.family'] = 'Times New Roman'\n",
    "sns.violinplot(\n",
    "    data=df_long,\n",
    "    x='model_id',\n",
    "    y='prob_mean',\n",
    "    palette=model_palette,\n",
    "    inner=None,\n",
    "    cut=0,\n",
    "    linewidth=1.2\n",
    ")\n",
    "\n",
    "norm = plt.Normalize(df_long['layer_index'].min(), df_long['layer_index'].max())\n",
    "sm = plt.cm.ScalarMappable(cmap=\"coolwarm\", norm=norm)\n",
    "layer_colors = sm.to_rgba(df_long['layer_index'])\n",
    "\n",
    "sns.stripplot(\n",
    "    data=df_long,\n",
    "    x='model_id',\n",
    "    y='prob_mean',\n",
    "    hue='layer_index',\n",
    "    palette=\"coolwarm\",\n",
    "    size=3,\n",
    "    jitter=True,\n",
    "    dodge=False,\n",
    "    alpha=0.8,\n",
    "    linewidth=0\n",
    ")\n",
    "\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.xlabel(\"Model\")\n",
    "plt.ylabel(\"Mean Probability per Layer\")\n",
    "plt.title(\"Layer-wise Mean Probabilities per Model (Colored by Layer Index)\")\n",
    "\n",
    "handles, labels = plt.gca().get_legend_handles_labels()\n",
    "n = df_long['layer_index'].nunique()\n",
    "plt.legend(handles[-n:], labels[-n:], title='Layer Index', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "\n",
    "plt.tight_layout()\n",
    "#plt.savefig('Outputs/all_datasets/boxplot_layerprobs_all_datasets.jpg')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as stats\n",
    "\n",
    "unique_models = df_long['model_id'].unique()\n",
    "n_models = len(unique_models)\n",
    "\n",
    "n_cols = 4\n",
    "n_rows = (n_models + n_cols - 1) // n_cols\n",
    "\n",
    "plt.rcParams['font.family'] = 'Times New Roman'\n",
    "fig, axes = plt.subplots(n_rows, n_cols, figsize=(n_cols*4, n_rows*4))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, model in enumerate(unique_models):\n",
    "    ax = axes[i]\n",
    "    model_data = df_long[df_long['model_id'] == model]['prob_mean'].dropna()\n",
    "\n",
    "    stats.probplot(model_data, dist=\"norm\", plot=ax)\n",
    "    ax.set_title(model, fontsize=12)\n",
    "\n",
    "for j in range(i + 1, len(axes)):\n",
    "    fig.delaxes(axes[j])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.suptitle(\"Q-Q Plots of Layer-wise Probabilities per Model\", fontsize=16, y=1.02)\n",
    "plt.savefig('Outputs/all_datasets/qq_plot_layerprobs_all_datasets.jpg')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_a = 'LLaMA-Instruct-8B'\n",
    "model_b = 'HF1BitLLM'\n",
    "\n",
    "\n",
    "data_a = df_long[df_long['model_id'] == model_a]['prob_mean'].sort_values().reset_index(drop=True)\n",
    "data_b = df_long[df_long['model_id'] == model_b]['prob_mean'].sort_values().reset_index(drop=True)\n",
    "\n",
    "min_len = min(len(data_a), len(data_b))\n",
    "\n",
    "plt.figure(figsize=(6,6))\n",
    "plt.plot(data_a[:min_len], data_b[:min_len], 'o')\n",
    "plt.plot([0, max(data_a.max(), data_b.max())], [0, max(data_a.max(), data_b.max())], 'r--')\n",
    "plt.xlabel(model_a)\n",
    "plt.ylabel(model_b)\n",
    "plt.title(f\"Q-Q Plot: {model_a} vs. {model_b}\")\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.savefig('Outputs/gsm8k_answers/qq_xllama_yhf1bitllmplot_layerprobs_gsm8k_answers.jpg')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import wasserstein_distance\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from itertools import combinations\n",
    "\n",
    "# Reshape to {model_id: [prob_mean values]}\n",
    "model_probs = df_long.groupby('model_id')['prob_mean'].apply(list).to_dict()\n",
    "\n",
    "# Build distance matrix\n",
    "model_ids = list(model_probs.keys())\n",
    "n_models = len(model_ids)\n",
    "nwd_matrix = np.zeros((n_models, n_models))\n",
    "\n",
    "# Use standard normal or uniform reference distribution for normalization\n",
    "# Here we use the union of all values as a reference\n",
    "all_values = np.concatenate(list(model_probs.values()))\n",
    "reference = np.random.permutation(all_values)\n",
    "\n",
    "for i, model_i in enumerate(model_ids):\n",
    "    for j, model_j in enumerate(model_ids):\n",
    "        if i >= j:\n",
    "            continue  # matrix is symmetric\n",
    "        a = np.array(model_probs[model_i])\n",
    "        b = np.array(model_probs[model_j])\n",
    "\n",
    "        # Match lengths if needed\n",
    "        min_len = min(len(a), len(b))\n",
    "        a, b = a[:min_len], b[:min_len]\n",
    "\n",
    "        # Wasserstein distance\n",
    "        w_dist = wasserstein_distance(a, b)\n",
    "\n",
    "        # Normalize\n",
    "        norm = max(\n",
    "            wasserstein_distance(a, reference[:min_len]),\n",
    "            wasserstein_distance(b, reference[:min_len])\n",
    "        )\n",
    "        nwd = w_dist / norm if norm != 0 else 0\n",
    "\n",
    "        # Fill matrix (symmetrically)\n",
    "        nwd_matrix[i, j] = nwd\n",
    "        nwd_matrix[j, i] = nwd\n",
    "\n",
    "# Turn into DataFrame for plotting\n",
    "nwd_df = pd.DataFrame(nwd_matrix, index=model_ids, columns=model_ids)\n",
    "\n",
    "# Plot as heatmap\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(nwd_df, cmap=\"coolwarm\", annot=True, fmt=\".2f\", square=True)\n",
    "plt.title(\"Normalized Wasserstein Distance Between Models (Layer-wise Probabilities)\")\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import wasserstein_distance\n",
    "\n",
    "# --- 1. Preprocess df_all['layer_names'] ---\n",
    "# Ensure each row of df_all has a list of layer names\n",
    "df_all = df_all_1.copy()\n",
    "\n",
    "# If entries are strings, safely evaluate them\n",
    "df_all['layer_names'] = df_all['layer_names'].apply(\n",
    "    lambda x: eval(x) if isinstance(x, str) else x\n",
    ")\n",
    "\n",
    "# Explode df_all into one row per layer, with normalized prob_mean\n",
    "records = []\n",
    "for _, row in df_all.iterrows():\n",
    "    model_id = row['model_id']\n",
    "    layer_names = row['layer_names']\n",
    "    prob_means = row['prob_mean']\n",
    "\n",
    "    # 🔹 Normalize over total (vocab) size\n",
    "    prob_means = np.array(prob_means)\n",
    "    if prob_means.sum() > 0:\n",
    "        prob_means = prob_means / prob_means.sum()\n",
    "\n",
    "    for idx, prob in enumerate(prob_means):\n",
    "        layer_name = layer_names[idx] if idx < len(layer_names) else f\"layer_{idx}\"\n",
    "        records.append({\n",
    "            'model_id': model_id,\n",
    "            'layer_idx': idx,\n",
    "            'prob_mean': prob,\n",
    "            'layer_name': layer_name\n",
    "        })\n",
    "\n",
    "df_long = pd.DataFrame.from_records(records)\n",
    "\n",
    "\n",
    "\n",
    "# --- 4. Group by layer_name and compute NWD matrices ---\n",
    "grouped = df_long.groupby('layer_name')\n",
    "n_layers = len(grouped)\n",
    "n_cols = 4\n",
    "n_rows = int(np.ceil(n_layers / n_cols))\n",
    "\n",
    "plt.rcParams['font.family'] = 'Times New Roman'\n",
    "fig, axes = plt.subplots(n_rows, n_cols, figsize=(n_cols * 4.5, n_rows * 4.5))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, (layer_name, group) in enumerate(grouped):\n",
    "    ax = axes[i]\n",
    "    \n",
    "    # Get per-model list of probs for this layer\n",
    "    model_probs = group.groupby('model_id')['prob_mean'].apply(list).to_dict()\n",
    "    model_ids = list(model_probs.keys())\n",
    "    n_models = len(model_ids)\n",
    "    \n",
    "    if n_models < 2:\n",
    "        ax.axis('off')\n",
    "        continue\n",
    "\n",
    "    # Compute reference distribution\n",
    "    all_values = np.concatenate(list(model_probs.values()))\n",
    "    reference = np.random.permutation(all_values)\n",
    "\n",
    "    # Compute NWD matrix\n",
    "    nwd_matrix = np.zeros((n_models, n_models))\n",
    "    for m, model_i in enumerate(model_ids):\n",
    "        for n, model_j in enumerate(model_ids):\n",
    "            if m >= n:\n",
    "                continue\n",
    "            a = np.array(model_probs[model_i])\n",
    "            b = np.array(model_probs[model_j])\n",
    "            min_len = min(len(a), len(b))\n",
    "            a, b = a[:min_len], b[:min_len]\n",
    "            w_dist = wasserstein_distance(a, b)\n",
    "            norm = max(\n",
    "                wasserstein_distance(a, reference[:min_len]),\n",
    "                wasserstein_distance(b, reference[:min_len])\n",
    "            )\n",
    "            nwd = w_dist / norm if norm != 0 else 0\n",
    "            nwd_matrix[m, n] = nwd\n",
    "            nwd_matrix[n, m] = nwd\n",
    "\n",
    "    # Plot heatmap\n",
    "    nwd_df = pd.DataFrame(nwd_matrix, index=model_ids, columns=model_ids)\n",
    "    sns.heatmap(nwd_df, ax=ax, cmap=\"coolwarm\", annot=True, fmt=\".2f\",  square=True, cbar=False,\n",
    "                xticklabels=False, yticklabels=False)\n",
    "    ax.set_title(layer_name, fontsize=16)\n",
    "\n",
    "# --- 5. Clean up layout ---\n",
    "for j in range(i + 1, len(axes)):\n",
    "    fig.delaxes(axes[j])\n",
    "\n",
    "plt.suptitle(\"Normalized Wasserstein Distances Per Layer\", fontsize=16)\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "plt.savefig('Outputs/all_datasets/nwd_layerwise_heatmap_layerprobs_normalized_gsm8kanswer_all_datasets.jpg')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all['model_id'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_all['prob_mean'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_all['model_id'].value_counts())\n",
    "print(df_all.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show random sample\n",
    "print(df_all.sample(10)[['model_id', 'file_name']])\n",
    "\n",
    "# Or show top N unique combinations\n",
    "print(df_all[['model_id', 'file_name']].drop_duplicates().head(12))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure your plotting DataFrame is grouped correctly\n",
    "grouped = df_all.groupby(\"model_id\")\n",
    "for name, group in grouped:\n",
    "    print(f\"{name}: {len(group)} rows\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_singleton_lists(df, columns):\n",
    "    \"\"\"\n",
    "    Replace singleton lists (e.g., [0.5]) with scalar values in specified columns.\n",
    "    \"\"\"\n",
    "    for col in columns:\n",
    "        df[col] = df[col].apply(lambda x: x[0] if isinstance(x, list) and len(x) == 1 else x)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "singleton_cols = ['entropy', 'normalized_entropy', 'logit_mean', 'prob_mean',\n",
    "                  'stability_top1', 'stability_topk']  # Add others if needed\n",
    "\n",
    "df_all = flatten_singleton_lists(df_all, singleton_cols)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "singleton_cols = ['prompt', 'decoded_prompt_str', 'tokens', 'prompt_type', 'target_ids',\n",
    "       'target_tokens', 'layer_names', 'correct_1', 'correct_topk',\n",
    "       'correct_1_std', 'correct_topk_std', 'correct_1_by_position',\n",
    "       'correct_topk_by_position', 'entropy', 'normalized_entropy',\n",
    "       'logit_mean', 'prob_mean', 'stability_top1', 'stability_topk',\n",
    "       'layer_kl_divergences', 'model_id', 'file_name']  # Add others if needed\n",
    "\n",
    "df_all = flatten_singleton_lists(df_all, singleton_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all['kl_layer_last'] = df_all['layer_kl_divergences'].apply(lambda x: x[-1] if isinstance(x, list) else np.nan)\n",
    "df_all['kl_layer_mean'] = df_all['layer_kl_divergences'].apply(lambda x: np.mean(x) if isinstance(x, list) else np.nan)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all['logit_std'] = df_all['logit_mean'].apply(np.std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only select needed columns\n",
    "sub_df = df_all[['model_id', 'layer_names', 'correct_1_by_position']].copy()\n",
    "\n",
    "# Sanity filter: only keep rows where lengths match\n",
    "sub_df = sub_df[sub_df['layer_names'].str.len() == sub_df['correct_1_by_position'].str.len()]\n",
    "\n",
    "# Repeat model_id per layer\n",
    "sub_df = sub_df.explode(['layer_names', 'correct_1_by_position'])\n",
    "\n",
    "# Rename for clarity\n",
    "sub_df = sub_df.rename(columns={'layer_names': 'layer_name', 'correct_1_by_position': 'correct_1'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "def flatten_nested(l):\n",
    "    return list(itertools.chain.from_iterable(l)) if isinstance(l[0], list) else l\n",
    "\n",
    "df['layer_names'] = df['layer_names'].apply(flatten_nested)\n",
    "df['correct_1_by_position'] = df['correct_1_by_position'].apply(flatten_nested)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check shapes\n",
    "print(\"Original shape:\", df_all.shape)\n",
    "\n",
    "# Show sample raw values\n",
    "print(\"\\nSample layer_names:\", df_all['layer_names'].iloc[0])\n",
    "print(\"Sample correct_1_by_position:\", df_all['correct_1_by_position'].iloc[0])\n",
    "print(\"Type layer_names:\", type(df_all['layer_names'].iloc[0]))\n",
    "print(\"Type correct_1_by_position:\", type(df_all['correct_1_by_position'].iloc[0]))\n",
    "\n",
    "# Show lengths (if list-like)\n",
    "print(\"\\nLength of layer_names:\", len(df_all['layer_names'].iloc[0]) if isinstance(df_all['layer_names'].iloc[0], list) else 'not list')\n",
    "print(\"Length of correct_1_by_position:\", len(df_all['correct_1_by_position'].iloc[0]) if isinstance(df_all['correct_1_by_position'].iloc[0], list) else 'not list')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate nested lists to their mean per layer\n",
    "df_all['correct_1_mean_by_layer'] = df_all['correct_1_by_position'].apply(\n",
    "    lambda layer_preds: [sum(layer)/len(layer) if len(layer) > 0 else 0 for layer in layer_preds]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Length layer_names:\", len(df_all['layer_names'].iloc[0]))\n",
    "print(\"Length correct_1_mean_by_layer:\", len(df_all['correct_1_mean_by_layer'].iloc[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate nested lists to their mean per layer\n",
    "df_all['kl_divergences_mean_by_layer'] = df_all['layer_kl_divergences'].apply(\n",
    "    lambda layer_preds: [sum(layer)/len(layer) if len(layer) > 0 else 0 for layer in layer_preds]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_df = df_all[['model_id', 'layer_names', 'correct_1_mean_by_layer']].copy()\n",
    "\n",
    "# Make sure lengths match, else truncate\n",
    "def truncate_lists(row):\n",
    "    min_len = min(len(row['layer_names']), len(row['correct_1_mean_by_layer']))\n",
    "    return pd.Series({\n",
    "        'model_id': row['model_id'],\n",
    "        'layer_names': row['layer_names'][:min_len],\n",
    "        'correct_1_mean_by_layer': row['correct_1_mean_by_layer'][:min_len]\n",
    "    })\n",
    "\n",
    "sub_df = sub_df.apply(truncate_lists, axis=1)\n",
    "\n",
    "# Explode both list columns\n",
    "sub_df = sub_df.explode(['layer_names', 'correct_1_mean_by_layer'])\n",
    "\n",
    "sub_df = sub_df.rename(columns={\n",
    "    'layer_names': 'layer_name',\n",
    "    'correct_1_mean_by_layer': 'correct_1'\n",
    "})\n",
    "\n",
    "# Optional: extract numeric layer index for plotting\n",
    "sub_df['layer_num'] = sub_df['layer_name'].str.extract(r'(\\d+)$').astype(int)\n",
    "\n",
    "# Plotting\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(14, 6))\n",
    "plt.rcParams['font.family'] = 'DejaVu Sans'\n",
    "sns.lineplot(\n",
    "    data=sub_df,\n",
    "    x='layer_num',\n",
    "    y='correct_1',\n",
    "    hue='model_id',\n",
    "    marker='o',\n",
    "    linestyle='-',\n",
    "    markersize=6,\n",
    "    linewidth=2\n",
    ")\n",
    "\n",
    "plt.xlabel(\"Layer\")\n",
    "plt.ylabel(\"Mean Correct@1 per Layer\")\n",
    "plt.title(\"Mean Correct@1 per Layer by Model\")\n",
    "\n",
    "plt.grid(True)\n",
    "\n",
    "# Optionally set x-limits to max max layer_num across models to keep same scale\n",
    "max_layer = sub_df['layer_num'].max()\n",
    "plt.xlim(0, max_layer + 1)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('mean_correct1_by_layer')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "plt.figure(figsize=(14, 6))\n",
    "\n",
    "markers = ['o', 's', 'D', '^', 'v', 'P', '*', 'X', 'h', '8']  # extend if needed\n",
    "\n",
    "# Get unique models\n",
    "models = sub_df['model_id'].unique()\n",
    "\n",
    "for i, model in enumerate(models):\n",
    "    marker = markers[i % len(markers)]\n",
    "    model_data = sub_df[sub_df['model_id'] == model]\n",
    "    plt.plot(\n",
    "        model_data['layer_num'],\n",
    "        model_data['correct_1'],\n",
    "        marker=marker,\n",
    "        label=model,\n",
    "        linewidth=2,\n",
    "        markersize=8\n",
    "    )\n",
    "\n",
    "plt.xlabel(\"Layer\")\n",
    "plt.ylabel(\"Mean Correct@1 per Layer\")\n",
    "plt.title(\"Mean Correct@1 per Layer by Model\")\n",
    "\n",
    "plt.grid(True)\n",
    "plt.legend(title='Model ID', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Optional: extract numeric layer index\n",
    "sub_df['layer_num'] = sub_df['layer_name'].str.extract(r'(\\d+)$').astype(int)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(14, 6))\n",
    "sns.lineplot(data=sub_df, x='layer_num', y='correct_1', hue='model_id', marker='o')\n",
    "plt.xlabel(\"Layer\")\n",
    "plt.ylabel(\"Correct@1\")\n",
    "plt.title(\"Correct@1 per Layer by Model\")\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "def compute_cosine_similarity_per_position(probs1, probs2):\n",
    "    return [1 - cosine(p1, p2) if not (np.all(p1 == 0) or np.all(p2 == 0)) else 0.0\n",
    "            for p1, p2 in zip(probs1, probs2)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langdetect import detect, DetectorFactory\n",
    "DetectorFactory.seed = 42  # ensure consistent results\n",
    "\n",
    "#df_all['language'] = df_all['decoded_prompt_str'].apply(lambda x: detect(x) if isinstance(x, str) else 'unknown')\n",
    "#df_all['language'] = df_all['tokens'].apply(lambda x: detect(x) if isinstance(x, str) else 'unknown')\n",
    "df_all['language'] = df_all['target_tokens'].apply(lambda x: detect(x) if isinstance(x, str) else 'unknown')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for w in df_all['language']:\n",
    "    if w != 'en':\n",
    "        print(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_config = [\n",
    "    #(\"prob_mean\", \"e) Mean Prob of TopK\", \"Mean Prob.\"),\n",
    "    (\"logit_mean\", \"a) Mean Logit Magnitude\", \"Mean Logit\"),\n",
    "    (\"normalized_entropy\", \"b) Normalized Entropy per Layer\", \"Norm. Entropy\"),\n",
    "    (\"layer_kl_divergences\", \"c) KL Divergence per Layer\", \"KL Divergence\"),\n",
    "    (\"correct_1\", \"d) Correct TopK-1\", \"Correct\"),\n",
    "    (\"correct_1_std\", \"e) Correct TopK-1 Std.\", \"Std.\"),\n",
    "    (\"stability_top1\", \"f) TopK-1 Stability\", \"Stability\"),\n",
    "    (\"correct_topk\", \"g) Correct TopK-5\", \"Correct\"),\n",
    "    (\"correct_topk_std\", \"h) Correct TopK-5 Std.\", \"Std.\"),\n",
    "    (\"stability_topk\", \"i) TopK-5 Stability\", \"Stability\"),\n",
    "]\n",
    "\n",
    "#plot_model_diagnostics_variable_depth(df_all, metrics_config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all['layer_kl_divergences']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kl_means = []\n",
    "for i in df_all['layer_kl_divergences']:\n",
    "    kl_mean = np.mean(i)\n",
    "    kl_means.append(kl_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kl_means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lineplot(df_all['entropy'])\n",
    "plt.legend(df_all['model_id'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract final-layer KL for overview\n",
    "df_all['kl_final_layer'] = df_all['layer_kl_divergences'].apply(lambda x: x[-1] if isinstance(x, list) else np.nan)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## With Font Controls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "# Set seaborn style\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "def flatten_layerwise_metric(row):\n",
    "    \"\"\"Flattens overly nested [[...]] or [[[...]]] metrics.\"\"\"\n",
    "    while isinstance(row, list) and len(row) == 1 and isinstance(row[0], list):\n",
    "        row = row[0]\n",
    "    return row\n",
    "\n",
    "def average_nested_per_layer(data):\n",
    "    \"\"\"Convert list-of-lists (per layer) to averaged list per layer.\"\"\"\n",
    "    return [np.mean(layer) if isinstance(layer, list) and layer else np.nan for layer in data]\n",
    "\n",
    "def plot_model_diagnostics_variable_depth(\n",
    "    df,\n",
    "    metrics_config,\n",
    "    title=\"TopK-5 Quantized Model Diagnostic Landscape\",\n",
    "    max_layer_len=32,\n",
    "    font_sizes=None\n",
    "):\n",
    "    # Default font sizes if none provided\n",
    "    if font_sizes is None:\n",
    "        font_sizes = {\n",
    "            \"title\": 24,\n",
    "            \"subtitle\": 18,\n",
    "            \"xlabel\": 16,\n",
    "            \"ylabel\": 16,\n",
    "            \"xtick\": 14,\n",
    "            \"ytick\": 14,\n",
    "            \"legend\": 16\n",
    "        }\n",
    "\n",
    "    num_plots = len(metrics_config)\n",
    "    num_rows = (num_plots + 2) // 3\n",
    "\n",
    "    # Use Times New Roman and set figure size\n",
    "    plt.rcParams['font.family'] = 'Times New Roman'\n",
    "    fig, axs = plt.subplots(num_rows, 3, figsize=(22, 5 * num_rows), sharex=False)\n",
    "    axs = axs.flatten()\n",
    "\n",
    "    model_ids = df['model_id'].unique()\n",
    "    model_colors = {\n",
    "        model: color for model, color in zip(model_ids, sns.color_palette(\"tab20\", len(model_ids)))\n",
    "    }\n",
    "\n",
    "    double_nested_metrics = {\"layer_kl_divergences\", \"normalized_entropy\"}\n",
    "\n",
    "    for i, (metric_key, subplot_title, y_label) in enumerate(metrics_config):\n",
    "        ax = axs[i]\n",
    "\n",
    "        for model_id, group in df.groupby(\"model_id\"):\n",
    "            layerwise_aggregates = []\n",
    "\n",
    "            for row in group[metric_key].dropna():\n",
    "                row = flatten_layerwise_metric(row)\n",
    "\n",
    "                if metric_key in double_nested_metrics:\n",
    "                    if not isinstance(row, list) or not all(isinstance(r, list) for r in row):\n",
    "                        continue\n",
    "                    averaged = average_nested_per_layer(row)\n",
    "                else:\n",
    "                    try:\n",
    "                        averaged = np.asarray(row, dtype=np.float32).flatten().tolist()\n",
    "                    except Exception:\n",
    "                        continue\n",
    "\n",
    "                if not averaged or np.any(np.isnan(averaged)) or np.any(np.isinf(averaged)):\n",
    "                    continue\n",
    "\n",
    "                if len(averaged) > max_layer_len:\n",
    "                    averaged = averaged[:max_layer_len]\n",
    "\n",
    "                layerwise_aggregates.append(averaged)\n",
    "\n",
    "            if not layerwise_aggregates:\n",
    "                continue\n",
    "\n",
    "            max_len = max(len(v) for v in layerwise_aggregates)\n",
    "            per_layer_values = [[] for _ in range(max_len)]\n",
    "\n",
    "            for v in layerwise_aggregates:\n",
    "                for j, val in enumerate(v):\n",
    "                    per_layer_values[j].append(val)\n",
    "\n",
    "            means = np.array([np.mean(l) if l else np.nan for l in per_layer_values])\n",
    "            stds = np.array([np.std(l) if l else np.nan for l in per_layer_values])\n",
    "            x = np.arange(len(means))\n",
    "\n",
    "            ax.plot(x, means, label=model_id, color=model_colors[model_id])\n",
    "            ax.fill_between(x, means - stds, means + stds, color=model_colors[model_id], alpha=0.2)\n",
    "\n",
    "            last_valid = np.where(~np.isnan(means))[0][-1]\n",
    "            ax.plot(x[last_valid], means[last_valid], 'o', color=model_colors[model_id], markersize=8)\n",
    "\n",
    "        ax.set_title(subplot_title, fontsize=font_sizes[\"subtitle\"])\n",
    "        ax.set_xlabel(\"Layer Index\", fontsize=font_sizes[\"xlabel\"])\n",
    "        ax.set_ylabel(y_label, fontsize=font_sizes[\"ylabel\"])\n",
    "        ax.tick_params(axis='x', labelsize=font_sizes[\"xtick\"])\n",
    "        ax.tick_params(axis='y', labelsize=font_sizes[\"ytick\"])\n",
    "        ax.grid(True)\n",
    "\n",
    "    # Remove unused axes\n",
    "    for j in range(len(metrics_config), len(axs)):\n",
    "        fig.delaxes(axs[j])\n",
    "\n",
    "    # Shared legend and figure title\n",
    "    handles, labels = axs[0].get_legend_handles_labels()\n",
    "    fig.legend(handles, labels, loc='upper center', ncol=4, fontsize=font_sizes[\"legend\"],\n",
    "               frameon=False, bbox_to_anchor=(0.5, 1.05))\n",
    "    #fig.suptitle(title, fontsize=font_sizes[\"title\"])\n",
    "\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.93])  # adjust to leave space for legend and title\n",
    "    fig.savefig('nq_answers_lineplots.jpg', bbox_inches='tight', dpi=300)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model_diagnostics_variable_depth(\n",
    "    df_all,\n",
    "    metrics_config,\n",
    "    font_sizes={\n",
    "        \"title\": 36,\n",
    "        \"subtitle\": 28,\n",
    "        \"xlabel\": 24,\n",
    "        \"ylabel\": 24,\n",
    "        \"xtick\": 20,\n",
    "        \"ytick\": 20,\n",
    "        \"legend\": 26\n",
    "    }\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "def compute_layerwise_metric_correlations(df, metrics_to_compare, max_layer_len=32):\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "\n",
    "    metric_data = {metric: [] for metric in metrics_to_compare}\n",
    "\n",
    "    for metric in metrics_to_compare:\n",
    "        for _, row in df.iterrows():\n",
    "            data = row[metric]\n",
    "            if data is None or (isinstance(data, float) and np.isnan(data)):\n",
    "                continue\n",
    "\n",
    "            data = flatten_layerwise_metric(data)\n",
    "\n",
    "            if isinstance(data, list) and all(isinstance(d, list) for d in data):\n",
    "                data = average_nested_per_layer(data)\n",
    "            try:\n",
    "                flattened = np.asarray(data, dtype=np.float32).flatten()\n",
    "                if len(flattened) < max_layer_len:\n",
    "                    padded = np.full((max_layer_len,), np.nan, dtype=np.float32)\n",
    "                    padded[:len(flattened)] = flattened\n",
    "                    flattened = padded\n",
    "                elif len(flattened) > max_layer_len:\n",
    "                    flattened = flattened[:max_layer_len]\n",
    "\n",
    "            except Exception:\n",
    "                continue\n",
    "\n",
    "            if len(flattened) > max_layer_len:\n",
    "                flattened = flattened[:max_layer_len]\n",
    "\n",
    "            if not len(flattened) or np.any(np.isnan(flattened)) or np.any(np.isinf(flattened)):\n",
    "                continue\n",
    "\n",
    "            metric_data[metric].append(flattened)\n",
    "\n",
    "    # Align all metrics by truncating/padding each run\n",
    "    min_samples = min(len(v) for v in metric_data.values())\n",
    "    for key in metric_data:\n",
    "        metric_data[key] = metric_data[key][:min_samples]\n",
    "\n",
    "    # Convert to matrix: shape = (num_samples, num_metrics, num_layers)\n",
    "    all_metrics = []\n",
    "    for key in metrics_to_compare:\n",
    "        all_metrics.append(np.stack(metric_data[key]))  # shape: (samples, layers)\n",
    "    all_metrics = np.stack(all_metrics, axis=1)  # shape: (samples, metrics, layers)\n",
    "\n",
    "    # Compute correlation per-layer and average across layers\n",
    "    correlations = np.full((len(metrics_to_compare), len(metrics_to_compare)), np.nan)\n",
    "    for i in range(len(metrics_to_compare)):\n",
    "        for j in range(len(metrics_to_compare)):\n",
    "            values_i = all_metrics[:, i, :].flatten()\n",
    "            values_j = all_metrics[:, j, :].flatten()\n",
    "            if np.std(values_i) > 0 and np.std(values_j) > 0:\n",
    "                correlations[i, j] = np.corrcoef(values_i, values_j)[0, 1]\n",
    "\n",
    "    return pd.DataFrame(\n",
    "        correlations,\n",
    "        index=metrics_to_compare,\n",
    "        columns=metrics_to_compare\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_config = [\n",
    "    #(\"prob_mean\", \"e) Mean Prob of TopK\", \"Mean Prob.\"),\n",
    "    (\"logit_mean\", \"a) Mean Logit Magnitude\", \"Mean Logit\"),\n",
    "    (\"normalized_entropy\", \"b) Normalized Entropy per Layer\", \"Norm. Entropy\"),\n",
    "    (\"layer_kl_divergences\", \"c) KL Divergence per Layer\", \"KL Divergence\"),\n",
    "    (\"correct_1\", \"d) Correct TopK-1\", \"Correct\"),\n",
    "    (\"correct_1_std\", \"e) Correct TopK-1 Std.\", \"Std.\"),\n",
    "    (\"correct_topk\", \"g) Correct TopK-5\", \"Correct\"),\n",
    "    (\"correct_topk_std\", \"h) Correct TopK-5 Std.\", \"Std.\"),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sns.set(style=\"whitegrid\")\n",
    "\n",
    "def plot_correlation_heatmap(corr_matrix, title=\"Metric Correlation Across Layers\"):\n",
    "    plt.rcParams['font.family'] = 'Times New Roman'\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    sns.heatmap(corr_matrix, annot=True, fmt=\".2f\", cmap=\"coolwarm\", square=True,\n",
    "                cbar_kws={\"shrink\": 0.8}, linewidths=0.5, linecolor=\"gray\")\n",
    "    plt.title(title, fontsize=20)\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.yticks(rotation=0)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_keys = [m[0] for m in corr_config]\n",
    "\n",
    "corr_matrix = compute_layerwise_metric_correlations(df_all, metrics_keys)\n",
    "plot_correlation_heatmap(corr_matrix)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datasets for calibrating activations and analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_inputs = [\n",
    "    # Language understanding\n",
    "    \"The quick brown fox jumps over the lazy dog.\",\n",
    "    \"Despite the rain, the event continued as planned.\",\n",
    "    \n",
    "    # Logic/reasoning\n",
    "    \"If all humans are mortal and Socrates is a human, then Socrates is mortal.\",\n",
    "    \"Either the lights are off or the power is out. The lights are on, so the power must be out.\",\n",
    "\n",
    "    # Math/numerical\n",
    "    \"The derivative of sin(x) with respect to x is cos(x).\",\n",
    "    \"What is the sum of the first 100 natural numbers?\",\n",
    "\n",
    "    # Programming\n",
    "    \"In Python, list comprehensions provide a concise way to create lists.\",\n",
    "    \"To define a function in JavaScript, use the 'function' keyword.\",\n",
    "\n",
    "    # Commonsense knowledge\n",
    "    \"You should refrigerate milk after opening it to keep it fresh.\",\n",
    "    \"People usually eat breakfast in the morning before starting their day.\",\n",
    "\n",
    "    # Scientific knowledge\n",
    "    \"Water boils at 100 degrees Celsius under standard atmospheric pressure.\",\n",
    "    \"Photosynthesis is the process by which plants convert sunlight into chemical energy.\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wikitext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = r'D:\\ThesisData\\wikitext'\n",
    "\n",
    "destination_path = str(Path(filepath))\n",
    "dataset = load_dataset(\n",
    "    'wikitext', 'wikitext-103-raw-v1',\n",
    "    split={\n",
    "        'train': 'train[:200]',\n",
    "    },\n",
    "    cache_dir=destination_path,\n",
    "    download_mode=DownloadMode.REUSE_DATASET_IF_EXISTS,\n",
    "    keep_in_memory=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_texts = dataset['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calibration_texts = [t for t in dataset['train'][\"text\"] if isinstance(t, str) and t.strip()]\n",
    "#calibration_texts = [t for t in sub_txts[\"text\"] if isinstance(t, str) and t.strip()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_txts = train_texts.take(200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Natural Questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = r'D:\\ThesisData\\nq'\n",
    "\n",
    "destination_path = str(Path(filepath))\n",
    "nq_dataset = load_dataset(\n",
    "    'sentence-transformers/natural-questions',\n",
    "    split={\n",
    "        'train': 'train[:20]'\n",
    "    },\n",
    "    cache_dir=destination_path,\n",
    "    download_mode=DownloadMode.REUSE_DATASET_IF_EXISTS,\n",
    "    keep_in_memory=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nq_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nq_queries= nq_dataset['train']['query']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nq_queries[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nq_queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nq_answers = nq_dataset['train']['answer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sae_input = nq_answers[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nq_answers[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nq_answers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GSM8K (Math)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = r'D:\\ThesisData\\gsm8k'\n",
    "\n",
    "destination_path = str(Path(filepath))\n",
    "gsm8k_dataset = load_dataset(\n",
    "    'gsm8k', 'main',\n",
    "    split={\n",
    "        'train': 'train[:20]'\n",
    "    },\n",
    "    cache_dir=destination_path,\n",
    "    download_mode=DownloadMode.REUSE_DATASET_IF_EXISTS,\n",
    "    keep_in_memory=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gsm8k_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gsm8k_questions = gsm8k_dataset['train']['question']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gsm8k_answers = gsm8k_dataset['train']['answer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gsm8k_answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gsm8k_questions_sae = \"\"\"\n",
    "Natalia sold clips to 48 of her friends in April, and then she sold half as many clips in May. How many clips did Natalia sell altogether in April and May?\n",
    "Weng earns $12 an hour for babysitting. Yesterday, she just did 50 minutes of babysitting. How much did she earn?\n",
    "Betty is saving money for a new wallet which costs $100. Betty has only half of the money she needs. Her parents decided to give her $15 for that purpose, and her grandparents twice as much as her parents. How much more money does Betty need to buy the wallet?\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models and Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_test_model(model_path:str, dtype=torch.dtype) -> AutoModelForCausalLM:\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_path,\n",
    "        return_dict=True,\n",
    "        output_hidden_states=True,\n",
    "        #torch_dtype=dtype,\n",
    "        low_cpu_mem_usage=True,\n",
    "        local_files_only=True,\n",
    "        use_safetensors=True,\n",
    "        #trust_remote_code=True\n",
    "    )\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hfbit1_tokenizer = AutoTokenizer.from_pretrained(FPKey.HFBIT1_TOKENIZER.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hfbit1_tokenizer.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hfbit1_fp32 = load_test_model(FPKey.HFBIT1_8B.value, dtype=torch.float32) # https://huggingface.co/HF1BitLLM/Llama3-8B-1.58-100B-tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access the input embedding layer\n",
    "embed_weight = hfbit1_fp32.model.embed_tokens.weight\n",
    "\n",
    "# Access the output projection layer\n",
    "output_weight = hfbit1_fp32.lm_head.weight\n",
    "\n",
    "# Check if they are tied\n",
    "print(embed_weight.data_ptr() == output_weight.data_ptr())  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hfbit1_fp32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print model layers to inspect their names\n",
    "for name, module in hfbit1_fp32.named_modules():\n",
    "    print(f\"Layer name: {name}, Module: {module}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llama8b_tokenizer = AutoTokenizer.from_pretrained(FPKey.LINSTRUCT_TOKENIZER.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llama8b_tokenizer.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llama8b_fp32 = load_test_model(FPKey.LINSTRUCT_8B.value, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access the input embedding layer\n",
    "embed_weight = llama8b_fp32.model.embed_tokens.weight\n",
    "\n",
    "# Access the output projection layer\n",
    "output_weight = llama8b_fp32.lm_head.weight\n",
    "\n",
    "# Check if they are tied\n",
    "print(embed_weight.data_ptr() == output_weight.data_ptr())  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_embedding_tie_info(model):\n",
    "    try:\n",
    "        embed = model.model.embed_tokens\n",
    "    except AttributeError:\n",
    "        embed = model.embed_tokens\n",
    "    lm_head = model.lm_head\n",
    "    print(f\"Embed shape: {embed.weight.shape}\")\n",
    "    print(f\"LM head shape: {lm_head.weight.shape}\")\n",
    "    print(\"Tied:\", embed.weight.data_ptr() == lm_head.weight.data_ptr())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_embedding_tie_info(llama8b_fp32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llama8b_fp32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model with 8-bit quantization using BNB\n",
    "llama8b_bnb8_float32 = AutoModelForCausalLM.from_pretrained(\n",
    "    ModelKey.LLINSTRUCT8B.value,           \n",
    "    #torch_dtype=torch.float32,     # Specify the dtype (for 8-bit, use uint8)\n",
    "    #device_map=\"cpu\",           # Automatically map the model to available devices (GPU/CPU)\n",
    "    load_in_8bit=True,\n",
    "    return_dict=True,\n",
    "    output_hidden_states=True,            # Set to True for 8-bit quantization\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llama8b_bnb8_float32.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model with 4-bit quantization using BNB\n",
    "llama8b_bnb4_float32 = AutoModelForCausalLM.from_pretrained(\n",
    "    ModelKey.LLINSTRUCT8B.value,           \n",
    "    #torch_dtype=torch.float32,     # This would still use uint8 or another type based on quantization method\n",
    "    #device_map=\"auto\",           # Automatically map the model to available devices (GPU/CPU)\n",
    "    load_in_4bit=True,            # Set to True for 4-bit quantization (if available)\n",
    "    return_dict=True,\n",
    "    output_hidden_states=True, \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llama8b_ptsq_float32 = applyPTQ(\n",
    "    load_test_model(ModelKey.LLINSTRUCT8B.value, dtype=torch.float32),\n",
    "    tokenizer=llama8b_tokenizer,\n",
    "    #calibration_input=None,\n",
    "    #calibration_input=sub_txts['text'],\n",
    "    calibration_input=Texts.T1.value,\n",
    "    mode='1.58bit',\n",
    "    safer_quant=True,\n",
    "    q_lmhead=True,\n",
    "    model_half=False,\n",
    "    quant_half=False,\n",
    "    layers_to_quant_weights=QuantStyle.BITNET.value,\n",
    "    layers_to_quant_activations=QuantStyle.BITNET.value,\n",
    "    fragile_layers=False,\n",
    "    act_quant=True,\n",
    "    act_bits=8,\n",
    "    torch_backends=False,\n",
    "    debugging=True,\n",
    "    plot_debugging=False,\n",
    "    plot_quantization=False,\n",
    "    freeze_modules=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hfbit1_tokenizer.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llama8b_tokenizer.vocab_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### allenai/OLMo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "olmo1b_tokenizer = AutoTokenizer.from_pretrained(FPKey.OLMO1B_TOKENIZER.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "olmo1b_tokenizer.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "olmo2t_tokenizer = AutoTokenizer.from_pretrained(FPKey.OLMO7B2T_TOKENIZER.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "olmo2t_tokenizer.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "olmo1b_fp32 = load_test_model(FPKey.OLMO1B_FP.value, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.functional as F\n",
    "# Check self-tying of embeddings for training-time hints\n",
    "embedding_weight = olmo1b_fp32.model.embed_tokens\n",
    "first_hidden = olmo1b_fp32.model(...)[0][:, 0, :]         # or use a dummy forward pass\n",
    "cos_sim = torch.nn.functional.cosine_similarity(\n",
    "    first_hidden @ embedding_weight.T,  # approximates logits\n",
    "    embedding_weight, dim=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_embedding_tie_info(olmo1b_fp32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diagnostic to check weight tying at inference\n",
    "print(\"Inference-time tying:\", olmo1b_fp32.model.embed_tokens.weight is olmo1b_fp32.model.lm_head.weight)\n",
    "\n",
    "# Diagnostic for training-time behavior: check cosine similarity\n",
    "cos_sim = F.cosine_similarity(\n",
    "    olmo1b_fp32.model.embed_tokens.weight, model.lm_head.weight, dim=1\n",
    ")\n",
    "print(\"Cosine similarity (mean):\", cos_sim.mean().item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access the input embedding layer\n",
    "embed_weight = llama8b_fp32.model.embed_tokens.weight\n",
    "\n",
    "# Access the output projection layer\n",
    "output_weight = llama8b_fp32.lm_head.weight\n",
    "\n",
    "# Check if they are tied\n",
    "print(embed_weight.data_ptr() == output_weight.data_ptr())  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "olmo1b_fp32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "olmo2t_fp32 = load_test_model(FPKey.OLMO7B2T_FP.value, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "olmo2t_fp32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def are_embeddings_tied(model):\n",
    "    # Access correctly for various subclasses\n",
    "    try:\n",
    "        embed = model.model.embed_tokens\n",
    "    except AttributeError:\n",
    "        embed = model.embed_tokens\n",
    "\n",
    "    try:\n",
    "        lm_head = model.lm_head\n",
    "    except AttributeError:\n",
    "        raise ValueError(\"Model has no `lm_head`.\")\n",
    "\n",
    "    return embed.weight.data_ptr() == lm_head.weight.data_ptr()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "are_embeddings_tied(olmo1b_fp32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "olmo1b_bitnet_fp32_ptsq_safety = applyPTQ(\n",
    "    load_test_model(FPKey.OLMO1B_FP.value, dtype=torch.float32),\n",
    "    tokenizer=olmo1b_tokenizer,\n",
    "    #calibration_input=None,\n",
    "    #calibration_input=sub_txts['text'],\n",
    "    calibration_input=Texts.T1.value,\n",
    "    mode='1.58bit',\n",
    "    safer_quant=True,\n",
    "    model_half=False,\n",
    "    quant_half=False,\n",
    "    layers_to_quant_weights=QuantStyle.BITNET.value,\n",
    "    layers_to_quant_activations=QuantStyle.BITNET.value,\n",
    "    fragile_layers=False,\n",
    "    act_quant=True,\n",
    "    act_bits=8,\n",
    "    torch_backends=True,\n",
    "    debugging=True,\n",
    "    plot_debugging=False,\n",
    "    plot_quantization=False,\n",
    "    freeze_modules=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "olmo1b_bitnet_fp32_ptsq = applyPTQ(\n",
    "    load_test_model(FPKey.OLMO1B_FP.value, dtype=torch.float32),\n",
    "    tokenizer=olmo1b_tokenizer,\n",
    "    #calibration_input=None,\n",
    "    #calibration_input=sub_txts['text'],\n",
    "    calibration_input=Texts.T1.value,\n",
    "    mode='1.58bit',\n",
    "    safer_quant=False,\n",
    "    q_safety_layers=None,\n",
    "    model_half=False,\n",
    "    quant_half=False,\n",
    "    layers_to_quant_weights=QuantStyle.BITNET.value,\n",
    "    layers_to_quant_activations=QuantStyle.BITNET.value,\n",
    "    fragile_layers=False,\n",
    "    act_quant=True,\n",
    "    act_bits=8,\n",
    "    torch_backends=False,\n",
    "    debugging=True,\n",
    "    plot_debugging=False,\n",
    "    plot_quantization=False,\n",
    "    freeze_modules=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "olmo2t_bitnet_fp32_ptsq = applyPTQ(\n",
    "    load_test_model(FPKey.OLMO7B2T_FP.value, dtype=torch.float32),\n",
    "    tokenizer=olmo2t_tokenizer,\n",
    "    #calibration_input=None,\n",
    "    #calibration_input=sub_txts['text'],\n",
    "    calibration_input=Texts.T1.value,\n",
    "    mode='1.58bit',\n",
    "    safer_quant=True,\n",
    "    q_lmhead=True,\n",
    "    model_half=False,\n",
    "    quant_half=False,\n",
    "    layers_to_quant_weights=QuantStyle.BITNET.value,\n",
    "    layers_to_quant_activations=QuantStyle.BITNET.value,\n",
    "    fragile_layers=False,\n",
    "    act_quant=True,\n",
    "    act_bits=8,\n",
    "    torch_backends=False,\n",
    "    debugging=True,\n",
    "    plot_debugging=False,\n",
    "    plot_quantization=False,\n",
    "    freeze_modules=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model with 8-bit quantization using BNB\n",
    "olmo1b_bnb8_float32 = AutoModelForCausalLM.from_pretrained(\n",
    "    FPKey.OLMO1B_FP.value,           \n",
    "    #torch_dtype=torch.uint8,     # Specify the dtype (for 8-bit, use uint8)\n",
    "    #device_map=\"cpu\",           # Automatically map the model to available devices (GPU/CPU)\n",
    "    load_in_8bit=True,\n",
    "    return_dict=True,\n",
    "    output_hidden_states=True,            # Set to True for 8-bit quantization\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "olmo1b_bnb8_float32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.check_model_dtypes(olmo1b_bnb8_float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model with 4-bit quantization using BNB\n",
    "olmo1b_bnb4_float32 = AutoModelForCausalLM.from_pretrained(\n",
    "    FPKey.OLMO1B_FP.value,           \n",
    "    #torch_dtype=torch.float32,     # This would still use uint8 or another type based on quantization method\n",
    "    #device_map=\"auto\",           # Automatically map the model to available devices (GPU/CPU)\n",
    "    load_in_4bit=True,            # Set to True for 4-bit quantization (if available)\n",
    "    return_dict=True,\n",
    "    output_hidden_states=True, \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "olmo1b_bnb4_float32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.check_model_dtypes(olmo1b_bnb4_float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.check_model_dtypes(olmo1b_bnb4_float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NousResearch/DeepHermes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dh3b_tokenizer = AutoTokenizer.from_pretrained(FPKey.TOKENIZER_3B.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dh3b_tokenizer.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dh8b_tokenizer = AutoTokenizer.from_pretrained(FPKey.TOKENIZER_8B.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dh8b_tokenizer.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dh3b_fp32 = load_test_model(FPKey.FP_3B.value, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "are_embeddings_tied(dh3b_fp32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "are_embeddings_tied(llama8b_fp32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "are_embeddings_tied(hfbit1_fp32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access the input embedding layer\n",
    "embed_weight = llama8b_fp32.model.embed_tokens.weight\n",
    "\n",
    "# Access the output projection layer\n",
    "output_weight = llama8b_fp32.lm_head.weight\n",
    "\n",
    "# Check if they are tied\n",
    "print(embed_weight.data_ptr() == output_weight.data_ptr())  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dh3b_fp32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dh8b_fp32 = load_test_model(FPKey.FP_8B.value, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dh8b_fp32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dh3b_bitnet_fp32_ptsq = applyPTQ(\n",
    "    load_test_model(FPKey.FP_3B.value, dtype=torch.float32),\n",
    "    tokenizer=dh3b_tokenizer,\n",
    "    #calibration_input=None,\n",
    "    #calibration_input=sub_txts['text'],\n",
    "    calibration_input=Texts.T1.value,\n",
    "    mode='1.58bit',\n",
    "    safer_quant=False,\n",
    "    model_half=False,\n",
    "    q_safety_layers=None,\n",
    "    quant_half=False,\n",
    "    layers_to_quant_weights=QuantStyle.BITNET.value,\n",
    "    layers_to_quant_activations=QuantStyle.BITNET.value,\n",
    "    fragile_layers=False,\n",
    "    act_quant=True,\n",
    "    act_bits=8,\n",
    "    torch_backends=False,\n",
    "    debugging=True,\n",
    "    plot_debugging=False,\n",
    "    plot_quantization=False,\n",
    "    freeze_modules=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model with 8-bit quantization using BNB\n",
    "dh3b_bnb8_float32 = AutoModelForCausalLM.from_pretrained(\n",
    "    FPKey.FP_3B.value,           \n",
    "    #torch_dtype=torch.float32,     # Specify the dtype (for 8-bit, use uint8)\n",
    "    #device_map=\"cpu\",           # Automatically map the model to available devices (GPU/CPU)\n",
    "    load_in_8bit=True,\n",
    "    return_dict=True,\n",
    "    output_hidden_states=True,            # Set to True for 8-bit quantization\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model with 4-bit quantization using BNB\n",
    "dh3b_bnb4_float32 = AutoModelForCausalLM.from_pretrained(\n",
    "    FPKey.FP_3B.value,           \n",
    "    #torch_dtype=torch.float32,     # This would still use uint8 or another type based on quantization method\n",
    "    #device_map=\"auto\",           # Automatically map the model to available devices (GPU/CPU)\n",
    "    load_in_4bit=True,            # Set to True for 4-bit quantization (if available)\n",
    "    return_dict=True,\n",
    "    output_hidden_states=True, \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model with 8-bit quantization using BNB\n",
    "dh8b_bnb8_float32 = AutoModelForCausalLM.from_pretrained(\n",
    "    FPKey.FP_8B.value,           \n",
    "    #torch_dtype=torch.float32,     # Specify the dtype (for 8-bit, use uint8)\n",
    "    #device_map=\"cpu\",           # Automatically map the model to available devices (GPU/CPU)\n",
    "    load_in_8bit=True,\n",
    "    return_dict=True,\n",
    "    output_hidden_states=True,            # Set to True for 8-bit quantization\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model with 4-bit quantization using BNB\n",
    "dh8b_bnb4_float32 = AutoModelForCausalLM.from_pretrained(\n",
    "    FPKey.FP_8B.value,           \n",
    "    #torch_dtype=torch.float32,     # This would still use uint8 or another type based on quantization method\n",
    "    #device_map=\"auto\",           # Automatically map the model to available devices (GPU/CPU)\n",
    "    load_in_4bit=True,            # Set to True for 4-bit quantization (if available)\n",
    "    return_dict=True,\n",
    "    output_hidden_states=True, \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dh8b_bnb4_float32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logit Lens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gsm8k_questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gsm8k_answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nq_queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch._dynamo\n",
    "torch._dynamo.config.suppress_errors = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logit_lens.plot_topk_comparing_lens(\n",
    "    models=(hfbit1_fp32, llama8b_fp32),\n",
    "    tokenizers=(hfbit1_tokenizer, llama8b_tokenizer),\n",
    "    inputs=nq_answers[0],\n",
    "    start_ix=0, end_ix=15,\n",
    "    topk=5,\n",
    "    topk_mean=False,\n",
    "    #js=True,\n",
    "    block_step=1,\n",
    "    token_font_size=18,\n",
    "    label_font_size=20,\n",
    "    #json_log_path=None,\n",
    "    #json_log_path='logs/nq_answers/dh.3b-bnb4bit.fp32', # 20 samples\n",
    "    #save_fig_path=None,\n",
    "    #save_fig_path='Outputs/LogitLens/DH3B/logits_3b_fp32_math.jpg',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logit_lens.plot_topk_comparing_lens(\n",
    "    models=(llama8b_fp32, hfbit1_fp32),\n",
    "    tokenizers=(llama8b_tokenizer, hfbit1_tokenizer),\n",
    "    inputs=nq_answers[0],\n",
    "    start_ix=0, end_ix=15,\n",
    "    topk=5,\n",
    "    topk_mean=False,\n",
    "    #js=True,\n",
    "    block_step=1,\n",
    "    token_font_size=18,\n",
    "    label_font_size=20,\n",
    "    #json_log_path=None,\n",
    "    #json_log_path='logs/nq_answers/dh.3b-bnb4bit.fp32', # 20 samples\n",
    "    #save_fig_path=None,\n",
    "    #save_fig_path='Outputs/LogitLens/DH3B/logits_3b_fp32_math.jpg',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logit_lens.plot_topk_comparing_lens(\n",
    "    models=(dh3b_fp32, dh3b_bnb4_float32),\n",
    "    tokenizers=(dh3b_tokenizer, dh3b_tokenizer),\n",
    "    inputs=nq_queries[0],\n",
    "    start_ix=0, end_ix=5,\n",
    "    topk=5,\n",
    "    topk_mean=True,\n",
    "    js=True,\n",
    "    block_step=2,\n",
    "    token_font_size=22,\n",
    "    label_font_size=22,\n",
    "    #json_log_path=None,\n",
    "    #json_log_path='logs/nq_answers/dh.3b-bnb4bit.fp32', # 20 samples\n",
    "    #save_fig_path=None,\n",
    "    #save_fig_path='Outputs/LogitLens/DH3B/logits_3b_fp32_math.jpg',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logit_lens.plot_topk_logit_lens(\n",
    "    model=hfbit1_fp32,\n",
    "    tokenizer=hfbit1_tokenizer,\n",
    "    inputs=\"The cat is on the mat\",\n",
    "    start_ix=0, end_ix=5,\n",
    "    topk=5,\n",
    "    topk_mean=False,\n",
    "    plot_topk_lens=True,\n",
    "    #entropy=True,\n",
    "    block_step=2,\n",
    "    token_font_size=22,\n",
    "    label_font_size=22,\n",
    "    #json_log_path=None,\n",
    "    #json_log_path='logs/nq_answers/dh.3b-bnb4bit.fp32', # 20 samples\n",
    "    #save_fig_path=None,\n",
    "    #save_fig_path='Outputs/LogitLens/DH3B/logits_3b_fp32_math.jpg',\n",
    "    model_precision=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logit_lens.plot_topk_logit_lens(\n",
    "    model=dh3b_bnb4_float32,\n",
    "    tokenizer=dh3b_tokenizer,\n",
    "    inputs=nq_queries[0],\n",
    "    start_ix=0, end_ix=5,\n",
    "    topk=5,\n",
    "    topk_mean=False,\n",
    "    plot_topk_lens=True,\n",
    "    entropy=True,\n",
    "    block_step=2,\n",
    "    token_font_size=22,\n",
    "    label_font_size=22,\n",
    "    #json_log_path=None,\n",
    "    #json_log_path='logs/nq_answers/dh.3b-bnb4bit.fp32', # 20 samples\n",
    "    #save_fig_path=None,\n",
    "    #save_fig_path='Outputs/LogitLens/DH3B/logits_3b_fp32_math.jpg',\n",
    "    model_precision=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logit_lens.plot_topk_logit_lens(\n",
    "    model=hfbit1_fp32,\n",
    "    tokenizer=hfbit1_tokenizer,\n",
    "    inputs=MiscPrompts.Q11.value,\n",
    "    start_ix=0, end_ix=15,\n",
    "    topk=5,\n",
    "    topk_mean=True,\n",
    "    plot_topk_lens=True,\n",
    "    entropy=True,\n",
    "    block_step=1,\n",
    "    token_font_size=18,\n",
    "    #json_log_path=None,\n",
    "    #json_log_path='logs/nq_answers/dh.3b-bnb4bit.fp32', # 20 samples\n",
    "    #save_fig_path=None,\n",
    "    #save_fig_path='Outputs/LogitLens/DH3B/logits_3b_fp32_math.jpg',\n",
    "    model_precision=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logit_lens.plot_topk_logit_lens(\n",
    "    model=dh3b_bitnet_fp32_ptsq,\n",
    "    tokenizer=dh3b_tokenizer,\n",
    "    inputs=nq_answers,\n",
    "    start_ix=0, end_ix=15,\n",
    "    topk=5,\n",
    "    topk_mean=True,\n",
    "    plot_topk_lens=False,\n",
    "    #entropy=True,\n",
    "    block_step=1,\n",
    "    token_font_size=18,\n",
    "    #json_log_path=None,\n",
    "    json_log_path='logs/nq_answers/dh.3b-ptsq.fp32.json', # 20 samples\n",
    "    #save_fig_path=None,\n",
    "    #save_fig_path='Outputs/LogitLens/DH3B/logits_3b_fp32_math.jpg',\n",
    "    model_precision=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dh8b_bnb8_float32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_path = 'logs/gsm8k/llama.8b-1.58.fp32'\n",
    "\n",
    "with open(json_path, 'r') as f:\n",
    "    log_data = json.load(f)\n",
    "\n",
    "# Convert the loaded JSON data to a DataFrame\n",
    "df = pd.json_normalize(log_data)\n",
    "\n",
    "\n",
    "print(df)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('logs/gsm8k/dh.3b-ptsq.fp32', 'r') as f:\n",
    "    log_data = json.load(f)\n",
    "df = pd.json_normalize(log_data)\n",
    "\n",
    "# Ensure each row's layer_names and entropy have matching length\n",
    "num_layers = len(df.loc[0, 'layer_names'])\n",
    "sum_entropy = [0.0] * num_layers\n",
    "valid_rows = 0\n",
    "\n",
    "for _, row in df.iterrows():\n",
    "    layer_names = row['layer_names']\n",
    "    entropy = row['entropy']\n",
    "    \n",
    "    if isinstance(entropy, list) and len(entropy) == num_layers:\n",
    "        sum_entropy = [s + e for s, e in zip(sum_entropy, entropy)]\n",
    "        valid_rows += 1\n",
    "\n",
    "# Compute average\n",
    "if valid_rows > 0:\n",
    "    avg_entropy = [e / valid_rows for e in sum_entropy]\n",
    "    layer_labels = df.loc[0, 'layer_names']\n",
    "\n",
    "    # Plotting\n",
    "    plt.figure(figsize=(14, 6))\n",
    "    plt.plot(layer_labels, avg_entropy, marker='o', linestyle='-', color='b')\n",
    "    plt.xlabel('Layer Name')\n",
    "    plt.ylabel('Average Entropy')\n",
    "    plt.title(f'Average Entropy Across Layers (n = {valid_rows} samples)')\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No valid rows matched expected layer length.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['entropy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['normalized_entropy']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dictionary Learning: SAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sae_input = nq_answers[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sae_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch._dynamo\n",
    "torch._dynamo.config.suppress_errors = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dh3b_bitnet_fp32_ptsq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary_learning.plot_sae_heatmap(\n",
    "    model=dh3b_fp32,\n",
    "    tokenizer=dh3b_tokenizer,\n",
    "    inputs=gsm8k_questions_sae,\n",
    "    plot_sae=True,\n",
    "    do_log=True,\n",
    "    top_k=5,\n",
    "    tokens_per_row=30,\n",
    "    target_layers=[2, 9, 16, 20, 26],\n",
    "    log_path='logs/sae_logs/DH3B/fp',\n",
    "    log_name='dh.3b-ptsq.fp32',\n",
    "    fig_path=None,\n",
    "    deterministic_sae=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary_learning.plot_sae_heatmap(\n",
    "    model=hfbit1_fp32,\n",
    "    tokenizer=hfbit1_tokenizer,\n",
    "    inputs=sae_input,\n",
    "    plot_sae=True,\n",
    "    do_log=False,\n",
    "    top_k=5,\n",
    "    tokens_per_row=25,\n",
    "    target_layers=[0, 1, 10, 27, 30],\n",
    "    log_path=None,\n",
    "    log_name=None,\n",
    "    fig_path=None,\n",
    "    deterministic_sae=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary_learning.plot_comparing_heatmap(\n",
    "    models=(llama8b_fp32, hfbit1_fp32),\n",
    "    tokenizer=llama8b_tokenizer,\n",
    "    inputs=sae_input,\n",
    "    top_k=5,\n",
    "    tokens_per_row=25,\n",
    "    target_layers=[0, 1, 10, 27, 30],\n",
    "    fig_path=None,\n",
    "    deterministic_sae=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary_learning.plot_comparing_heatmap(\n",
    "    models=(llama8b_fp32, hfbit1_fp32),\n",
    "    tokenizer=llama8b_tokenizer,\n",
    "    inputs=gsm8k_questions_sae,\n",
    "    top_k=5,\n",
    "    tokens_per_row=30,\n",
    "    target_layers=[2, 9, 16, 23, 30],\n",
    "    fig_path=None\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# QA on Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_dict = {\n",
    "    #'dh.3b-llama.fp32': dh3b_fp32,\n",
    "    #'dh.3b-bnb4bit.fp16': dh3b_bnb4_fp16,\n",
    "    #'dh.3b-1.58.ptdq': dh3b_bitnet_fp32, \n",
    "    #'dh.3b-1.58.ptsq': dh3b_bitnet_fp32,\n",
    "    #'dh.8b-llama.fp32': dh8b_fp32,\n",
    "    #'dh.8b-bnb4bit.fp16': dh8b_bnb4_fp16,\n",
    "    #'dh.8b-1.58.ptdq': dh8b_bitnet_fp32,\n",
    "    #'dh.8b-1.58.ptsq': dh8b_bitnet_fp32,\n",
    "    #'llama.8b-instruct.fp32': llama8b_fp32,\n",
    "    #'llama.8b-bnb4bit.fp16': llama8b_bnb4_fp16,\n",
    "    #'llama.8b-1.58.fp32': hfbit1_fp32,\n",
    "    #'llama.8b-1.58.ptdq': llama8b_bitnet_fp32,\n",
    "    #'llama.8b-1.58.ptsq': llama8b_bitnet_fp32,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PARAMS:Dict = {\n",
    "    'context': Contexts.C1.value,\n",
    "    'prompt': MiscPrompts.Q2.value,\n",
    "    'max_new_tokens': 100,\n",
    "    'temperature': 0.8,\n",
    "    'repetition_penalty': 1.1,\n",
    "    'sample': True,\n",
    "    'device': None\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chatbot_analysis.run_nq_analysis(\n",
    "    model=dh3b_bnb4_float32,\n",
    "    tokenizer=dh3b_tokenizer,\n",
    "    model_name='dh.3b-bnb4bit.fp32',\n",
    "    dataset=nq_dataset['train'],\n",
    "    save_path='logs/nq_logs/DH3B',\n",
    "    num_samples=10,\n",
    "    deterministic_backend=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chatbot_analysis.run_gsm8k_analysis(\n",
    "    model=dh3b_bnb4_float32,\n",
    "    tokenizer=dh3b_tokenizer,\n",
    "    model_name='dh.3b-bnb4bit.fp32',\n",
    "    dataset=gsm8k_dataset['train'],\n",
    "    save_path='logs/gsm8k_logs/DH3B',\n",
    "    num_samples=10,\n",
    "    deterministic_backend=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chatbot_analysis.run_chatbot_analysis(\n",
    "    models=chat_dict,\n",
    "    tokenizer=dh3b_tokenizer,\n",
    "    deep_thinking=False,\n",
    "    full_path='logs/chatbot_logs',\n",
    "    deterministic_backend=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chatbot_analysis.plot_chatbot_analysis(\n",
    "    json_logs='logs/gsm8k_logs',\n",
    "    parallel_plot=True,\n",
    "    reference_file='logs/gsm8k_logs/llama.8b-1.58.fp32.json'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chatbot_analysis.plot_chatbot_analysis(\n",
    "    json_logs='logs/gsm8k_logs',\n",
    "    parallel_plot=False,\n",
    "    reference_file='logs/gsm8k_logs/llama.8b-1.58.fp32.json',\n",
    "    title=\"Model Metrics ('What is y if y=2*2-4+(3*2)')\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting Analysis Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dir = \"logs/gsm8k_logs\"\n",
    "\n",
    "# Load all JSONs into a DataFrame\n",
    "all_results = []\n",
    "for filename in os.listdir(results_dir):\n",
    "    if filename.endswith(\".json\"):\n",
    "        with open(os.path.join(results_dir, filename), 'r') as f:\n",
    "            data = json.load(f)\n",
    "            all_results.append(data)\n",
    "\n",
    "df = pd.DataFrame(all_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "def normalize_name(name):\n",
    "    # Remove .json suffix if present\n",
    "    if name.endswith('.json'):\n",
    "        name = name[:-5]\n",
    "\n",
    "    # Fix prefix ldh -> dh\n",
    "    if name.startswith('ldh'):\n",
    "        name = 'dh' + name[3:]\n",
    "\n",
    "    # For names starting with 'dh.3b', replace dots with dashes after first two parts\n",
    "    if name.startswith('dh.3b'):\n",
    "        parts = name.split('.')\n",
    "        if len(parts) > 2:\n",
    "            # Join first two parts with dot, rest joined by dash\n",
    "            name = parts[0] + '.' + parts[1] + '-' + '-'.join(parts[2:])\n",
    "    \n",
    "    return name\n",
    "\n",
    "def plot_flat_metrics_by_model(\n",
    "    df,\n",
    "    metrics,\n",
    "    model_col='Model',\n",
    "    model_name_map=None,\n",
    "    title=\"Model Performance & Resource Metrics\",\n",
    "    font_sizes=None,\n",
    "    log_metrics=None,\n",
    "    col_wrap=5,\n",
    "    figsize=(22, 10),\n",
    "    save_path=None\n",
    "):\n",
    "\n",
    "\n",
    "    if font_sizes is None:\n",
    "        font_sizes = {\n",
    "            \"title\": 20,\n",
    "            \"subtitle\": 22,\n",
    "            \"xlabel\": 20,\n",
    "            \"ylabel\": 20,\n",
    "            \"xtick\": 14,\n",
    "            \"ytick\": 14,\n",
    "            \"legend\": 22\n",
    "        }\n",
    "\n",
    "    if log_metrics is None:\n",
    "        log_metrics = [\"Perplexity\"]\n",
    "\n",
    "    df = df.copy()\n",
    "    df[model_col] = df[model_col].str.strip()\n",
    "\n",
    "    # Normalize df model names\n",
    "    df['NormalizedModel'] = df[model_col].apply(normalize_name)\n",
    "\n",
    "    # Normalize model_name_map values\n",
    "    model_name_map_fixed = {k: normalize_name(v) for k, v in model_name_map.items()}\n",
    "\n",
    "    # Reverse map: normalized model name -> friendly key\n",
    "    json_to_key = {v: k for k, v in model_name_map_fixed.items()}\n",
    "\n",
    "    # Map normalized df model names to friendly keys for legend\n",
    "    df['FriendlyName'] = df['NormalizedModel'].map(json_to_key).fillna(df[model_col])\n",
    "\n",
    "    friendly_names = df['FriendlyName'].unique()\n",
    "    plt.rcParams['font.family'] = 'Times New Roman'\n",
    "    cmap = cm.get_cmap('tab20', len(friendly_names))\n",
    "    color_map = {name: cmap(i) for i, name in enumerate(friendly_names)}\n",
    "\n",
    "    n_metrics = len(metrics)\n",
    "    nrows = int(np.ceil(n_metrics / col_wrap))\n",
    "    ncols = min(col_wrap, n_metrics)\n",
    "    fig, axes = plt.subplots(nrows=nrows, ncols=ncols, figsize=figsize)\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    plt.rcParams['font.family'] = 'Times New Roman'\n",
    "\n",
    "    for i, metric in enumerate(metrics):\n",
    "        ax = axes[i]\n",
    "        if metric not in df.columns:\n",
    "            ax.set_visible(False)\n",
    "            continue\n",
    "\n",
    "        for j, friendly_name in enumerate(friendly_names):\n",
    "            vals = df.loc[df['FriendlyName'] == friendly_name, metric].values\n",
    "            if len(vals) == 0:\n",
    "                continue\n",
    "            val = vals[0]\n",
    "            if metric in log_metrics:\n",
    "                val = np.log1p(val)\n",
    "\n",
    "            label = friendly_name if i == 0 else None\n",
    "            ax.bar(j, val, color=color_map[friendly_name], label=label)\n",
    "\n",
    "        ax.set_title(metric + (\" (log)\" if metric in log_metrics else \"\"), fontsize=font_sizes[\"subtitle\"])\n",
    "        ax.set_xticks([])\n",
    "        ax.grid(True)\n",
    "\n",
    "    for i in range(n_metrics, len(axes)):\n",
    "        fig.delaxes(axes[i])\n",
    "\n",
    "    handles = [plt.Rectangle((0, 0), 1, 1, color=color_map[name]) for name in friendly_names]\n",
    "    labels = list(friendly_names)\n",
    "\n",
    "    fig.legend(handles, labels, loc='upper center', ncol=min(5, len(labels)),\n",
    "               fontsize=font_sizes[\"legend\"], frameon=False, bbox_to_anchor=(0.5, 1.05))\n",
    "\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.93])\n",
    "\n",
    "    if save_path:\n",
    "        fig.savefig(save_path, bbox_inches='tight', dpi=300)\n",
    "\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name_map = {\n",
    "    'DeepHermes-LLaMA-3B': 'dh.3b-llama.fp32.json',\n",
    "    'DeepHermes-3B-8bit': 'dh.3b.bnb8bit.fp32.json',\n",
    "    'DeepHermes-3B-4bit': 'ldh.3b.bnb4bit.fp32.json',\n",
    "    'DeepHermes-3B-1.58bit': 'dh.3b.ptsq.fp32.json',\n",
    "    'LLaMA-Instruct-8B': 'llama.8b-instruct.fp32.json',\n",
    "    'LLaMA-Instruct-8B-8bit': 'llama.8b-bnb8bit.fp32.json',\n",
    "    'LLaMA-Instruct-8B-4bit': 'llama.8b-bnb4bit.fp32.json',\n",
    "    'HF1BitLLM': 'llama.8b-1.58.fp32.json',\n",
    "}\n",
    "\n",
    "\"\"\"model_name_map_fixed = {\n",
    "    key: val.replace(\".json\", \"\") for key, val in model_name_map.items()\n",
    "}\"\"\"\n",
    "\n",
    "metrics = [\n",
    "    'Perplexity',\n",
    "    'CPU Usage (%)',\n",
    "    'RAM Usage (%)',\n",
    "    'GPU Memory (MB)',\n",
    "    'Activation Similarity',\n",
    "    'Latency (s)',\n",
    "    \"Last Layer Mean Activation\",\n",
    "    \"Last Layer Activation Std\",\n",
    "    \"Mean Logits\",\n",
    "    \"Logit Std\",\n",
    "]\n",
    "\n",
    "plot_flat_metrics_by_model(\n",
    "    df,\n",
    "    metrics=metrics,\n",
    "    model_col=\"Model\",\n",
    "    model_name_map=model_name_map,\n",
    "    title=\"Flat Metrics Across Models (GSM8K)\",\n",
    "    log_metrics=[\"Perplexity\"],\n",
    "    save_path='GSM8K_QA'\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = [\n",
    "    'Perplexity',\n",
    "    'CPU Usage (%)',\n",
    "    'RAM Usage (%)',\n",
    "    'GPU Memory (MB)',\n",
    "    'Activation Similarity',\n",
    "    'Latency (s)',\n",
    "    \"Last Layer Mean Activation\",\n",
    "    \"Last Layer Activation Std\",\n",
    "    \"Mean Logits\",\n",
    "    \"Logit Std\",\n",
    "]\n",
    "\n",
    "# Generate a color map\n",
    "models = df['Model'].tolist()\n",
    "num_models = len(models)\n",
    "colors = cm.get_cmap('coolwarm', num_models)\n",
    "model_colors = {model: colors(i) for i, model in enumerate(models)}\n",
    "\n",
    "# Prepare subplot grid dynamically\n",
    "n_metrics = len(metrics)\n",
    "ncols = 5\n",
    "nrows = int(np.ceil(n_metrics / ncols))\n",
    "\n",
    "plt.rcParams['font.family'] = 'Times New Roman'\n",
    "fig, axes = plt.subplots(nrows=nrows, ncols=ncols, figsize=(20, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, metric in enumerate(metrics):\n",
    "    ax = axes[i]\n",
    "    if metric in df.columns:\n",
    "        for j, model in enumerate(models):\n",
    "            ax.bar(model, df.loc[j, metric], color=model_colors[model])\n",
    "        ax.set_title(metric, fontsize=12)\n",
    "        ax.set_xticks(range(len(models)))\n",
    "        ax.set_xticklabels(models, rotation=45, ha='right', fontsize=10)\n",
    "        ax.grid(True)\n",
    "\n",
    "# Hide unused axes\n",
    "for j in range(i + 1, len(axes)):\n",
    "    fig.delaxes(axes[j])\n",
    "\n",
    "# Legend\n",
    "##handles = [plt.Rectangle((0, 0), 1, 1, color=model_colors[model]) for model in models]\n",
    "#fig.legend(handles, models, loc='upper center', ncol=min(num_models, 5))\n",
    "\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.93])\n",
    "#plt.suptitle(\"Deep Hermes LLaMA 3B & LLaMA Instruct 8B GSM8K (n=10)\", fontsize=12)\n",
    "plt.suptitle(\"LLaMA 8B Instruct GSM8K (n=10)\", fontsize=14)\n",
    "plt.savefig('Outputs/Report/gsm8k_qa/llama8b_subplots.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = [\n",
    "    'Perplexity',\n",
    "    'CPU Usage (%)',\n",
    "    'RAM Usage (%)',\n",
    "    'GPU Memory (MB)',\n",
    "    'Activation Similarity',\n",
    "    'Latency (s)',\n",
    "    \"Last Layer Mean Activation\",\n",
    "    \"Last Layer Activation Std\",\n",
    "    \"Mean Logits\",\n",
    "    \"Logit Std\",\n",
    "]\n",
    "\n",
    "# Generate a color map\n",
    "models = df['Model'].tolist()\n",
    "num_models = len(models)\n",
    "colors = cm.get_cmap('coolwarm', num_models)\n",
    "model_colors = {model: colors(i) for i, model in enumerate(models)}\n",
    "\n",
    "# Prepare subplot grid dynamically\n",
    "n_metrics = len(metrics)\n",
    "ncols = 5\n",
    "nrows = int(np.ceil(n_metrics / ncols))\n",
    "\n",
    "plt.rcParams['font.family'] = 'Times New Roman'\n",
    "fig, axes = plt.subplots(nrows=nrows, ncols=ncols, figsize=(20, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, metric in enumerate(metrics):\n",
    "    ax = axes[i]\n",
    "    if metric in df.columns:\n",
    "        values = df[metric].copy()\n",
    "\n",
    "        # Normalize perplexity via log-scale\n",
    "        if metric == \"Perplexity\":\n",
    "            values = np.log1p(values)  # log1p handles 0 gracefully\n",
    "\n",
    "        for j, model in enumerate(models):\n",
    "            ax.bar(model, values[j], color=model_colors[model])\n",
    "        ax.set_title(metric + (\" (log)\" if metric == \"Perplexity\" else \"\"), fontsize=12)\n",
    "        ax.set_xticks(range(len(models)))\n",
    "        ax.set_xticklabels(models, rotation=45, ha='right', fontsize=10)\n",
    "        ax.grid(True)\n",
    "\n",
    "# Hide unused axes\n",
    "for j in range(i + 1, len(axes)):\n",
    "    fig.delaxes(axes[j])\n",
    "\n",
    "# Legend\n",
    "##handles = [plt.Rectangle((0, 0), 1, 1, color=model_colors[model]) for model in models]\n",
    "#fig.legend(handles, models, loc='upper center', ncol=min(num_models, 5))\n",
    "\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.93])\n",
    "plt.suptitle(\"Deep Hermes 3B LLaMA & LLaMA 8B Instruct GSM8K (n=10)\", fontsize=14)\n",
    "plt.savefig('Outputs/Report/gsm8k_qa/llamadh3b_subplots_normalized_perplexity.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metrics to include (excluding 'Activation Similarity')\n",
    "metrics_for_corr = [\n",
    "    'Perplexity',\n",
    "    'CPU Usage (%)',\n",
    "    'RAM Usage (%)',\n",
    "    'GPU Memory (MB)',\n",
    "    'Latency (s)',\n",
    "    \"Last Layer Mean Activation\",\n",
    "    \"Last Layer Activation Std\",\n",
    "    \"Mean Logits\",\n",
    "    \"Logit Std\",\n",
    "]\n",
    "\n",
    "# Compute correlation\n",
    "corr_matrix = df[metrics_for_corr].corr()\n",
    "\n",
    "# Plot\n",
    "plt.rcParams['font.family'] = 'Times New Roman'\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(\n",
    "    corr_matrix, \n",
    "    annot=True, \n",
    "    cmap='coolwarm', \n",
    "    fmt=\".2f\", \n",
    "    linewidths=0.5, \n",
    "    square=True,\n",
    "    cbar_kws={\"shrink\": 0.75}\n",
    ")\n",
    "plt.title(\"LLaMA 8B Instruct GSM8K (n=10)\", fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.savefig('Outputs/Report/gsm8k_qa/llama8b_corr_heatmap.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "original = torch.randn(512) * 0.5  # Original activations\n",
    "\n",
    "def quantize_dequantize(tensor, scale_value):\n",
    "    scale = max(scale_value, 1e-8)\n",
    "    qmin, qmax = -127, 127\n",
    "    tensor_int = (tensor / scale).round().clamp(qmin, qmax).to(torch.int8)\n",
    "    tensor_dequant = tensor_int.float() * scale\n",
    "    return tensor_int, tensor_dequant\n",
    "\n",
    "# Quantize with different scales\n",
    "_, dequant_1e2 = quantize_dequantize(original, 1e-2)\n",
    "_, dequant_1e5 = quantize_dequantize(original, 1e-5)\n",
    "\n",
    "# L2 distance\n",
    "print(\"L2 Distance (scale=1e-2):\", torch.norm(original - dequant_1e2).item())\n",
    "print(\"L2 Distance (scale=1e-5):\", torch.norm(original - dequant_1e5).item())\n",
    "\n",
    "# Plot histograms + KDEs\n",
    "plt.rcParams['font.family'] = 'Times New Roman'\n",
    "plt.figure(figsize=(14, 6))\n",
    "sns.histplot(original.numpy(),bel='Original', kde=True, stat=\"count\", bins=50, color='black', alpha=0.5)\n",
    "sns.histplot(dequant_1e2.numpy(), label='Dequant (scale=1e-2)', kde=True, stat=\"count\", bins=50, color='red', alpha=0.5)\n",
    "sns.histplot(dequant_1e5.numpy(), label='Dequant (scale=1e-5)', kde=True, stat=\"count\", bins=50, color='blue', alpha=0.5)\n",
    "\n",
    "plt.title(\"Histogram (Count) + KDE of Quantized vs Original Activations\")\n",
    "plt.xlabel(\"Activation Value\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.ylim(0, 40)  \n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = {\n",
    "    \"GSM8K Query\": gsm8k_questions,  # 20 samples\n",
    "    \"GSM8K Answer\": gsm8k_answers,\n",
    "    \"Natural Questions Query\": nq_queries,\n",
    "    \"Natural Questions Answer\": nq_answers,\n",
    "}\n",
    "\n",
    "\n",
    "length_data = []\n",
    "for name, samples in datasets.items():\n",
    "    for sample in samples:\n",
    "        length_data.append({\n",
    "            \"Dataset\": name,\n",
    "            \"Length\": len(sample) \n",
    "        })\n",
    "\n",
    "\n",
    "df = pd.DataFrame(length_data)\n",
    "\n",
    "sns.set(style=\"whitegrid\")\n",
    "plt.rcParams['font.family'] = 'Times New Roman'\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.boxplot(data=df, x=\"Dataset\", y=\"Length\", palette=\"tab20\")\n",
    "sns.stripplot(data=df, x=\"Dataset\", y=\"Length\", color=\"black\", alpha=0.5, jitter=True)\n",
    "\n",
    "#plt.title(\"Sequence Length Distribution by Dataset\")\n",
    "plt.legend(df['Dataset'].unique(), fontsize=20)\n",
    "plt.ylabel(\"Length\")\n",
    "plt.grid(True, linestyle=\"--\", alpha=0.4)\n",
    "plt.tight_layout()\n",
    "#plt.savefig('datasets_lengths.jpg')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = {\n",
    "    \"GSM8K Query\": gsm8k_questions,  # 20 samples\n",
    "    \"GSM8K Answer\": gsm8k_answers,\n",
    "    \"Natural Questions Query\": nq_queries,\n",
    "    \"Natural Questions Answer\": nq_answers,\n",
    "}\n",
    "\n",
    "\n",
    "length_data = []\n",
    "for name, samples in datasets.items():\n",
    "    for sample in samples:\n",
    "        length_data.append({\n",
    "            \"Dataset\": name,\n",
    "            \"Length\": len(sample) \n",
    "        })\n",
    "\n",
    "\n",
    "df = pd.DataFrame(length_data)\n",
    "\n",
    "\n",
    "fig = px.box(df, x=\"Dataset\", y=\"Length\", points=\"all\", color=\"Dataset\",\n",
    "             color_discrete_sequence=px.colors.diverging.Spectral)\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"Sequence Length Distribution by Dataset\",\n",
    "    yaxis_title=\"Length\",\n",
    "    font_family=\"DejaVu Sans\",\n",
    "    showlegend=False,\n",
    "    plot_bgcolor='white'\n",
    ")\n",
    "\n",
    "fig.update_yaxes(gridcolor='lightgray')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute correlation matrix\n",
    "corr_matrix = df[metrics_for_corr].corr().round(2)\n",
    "\n",
    "# Plot heatmap\n",
    "fig = go.Figure(data=go.Heatmap(\n",
    "    z=corr_matrix.values,\n",
    "    x=corr_matrix.columns,\n",
    "    y=corr_matrix.index,\n",
    "    colorscale='RdBu',\n",
    "    zmin=-1, zmax=1,\n",
    "    colorbar=dict(title='Correlation'),\n",
    "    hovertemplate='Correlation: %{z}<extra></extra>'\n",
    "))\n",
    "\n",
    "# Add text annotations\n",
    "for i in range(len(corr_matrix)):\n",
    "    for j in range(len(corr_matrix)):\n",
    "        fig.add_annotation(dict(\n",
    "            x=corr_matrix.columns[j],\n",
    "            y=corr_matrix.index[i],\n",
    "            text=str(corr_matrix.values[i][j]),\n",
    "            showarrow=False,\n",
    "            font=dict(color=\"black\" if abs(corr_matrix.values[i][j]) < 0.7 else \"white\")\n",
    "        ))\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"LLaMA 8B Instruct GSM8K (n=10)\",\n",
    "    font_family=\"Times New Roman\",\n",
    "    height=700,\n",
    "    width=800,\n",
    "    plot_bgcolor=\"white\"\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# Define models and metrics\n",
    "models = df['Model'].tolist()\n",
    "n_models = len(models)\n",
    "n_metrics = len(metrics)\n",
    "ncols = 5\n",
    "nrows = int(np.ceil(n_metrics / ncols))\n",
    "\n",
    "# Create subplot grid\n",
    "fig = make_subplots(rows=nrows, cols=ncols, subplot_titles=metrics)\n",
    "\n",
    "# Assign each metric to subplot\n",
    "for idx, metric in enumerate(metrics):\n",
    "    row = idx // ncols + 1\n",
    "    col = idx % ncols + 1\n",
    "\n",
    "    if metric in df.columns:\n",
    "        values = df[metric].copy()\n",
    "\n",
    "        # Normalize perplexity via log-scale\n",
    "        if metric == \"Perplexity\":\n",
    "            values = np.log1p(values)\n",
    "\n",
    "        fig.add_trace(go.Bar(\n",
    "            x=models,\n",
    "            y=values,\n",
    "            marker_color='indianred',\n",
    "            text=values.round(2),\n",
    "            textposition='outside',\n",
    "            name=metric\n",
    "        ), row=row, col=col)\n",
    "\n",
    "# Layout aesthetics\n",
    "fig.update_layout(\n",
    "    height=1000,\n",
    "    width=1500,\n",
    "    title_text=\"Deep Hermes 3B LLaMA & LLaMA 8B Instruct GSM8K (n=10)\",\n",
    "    showlegend=False,\n",
    "    font=dict(family=\"Times New Roman\")\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a pirate chatbot who always responds in pirate speak!\"},\n",
    "    {\"role\": \"user\", \"content\": \"Who are you?\"},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = llama8b_tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    add_generation_prompt=True,\n",
    "    return_tensors=\"pt\"\n",
    ").to(llama8b_fp32.device)\n",
    "\n",
    "terminators = [\n",
    "    llama8b_tokenizer.eos_token_id,\n",
    "    llama8b_tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "]\n",
    "\n",
    "outputs = llama8b_fp32.generate(\n",
    "    input_ids,\n",
    "    max_new_tokens=256,\n",
    "    eos_token_id=terminators,\n",
    "    do_sample=True,\n",
    "    temperature=0.6,\n",
    "    top_p=0.9,\n",
    ")\n",
    "response = outputs[0][input_ids.shape[-1]:]\n",
    "print(llama8b_tokenizer.decode(response, skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids_0 = hfbit1_tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    add_generation_prompt=True,\n",
    "    return_tensors=\"pt\"\n",
    ").to(hfbit1_fp32.device)\n",
    "\n",
    "terminators_0 = [\n",
    "    hfbit1_tokenizer.eos_token_id,\n",
    "    hfbit1_tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "]\n",
    "\n",
    "outputs_0 = hfbit1_fp32.generate(\n",
    "    input_ids_0,\n",
    "    max_new_tokens=256,\n",
    "    eos_token_id=terminators_0,\n",
    "    do_sample=True,\n",
    "    temperature=0.6,\n",
    "    top_p=0.9,\n",
    ")\n",
    "response_0 = outputs_0[0][input_ids_0.shape[-1]:]\n",
    "print(hfbit1_tokenizer.decode(response_0, skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(response_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_fp = \"\"\"\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_bit = \"\"\"\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from sklearn.metrics import pairwise_distances\n",
    "\n",
    "\n",
    "response_fp = response\n",
    "response_quantized = response_0\n",
    "\n",
    "# Decode the tokens to get the words/subwords\n",
    "tokens_fp = llama8b_tokenizer.decode(response_fp[0], skip_special_tokens=True).split()\n",
    "tokens_quantized = hfbit1_tokenizer.decode(response_quantized[0], skip_special_tokens=True).split()\n",
    "\n",
    "# Convert to sets for Jaccard distance calculation\n",
    "tokens_fp_set = set(tokens_fp)\n",
    "tokens_quantized_set = set(tokens_quantized)\n",
    "\n",
    "# Calculate Jaccard Distance\n",
    "intersection = len(tokens_fp_set & tokens_quantized_set)\n",
    "union = len(tokens_fp_set | tokens_quantized_set)\n",
    "jaccard_similarity = intersection / union\n",
    "jaccard_distance = 1 - jaccard_similarity\n",
    "\n",
    "print(f\"Jaccard Distance: {jaccard_distance:.3f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#input_text = \"Daniel went back to the the the garden. Mary travelled to the kitchen. Sandra journeyed to the kitchen. Sandra went to the hallway. John went to the bedroom. Mary went back to the garden. Where is Mary?\\nAnswer:\"\n",
    "input_text = \"What is y if y=2*2-4+(3*2)?\\nAnswer: \"\n",
    "input_ids = llama8b_tokenizer.encode(input_text, return_tensors=\"pt\").to(llama8b_fp32.device)\n",
    "output = llama8b_fp32.generate(input_ids, max_new_tokens=50, do_sample=False)\n",
    "generated_text = llama8b_tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "print(generated_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MechInterp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
